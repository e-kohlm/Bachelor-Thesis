hyper-parameter




VUDENC w2v train_model
#trying out different hyper parameters
mincount= wie oft muss ein token mindestens auftauchen im corpus, um es zu integrieren
iterationen = epochs = wich oft wird durch das Datenset ruchgelaufen
s = dimensionality = Code in vector numeric representation, vector hat best. Länge,
 je höher dimensionality, desto feiner kann Beziehung zwischen token dargestellt werden 
 semantic

#for mincount in [10,30,50,100,300,500,5000]:
  #for iterationen in [1,5,10,30,50,100]:
    #for s in [5,10,15,30,50,75,100,200,300]:

# just one hyperparameter, the winning one (und string bleibt erhalten)
for mincount in [10]:
  for iterationen in [100]:
    for s in [200]:

LSTM hyper - parameter:
F1 score als optimization Kriterium
loss function
epochs

ba_Jandieri:
character LSTM:
200 Neurons
35 Vector size
0.1 dropout
200 epochs
adam optimizer
128 batch size

basline transformer model:
epochs: 100
batch size: 1024
vocabulary size: 30000
window length: 200
step size: 5
embedding dim: 32
number additional heads: 5
dropout: 0.1
optimizer: adam

optimal für transformer:
epochs: 50
batch size: 1000
vocabulary size: 20000
window length: 250
step size: 5
embedding dim: 64
number additional heads: 0
dropout: 0.05
optimizer: RMSProp

baseline model (estimated by???) und dann hyper-parameter einzeln verändert um ihren Einfluss zu testen
ba_Jandieri(25) erklärt die alle recht ausführlich



MEINS
1. learning rate: default 5e-5 = 0.00005
0.01
0.0001 sagt tutorial

irgnedwo aus Literatur
2. batch size
32, 64, 128

3. epochs
10, 50, 100, 500, 1000

kleinstes dataset: xss

loss = loss function?
learning-rate?
grad_norm?
