{
  "best_metric": 0.7384196185286105,
  "best_model_checkpoint": "../saved_models/gpu_open_redirect_770/checkpoint-69990",
  "epoch": 15.0,
  "eval_steps": 500,
  "global_step": 69990,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0002143163309044149,
      "grad_norm": 108.74227142333984,
      "learning_rate": 1.999971424489213e-05,
      "loss": 1.3227,
      "step": 1
    },
    {
      "epoch": 0.0021431633090441492,
      "grad_norm": 13.69068431854248,
      "learning_rate": 1.9997142448921276e-05,
      "loss": 0.5448,
      "step": 10
    },
    {
      "epoch": 0.0042863266180882984,
      "grad_norm": 1.2746410369873047,
      "learning_rate": 1.999428489784255e-05,
      "loss": 0.677,
      "step": 20
    },
    {
      "epoch": 0.006429489927132447,
      "grad_norm": 102.15296936035156,
      "learning_rate": 1.9991427346763827e-05,
      "loss": 0.4769,
      "step": 30
    },
    {
      "epoch": 0.008572653236176597,
      "grad_norm": 75.85353088378906,
      "learning_rate": 1.99885697956851e-05,
      "loss": 1.9124,
      "step": 40
    },
    {
      "epoch": 0.010715816545220747,
      "grad_norm": 91.30907440185547,
      "learning_rate": 1.9985712244606375e-05,
      "loss": 0.8916,
      "step": 50
    },
    {
      "epoch": 0.012858979854264894,
      "grad_norm": 13.875139236450195,
      "learning_rate": 1.998285469352765e-05,
      "loss": 0.4667,
      "step": 60
    },
    {
      "epoch": 0.015002143163309044,
      "grad_norm": 0.09951800853013992,
      "learning_rate": 1.9979997142448923e-05,
      "loss": 0.0014,
      "step": 70
    },
    {
      "epoch": 0.017145306472353194,
      "grad_norm": 0.057832300662994385,
      "learning_rate": 1.99771395913702e-05,
      "loss": 1.661,
      "step": 80
    },
    {
      "epoch": 0.01928846978139734,
      "grad_norm": 0.25324851274490356,
      "learning_rate": 1.9974282040291474e-05,
      "loss": 0.8208,
      "step": 90
    },
    {
      "epoch": 0.021431633090441493,
      "grad_norm": 237.60829162597656,
      "learning_rate": 1.9971424489212748e-05,
      "loss": 0.4046,
      "step": 100
    },
    {
      "epoch": 0.02357479639948564,
      "grad_norm": 1.0149897336959839,
      "learning_rate": 1.9968566938134018e-05,
      "loss": 0.9789,
      "step": 110
    },
    {
      "epoch": 0.02571795970852979,
      "grad_norm": 0.718233585357666,
      "learning_rate": 1.9965709387055296e-05,
      "loss": 0.8741,
      "step": 120
    },
    {
      "epoch": 0.02786112301757394,
      "grad_norm": 86.83102416992188,
      "learning_rate": 1.996285183597657e-05,
      "loss": 0.9364,
      "step": 130
    },
    {
      "epoch": 0.03000428632661809,
      "grad_norm": 58.41999053955078,
      "learning_rate": 1.9959994284897843e-05,
      "loss": 1.1928,
      "step": 140
    },
    {
      "epoch": 0.03214744963566224,
      "grad_norm": 3.248568058013916,
      "learning_rate": 1.9957136733819117e-05,
      "loss": 0.5997,
      "step": 150
    },
    {
      "epoch": 0.03429061294470639,
      "grad_norm": 42.4936637878418,
      "learning_rate": 1.995427918274039e-05,
      "loss": 0.6638,
      "step": 160
    },
    {
      "epoch": 0.036433776253750536,
      "grad_norm": 89.10629272460938,
      "learning_rate": 1.995142163166167e-05,
      "loss": 0.7821,
      "step": 170
    },
    {
      "epoch": 0.03857693956279468,
      "grad_norm": 0.14859239757061005,
      "learning_rate": 1.9948564080582943e-05,
      "loss": 0.6563,
      "step": 180
    },
    {
      "epoch": 0.04072010287183883,
      "grad_norm": 0.10790988057851791,
      "learning_rate": 1.9945706529504216e-05,
      "loss": 0.9326,
      "step": 190
    },
    {
      "epoch": 0.042863266180882986,
      "grad_norm": 1.3063273429870605,
      "learning_rate": 1.994284897842549e-05,
      "loss": 0.6057,
      "step": 200
    },
    {
      "epoch": 0.045006429489927134,
      "grad_norm": 57.72392654418945,
      "learning_rate": 1.9939991427346764e-05,
      "loss": 1.4717,
      "step": 210
    },
    {
      "epoch": 0.04714959279897128,
      "grad_norm": 40.356693267822266,
      "learning_rate": 1.993713387626804e-05,
      "loss": 1.1067,
      "step": 220
    },
    {
      "epoch": 0.04929275610801543,
      "grad_norm": 0.08886521309614182,
      "learning_rate": 1.9934276325189316e-05,
      "loss": 0.3755,
      "step": 230
    },
    {
      "epoch": 0.05143591941705958,
      "grad_norm": 28.53041648864746,
      "learning_rate": 1.993141877411059e-05,
      "loss": 1.1401,
      "step": 240
    },
    {
      "epoch": 0.053579082726103726,
      "grad_norm": 0.672231912612915,
      "learning_rate": 1.9928561223031863e-05,
      "loss": 0.356,
      "step": 250
    },
    {
      "epoch": 0.05572224603514788,
      "grad_norm": 0.1890437752008438,
      "learning_rate": 1.9925703671953137e-05,
      "loss": 0.7727,
      "step": 260
    },
    {
      "epoch": 0.05786540934419203,
      "grad_norm": 0.17709454894065857,
      "learning_rate": 1.992284612087441e-05,
      "loss": 0.3912,
      "step": 270
    },
    {
      "epoch": 0.06000857265323618,
      "grad_norm": 0.14402009546756744,
      "learning_rate": 1.991998856979569e-05,
      "loss": 0.6797,
      "step": 280
    },
    {
      "epoch": 0.062151735962280324,
      "grad_norm": 0.5000960826873779,
      "learning_rate": 1.9917131018716962e-05,
      "loss": 0.3714,
      "step": 290
    },
    {
      "epoch": 0.06429489927132448,
      "grad_norm": 47.45547866821289,
      "learning_rate": 1.9914273467638236e-05,
      "loss": 1.0372,
      "step": 300
    },
    {
      "epoch": 0.06643806258036862,
      "grad_norm": 0.3396717607975006,
      "learning_rate": 1.991141591655951e-05,
      "loss": 0.3276,
      "step": 310
    },
    {
      "epoch": 0.06858122588941278,
      "grad_norm": 0.24581480026245117,
      "learning_rate": 1.9908558365480784e-05,
      "loss": 0.7827,
      "step": 320
    },
    {
      "epoch": 0.07072438919845692,
      "grad_norm": 0.8360788226127625,
      "learning_rate": 1.9905700814402058e-05,
      "loss": 0.8345,
      "step": 330
    },
    {
      "epoch": 0.07286755250750107,
      "grad_norm": 24.45538330078125,
      "learning_rate": 1.9902843263323332e-05,
      "loss": 0.9084,
      "step": 340
    },
    {
      "epoch": 0.07501071581654523,
      "grad_norm": 0.5133023262023926,
      "learning_rate": 1.9899985712244606e-05,
      "loss": 0.8216,
      "step": 350
    },
    {
      "epoch": 0.07715387912558937,
      "grad_norm": 0.4311520457267761,
      "learning_rate": 1.9897128161165883e-05,
      "loss": 0.8902,
      "step": 360
    },
    {
      "epoch": 0.07929704243463352,
      "grad_norm": 0.4004386365413666,
      "learning_rate": 1.9894270610087157e-05,
      "loss": 0.7512,
      "step": 370
    },
    {
      "epoch": 0.08144020574367766,
      "grad_norm": 1.0636080503463745,
      "learning_rate": 1.989141305900843e-05,
      "loss": 1.289,
      "step": 380
    },
    {
      "epoch": 0.08358336905272182,
      "grad_norm": 23.749853134155273,
      "learning_rate": 1.9888555507929705e-05,
      "loss": 0.5512,
      "step": 390
    },
    {
      "epoch": 0.08572653236176597,
      "grad_norm": 22.172422409057617,
      "learning_rate": 1.988569795685098e-05,
      "loss": 0.9766,
      "step": 400
    },
    {
      "epoch": 0.08786969567081011,
      "grad_norm": 0.4682944416999817,
      "learning_rate": 1.9882840405772253e-05,
      "loss": 0.552,
      "step": 410
    },
    {
      "epoch": 0.09001285897985427,
      "grad_norm": 0.4135175943374634,
      "learning_rate": 1.987998285469353e-05,
      "loss": 0.5095,
      "step": 420
    },
    {
      "epoch": 0.09215602228889841,
      "grad_norm": 41.76786422729492,
      "learning_rate": 1.9877125303614804e-05,
      "loss": 1.0129,
      "step": 430
    },
    {
      "epoch": 0.09429918559794256,
      "grad_norm": 26.238168716430664,
      "learning_rate": 1.9874267752536078e-05,
      "loss": 0.7516,
      "step": 440
    },
    {
      "epoch": 0.09644234890698672,
      "grad_norm": 0.4780789911746979,
      "learning_rate": 1.9871410201457352e-05,
      "loss": 0.4837,
      "step": 450
    },
    {
      "epoch": 0.09858551221603086,
      "grad_norm": 21.827919006347656,
      "learning_rate": 1.9868552650378626e-05,
      "loss": 0.992,
      "step": 460
    },
    {
      "epoch": 0.10072867552507501,
      "grad_norm": 1.870977759361267,
      "learning_rate": 1.9865695099299903e-05,
      "loss": 1.0202,
      "step": 470
    },
    {
      "epoch": 0.10287183883411916,
      "grad_norm": 21.795692443847656,
      "learning_rate": 1.9862837548221177e-05,
      "loss": 0.7516,
      "step": 480
    },
    {
      "epoch": 0.10501500214316331,
      "grad_norm": 22.19889259338379,
      "learning_rate": 1.985997999714245e-05,
      "loss": 0.5644,
      "step": 490
    },
    {
      "epoch": 0.10715816545220745,
      "grad_norm": 0.8180385231971741,
      "learning_rate": 1.9857122446063725e-05,
      "loss": 1.0147,
      "step": 500
    },
    {
      "epoch": 0.1093013287612516,
      "grad_norm": 27.50656509399414,
      "learning_rate": 1.9854264894985e-05,
      "loss": 0.8642,
      "step": 510
    },
    {
      "epoch": 0.11144449207029576,
      "grad_norm": 0.406727135181427,
      "learning_rate": 1.9851407343906276e-05,
      "loss": 0.3109,
      "step": 520
    },
    {
      "epoch": 0.1135876553793399,
      "grad_norm": 28.071880340576172,
      "learning_rate": 1.984854979282755e-05,
      "loss": 1.0072,
      "step": 530
    },
    {
      "epoch": 0.11573081868838406,
      "grad_norm": 1.052290439605713,
      "learning_rate": 1.984569224174882e-05,
      "loss": 0.6981,
      "step": 540
    },
    {
      "epoch": 0.1178739819974282,
      "grad_norm": 21.286706924438477,
      "learning_rate": 1.9842834690670095e-05,
      "loss": 0.6081,
      "step": 550
    },
    {
      "epoch": 0.12001714530647235,
      "grad_norm": 21.65831184387207,
      "learning_rate": 1.9839977139591372e-05,
      "loss": 0.8988,
      "step": 560
    },
    {
      "epoch": 0.12216030861551651,
      "grad_norm": 1.602347493171692,
      "learning_rate": 1.9837119588512646e-05,
      "loss": 1.1124,
      "step": 570
    },
    {
      "epoch": 0.12430347192456065,
      "grad_norm": 21.157527923583984,
      "learning_rate": 1.983426203743392e-05,
      "loss": 0.6004,
      "step": 580
    },
    {
      "epoch": 0.1264466352336048,
      "grad_norm": 0.8783015608787537,
      "learning_rate": 1.9831404486355194e-05,
      "loss": 0.8155,
      "step": 590
    },
    {
      "epoch": 0.12858979854264896,
      "grad_norm": 0.7930094599723816,
      "learning_rate": 1.9828546935276468e-05,
      "loss": 0.8518,
      "step": 600
    },
    {
      "epoch": 0.13073296185169309,
      "grad_norm": 22.00298500061035,
      "learning_rate": 1.9825689384197745e-05,
      "loss": 0.6933,
      "step": 610
    },
    {
      "epoch": 0.13287612516073724,
      "grad_norm": 0.20662853121757507,
      "learning_rate": 1.982283183311902e-05,
      "loss": 0.1711,
      "step": 620
    },
    {
      "epoch": 0.1350192884697814,
      "grad_norm": 0.10173702239990234,
      "learning_rate": 1.9819974282040293e-05,
      "loss": 0.2056,
      "step": 630
    },
    {
      "epoch": 0.13716245177882555,
      "grad_norm": 0.2600620985031128,
      "learning_rate": 1.9817116730961567e-05,
      "loss": 0.8313,
      "step": 640
    },
    {
      "epoch": 0.1393056150878697,
      "grad_norm": 0.399450421333313,
      "learning_rate": 1.981425917988284e-05,
      "loss": 0.3579,
      "step": 650
    },
    {
      "epoch": 0.14144877839691383,
      "grad_norm": 17.906827926635742,
      "learning_rate": 1.9811401628804118e-05,
      "loss": 1.2096,
      "step": 660
    },
    {
      "epoch": 0.143591941705958,
      "grad_norm": 0.3899351954460144,
      "learning_rate": 1.9808544077725392e-05,
      "loss": 0.3409,
      "step": 670
    },
    {
      "epoch": 0.14573510501500214,
      "grad_norm": 0.5806624293327332,
      "learning_rate": 1.9805686526646666e-05,
      "loss": 0.8147,
      "step": 680
    },
    {
      "epoch": 0.1478782683240463,
      "grad_norm": 0.2974797487258911,
      "learning_rate": 1.980282897556794e-05,
      "loss": 0.7063,
      "step": 690
    },
    {
      "epoch": 0.15002143163309045,
      "grad_norm": 0.18462564051151276,
      "learning_rate": 1.9799971424489214e-05,
      "loss": 0.5364,
      "step": 700
    },
    {
      "epoch": 0.15216459494213458,
      "grad_norm": 38.393455505371094,
      "learning_rate": 1.979711387341049e-05,
      "loss": 0.7129,
      "step": 710
    },
    {
      "epoch": 0.15430775825117873,
      "grad_norm": 0.5175493359565735,
      "learning_rate": 1.9794256322331765e-05,
      "loss": 0.5237,
      "step": 720
    },
    {
      "epoch": 0.1564509215602229,
      "grad_norm": 0.3348137140274048,
      "learning_rate": 1.979139877125304e-05,
      "loss": 0.542,
      "step": 730
    },
    {
      "epoch": 0.15859408486926704,
      "grad_norm": 0.2840649485588074,
      "learning_rate": 1.9788541220174313e-05,
      "loss": 0.909,
      "step": 740
    },
    {
      "epoch": 0.1607372481783112,
      "grad_norm": 19.679222106933594,
      "learning_rate": 1.9785683669095587e-05,
      "loss": 0.9749,
      "step": 750
    },
    {
      "epoch": 0.16288041148735533,
      "grad_norm": 0.26309841871261597,
      "learning_rate": 1.978282611801686e-05,
      "loss": 0.3441,
      "step": 760
    },
    {
      "epoch": 0.16502357479639948,
      "grad_norm": 0.29735028743743896,
      "learning_rate": 1.9779968566938135e-05,
      "loss": 0.5276,
      "step": 770
    },
    {
      "epoch": 0.16716673810544364,
      "grad_norm": 0.30286917090415955,
      "learning_rate": 1.977711101585941e-05,
      "loss": 0.5111,
      "step": 780
    },
    {
      "epoch": 0.1693099014144878,
      "grad_norm": 0.13497509062290192,
      "learning_rate": 1.9774253464780683e-05,
      "loss": 0.1876,
      "step": 790
    },
    {
      "epoch": 0.17145306472353194,
      "grad_norm": 0.24614019691944122,
      "learning_rate": 1.977139591370196e-05,
      "loss": 0.9922,
      "step": 800
    },
    {
      "epoch": 0.17359622803257607,
      "grad_norm": 21.817914962768555,
      "learning_rate": 1.9768538362623234e-05,
      "loss": 0.3667,
      "step": 810
    },
    {
      "epoch": 0.17573939134162023,
      "grad_norm": 0.16685864329338074,
      "learning_rate": 1.9765680811544508e-05,
      "loss": 0.1987,
      "step": 820
    },
    {
      "epoch": 0.17788255465066438,
      "grad_norm": 22.0657901763916,
      "learning_rate": 1.9762823260465782e-05,
      "loss": 1.1427,
      "step": 830
    },
    {
      "epoch": 0.18002571795970854,
      "grad_norm": 0.30990394949913025,
      "learning_rate": 1.9759965709387056e-05,
      "loss": 0.5489,
      "step": 840
    },
    {
      "epoch": 0.1821688812687527,
      "grad_norm": 0.939293622970581,
      "learning_rate": 1.9757108158308333e-05,
      "loss": 0.7979,
      "step": 850
    },
    {
      "epoch": 0.18431204457779682,
      "grad_norm": 0.667521059513092,
      "learning_rate": 1.9754250607229607e-05,
      "loss": 0.5409,
      "step": 860
    },
    {
      "epoch": 0.18645520788684097,
      "grad_norm": 0.2777550220489502,
      "learning_rate": 1.975139305615088e-05,
      "loss": 0.808,
      "step": 870
    },
    {
      "epoch": 0.18859837119588513,
      "grad_norm": 19.17713165283203,
      "learning_rate": 1.9748535505072155e-05,
      "loss": 0.5296,
      "step": 880
    },
    {
      "epoch": 0.19074153450492928,
      "grad_norm": 17.831693649291992,
      "learning_rate": 1.974567795399343e-05,
      "loss": 0.7392,
      "step": 890
    },
    {
      "epoch": 0.19288469781397344,
      "grad_norm": 1.0268975496292114,
      "learning_rate": 1.9742820402914703e-05,
      "loss": 0.9661,
      "step": 900
    },
    {
      "epoch": 0.19502786112301757,
      "grad_norm": 0.32719886302948,
      "learning_rate": 1.973996285183598e-05,
      "loss": 0.4107,
      "step": 910
    },
    {
      "epoch": 0.19717102443206172,
      "grad_norm": 0.48145848512649536,
      "learning_rate": 1.9737105300757254e-05,
      "loss": 1.1915,
      "step": 920
    },
    {
      "epoch": 0.19931418774110587,
      "grad_norm": 0.9125805497169495,
      "learning_rate": 1.9734247749678528e-05,
      "loss": 0.6008,
      "step": 930
    },
    {
      "epoch": 0.20145735105015003,
      "grad_norm": 0.48136967420578003,
      "learning_rate": 1.9731390198599802e-05,
      "loss": 0.806,
      "step": 940
    },
    {
      "epoch": 0.20360051435919416,
      "grad_norm": 0.5395050644874573,
      "learning_rate": 1.9728532647521076e-05,
      "loss": 0.7762,
      "step": 950
    },
    {
      "epoch": 0.2057436776682383,
      "grad_norm": 0.5527663826942444,
      "learning_rate": 1.9725675096442353e-05,
      "loss": 0.9694,
      "step": 960
    },
    {
      "epoch": 0.20788684097728247,
      "grad_norm": 0.26597559452056885,
      "learning_rate": 1.9722817545363624e-05,
      "loss": 0.7368,
      "step": 970
    },
    {
      "epoch": 0.21003000428632662,
      "grad_norm": 0.45031675696372986,
      "learning_rate": 1.9719959994284897e-05,
      "loss": 0.347,
      "step": 980
    },
    {
      "epoch": 0.21217316759537078,
      "grad_norm": 0.38326728343963623,
      "learning_rate": 1.9717102443206175e-05,
      "loss": 0.8363,
      "step": 990
    },
    {
      "epoch": 0.2143163309044149,
      "grad_norm": 0.31488466262817383,
      "learning_rate": 1.971424489212745e-05,
      "loss": 0.3364,
      "step": 1000
    },
    {
      "epoch": 0.21645949421345906,
      "grad_norm": 21.012189865112305,
      "learning_rate": 1.9711387341048723e-05,
      "loss": 0.7574,
      "step": 1010
    },
    {
      "epoch": 0.2186026575225032,
      "grad_norm": 0.5029865503311157,
      "learning_rate": 1.9708529789969997e-05,
      "loss": 0.3636,
      "step": 1020
    },
    {
      "epoch": 0.22074582083154737,
      "grad_norm": 0.2174490988254547,
      "learning_rate": 1.970567223889127e-05,
      "loss": 0.5027,
      "step": 1030
    },
    {
      "epoch": 0.22288898414059152,
      "grad_norm": 19.78447723388672,
      "learning_rate": 1.9702814687812544e-05,
      "loss": 0.554,
      "step": 1040
    },
    {
      "epoch": 0.22503214744963565,
      "grad_norm": 0.1752939373254776,
      "learning_rate": 1.9699957136733822e-05,
      "loss": 0.3856,
      "step": 1050
    },
    {
      "epoch": 0.2271753107586798,
      "grad_norm": 25.140586853027344,
      "learning_rate": 1.9697099585655096e-05,
      "loss": 0.5737,
      "step": 1060
    },
    {
      "epoch": 0.22931847406772396,
      "grad_norm": 0.416639506816864,
      "learning_rate": 1.969424203457637e-05,
      "loss": 0.5182,
      "step": 1070
    },
    {
      "epoch": 0.23146163737676811,
      "grad_norm": 0.25978317856788635,
      "learning_rate": 1.9691384483497643e-05,
      "loss": 0.3571,
      "step": 1080
    },
    {
      "epoch": 0.23360480068581227,
      "grad_norm": 23.351160049438477,
      "learning_rate": 1.9688526932418917e-05,
      "loss": 0.5731,
      "step": 1090
    },
    {
      "epoch": 0.2357479639948564,
      "grad_norm": 22.848247528076172,
      "learning_rate": 1.9685669381340195e-05,
      "loss": 0.938,
      "step": 1100
    },
    {
      "epoch": 0.23789112730390055,
      "grad_norm": 21.195642471313477,
      "learning_rate": 1.968281183026147e-05,
      "loss": 0.7913,
      "step": 1110
    },
    {
      "epoch": 0.2400342906129447,
      "grad_norm": 2.84472393989563,
      "learning_rate": 1.9679954279182743e-05,
      "loss": 0.8992,
      "step": 1120
    },
    {
      "epoch": 0.24217745392198886,
      "grad_norm": 0.9689690470695496,
      "learning_rate": 1.9677096728104017e-05,
      "loss": 0.2681,
      "step": 1130
    },
    {
      "epoch": 0.24432061723103302,
      "grad_norm": 0.5237540602684021,
      "learning_rate": 1.967423917702529e-05,
      "loss": 0.7628,
      "step": 1140
    },
    {
      "epoch": 0.24646378054007714,
      "grad_norm": 0.20764678716659546,
      "learning_rate": 1.9671381625946568e-05,
      "loss": 0.1522,
      "step": 1150
    },
    {
      "epoch": 0.2486069438491213,
      "grad_norm": 0.22307923436164856,
      "learning_rate": 1.966852407486784e-05,
      "loss": 0.5669,
      "step": 1160
    },
    {
      "epoch": 0.2507501071581654,
      "grad_norm": 25.733726501464844,
      "learning_rate": 1.9665666523789116e-05,
      "loss": 0.5775,
      "step": 1170
    },
    {
      "epoch": 0.2528932704672096,
      "grad_norm": 22.13439178466797,
      "learning_rate": 1.966280897271039e-05,
      "loss": 0.9442,
      "step": 1180
    },
    {
      "epoch": 0.25503643377625373,
      "grad_norm": 0.8015530705451965,
      "learning_rate": 1.9659951421631663e-05,
      "loss": 0.5245,
      "step": 1190
    },
    {
      "epoch": 0.2571795970852979,
      "grad_norm": 0.17255637049674988,
      "learning_rate": 1.9657093870552937e-05,
      "loss": 0.0076,
      "step": 1200
    },
    {
      "epoch": 0.25932276039434204,
      "grad_norm": 0.16998589038848877,
      "learning_rate": 1.965423631947421e-05,
      "loss": 1.4056,
      "step": 1210
    },
    {
      "epoch": 0.26146592370338617,
      "grad_norm": 21.311548233032227,
      "learning_rate": 1.9651378768395485e-05,
      "loss": 0.7186,
      "step": 1220
    },
    {
      "epoch": 0.26360908701243035,
      "grad_norm": 0.6916028261184692,
      "learning_rate": 1.964852121731676e-05,
      "loss": 0.9618,
      "step": 1230
    },
    {
      "epoch": 0.2657522503214745,
      "grad_norm": 19.61890983581543,
      "learning_rate": 1.9645663666238037e-05,
      "loss": 0.5753,
      "step": 1240
    },
    {
      "epoch": 0.26789541363051866,
      "grad_norm": 20.557767868041992,
      "learning_rate": 1.964280611515931e-05,
      "loss": 0.3865,
      "step": 1250
    },
    {
      "epoch": 0.2700385769395628,
      "grad_norm": 18.346664428710938,
      "learning_rate": 1.9639948564080584e-05,
      "loss": 0.7635,
      "step": 1260
    },
    {
      "epoch": 0.2721817402486069,
      "grad_norm": 0.2592717707157135,
      "learning_rate": 1.9637091013001858e-05,
      "loss": 0.7477,
      "step": 1270
    },
    {
      "epoch": 0.2743249035576511,
      "grad_norm": 0.8384079933166504,
      "learning_rate": 1.9634233461923132e-05,
      "loss": 1.1209,
      "step": 1280
    },
    {
      "epoch": 0.27646806686669523,
      "grad_norm": 0.6916553378105164,
      "learning_rate": 1.963137591084441e-05,
      "loss": 0.5597,
      "step": 1290
    },
    {
      "epoch": 0.2786112301757394,
      "grad_norm": 0.3104565739631653,
      "learning_rate": 1.9628518359765683e-05,
      "loss": 0.6645,
      "step": 1300
    },
    {
      "epoch": 0.28075439348478354,
      "grad_norm": 0.35119324922561646,
      "learning_rate": 1.9625660808686957e-05,
      "loss": 0.644,
      "step": 1310
    },
    {
      "epoch": 0.28289755679382766,
      "grad_norm": 17.518253326416016,
      "learning_rate": 1.962280325760823e-05,
      "loss": 0.5511,
      "step": 1320
    },
    {
      "epoch": 0.28504072010287185,
      "grad_norm": 20.39082908630371,
      "learning_rate": 1.9619945706529505e-05,
      "loss": 0.6718,
      "step": 1330
    },
    {
      "epoch": 0.287183883411916,
      "grad_norm": 16.67474937438965,
      "learning_rate": 1.961708815545078e-05,
      "loss": 0.7718,
      "step": 1340
    },
    {
      "epoch": 0.28932704672096016,
      "grad_norm": 18.81881332397461,
      "learning_rate": 1.9614230604372056e-05,
      "loss": 0.5927,
      "step": 1350
    },
    {
      "epoch": 0.2914702100300043,
      "grad_norm": 0.6296676993370056,
      "learning_rate": 1.961137305329333e-05,
      "loss": 0.7661,
      "step": 1360
    },
    {
      "epoch": 0.2936133733390484,
      "grad_norm": 0.2741720676422119,
      "learning_rate": 1.9608515502214604e-05,
      "loss": 0.0096,
      "step": 1370
    },
    {
      "epoch": 0.2957565366480926,
      "grad_norm": 32.85369110107422,
      "learning_rate": 1.9605657951135878e-05,
      "loss": 1.5596,
      "step": 1380
    },
    {
      "epoch": 0.2978996999571367,
      "grad_norm": 18.47400665283203,
      "learning_rate": 1.9602800400057152e-05,
      "loss": 0.4794,
      "step": 1390
    },
    {
      "epoch": 0.3000428632661809,
      "grad_norm": 0.5945702791213989,
      "learning_rate": 1.9599942848978426e-05,
      "loss": 0.6535,
      "step": 1400
    },
    {
      "epoch": 0.30218602657522503,
      "grad_norm": 15.844954490661621,
      "learning_rate": 1.95970852978997e-05,
      "loss": 0.7125,
      "step": 1410
    },
    {
      "epoch": 0.30432918988426916,
      "grad_norm": 0.4011459946632385,
      "learning_rate": 1.9594227746820974e-05,
      "loss": 0.1482,
      "step": 1420
    },
    {
      "epoch": 0.30647235319331334,
      "grad_norm": 0.20737364888191223,
      "learning_rate": 1.959137019574225e-05,
      "loss": 0.7874,
      "step": 1430
    },
    {
      "epoch": 0.30861551650235747,
      "grad_norm": 0.3141721189022064,
      "learning_rate": 1.9588512644663525e-05,
      "loss": 0.5393,
      "step": 1440
    },
    {
      "epoch": 0.31075867981140165,
      "grad_norm": 0.7269458174705505,
      "learning_rate": 1.95856550935848e-05,
      "loss": 1.1449,
      "step": 1450
    },
    {
      "epoch": 0.3129018431204458,
      "grad_norm": 20.513376235961914,
      "learning_rate": 1.9582797542506073e-05,
      "loss": 0.4777,
      "step": 1460
    },
    {
      "epoch": 0.3150450064294899,
      "grad_norm": 0.1904131919145584,
      "learning_rate": 1.9579939991427347e-05,
      "loss": 0.1814,
      "step": 1470
    },
    {
      "epoch": 0.3171881697385341,
      "grad_norm": 0.32221829891204834,
      "learning_rate": 1.957708244034862e-05,
      "loss": 0.9062,
      "step": 1480
    },
    {
      "epoch": 0.3193313330475782,
      "grad_norm": 0.40456846356391907,
      "learning_rate": 1.9574224889269898e-05,
      "loss": 0.4942,
      "step": 1490
    },
    {
      "epoch": 0.3214744963566224,
      "grad_norm": 0.2590426504611969,
      "learning_rate": 1.9571367338191172e-05,
      "loss": 0.3598,
      "step": 1500
    },
    {
      "epoch": 0.3236176596656665,
      "grad_norm": 17.779926300048828,
      "learning_rate": 1.9568509787112446e-05,
      "loss": 0.8743,
      "step": 1510
    },
    {
      "epoch": 0.32576082297471065,
      "grad_norm": 18.12241554260254,
      "learning_rate": 1.956565223603372e-05,
      "loss": 0.1844,
      "step": 1520
    },
    {
      "epoch": 0.32790398628375483,
      "grad_norm": 0.2580178380012512,
      "learning_rate": 1.9562794684954994e-05,
      "loss": 0.5495,
      "step": 1530
    },
    {
      "epoch": 0.33004714959279896,
      "grad_norm": 0.55269855260849,
      "learning_rate": 1.955993713387627e-05,
      "loss": 1.5524,
      "step": 1540
    },
    {
      "epoch": 0.33219031290184314,
      "grad_norm": 19.400638580322266,
      "learning_rate": 1.9557079582797545e-05,
      "loss": 0.9675,
      "step": 1550
    },
    {
      "epoch": 0.33433347621088727,
      "grad_norm": 0.683028519153595,
      "learning_rate": 1.955422203171882e-05,
      "loss": 0.2684,
      "step": 1560
    },
    {
      "epoch": 0.3364766395199314,
      "grad_norm": 32.736000061035156,
      "learning_rate": 1.9551364480640093e-05,
      "loss": 0.3738,
      "step": 1570
    },
    {
      "epoch": 0.3386198028289756,
      "grad_norm": 18.060569763183594,
      "learning_rate": 1.9548506929561367e-05,
      "loss": 0.3955,
      "step": 1580
    },
    {
      "epoch": 0.3407629661380197,
      "grad_norm": 20.817899703979492,
      "learning_rate": 1.9545649378482644e-05,
      "loss": 0.5755,
      "step": 1590
    },
    {
      "epoch": 0.3429061294470639,
      "grad_norm": 0.6704222559928894,
      "learning_rate": 1.9542791827403918e-05,
      "loss": 0.9074,
      "step": 1600
    },
    {
      "epoch": 0.345049292756108,
      "grad_norm": 0.5301662087440491,
      "learning_rate": 1.9539934276325192e-05,
      "loss": 0.4674,
      "step": 1610
    },
    {
      "epoch": 0.34719245606515214,
      "grad_norm": 0.29622021317481995,
      "learning_rate": 1.9537076725246463e-05,
      "loss": 0.6671,
      "step": 1620
    },
    {
      "epoch": 0.3493356193741963,
      "grad_norm": 0.6563720703125,
      "learning_rate": 1.953421917416774e-05,
      "loss": 0.7801,
      "step": 1630
    },
    {
      "epoch": 0.35147878268324045,
      "grad_norm": 0.8302628397941589,
      "learning_rate": 1.9531361623089014e-05,
      "loss": 0.5721,
      "step": 1640
    },
    {
      "epoch": 0.35362194599228464,
      "grad_norm": 0.3768135607242584,
      "learning_rate": 1.9528504072010288e-05,
      "loss": 0.5881,
      "step": 1650
    },
    {
      "epoch": 0.35576510930132876,
      "grad_norm": 16.489238739013672,
      "learning_rate": 1.9525646520931562e-05,
      "loss": 1.088,
      "step": 1660
    },
    {
      "epoch": 0.3579082726103729,
      "grad_norm": 18.013870239257812,
      "learning_rate": 1.9522788969852836e-05,
      "loss": 0.5923,
      "step": 1670
    },
    {
      "epoch": 0.3600514359194171,
      "grad_norm": 0.6706653237342834,
      "learning_rate": 1.9519931418774113e-05,
      "loss": 0.4353,
      "step": 1680
    },
    {
      "epoch": 0.3621945992284612,
      "grad_norm": 17.19152069091797,
      "learning_rate": 1.9517073867695387e-05,
      "loss": 0.3509,
      "step": 1690
    },
    {
      "epoch": 0.3643377625375054,
      "grad_norm": 16.759632110595703,
      "learning_rate": 1.951421631661666e-05,
      "loss": 0.5523,
      "step": 1700
    },
    {
      "epoch": 0.3664809258465495,
      "grad_norm": 0.2457166463136673,
      "learning_rate": 1.9511358765537935e-05,
      "loss": 0.5594,
      "step": 1710
    },
    {
      "epoch": 0.36862408915559364,
      "grad_norm": 19.31643295288086,
      "learning_rate": 1.950850121445921e-05,
      "loss": 0.8486,
      "step": 1720
    },
    {
      "epoch": 0.3707672524646378,
      "grad_norm": 0.46434786915779114,
      "learning_rate": 1.9505643663380486e-05,
      "loss": 0.3264,
      "step": 1730
    },
    {
      "epoch": 0.37291041577368195,
      "grad_norm": 0.5357568860054016,
      "learning_rate": 1.950278611230176e-05,
      "loss": 1.2255,
      "step": 1740
    },
    {
      "epoch": 0.37505357908272613,
      "grad_norm": 0.7692806124687195,
      "learning_rate": 1.9499928561223034e-05,
      "loss": 0.5968,
      "step": 1750
    },
    {
      "epoch": 0.37719674239177026,
      "grad_norm": 0.8860024213790894,
      "learning_rate": 1.9497071010144308e-05,
      "loss": 0.537,
      "step": 1760
    },
    {
      "epoch": 0.3793399057008144,
      "grad_norm": 15.583025932312012,
      "learning_rate": 1.9494213459065582e-05,
      "loss": 1.4084,
      "step": 1770
    },
    {
      "epoch": 0.38148306900985857,
      "grad_norm": 32.293670654296875,
      "learning_rate": 1.949135590798686e-05,
      "loss": 0.9148,
      "step": 1780
    },
    {
      "epoch": 0.3836262323189027,
      "grad_norm": 0.4125935137271881,
      "learning_rate": 1.9488498356908133e-05,
      "loss": 0.3287,
      "step": 1790
    },
    {
      "epoch": 0.3857693956279469,
      "grad_norm": 16.265798568725586,
      "learning_rate": 1.9485640805829407e-05,
      "loss": 0.522,
      "step": 1800
    },
    {
      "epoch": 0.387912558936991,
      "grad_norm": 0.4593167006969452,
      "learning_rate": 1.948278325475068e-05,
      "loss": 1.2113,
      "step": 1810
    },
    {
      "epoch": 0.39005572224603513,
      "grad_norm": 0.49820494651794434,
      "learning_rate": 1.9479925703671955e-05,
      "loss": 0.3298,
      "step": 1820
    },
    {
      "epoch": 0.3921988855550793,
      "grad_norm": 0.47591570019721985,
      "learning_rate": 1.947706815259323e-05,
      "loss": 0.5991,
      "step": 1830
    },
    {
      "epoch": 0.39434204886412344,
      "grad_norm": 20.404918670654297,
      "learning_rate": 1.9474210601514503e-05,
      "loss": 0.902,
      "step": 1840
    },
    {
      "epoch": 0.3964852121731676,
      "grad_norm": 16.083690643310547,
      "learning_rate": 1.9471353050435777e-05,
      "loss": 0.9404,
      "step": 1850
    },
    {
      "epoch": 0.39862837548221175,
      "grad_norm": 0.8544394373893738,
      "learning_rate": 1.946849549935705e-05,
      "loss": 0.6944,
      "step": 1860
    },
    {
      "epoch": 0.4007715387912559,
      "grad_norm": 0.643515944480896,
      "learning_rate": 1.9465637948278328e-05,
      "loss": 0.3188,
      "step": 1870
    },
    {
      "epoch": 0.40291470210030006,
      "grad_norm": 0.3505954444408417,
      "learning_rate": 1.9462780397199602e-05,
      "loss": 0.7967,
      "step": 1880
    },
    {
      "epoch": 0.4050578654093442,
      "grad_norm": 15.410983085632324,
      "learning_rate": 1.9459922846120876e-05,
      "loss": 0.5051,
      "step": 1890
    },
    {
      "epoch": 0.4072010287183883,
      "grad_norm": 0.5845857858657837,
      "learning_rate": 1.945706529504215e-05,
      "loss": 1.1606,
      "step": 1900
    },
    {
      "epoch": 0.4093441920274325,
      "grad_norm": 17.612794876098633,
      "learning_rate": 1.9454207743963424e-05,
      "loss": 0.7411,
      "step": 1910
    },
    {
      "epoch": 0.4114873553364766,
      "grad_norm": 32.995670318603516,
      "learning_rate": 1.94513501928847e-05,
      "loss": 0.7205,
      "step": 1920
    },
    {
      "epoch": 0.4136305186455208,
      "grad_norm": 0.532421886920929,
      "learning_rate": 1.9448492641805975e-05,
      "loss": 0.1593,
      "step": 1930
    },
    {
      "epoch": 0.41577368195456493,
      "grad_norm": 17.05316925048828,
      "learning_rate": 1.944563509072725e-05,
      "loss": 0.5069,
      "step": 1940
    },
    {
      "epoch": 0.41791684526360906,
      "grad_norm": 0.459185391664505,
      "learning_rate": 1.9442777539648523e-05,
      "loss": 1.0096,
      "step": 1950
    },
    {
      "epoch": 0.42006000857265324,
      "grad_norm": 0.501157283782959,
      "learning_rate": 1.9439919988569797e-05,
      "loss": 0.461,
      "step": 1960
    },
    {
      "epoch": 0.42220317188169737,
      "grad_norm": 0.6833311915397644,
      "learning_rate": 1.943706243749107e-05,
      "loss": 0.6034,
      "step": 1970
    },
    {
      "epoch": 0.42434633519074155,
      "grad_norm": 0.5576412081718445,
      "learning_rate": 1.9434204886412348e-05,
      "loss": 0.7615,
      "step": 1980
    },
    {
      "epoch": 0.4264894984997857,
      "grad_norm": 0.6783584952354431,
      "learning_rate": 1.9431347335333622e-05,
      "loss": 0.9133,
      "step": 1990
    },
    {
      "epoch": 0.4286326618088298,
      "grad_norm": 0.731269359588623,
      "learning_rate": 1.9428489784254896e-05,
      "loss": 0.8394,
      "step": 2000
    },
    {
      "epoch": 0.430775825117874,
      "grad_norm": 15.899378776550293,
      "learning_rate": 1.942563223317617e-05,
      "loss": 1.0158,
      "step": 2010
    },
    {
      "epoch": 0.4329189884269181,
      "grad_norm": 1.5210349559783936,
      "learning_rate": 1.9422774682097444e-05,
      "loss": 0.7878,
      "step": 2020
    },
    {
      "epoch": 0.4350621517359623,
      "grad_norm": 15.167030334472656,
      "learning_rate": 1.941991713101872e-05,
      "loss": 0.2907,
      "step": 2030
    },
    {
      "epoch": 0.4372053150450064,
      "grad_norm": 0.4660798907279968,
      "learning_rate": 1.941705957993999e-05,
      "loss": 0.5473,
      "step": 2040
    },
    {
      "epoch": 0.43934847835405055,
      "grad_norm": 0.29799363017082214,
      "learning_rate": 1.9414202028861265e-05,
      "loss": 0.331,
      "step": 2050
    },
    {
      "epoch": 0.44149164166309474,
      "grad_norm": 18.529203414916992,
      "learning_rate": 1.9411344477782543e-05,
      "loss": 1.1114,
      "step": 2060
    },
    {
      "epoch": 0.44363480497213886,
      "grad_norm": 18.712661743164062,
      "learning_rate": 1.9408486926703817e-05,
      "loss": 0.6204,
      "step": 2070
    },
    {
      "epoch": 0.44577796828118305,
      "grad_norm": 0.8669050335884094,
      "learning_rate": 1.940562937562509e-05,
      "loss": 0.5826,
      "step": 2080
    },
    {
      "epoch": 0.4479211315902272,
      "grad_norm": 1.2655491828918457,
      "learning_rate": 1.9402771824546364e-05,
      "loss": 1.0838,
      "step": 2090
    },
    {
      "epoch": 0.4500642948992713,
      "grad_norm": 0.8998121023178101,
      "learning_rate": 1.939991427346764e-05,
      "loss": 0.5551,
      "step": 2100
    },
    {
      "epoch": 0.4522074582083155,
      "grad_norm": 0.2635531425476074,
      "learning_rate": 1.9397056722388912e-05,
      "loss": 0.1793,
      "step": 2110
    },
    {
      "epoch": 0.4543506215173596,
      "grad_norm": 0.21280933916568756,
      "learning_rate": 1.939419917131019e-05,
      "loss": 0.7132,
      "step": 2120
    },
    {
      "epoch": 0.4564937848264038,
      "grad_norm": 19.1209659576416,
      "learning_rate": 1.9391341620231464e-05,
      "loss": 0.695,
      "step": 2130
    },
    {
      "epoch": 0.4586369481354479,
      "grad_norm": 0.8295053839683533,
      "learning_rate": 1.9388484069152737e-05,
      "loss": 1.044,
      "step": 2140
    },
    {
      "epoch": 0.46078011144449205,
      "grad_norm": 0.45082950592041016,
      "learning_rate": 1.938562651807401e-05,
      "loss": 0.1542,
      "step": 2150
    },
    {
      "epoch": 0.46292327475353623,
      "grad_norm": 0.2447664737701416,
      "learning_rate": 1.9382768966995285e-05,
      "loss": 0.3553,
      "step": 2160
    },
    {
      "epoch": 0.46506643806258036,
      "grad_norm": 16.64194107055664,
      "learning_rate": 1.9379911415916563e-05,
      "loss": 0.9157,
      "step": 2170
    },
    {
      "epoch": 0.46720960137162454,
      "grad_norm": 0.321864515542984,
      "learning_rate": 1.9377053864837837e-05,
      "loss": 0.8886,
      "step": 2180
    },
    {
      "epoch": 0.46935276468066867,
      "grad_norm": 34.061492919921875,
      "learning_rate": 1.937419631375911e-05,
      "loss": 1.0803,
      "step": 2190
    },
    {
      "epoch": 0.4714959279897128,
      "grad_norm": 1.0606733560562134,
      "learning_rate": 1.9371338762680384e-05,
      "loss": 0.5422,
      "step": 2200
    },
    {
      "epoch": 0.473639091298757,
      "grad_norm": 0.482894629240036,
      "learning_rate": 1.936848121160166e-05,
      "loss": 0.1527,
      "step": 2210
    },
    {
      "epoch": 0.4757822546078011,
      "grad_norm": 0.38957181572914124,
      "learning_rate": 1.9365623660522936e-05,
      "loss": 0.5988,
      "step": 2220
    },
    {
      "epoch": 0.4779254179168453,
      "grad_norm": 0.20242339372634888,
      "learning_rate": 1.936276610944421e-05,
      "loss": 0.3385,
      "step": 2230
    },
    {
      "epoch": 0.4800685812258894,
      "grad_norm": 0.13444477319717407,
      "learning_rate": 1.9359908558365484e-05,
      "loss": 0.1592,
      "step": 2240
    },
    {
      "epoch": 0.48221174453493354,
      "grad_norm": 0.21349911391735077,
      "learning_rate": 1.9357051007286757e-05,
      "loss": 0.9617,
      "step": 2250
    },
    {
      "epoch": 0.4843549078439777,
      "grad_norm": 18.93588638305664,
      "learning_rate": 1.935419345620803e-05,
      "loss": 0.2931,
      "step": 2260
    },
    {
      "epoch": 0.48649807115302185,
      "grad_norm": 0.37048953771591187,
      "learning_rate": 1.9351335905129305e-05,
      "loss": 0.4626,
      "step": 2270
    },
    {
      "epoch": 0.48864123446206603,
      "grad_norm": 17.922521591186523,
      "learning_rate": 1.934847835405058e-05,
      "loss": 0.7968,
      "step": 2280
    },
    {
      "epoch": 0.49078439777111016,
      "grad_norm": 0.43944084644317627,
      "learning_rate": 1.9345620802971853e-05,
      "loss": 0.2714,
      "step": 2290
    },
    {
      "epoch": 0.4929275610801543,
      "grad_norm": 0.2944529950618744,
      "learning_rate": 1.9342763251893127e-05,
      "loss": 0.3441,
      "step": 2300
    },
    {
      "epoch": 0.49507072438919847,
      "grad_norm": 0.3529971241950989,
      "learning_rate": 1.9339905700814404e-05,
      "loss": 0.8637,
      "step": 2310
    },
    {
      "epoch": 0.4972138876982426,
      "grad_norm": 33.27893829345703,
      "learning_rate": 1.933704814973568e-05,
      "loss": 0.7126,
      "step": 2320
    },
    {
      "epoch": 0.4993570510072868,
      "grad_norm": 0.38189199566841125,
      "learning_rate": 1.9334190598656952e-05,
      "loss": 0.479,
      "step": 2330
    },
    {
      "epoch": 0.5015002143163309,
      "grad_norm": 17.92978858947754,
      "learning_rate": 1.9331333047578226e-05,
      "loss": 1.1392,
      "step": 2340
    },
    {
      "epoch": 0.503643377625375,
      "grad_norm": 0.7053391337394714,
      "learning_rate": 1.93284754964995e-05,
      "loss": 0.488,
      "step": 2350
    },
    {
      "epoch": 0.5057865409344192,
      "grad_norm": 0.8444400429725647,
      "learning_rate": 1.9325617945420777e-05,
      "loss": 0.8655,
      "step": 2360
    },
    {
      "epoch": 0.5079297042434634,
      "grad_norm": 0.727805495262146,
      "learning_rate": 1.932276039434205e-05,
      "loss": 0.5622,
      "step": 2370
    },
    {
      "epoch": 0.5100728675525075,
      "grad_norm": 0.4749455749988556,
      "learning_rate": 1.9319902843263325e-05,
      "loss": 0.4664,
      "step": 2380
    },
    {
      "epoch": 0.5122160308615517,
      "grad_norm": 36.5643310546875,
      "learning_rate": 1.93170452921846e-05,
      "loss": 0.4147,
      "step": 2390
    },
    {
      "epoch": 0.5143591941705958,
      "grad_norm": 0.17226086556911469,
      "learning_rate": 1.9314187741105873e-05,
      "loss": 0.7414,
      "step": 2400
    },
    {
      "epoch": 0.5165023574796399,
      "grad_norm": 26.834243774414062,
      "learning_rate": 1.931133019002715e-05,
      "loss": 0.7196,
      "step": 2410
    },
    {
      "epoch": 0.5186455207886841,
      "grad_norm": 40.95021438598633,
      "learning_rate": 1.9308472638948424e-05,
      "loss": 1.1819,
      "step": 2420
    },
    {
      "epoch": 0.5207886840977283,
      "grad_norm": 25.72755241394043,
      "learning_rate": 1.93056150878697e-05,
      "loss": 0.6064,
      "step": 2430
    },
    {
      "epoch": 0.5229318474067723,
      "grad_norm": 33.95076370239258,
      "learning_rate": 1.9302757536790972e-05,
      "loss": 1.3287,
      "step": 2440
    },
    {
      "epoch": 0.5250750107158165,
      "grad_norm": 0.4130364954471588,
      "learning_rate": 1.9299899985712246e-05,
      "loss": 0.4075,
      "step": 2450
    },
    {
      "epoch": 0.5272181740248607,
      "grad_norm": 0.2888176739215851,
      "learning_rate": 1.929704243463352e-05,
      "loss": 0.8355,
      "step": 2460
    },
    {
      "epoch": 0.5293613373339049,
      "grad_norm": 0.186705082654953,
      "learning_rate": 1.9294184883554794e-05,
      "loss": 0.1856,
      "step": 2470
    },
    {
      "epoch": 0.531504500642949,
      "grad_norm": 0.21432171761989594,
      "learning_rate": 1.9291327332476068e-05,
      "loss": 0.8508,
      "step": 2480
    },
    {
      "epoch": 0.5336476639519931,
      "grad_norm": 18.019479751586914,
      "learning_rate": 1.9288469781397342e-05,
      "loss": 0.7338,
      "step": 2490
    },
    {
      "epoch": 0.5357908272610373,
      "grad_norm": 16.388582229614258,
      "learning_rate": 1.928561223031862e-05,
      "loss": 1.1439,
      "step": 2500
    },
    {
      "epoch": 0.5379339905700814,
      "grad_norm": 18.1151123046875,
      "learning_rate": 1.9282754679239893e-05,
      "loss": 0.673,
      "step": 2510
    },
    {
      "epoch": 0.5400771538791256,
      "grad_norm": 33.99515151977539,
      "learning_rate": 1.9279897128161167e-05,
      "loss": 0.6807,
      "step": 2520
    },
    {
      "epoch": 0.5422203171881698,
      "grad_norm": 0.47603628039360046,
      "learning_rate": 1.927703957708244e-05,
      "loss": 0.5996,
      "step": 2530
    },
    {
      "epoch": 0.5443634804972138,
      "grad_norm": 0.6502681374549866,
      "learning_rate": 1.9274182026003715e-05,
      "loss": 0.6417,
      "step": 2540
    },
    {
      "epoch": 0.546506643806258,
      "grad_norm": 16.090435028076172,
      "learning_rate": 1.9271324474924992e-05,
      "loss": 1.2236,
      "step": 2550
    },
    {
      "epoch": 0.5486498071153022,
      "grad_norm": 0.5962822437286377,
      "learning_rate": 1.9268466923846266e-05,
      "loss": 0.4434,
      "step": 2560
    },
    {
      "epoch": 0.5507929704243464,
      "grad_norm": 0.2714677155017853,
      "learning_rate": 1.926560937276754e-05,
      "loss": 0.3285,
      "step": 2570
    },
    {
      "epoch": 0.5529361337333905,
      "grad_norm": 0.22838623821735382,
      "learning_rate": 1.9262751821688814e-05,
      "loss": 0.7055,
      "step": 2580
    },
    {
      "epoch": 0.5550792970424346,
      "grad_norm": 0.2691568434238434,
      "learning_rate": 1.9259894270610088e-05,
      "loss": 0.6528,
      "step": 2590
    },
    {
      "epoch": 0.5572224603514788,
      "grad_norm": 0.4335000514984131,
      "learning_rate": 1.9257036719531362e-05,
      "loss": 0.5138,
      "step": 2600
    },
    {
      "epoch": 0.5593656236605229,
      "grad_norm": 31.35442352294922,
      "learning_rate": 1.925417916845264e-05,
      "loss": 0.812,
      "step": 2610
    },
    {
      "epoch": 0.5615087869695671,
      "grad_norm": 0.789294421672821,
      "learning_rate": 1.9251321617373913e-05,
      "loss": 0.5059,
      "step": 2620
    },
    {
      "epoch": 0.5636519502786113,
      "grad_norm": 0.6080229878425598,
      "learning_rate": 1.9248464066295187e-05,
      "loss": 0.8713,
      "step": 2630
    },
    {
      "epoch": 0.5657951135876553,
      "grad_norm": 0.49569281935691833,
      "learning_rate": 1.924560651521646e-05,
      "loss": 0.4588,
      "step": 2640
    },
    {
      "epoch": 0.5679382768966995,
      "grad_norm": 16.670475006103516,
      "learning_rate": 1.9242748964137735e-05,
      "loss": 0.454,
      "step": 2650
    },
    {
      "epoch": 0.5700814402057437,
      "grad_norm": 0.23151353001594543,
      "learning_rate": 1.9239891413059012e-05,
      "loss": 0.0066,
      "step": 2660
    },
    {
      "epoch": 0.5722246035147879,
      "grad_norm": 32.188331604003906,
      "learning_rate": 1.9237033861980286e-05,
      "loss": 1.6607,
      "step": 2670
    },
    {
      "epoch": 0.574367766823832,
      "grad_norm": 16.5575008392334,
      "learning_rate": 1.923417631090156e-05,
      "loss": 0.5026,
      "step": 2680
    },
    {
      "epoch": 0.5765109301328761,
      "grad_norm": 0.45031750202178955,
      "learning_rate": 1.9231318759822834e-05,
      "loss": 0.3332,
      "step": 2690
    },
    {
      "epoch": 0.5786540934419203,
      "grad_norm": 0.38887935876846313,
      "learning_rate": 1.9228461208744108e-05,
      "loss": 0.3078,
      "step": 2700
    },
    {
      "epoch": 0.5807972567509644,
      "grad_norm": 0.22910046577453613,
      "learning_rate": 1.9225603657665382e-05,
      "loss": 0.5162,
      "step": 2710
    },
    {
      "epoch": 0.5829404200600086,
      "grad_norm": 18.010726928710938,
      "learning_rate": 1.9222746106586656e-05,
      "loss": 0.3861,
      "step": 2720
    },
    {
      "epoch": 0.5850835833690528,
      "grad_norm": 0.2193668931722641,
      "learning_rate": 1.921988855550793e-05,
      "loss": 0.5817,
      "step": 2730
    },
    {
      "epoch": 0.5872267466780968,
      "grad_norm": 17.700063705444336,
      "learning_rate": 1.9217031004429204e-05,
      "loss": 0.9067,
      "step": 2740
    },
    {
      "epoch": 0.589369909987141,
      "grad_norm": 0.4112771153450012,
      "learning_rate": 1.921417345335048e-05,
      "loss": 0.6799,
      "step": 2750
    },
    {
      "epoch": 0.5915130732961852,
      "grad_norm": 0.3912210166454315,
      "learning_rate": 1.9211315902271755e-05,
      "loss": 0.3227,
      "step": 2760
    },
    {
      "epoch": 0.5936562366052294,
      "grad_norm": 48.807857513427734,
      "learning_rate": 1.920845835119303e-05,
      "loss": 1.0912,
      "step": 2770
    },
    {
      "epoch": 0.5957993999142734,
      "grad_norm": 0.5136880874633789,
      "learning_rate": 1.9205600800114303e-05,
      "loss": 0.6008,
      "step": 2780
    },
    {
      "epoch": 0.5979425632233176,
      "grad_norm": 18.09217643737793,
      "learning_rate": 1.9202743249035577e-05,
      "loss": 0.5998,
      "step": 2790
    },
    {
      "epoch": 0.6000857265323618,
      "grad_norm": 15.337687492370605,
      "learning_rate": 1.9199885697956854e-05,
      "loss": 0.9102,
      "step": 2800
    },
    {
      "epoch": 0.6022288898414059,
      "grad_norm": 0.764876127243042,
      "learning_rate": 1.9197028146878128e-05,
      "loss": 1.0164,
      "step": 2810
    },
    {
      "epoch": 0.6043720531504501,
      "grad_norm": 0.7229796648025513,
      "learning_rate": 1.9194170595799402e-05,
      "loss": 0.5662,
      "step": 2820
    },
    {
      "epoch": 0.6065152164594942,
      "grad_norm": 0.7525395750999451,
      "learning_rate": 1.9191313044720676e-05,
      "loss": 0.8346,
      "step": 2830
    },
    {
      "epoch": 0.6086583797685383,
      "grad_norm": 0.7214665412902832,
      "learning_rate": 1.918845549364195e-05,
      "loss": 0.4418,
      "step": 2840
    },
    {
      "epoch": 0.6108015430775825,
      "grad_norm": 0.4738391041755676,
      "learning_rate": 1.9185597942563227e-05,
      "loss": 0.9399,
      "step": 2850
    },
    {
      "epoch": 0.6129447063866267,
      "grad_norm": 0.5990416407585144,
      "learning_rate": 1.91827403914845e-05,
      "loss": 0.6065,
      "step": 2860
    },
    {
      "epoch": 0.6150878696956709,
      "grad_norm": 16.101713180541992,
      "learning_rate": 1.9179882840405775e-05,
      "loss": 0.8927,
      "step": 2870
    },
    {
      "epoch": 0.6172310330047149,
      "grad_norm": 0.87830650806427,
      "learning_rate": 1.917702528932705e-05,
      "loss": 0.7364,
      "step": 2880
    },
    {
      "epoch": 0.6193741963137591,
      "grad_norm": 0.3491532504558563,
      "learning_rate": 1.9174167738248323e-05,
      "loss": 0.0114,
      "step": 2890
    },
    {
      "epoch": 0.6215173596228033,
      "grad_norm": 0.3153950273990631,
      "learning_rate": 1.9171310187169597e-05,
      "loss": 1.4101,
      "step": 2900
    },
    {
      "epoch": 0.6236605229318474,
      "grad_norm": 0.641681969165802,
      "learning_rate": 1.916845263609087e-05,
      "loss": 0.9312,
      "step": 2910
    },
    {
      "epoch": 0.6258036862408916,
      "grad_norm": 15.361539840698242,
      "learning_rate": 1.9165595085012145e-05,
      "loss": 0.9768,
      "step": 2920
    },
    {
      "epoch": 0.6279468495499357,
      "grad_norm": 1.4815137386322021,
      "learning_rate": 1.916273753393342e-05,
      "loss": 0.6811,
      "step": 2930
    },
    {
      "epoch": 0.6300900128589798,
      "grad_norm": 14.084535598754883,
      "learning_rate": 1.9159879982854696e-05,
      "loss": 0.503,
      "step": 2940
    },
    {
      "epoch": 0.632233176168024,
      "grad_norm": 1.4285558462142944,
      "learning_rate": 1.915702243177597e-05,
      "loss": 0.4078,
      "step": 2950
    },
    {
      "epoch": 0.6343763394770682,
      "grad_norm": 0.45599478483200073,
      "learning_rate": 1.9154164880697244e-05,
      "loss": 0.5511,
      "step": 2960
    },
    {
      "epoch": 0.6365195027861124,
      "grad_norm": 0.3716851472854614,
      "learning_rate": 1.9151307329618518e-05,
      "loss": 0.3212,
      "step": 2970
    },
    {
      "epoch": 0.6386626660951564,
      "grad_norm": 0.35515695810317993,
      "learning_rate": 1.914844977853979e-05,
      "loss": 0.6838,
      "step": 2980
    },
    {
      "epoch": 0.6408058294042006,
      "grad_norm": 16.021909713745117,
      "learning_rate": 1.914559222746107e-05,
      "loss": 0.6647,
      "step": 2990
    },
    {
      "epoch": 0.6429489927132448,
      "grad_norm": 16.609289169311523,
      "learning_rate": 1.9142734676382343e-05,
      "loss": 0.8282,
      "step": 3000
    },
    {
      "epoch": 0.6450921560222889,
      "grad_norm": 16.81590461730957,
      "learning_rate": 1.9139877125303617e-05,
      "loss": 0.6009,
      "step": 3010
    },
    {
      "epoch": 0.647235319331333,
      "grad_norm": 0.49302589893341064,
      "learning_rate": 1.913701957422489e-05,
      "loss": 0.4525,
      "step": 3020
    },
    {
      "epoch": 0.6493784826403772,
      "grad_norm": 0.457011878490448,
      "learning_rate": 1.9134162023146165e-05,
      "loss": 0.4677,
      "step": 3030
    },
    {
      "epoch": 0.6515216459494213,
      "grad_norm": 19.48551368713379,
      "learning_rate": 1.913130447206744e-05,
      "loss": 0.4763,
      "step": 3040
    },
    {
      "epoch": 0.6536648092584655,
      "grad_norm": 0.3241436183452606,
      "learning_rate": 1.9128446920988716e-05,
      "loss": 0.8392,
      "step": 3050
    },
    {
      "epoch": 0.6558079725675097,
      "grad_norm": 0.4252895712852478,
      "learning_rate": 1.912558936990999e-05,
      "loss": 0.6645,
      "step": 3060
    },
    {
      "epoch": 0.6579511358765537,
      "grad_norm": 0.6428619027137756,
      "learning_rate": 1.9122731818831264e-05,
      "loss": 0.4798,
      "step": 3070
    },
    {
      "epoch": 0.6600942991855979,
      "grad_norm": 0.2510993778705597,
      "learning_rate": 1.9119874267752538e-05,
      "loss": 0.3396,
      "step": 3080
    },
    {
      "epoch": 0.6622374624946421,
      "grad_norm": 32.96749496459961,
      "learning_rate": 1.911701671667381e-05,
      "loss": 0.6823,
      "step": 3090
    },
    {
      "epoch": 0.6643806258036863,
      "grad_norm": 15.271646499633789,
      "learning_rate": 1.911415916559509e-05,
      "loss": 1.1327,
      "step": 3100
    },
    {
      "epoch": 0.6665237891127304,
      "grad_norm": 0.6198201179504395,
      "learning_rate": 1.9111301614516363e-05,
      "loss": 0.7267,
      "step": 3110
    },
    {
      "epoch": 0.6686669524217745,
      "grad_norm": 15.088709831237793,
      "learning_rate": 1.9108444063437633e-05,
      "loss": 0.3033,
      "step": 3120
    },
    {
      "epoch": 0.6708101157308187,
      "grad_norm": 16.345094680786133,
      "learning_rate": 1.910558651235891e-05,
      "loss": 0.4986,
      "step": 3130
    },
    {
      "epoch": 0.6729532790398628,
      "grad_norm": 0.6295785307884216,
      "learning_rate": 1.9102728961280185e-05,
      "loss": 0.6517,
      "step": 3140
    },
    {
      "epoch": 0.675096442348907,
      "grad_norm": 0.43704310059547424,
      "learning_rate": 1.909987141020146e-05,
      "loss": 0.4863,
      "step": 3150
    },
    {
      "epoch": 0.6772396056579512,
      "grad_norm": 0.37970492243766785,
      "learning_rate": 1.9097013859122732e-05,
      "loss": 0.1641,
      "step": 3160
    },
    {
      "epoch": 0.6793827689669952,
      "grad_norm": 0.3960638642311096,
      "learning_rate": 1.9094156308044006e-05,
      "loss": 0.8858,
      "step": 3170
    },
    {
      "epoch": 0.6815259322760394,
      "grad_norm": 30.98761558532715,
      "learning_rate": 1.909129875696528e-05,
      "loss": 0.4823,
      "step": 3180
    },
    {
      "epoch": 0.6836690955850836,
      "grad_norm": 0.236566424369812,
      "learning_rate": 1.9088441205886558e-05,
      "loss": 0.3629,
      "step": 3190
    },
    {
      "epoch": 0.6858122588941278,
      "grad_norm": 0.21335341036319733,
      "learning_rate": 1.908558365480783e-05,
      "loss": 0.1863,
      "step": 3200
    },
    {
      "epoch": 0.6879554222031719,
      "grad_norm": 0.29302293062210083,
      "learning_rate": 1.9082726103729105e-05,
      "loss": 0.7217,
      "step": 3210
    },
    {
      "epoch": 0.690098585512216,
      "grad_norm": 0.3702264726161957,
      "learning_rate": 1.907986855265038e-05,
      "loss": 1.5279,
      "step": 3220
    },
    {
      "epoch": 0.6922417488212602,
      "grad_norm": 15.018170356750488,
      "learning_rate": 1.9077011001571653e-05,
      "loss": 0.7795,
      "step": 3230
    },
    {
      "epoch": 0.6943849121303043,
      "grad_norm": 0.717421293258667,
      "learning_rate": 1.907415345049293e-05,
      "loss": 0.8489,
      "step": 3240
    },
    {
      "epoch": 0.6965280754393485,
      "grad_norm": 15.228002548217773,
      "learning_rate": 1.9071295899414205e-05,
      "loss": 0.5592,
      "step": 3250
    },
    {
      "epoch": 0.6986712387483927,
      "grad_norm": 16.64589500427246,
      "learning_rate": 1.906843834833548e-05,
      "loss": 0.8686,
      "step": 3260
    },
    {
      "epoch": 0.7008144020574367,
      "grad_norm": 15.201656341552734,
      "learning_rate": 1.9065580797256752e-05,
      "loss": 0.5922,
      "step": 3270
    },
    {
      "epoch": 0.7029575653664809,
      "grad_norm": 0.7656148076057434,
      "learning_rate": 1.9062723246178026e-05,
      "loss": 0.7293,
      "step": 3280
    },
    {
      "epoch": 0.7051007286755251,
      "grad_norm": 0.7643485069274902,
      "learning_rate": 1.9059865695099304e-05,
      "loss": 0.7041,
      "step": 3290
    },
    {
      "epoch": 0.7072438919845693,
      "grad_norm": 17.692176818847656,
      "learning_rate": 1.9057008144020578e-05,
      "loss": 0.4628,
      "step": 3300
    },
    {
      "epoch": 0.7093870552936133,
      "grad_norm": 17.311838150024414,
      "learning_rate": 1.905415059294185e-05,
      "loss": 0.6567,
      "step": 3310
    },
    {
      "epoch": 0.7115302186026575,
      "grad_norm": 21.85044288635254,
      "learning_rate": 1.9051293041863125e-05,
      "loss": 0.4868,
      "step": 3320
    },
    {
      "epoch": 0.7136733819117017,
      "grad_norm": 0.6145869493484497,
      "learning_rate": 1.90484354907844e-05,
      "loss": 0.6388,
      "step": 3330
    },
    {
      "epoch": 0.7158165452207458,
      "grad_norm": 0.3253024220466614,
      "learning_rate": 1.9045577939705673e-05,
      "loss": 0.6153,
      "step": 3340
    },
    {
      "epoch": 0.71795970852979,
      "grad_norm": 0.27262216806411743,
      "learning_rate": 1.9042720388626947e-05,
      "loss": 0.6002,
      "step": 3350
    },
    {
      "epoch": 0.7201028718388341,
      "grad_norm": 0.24811631441116333,
      "learning_rate": 1.903986283754822e-05,
      "loss": 0.3538,
      "step": 3360
    },
    {
      "epoch": 0.7222460351478782,
      "grad_norm": 0.6926344633102417,
      "learning_rate": 1.9037005286469495e-05,
      "loss": 0.6635,
      "step": 3370
    },
    {
      "epoch": 0.7243891984569224,
      "grad_norm": 0.45647376775741577,
      "learning_rate": 1.9034147735390772e-05,
      "loss": 0.3228,
      "step": 3380
    },
    {
      "epoch": 0.7265323617659666,
      "grad_norm": 20.024991989135742,
      "learning_rate": 1.9031290184312046e-05,
      "loss": 1.1999,
      "step": 3390
    },
    {
      "epoch": 0.7286755250750108,
      "grad_norm": 0.4459759593009949,
      "learning_rate": 1.902843263323332e-05,
      "loss": 0.3342,
      "step": 3400
    },
    {
      "epoch": 0.7308186883840548,
      "grad_norm": 0.17900286614894867,
      "learning_rate": 1.9025575082154594e-05,
      "loss": 0.3357,
      "step": 3410
    },
    {
      "epoch": 0.732961851693099,
      "grad_norm": 0.16712817549705505,
      "learning_rate": 1.9022717531075868e-05,
      "loss": 0.5339,
      "step": 3420
    },
    {
      "epoch": 0.7351050150021432,
      "grad_norm": 21.241744995117188,
      "learning_rate": 1.9019859979997145e-05,
      "loss": 0.5736,
      "step": 3430
    },
    {
      "epoch": 0.7372481783111873,
      "grad_norm": 16.27745246887207,
      "learning_rate": 1.901700242891842e-05,
      "loss": 0.3705,
      "step": 3440
    },
    {
      "epoch": 0.7393913416202315,
      "grad_norm": 0.27122700214385986,
      "learning_rate": 1.9014144877839693e-05,
      "loss": 0.7053,
      "step": 3450
    },
    {
      "epoch": 0.7415345049292756,
      "grad_norm": 0.25684988498687744,
      "learning_rate": 1.9011287326760967e-05,
      "loss": 0.3638,
      "step": 3460
    },
    {
      "epoch": 0.7436776682383197,
      "grad_norm": 0.24262739717960358,
      "learning_rate": 1.900842977568224e-05,
      "loss": 0.3372,
      "step": 3470
    },
    {
      "epoch": 0.7458208315473639,
      "grad_norm": 0.3363901376724243,
      "learning_rate": 1.900557222460352e-05,
      "loss": 0.8553,
      "step": 3480
    },
    {
      "epoch": 0.7479639948564081,
      "grad_norm": 1.616728663444519,
      "learning_rate": 1.9002714673524792e-05,
      "loss": 0.5057,
      "step": 3490
    },
    {
      "epoch": 0.7501071581654523,
      "grad_norm": 20.947172164916992,
      "learning_rate": 1.8999857122446066e-05,
      "loss": 0.5857,
      "step": 3500
    },
    {
      "epoch": 0.7522503214744963,
      "grad_norm": 1.1092089414596558,
      "learning_rate": 1.899699957136734e-05,
      "loss": 0.9353,
      "step": 3510
    },
    {
      "epoch": 0.7543934847835405,
      "grad_norm": 17.1202449798584,
      "learning_rate": 1.8994142020288614e-05,
      "loss": 0.5303,
      "step": 3520
    },
    {
      "epoch": 0.7565366480925847,
      "grad_norm": 20.240631103515625,
      "learning_rate": 1.8991284469209888e-05,
      "loss": 0.7669,
      "step": 3530
    },
    {
      "epoch": 0.7586798114016288,
      "grad_norm": 1.0013957023620605,
      "learning_rate": 1.8988426918131165e-05,
      "loss": 0.656,
      "step": 3540
    },
    {
      "epoch": 0.760822974710673,
      "grad_norm": 20.386390686035156,
      "learning_rate": 1.8985569367052436e-05,
      "loss": 0.8629,
      "step": 3550
    },
    {
      "epoch": 0.7629661380197171,
      "grad_norm": 0.8631309270858765,
      "learning_rate": 1.898271181597371e-05,
      "loss": 0.6248,
      "step": 3560
    },
    {
      "epoch": 0.7651093013287612,
      "grad_norm": 0.34685930609703064,
      "learning_rate": 1.8979854264894987e-05,
      "loss": 0.3119,
      "step": 3570
    },
    {
      "epoch": 0.7672524646378054,
      "grad_norm": 0.42725154757499695,
      "learning_rate": 1.897699671381626e-05,
      "loss": 1.2177,
      "step": 3580
    },
    {
      "epoch": 0.7693956279468496,
      "grad_norm": 1.1751329898834229,
      "learning_rate": 1.8974139162737535e-05,
      "loss": 1.0474,
      "step": 3590
    },
    {
      "epoch": 0.7715387912558938,
      "grad_norm": 15.517340660095215,
      "learning_rate": 1.897128161165881e-05,
      "loss": 0.551,
      "step": 3600
    },
    {
      "epoch": 0.7736819545649378,
      "grad_norm": 16.49602699279785,
      "learning_rate": 1.8968424060580083e-05,
      "loss": 0.8549,
      "step": 3610
    },
    {
      "epoch": 0.775825117873982,
      "grad_norm": 1.5558544397354126,
      "learning_rate": 1.896556650950136e-05,
      "loss": 1.1773,
      "step": 3620
    },
    {
      "epoch": 0.7779682811830262,
      "grad_norm": 0.934037446975708,
      "learning_rate": 1.8962708958422634e-05,
      "loss": 0.5171,
      "step": 3630
    },
    {
      "epoch": 0.7801114444920703,
      "grad_norm": 18.967212677001953,
      "learning_rate": 1.8959851407343908e-05,
      "loss": 0.8569,
      "step": 3640
    },
    {
      "epoch": 0.7822546078011144,
      "grad_norm": 15.910256385803223,
      "learning_rate": 1.8956993856265182e-05,
      "loss": 1.2946,
      "step": 3650
    },
    {
      "epoch": 0.7843977711101586,
      "grad_norm": 0.599548876285553,
      "learning_rate": 1.8954136305186456e-05,
      "loss": 0.4417,
      "step": 3660
    },
    {
      "epoch": 0.7865409344192027,
      "grad_norm": 16.272851943969727,
      "learning_rate": 1.895127875410773e-05,
      "loss": 1.2455,
      "step": 3670
    },
    {
      "epoch": 0.7886840977282469,
      "grad_norm": 0.6278236508369446,
      "learning_rate": 1.8948421203029007e-05,
      "loss": 0.763,
      "step": 3680
    },
    {
      "epoch": 0.7908272610372911,
      "grad_norm": 0.8828583359718323,
      "learning_rate": 1.894556365195028e-05,
      "loss": 1.0078,
      "step": 3690
    },
    {
      "epoch": 0.7929704243463352,
      "grad_norm": 0.6502124667167664,
      "learning_rate": 1.8942706100871555e-05,
      "loss": 0.2845,
      "step": 3700
    },
    {
      "epoch": 0.7951135876553793,
      "grad_norm": 0.5260960459709167,
      "learning_rate": 1.893984854979283e-05,
      "loss": 1.0718,
      "step": 3710
    },
    {
      "epoch": 0.7972567509644235,
      "grad_norm": 0.5793460607528687,
      "learning_rate": 1.8936990998714103e-05,
      "loss": 0.4466,
      "step": 3720
    },
    {
      "epoch": 0.7993999142734677,
      "grad_norm": 0.4463404417037964,
      "learning_rate": 1.893413344763538e-05,
      "loss": 0.7513,
      "step": 3730
    },
    {
      "epoch": 0.8015430775825118,
      "grad_norm": 17.157724380493164,
      "learning_rate": 1.8931275896556654e-05,
      "loss": 0.3238,
      "step": 3740
    },
    {
      "epoch": 0.8036862408915559,
      "grad_norm": 30.30605125427246,
      "learning_rate": 1.8928418345477928e-05,
      "loss": 1.459,
      "step": 3750
    },
    {
      "epoch": 0.8058294042006001,
      "grad_norm": 16.086713790893555,
      "learning_rate": 1.8925560794399202e-05,
      "loss": 1.0334,
      "step": 3760
    },
    {
      "epoch": 0.8079725675096442,
      "grad_norm": 0.9812184572219849,
      "learning_rate": 1.8922703243320476e-05,
      "loss": 0.5738,
      "step": 3770
    },
    {
      "epoch": 0.8101157308186884,
      "grad_norm": 0.6521787047386169,
      "learning_rate": 1.891984569224175e-05,
      "loss": 0.5684,
      "step": 3780
    },
    {
      "epoch": 0.8122588941277326,
      "grad_norm": 0.5068869590759277,
      "learning_rate": 1.8916988141163024e-05,
      "loss": 0.5745,
      "step": 3790
    },
    {
      "epoch": 0.8144020574367766,
      "grad_norm": 14.85358715057373,
      "learning_rate": 1.8914130590084298e-05,
      "loss": 1.282,
      "step": 3800
    },
    {
      "epoch": 0.8165452207458208,
      "grad_norm": 1.240053653717041,
      "learning_rate": 1.891127303900557e-05,
      "loss": 0.2742,
      "step": 3810
    },
    {
      "epoch": 0.818688384054865,
      "grad_norm": 0.3632786273956299,
      "learning_rate": 1.890841548792685e-05,
      "loss": 0.2991,
      "step": 3820
    },
    {
      "epoch": 0.8208315473639092,
      "grad_norm": 30.93731117248535,
      "learning_rate": 1.8905557936848123e-05,
      "loss": 0.8411,
      "step": 3830
    },
    {
      "epoch": 0.8229747106729532,
      "grad_norm": 0.5700350403785706,
      "learning_rate": 1.8902700385769397e-05,
      "loss": 1.0078,
      "step": 3840
    },
    {
      "epoch": 0.8251178739819974,
      "grad_norm": 0.4496258795261383,
      "learning_rate": 1.889984283469067e-05,
      "loss": 0.4641,
      "step": 3850
    },
    {
      "epoch": 0.8272610372910416,
      "grad_norm": 0.803649365901947,
      "learning_rate": 1.8896985283611945e-05,
      "loss": 0.8936,
      "step": 3860
    },
    {
      "epoch": 0.8294042006000857,
      "grad_norm": 0.4414524435997009,
      "learning_rate": 1.8894127732533222e-05,
      "loss": 0.1562,
      "step": 3870
    },
    {
      "epoch": 0.8315473639091299,
      "grad_norm": 15.492035865783691,
      "learning_rate": 1.8891270181454496e-05,
      "loss": 0.516,
      "step": 3880
    },
    {
      "epoch": 0.833690527218174,
      "grad_norm": 0.26375043392181396,
      "learning_rate": 1.888841263037577e-05,
      "loss": 0.5323,
      "step": 3890
    },
    {
      "epoch": 0.8358336905272181,
      "grad_norm": 30.668519973754883,
      "learning_rate": 1.8885555079297044e-05,
      "loss": 0.7064,
      "step": 3900
    },
    {
      "epoch": 0.8379768538362623,
      "grad_norm": 0.30833661556243896,
      "learning_rate": 1.8882697528218318e-05,
      "loss": 0.8708,
      "step": 3910
    },
    {
      "epoch": 0.8401200171453065,
      "grad_norm": 0.3802391588687897,
      "learning_rate": 1.8879839977139595e-05,
      "loss": 0.1737,
      "step": 3920
    },
    {
      "epoch": 0.8422631804543507,
      "grad_norm": 0.39965933561325073,
      "learning_rate": 1.887698242606087e-05,
      "loss": 0.493,
      "step": 3930
    },
    {
      "epoch": 0.8444063437633947,
      "grad_norm": 0.36352935433387756,
      "learning_rate": 1.8874124874982143e-05,
      "loss": 0.4922,
      "step": 3940
    },
    {
      "epoch": 0.8465495070724389,
      "grad_norm": 0.21844586730003357,
      "learning_rate": 1.8871267323903417e-05,
      "loss": 0.1827,
      "step": 3950
    },
    {
      "epoch": 0.8486926703814831,
      "grad_norm": 15.243309020996094,
      "learning_rate": 1.886840977282469e-05,
      "loss": 0.8989,
      "step": 3960
    },
    {
      "epoch": 0.8508358336905272,
      "grad_norm": 16.967374801635742,
      "learning_rate": 1.8865552221745968e-05,
      "loss": 0.9211,
      "step": 3970
    },
    {
      "epoch": 0.8529789969995714,
      "grad_norm": 15.342059135437012,
      "learning_rate": 1.886269467066724e-05,
      "loss": 0.8839,
      "step": 3980
    },
    {
      "epoch": 0.8551221603086155,
      "grad_norm": 0.4021594822406769,
      "learning_rate": 1.8859837119588512e-05,
      "loss": 0.6656,
      "step": 3990
    },
    {
      "epoch": 0.8572653236176596,
      "grad_norm": 0.559852659702301,
      "learning_rate": 1.8856979568509786e-05,
      "loss": 0.629,
      "step": 4000
    },
    {
      "epoch": 0.8594084869267038,
      "grad_norm": 0.49171197414398193,
      "learning_rate": 1.8854122017431064e-05,
      "loss": 0.6143,
      "step": 4010
    },
    {
      "epoch": 0.861551650235748,
      "grad_norm": 0.6456260681152344,
      "learning_rate": 1.8851264466352338e-05,
      "loss": 0.7485,
      "step": 4020
    },
    {
      "epoch": 0.8636948135447922,
      "grad_norm": 0.6498191952705383,
      "learning_rate": 1.884840691527361e-05,
      "loss": 0.7293,
      "step": 4030
    },
    {
      "epoch": 0.8658379768538362,
      "grad_norm": 0.38074150681495667,
      "learning_rate": 1.8845549364194885e-05,
      "loss": 0.1695,
      "step": 4040
    },
    {
      "epoch": 0.8679811401628804,
      "grad_norm": 16.05178451538086,
      "learning_rate": 1.884269181311616e-05,
      "loss": 1.1255,
      "step": 4050
    },
    {
      "epoch": 0.8701243034719246,
      "grad_norm": 0.5669714212417603,
      "learning_rate": 1.8839834262037437e-05,
      "loss": 0.4708,
      "step": 4060
    },
    {
      "epoch": 0.8722674667809687,
      "grad_norm": 0.7999948859214783,
      "learning_rate": 1.883697671095871e-05,
      "loss": 1.0056,
      "step": 4070
    },
    {
      "epoch": 0.8744106300900129,
      "grad_norm": 0.6880901455879211,
      "learning_rate": 1.8834119159879985e-05,
      "loss": 0.5574,
      "step": 4080
    },
    {
      "epoch": 0.876553793399057,
      "grad_norm": 0.4480559825897217,
      "learning_rate": 1.883126160880126e-05,
      "loss": 0.153,
      "step": 4090
    },
    {
      "epoch": 0.8786969567081011,
      "grad_norm": 15.639094352722168,
      "learning_rate": 1.8828404057722532e-05,
      "loss": 0.5537,
      "step": 4100
    },
    {
      "epoch": 0.8808401200171453,
      "grad_norm": 0.23547790944576263,
      "learning_rate": 1.882554650664381e-05,
      "loss": 0.538,
      "step": 4110
    },
    {
      "epoch": 0.8829832833261895,
      "grad_norm": 0.2613126039505005,
      "learning_rate": 1.8822688955565084e-05,
      "loss": 0.7098,
      "step": 4120
    },
    {
      "epoch": 0.8851264466352337,
      "grad_norm": 0.2692550718784332,
      "learning_rate": 1.8819831404486358e-05,
      "loss": 0.3496,
      "step": 4130
    },
    {
      "epoch": 0.8872696099442777,
      "grad_norm": 0.3181382119655609,
      "learning_rate": 1.881697385340763e-05,
      "loss": 0.5273,
      "step": 4140
    },
    {
      "epoch": 0.8894127732533219,
      "grad_norm": 0.4949640929698944,
      "learning_rate": 1.8814116302328905e-05,
      "loss": 0.6524,
      "step": 4150
    },
    {
      "epoch": 0.8915559365623661,
      "grad_norm": 0.2886655926704407,
      "learning_rate": 1.881125875125018e-05,
      "loss": 0.0074,
      "step": 4160
    },
    {
      "epoch": 0.8936990998714102,
      "grad_norm": 0.2041705697774887,
      "learning_rate": 1.8808401200171457e-05,
      "loss": 0.5482,
      "step": 4170
    },
    {
      "epoch": 0.8958422631804543,
      "grad_norm": 0.3172416388988495,
      "learning_rate": 1.880554364909273e-05,
      "loss": 1.0343,
      "step": 4180
    },
    {
      "epoch": 0.8979854264894985,
      "grad_norm": 0.5182012319564819,
      "learning_rate": 1.8802686098014e-05,
      "loss": 1.2666,
      "step": 4190
    },
    {
      "epoch": 0.9001285897985426,
      "grad_norm": 16.030168533325195,
      "learning_rate": 1.879982854693528e-05,
      "loss": 0.5959,
      "step": 4200
    },
    {
      "epoch": 0.9022717531075868,
      "grad_norm": 0.5850778222084045,
      "learning_rate": 1.8796970995856552e-05,
      "loss": 0.1586,
      "step": 4210
    },
    {
      "epoch": 0.904414916416631,
      "grad_norm": 29.74312973022461,
      "learning_rate": 1.8794113444777826e-05,
      "loss": 0.9082,
      "step": 4220
    },
    {
      "epoch": 0.9065580797256751,
      "grad_norm": 0.6156865954399109,
      "learning_rate": 1.87912558936991e-05,
      "loss": 1.0401,
      "step": 4230
    },
    {
      "epoch": 0.9087012430347192,
      "grad_norm": 1.0245081186294556,
      "learning_rate": 1.8788398342620374e-05,
      "loss": 0.9593,
      "step": 4240
    },
    {
      "epoch": 0.9108444063437634,
      "grad_norm": 0.6609655618667603,
      "learning_rate": 1.878554079154165e-05,
      "loss": 0.4189,
      "step": 4250
    },
    {
      "epoch": 0.9129875696528076,
      "grad_norm": 0.3861662745475769,
      "learning_rate": 1.8782683240462925e-05,
      "loss": 0.2979,
      "step": 4260
    },
    {
      "epoch": 0.9151307329618517,
      "grad_norm": 16.183061599731445,
      "learning_rate": 1.87798256893842e-05,
      "loss": 0.9752,
      "step": 4270
    },
    {
      "epoch": 0.9172738962708958,
      "grad_norm": 0.3156064748764038,
      "learning_rate": 1.8776968138305473e-05,
      "loss": 0.1706,
      "step": 4280
    },
    {
      "epoch": 0.91941705957994,
      "grad_norm": 15.8319730758667,
      "learning_rate": 1.8774110587226747e-05,
      "loss": 0.3657,
      "step": 4290
    },
    {
      "epoch": 0.9215602228889841,
      "grad_norm": 0.23594999313354492,
      "learning_rate": 1.877125303614802e-05,
      "loss": 0.7238,
      "step": 4300
    },
    {
      "epoch": 0.9237033861980283,
      "grad_norm": 15.528304100036621,
      "learning_rate": 1.87683954850693e-05,
      "loss": 0.3446,
      "step": 4310
    },
    {
      "epoch": 0.9258465495070725,
      "grad_norm": 0.22452279925346375,
      "learning_rate": 1.8765537933990572e-05,
      "loss": 0.1839,
      "step": 4320
    },
    {
      "epoch": 0.9279897128161166,
      "grad_norm": 0.2771506607532501,
      "learning_rate": 1.8762680382911846e-05,
      "loss": 0.5283,
      "step": 4330
    },
    {
      "epoch": 0.9301328761251607,
      "grad_norm": 0.4401223659515381,
      "learning_rate": 1.875982283183312e-05,
      "loss": 1.1707,
      "step": 4340
    },
    {
      "epoch": 0.9322760394342049,
      "grad_norm": 15.430473327636719,
      "learning_rate": 1.8756965280754394e-05,
      "loss": 1.0785,
      "step": 4350
    },
    {
      "epoch": 0.9344192027432491,
      "grad_norm": 14.942784309387207,
      "learning_rate": 1.875410772967567e-05,
      "loss": 0.6927,
      "step": 4360
    },
    {
      "epoch": 0.9365623660522931,
      "grad_norm": 14.308879852294922,
      "learning_rate": 1.8751250178596945e-05,
      "loss": 0.6503,
      "step": 4370
    },
    {
      "epoch": 0.9387055293613373,
      "grad_norm": 0.6348748207092285,
      "learning_rate": 1.874839262751822e-05,
      "loss": 0.1505,
      "step": 4380
    },
    {
      "epoch": 0.9408486926703815,
      "grad_norm": 0.3283365070819855,
      "learning_rate": 1.8745535076439493e-05,
      "loss": 0.3129,
      "step": 4390
    },
    {
      "epoch": 0.9429918559794256,
      "grad_norm": 0.355984628200531,
      "learning_rate": 1.8742677525360767e-05,
      "loss": 0.6883,
      "step": 4400
    },
    {
      "epoch": 0.9451350192884698,
      "grad_norm": 0.28553107380867004,
      "learning_rate": 1.873981997428204e-05,
      "loss": 0.5142,
      "step": 4410
    },
    {
      "epoch": 0.947278182597514,
      "grad_norm": 15.959007263183594,
      "learning_rate": 1.8736962423203315e-05,
      "loss": 0.6647,
      "step": 4420
    },
    {
      "epoch": 0.9494213459065581,
      "grad_norm": 29.699865341186523,
      "learning_rate": 1.873410487212459e-05,
      "loss": 0.797,
      "step": 4430
    },
    {
      "epoch": 0.9515645092156022,
      "grad_norm": 16.15754508972168,
      "learning_rate": 1.8731247321045863e-05,
      "loss": 1.0325,
      "step": 4440
    },
    {
      "epoch": 0.9537076725246464,
      "grad_norm": 16.016647338867188,
      "learning_rate": 1.872838976996714e-05,
      "loss": 0.8713,
      "step": 4450
    },
    {
      "epoch": 0.9558508358336906,
      "grad_norm": 16.625411987304688,
      "learning_rate": 1.8725532218888414e-05,
      "loss": 0.5466,
      "step": 4460
    },
    {
      "epoch": 0.9579939991427346,
      "grad_norm": 34.84170913696289,
      "learning_rate": 1.8722674667809688e-05,
      "loss": 0.5704,
      "step": 4470
    },
    {
      "epoch": 0.9601371624517788,
      "grad_norm": 17.19805145263672,
      "learning_rate": 1.8719817116730962e-05,
      "loss": 0.8077,
      "step": 4480
    },
    {
      "epoch": 0.962280325760823,
      "grad_norm": 18.545963287353516,
      "learning_rate": 1.8716959565652236e-05,
      "loss": 0.6128,
      "step": 4490
    },
    {
      "epoch": 0.9644234890698671,
      "grad_norm": 17.224672317504883,
      "learning_rate": 1.8714102014573513e-05,
      "loss": 0.5375,
      "step": 4500
    },
    {
      "epoch": 0.9665666523789113,
      "grad_norm": 0.7537663578987122,
      "learning_rate": 1.8711244463494787e-05,
      "loss": 0.7719,
      "step": 4510
    },
    {
      "epoch": 0.9687098156879554,
      "grad_norm": 0.6398955583572388,
      "learning_rate": 1.870838691241606e-05,
      "loss": 0.5666,
      "step": 4520
    },
    {
      "epoch": 0.9708529789969995,
      "grad_norm": 16.806777954101562,
      "learning_rate": 1.8705529361337335e-05,
      "loss": 0.3249,
      "step": 4530
    },
    {
      "epoch": 0.9729961423060437,
      "grad_norm": 0.49999862909317017,
      "learning_rate": 1.870267181025861e-05,
      "loss": 0.4923,
      "step": 4540
    },
    {
      "epoch": 0.9751393056150879,
      "grad_norm": 0.21940672397613525,
      "learning_rate": 1.8699814259179886e-05,
      "loss": 0.3569,
      "step": 4550
    },
    {
      "epoch": 0.9772824689241321,
      "grad_norm": 0.23552873730659485,
      "learning_rate": 1.869695670810116e-05,
      "loss": 0.7436,
      "step": 4560
    },
    {
      "epoch": 0.9794256322331761,
      "grad_norm": 0.26475998759269714,
      "learning_rate": 1.8694099157022434e-05,
      "loss": 1.0702,
      "step": 4570
    },
    {
      "epoch": 0.9815687955422203,
      "grad_norm": 16.405960083007812,
      "learning_rate": 1.8691241605943708e-05,
      "loss": 0.1784,
      "step": 4580
    },
    {
      "epoch": 0.9837119588512645,
      "grad_norm": 0.4226081371307373,
      "learning_rate": 1.8688384054864982e-05,
      "loss": 1.0422,
      "step": 4590
    },
    {
      "epoch": 0.9858551221603086,
      "grad_norm": 14.899789810180664,
      "learning_rate": 1.8685526503786256e-05,
      "loss": 0.9451,
      "step": 4600
    },
    {
      "epoch": 0.9879982854693528,
      "grad_norm": 30.408672332763672,
      "learning_rate": 1.8682668952707533e-05,
      "loss": 1.0018,
      "step": 4610
    },
    {
      "epoch": 0.9901414487783969,
      "grad_norm": 17.179426193237305,
      "learning_rate": 1.8679811401628804e-05,
      "loss": 0.9739,
      "step": 4620
    },
    {
      "epoch": 0.992284612087441,
      "grad_norm": 0.8157073259353638,
      "learning_rate": 1.8676953850550078e-05,
      "loss": 0.2803,
      "step": 4630
    },
    {
      "epoch": 0.9944277753964852,
      "grad_norm": 17.516036987304688,
      "learning_rate": 1.8674096299471355e-05,
      "loss": 0.3067,
      "step": 4640
    },
    {
      "epoch": 0.9965709387055294,
      "grad_norm": 0.32468289136886597,
      "learning_rate": 1.867123874839263e-05,
      "loss": 0.4976,
      "step": 4650
    },
    {
      "epoch": 0.9987141020145736,
      "grad_norm": 0.3515167534351349,
      "learning_rate": 1.8668381197313903e-05,
      "loss": 0.672,
      "step": 4660
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8676666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.6480844616889954,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 396.438,
      "eval_samples_per_second": 7.567,
      "eval_steps_per_second": 2.522,
      "step": 4666
    },
    {
      "epoch": 1.0008572653236176,
      "grad_norm": 0.3582441210746765,
      "learning_rate": 1.8665523646235177e-05,
      "loss": 0.8275,
      "step": 4670
    },
    {
      "epoch": 1.0030004286326617,
      "grad_norm": 0.3906213939189911,
      "learning_rate": 1.866266609515645e-05,
      "loss": 0.4842,
      "step": 4680
    },
    {
      "epoch": 1.005143591941706,
      "grad_norm": 15.082634925842285,
      "learning_rate": 1.8659808544077728e-05,
      "loss": 1.2345,
      "step": 4690
    },
    {
      "epoch": 1.00728675525075,
      "grad_norm": 0.6987248659133911,
      "learning_rate": 1.8656950992999002e-05,
      "loss": 0.567,
      "step": 4700
    },
    {
      "epoch": 1.0094299185597944,
      "grad_norm": 16.991479873657227,
      "learning_rate": 1.8654093441920276e-05,
      "loss": 0.7419,
      "step": 4710
    },
    {
      "epoch": 1.0115730818688384,
      "grad_norm": 1.4384907484054565,
      "learning_rate": 1.865123589084155e-05,
      "loss": 0.8414,
      "step": 4720
    },
    {
      "epoch": 1.0137162451778825,
      "grad_norm": 15.465160369873047,
      "learning_rate": 1.8648378339762824e-05,
      "loss": 0.3851,
      "step": 4730
    },
    {
      "epoch": 1.0158594084869268,
      "grad_norm": 0.7337610125541687,
      "learning_rate": 1.8645520788684098e-05,
      "loss": 0.4171,
      "step": 4740
    },
    {
      "epoch": 1.0180025717959709,
      "grad_norm": 0.4529237449169159,
      "learning_rate": 1.8642663237605375e-05,
      "loss": 0.3054,
      "step": 4750
    },
    {
      "epoch": 1.020145735105015,
      "grad_norm": 0.40433794260025024,
      "learning_rate": 1.863980568652665e-05,
      "loss": 0.65,
      "step": 4760
    },
    {
      "epoch": 1.0222888984140592,
      "grad_norm": 0.4135681688785553,
      "learning_rate": 1.8636948135447923e-05,
      "loss": 0.3235,
      "step": 4770
    },
    {
      "epoch": 1.0244320617231033,
      "grad_norm": 15.447749137878418,
      "learning_rate": 1.8634090584369197e-05,
      "loss": 0.6536,
      "step": 4780
    },
    {
      "epoch": 1.0265752250321474,
      "grad_norm": 0.3773897588253021,
      "learning_rate": 1.863123303329047e-05,
      "loss": 0.4948,
      "step": 4790
    },
    {
      "epoch": 1.0287183883411917,
      "grad_norm": 0.5049072504043579,
      "learning_rate": 1.8628375482211748e-05,
      "loss": 0.4883,
      "step": 4800
    },
    {
      "epoch": 1.0308615516502357,
      "grad_norm": 0.419135719537735,
      "learning_rate": 1.8625517931133022e-05,
      "loss": 0.6328,
      "step": 4810
    },
    {
      "epoch": 1.0330047149592798,
      "grad_norm": 0.42649075388908386,
      "learning_rate": 1.8622660380054296e-05,
      "loss": 0.7903,
      "step": 4820
    },
    {
      "epoch": 1.035147878268324,
      "grad_norm": 0.5817197561264038,
      "learning_rate": 1.861980282897557e-05,
      "loss": 0.7761,
      "step": 4830
    },
    {
      "epoch": 1.0372910415773682,
      "grad_norm": 0.4780133366584778,
      "learning_rate": 1.8616945277896844e-05,
      "loss": 0.3197,
      "step": 4840
    },
    {
      "epoch": 1.0394342048864122,
      "grad_norm": 0.386944442987442,
      "learning_rate": 1.8614087726818118e-05,
      "loss": 0.7978,
      "step": 4850
    },
    {
      "epoch": 1.0415773681954565,
      "grad_norm": 14.970415115356445,
      "learning_rate": 1.861123017573939e-05,
      "loss": 0.7746,
      "step": 4860
    },
    {
      "epoch": 1.0437205315045006,
      "grad_norm": 29.97639274597168,
      "learning_rate": 1.8608372624660666e-05,
      "loss": 0.8954,
      "step": 4870
    },
    {
      "epoch": 1.0458636948135447,
      "grad_norm": 0.535533607006073,
      "learning_rate": 1.860551507358194e-05,
      "loss": 0.161,
      "step": 4880
    },
    {
      "epoch": 1.048006858122589,
      "grad_norm": 0.4980306923389435,
      "learning_rate": 1.8602657522503217e-05,
      "loss": 0.7624,
      "step": 4890
    },
    {
      "epoch": 1.050150021431633,
      "grad_norm": 0.4390930235385895,
      "learning_rate": 1.859979997142449e-05,
      "loss": 0.4813,
      "step": 4900
    },
    {
      "epoch": 1.0522931847406773,
      "grad_norm": 0.5329053401947021,
      "learning_rate": 1.8596942420345765e-05,
      "loss": 0.6252,
      "step": 4910
    },
    {
      "epoch": 1.0544363480497214,
      "grad_norm": 0.4221884608268738,
      "learning_rate": 1.859408486926704e-05,
      "loss": 0.3216,
      "step": 4920
    },
    {
      "epoch": 1.0565795113587655,
      "grad_norm": 0.45095589756965637,
      "learning_rate": 1.8591227318188313e-05,
      "loss": 0.9598,
      "step": 4930
    },
    {
      "epoch": 1.0587226746678098,
      "grad_norm": 15.056190490722656,
      "learning_rate": 1.858836976710959e-05,
      "loss": 0.6171,
      "step": 4940
    },
    {
      "epoch": 1.0608658379768539,
      "grad_norm": 15.409238815307617,
      "learning_rate": 1.8585512216030864e-05,
      "loss": 0.6199,
      "step": 4950
    },
    {
      "epoch": 1.063009001285898,
      "grad_norm": 0.4984321892261505,
      "learning_rate": 1.8582654664952138e-05,
      "loss": 0.7588,
      "step": 4960
    },
    {
      "epoch": 1.0651521645949422,
      "grad_norm": 0.4635491669178009,
      "learning_rate": 1.857979711387341e-05,
      "loss": 0.4637,
      "step": 4970
    },
    {
      "epoch": 1.0672953279039863,
      "grad_norm": 0.3539073169231415,
      "learning_rate": 1.8576939562794686e-05,
      "loss": 0.1621,
      "step": 4980
    },
    {
      "epoch": 1.0694384912130304,
      "grad_norm": 0.24746966361999512,
      "learning_rate": 1.8574082011715963e-05,
      "loss": 0.185,
      "step": 4990
    },
    {
      "epoch": 1.0715816545220747,
      "grad_norm": 0.24177536368370056,
      "learning_rate": 1.8571224460637237e-05,
      "loss": 0.3562,
      "step": 5000
    },
    {
      "epoch": 1.0737248178311187,
      "grad_norm": 0.19074919819831848,
      "learning_rate": 1.856836690955851e-05,
      "loss": 0.5453,
      "step": 5010
    },
    {
      "epoch": 1.0758679811401628,
      "grad_norm": 0.20475369691848755,
      "learning_rate": 1.8565509358479785e-05,
      "loss": 0.3591,
      "step": 5020
    },
    {
      "epoch": 1.078011144449207,
      "grad_norm": 0.25357380509376526,
      "learning_rate": 1.856265180740106e-05,
      "loss": 0.5453,
      "step": 5030
    },
    {
      "epoch": 1.0801543077582512,
      "grad_norm": 15.537396430969238,
      "learning_rate": 1.8559794256322336e-05,
      "loss": 0.5101,
      "step": 5040
    },
    {
      "epoch": 1.0822974710672952,
      "grad_norm": 16.22052764892578,
      "learning_rate": 1.8556936705243606e-05,
      "loss": 0.3527,
      "step": 5050
    },
    {
      "epoch": 1.0844406343763395,
      "grad_norm": 0.3309762477874756,
      "learning_rate": 1.855407915416488e-05,
      "loss": 0.523,
      "step": 5060
    },
    {
      "epoch": 1.0865837976853836,
      "grad_norm": 0.24810726940631866,
      "learning_rate": 1.8551221603086154e-05,
      "loss": 0.1822,
      "step": 5070
    },
    {
      "epoch": 1.0887269609944277,
      "grad_norm": 0.2596298158168793,
      "learning_rate": 1.854836405200743e-05,
      "loss": 1.0348,
      "step": 5080
    },
    {
      "epoch": 1.090870124303472,
      "grad_norm": 0.3677724003791809,
      "learning_rate": 1.8545506500928706e-05,
      "loss": 0.8434,
      "step": 5090
    },
    {
      "epoch": 1.093013287612516,
      "grad_norm": 0.36622384190559387,
      "learning_rate": 1.854264894984998e-05,
      "loss": 0.1657,
      "step": 5100
    },
    {
      "epoch": 1.0951564509215603,
      "grad_norm": 0.29848751425743103,
      "learning_rate": 1.8539791398771253e-05,
      "loss": 0.1676,
      "step": 5110
    },
    {
      "epoch": 1.0972996142306044,
      "grad_norm": 0.22469106316566467,
      "learning_rate": 1.8536933847692527e-05,
      "loss": 0.5351,
      "step": 5120
    },
    {
      "epoch": 1.0994427775396485,
      "grad_norm": 15.258673667907715,
      "learning_rate": 1.8534076296613805e-05,
      "loss": 0.531,
      "step": 5130
    },
    {
      "epoch": 1.1015859408486928,
      "grad_norm": 0.3405302166938782,
      "learning_rate": 1.853121874553508e-05,
      "loss": 0.6911,
      "step": 5140
    },
    {
      "epoch": 1.1037291041577368,
      "grad_norm": 14.71780776977539,
      "learning_rate": 1.8528361194456353e-05,
      "loss": 0.6626,
      "step": 5150
    },
    {
      "epoch": 1.105872267466781,
      "grad_norm": 15.10587215423584,
      "learning_rate": 1.8525503643377626e-05,
      "loss": 0.4894,
      "step": 5160
    },
    {
      "epoch": 1.1080154307758252,
      "grad_norm": 0.3635942339897156,
      "learning_rate": 1.85226460922989e-05,
      "loss": 0.6461,
      "step": 5170
    },
    {
      "epoch": 1.1101585940848693,
      "grad_norm": 0.3147915005683899,
      "learning_rate": 1.8519788541220178e-05,
      "loss": 0.0079,
      "step": 5180
    },
    {
      "epoch": 1.1123017573939133,
      "grad_norm": 0.32105007767677307,
      "learning_rate": 1.851693099014145e-05,
      "loss": 0.6648,
      "step": 5190
    },
    {
      "epoch": 1.1144449207029576,
      "grad_norm": 15.802933692932129,
      "learning_rate": 1.8514073439062726e-05,
      "loss": 0.4933,
      "step": 5200
    },
    {
      "epoch": 1.1165880840120017,
      "grad_norm": 0.447365939617157,
      "learning_rate": 1.8511215887984e-05,
      "loss": 0.8014,
      "step": 5210
    },
    {
      "epoch": 1.1187312473210458,
      "grad_norm": 0.45798665285110474,
      "learning_rate": 1.8508358336905273e-05,
      "loss": 0.7932,
      "step": 5220
    },
    {
      "epoch": 1.12087441063009,
      "grad_norm": 0.4464431703090668,
      "learning_rate": 1.8505500785826547e-05,
      "loss": 0.3193,
      "step": 5230
    },
    {
      "epoch": 1.1230175739391342,
      "grad_norm": 30.580060958862305,
      "learning_rate": 1.8502643234747825e-05,
      "loss": 1.2103,
      "step": 5240
    },
    {
      "epoch": 1.1251607372481782,
      "grad_norm": 14.785694122314453,
      "learning_rate": 1.84997856836691e-05,
      "loss": 0.1692,
      "step": 5250
    },
    {
      "epoch": 1.1273039005572225,
      "grad_norm": 30.108932495117188,
      "learning_rate": 1.849692813259037e-05,
      "loss": 0.9685,
      "step": 5260
    },
    {
      "epoch": 1.1294470638662666,
      "grad_norm": 15.86138916015625,
      "learning_rate": 1.8494070581511646e-05,
      "loss": 0.6384,
      "step": 5270
    },
    {
      "epoch": 1.1315902271753107,
      "grad_norm": 0.5852295756340027,
      "learning_rate": 1.849121303043292e-05,
      "loss": 0.3159,
      "step": 5280
    },
    {
      "epoch": 1.133733390484355,
      "grad_norm": 0.5319037437438965,
      "learning_rate": 1.8488355479354194e-05,
      "loss": 0.6042,
      "step": 5290
    },
    {
      "epoch": 1.135876553793399,
      "grad_norm": 0.43290045857429504,
      "learning_rate": 1.8485497928275468e-05,
      "loss": 0.1632,
      "step": 5300
    },
    {
      "epoch": 1.138019717102443,
      "grad_norm": 0.34017324447631836,
      "learning_rate": 1.8482640377196742e-05,
      "loss": 0.6543,
      "step": 5310
    },
    {
      "epoch": 1.1401628804114874,
      "grad_norm": 15.407442092895508,
      "learning_rate": 1.847978282611802e-05,
      "loss": 1.0036,
      "step": 5320
    },
    {
      "epoch": 1.1423060437205315,
      "grad_norm": 0.4308353364467621,
      "learning_rate": 1.8476925275039293e-05,
      "loss": 0.494,
      "step": 5330
    },
    {
      "epoch": 1.1444492070295755,
      "grad_norm": 0.3819945752620697,
      "learning_rate": 1.8474067723960567e-05,
      "loss": 0.4805,
      "step": 5340
    },
    {
      "epoch": 1.1465923703386198,
      "grad_norm": 0.3991369307041168,
      "learning_rate": 1.847121017288184e-05,
      "loss": 0.4803,
      "step": 5350
    },
    {
      "epoch": 1.148735533647664,
      "grad_norm": 15.187050819396973,
      "learning_rate": 1.8468352621803115e-05,
      "loss": 0.3466,
      "step": 5360
    },
    {
      "epoch": 1.1508786969567082,
      "grad_norm": 0.3027176856994629,
      "learning_rate": 1.846549507072439e-05,
      "loss": 1.3639,
      "step": 5370
    },
    {
      "epoch": 1.1530218602657523,
      "grad_norm": 14.944106101989746,
      "learning_rate": 1.8462637519645666e-05,
      "loss": 0.8023,
      "step": 5380
    },
    {
      "epoch": 1.1551650235747963,
      "grad_norm": 14.754535675048828,
      "learning_rate": 1.845977996856694e-05,
      "loss": 1.5324,
      "step": 5390
    },
    {
      "epoch": 1.1573081868838406,
      "grad_norm": 15.224043846130371,
      "learning_rate": 1.8456922417488214e-05,
      "loss": 1.0103,
      "step": 5400
    },
    {
      "epoch": 1.1594513501928847,
      "grad_norm": 0.7727471590042114,
      "learning_rate": 1.8454064866409488e-05,
      "loss": 0.1544,
      "step": 5410
    },
    {
      "epoch": 1.1615945135019288,
      "grad_norm": 14.525917053222656,
      "learning_rate": 1.8451207315330762e-05,
      "loss": 0.5858,
      "step": 5420
    },
    {
      "epoch": 1.163737676810973,
      "grad_norm": 0.5050656795501709,
      "learning_rate": 1.844834976425204e-05,
      "loss": 0.3024,
      "step": 5430
    },
    {
      "epoch": 1.1658808401200171,
      "grad_norm": 14.982783317565918,
      "learning_rate": 1.8445492213173313e-05,
      "loss": 0.616,
      "step": 5440
    },
    {
      "epoch": 1.1680240034290612,
      "grad_norm": 0.43356892466545105,
      "learning_rate": 1.8442634662094587e-05,
      "loss": 0.635,
      "step": 5450
    },
    {
      "epoch": 1.1701671667381055,
      "grad_norm": 0.5451458096504211,
      "learning_rate": 1.843977711101586e-05,
      "loss": 1.0583,
      "step": 5460
    },
    {
      "epoch": 1.1723103300471496,
      "grad_norm": 0.7382145524024963,
      "learning_rate": 1.8436919559937135e-05,
      "loss": 0.5813,
      "step": 5470
    },
    {
      "epoch": 1.1744534933561936,
      "grad_norm": 0.6487867832183838,
      "learning_rate": 1.843406200885841e-05,
      "loss": 0.287,
      "step": 5480
    },
    {
      "epoch": 1.176596656665238,
      "grad_norm": 0.5084013938903809,
      "learning_rate": 1.8431204457779683e-05,
      "loss": 0.4644,
      "step": 5490
    },
    {
      "epoch": 1.178739819974282,
      "grad_norm": 0.5106433033943176,
      "learning_rate": 1.8428346906700957e-05,
      "loss": 0.6077,
      "step": 5500
    },
    {
      "epoch": 1.1808829832833263,
      "grad_norm": 0.40956926345825195,
      "learning_rate": 1.842548935562223e-05,
      "loss": 0.1551,
      "step": 5510
    },
    {
      "epoch": 1.1830261465923704,
      "grad_norm": 0.24300548434257507,
      "learning_rate": 1.8422631804543508e-05,
      "loss": 0.3405,
      "step": 5520
    },
    {
      "epoch": 1.1851693099014144,
      "grad_norm": 0.2847015857696533,
      "learning_rate": 1.8419774253464782e-05,
      "loss": 1.4126,
      "step": 5530
    },
    {
      "epoch": 1.1873124732104587,
      "grad_norm": 0.30866220593452454,
      "learning_rate": 1.8416916702386056e-05,
      "loss": 0.5093,
      "step": 5540
    },
    {
      "epoch": 1.1894556365195028,
      "grad_norm": 0.36583951115608215,
      "learning_rate": 1.841405915130733e-05,
      "loss": 0.4993,
      "step": 5550
    },
    {
      "epoch": 1.1915987998285469,
      "grad_norm": 0.3515361547470093,
      "learning_rate": 1.8411201600228604e-05,
      "loss": 0.8213,
      "step": 5560
    },
    {
      "epoch": 1.1937419631375912,
      "grad_norm": 0.42968904972076416,
      "learning_rate": 1.840834404914988e-05,
      "loss": 0.6495,
      "step": 5570
    },
    {
      "epoch": 1.1958851264466352,
      "grad_norm": 0.35380542278289795,
      "learning_rate": 1.8405486498071155e-05,
      "loss": 0.1659,
      "step": 5580
    },
    {
      "epoch": 1.1980282897556793,
      "grad_norm": 0.31544339656829834,
      "learning_rate": 1.840262894699243e-05,
      "loss": 0.5087,
      "step": 5590
    },
    {
      "epoch": 1.2001714530647236,
      "grad_norm": 0.3845170736312866,
      "learning_rate": 1.8399771395913703e-05,
      "loss": 0.345,
      "step": 5600
    },
    {
      "epoch": 1.2023146163737677,
      "grad_norm": 29.920072555541992,
      "learning_rate": 1.8396913844834977e-05,
      "loss": 0.8029,
      "step": 5610
    },
    {
      "epoch": 1.2044577796828118,
      "grad_norm": 29.834157943725586,
      "learning_rate": 1.8394056293756254e-05,
      "loss": 0.9485,
      "step": 5620
    },
    {
      "epoch": 1.206600942991856,
      "grad_norm": 0.5017291903495789,
      "learning_rate": 1.8391198742677528e-05,
      "loss": 0.9289,
      "step": 5630
    },
    {
      "epoch": 1.2087441063009001,
      "grad_norm": 0.6291685104370117,
      "learning_rate": 1.8388341191598802e-05,
      "loss": 0.8855,
      "step": 5640
    },
    {
      "epoch": 1.2108872696099442,
      "grad_norm": 0.6538482904434204,
      "learning_rate": 1.8385483640520076e-05,
      "loss": 0.3017,
      "step": 5650
    },
    {
      "epoch": 1.2130304329189885,
      "grad_norm": 0.4544849097728729,
      "learning_rate": 1.838262608944135e-05,
      "loss": 0.307,
      "step": 5660
    },
    {
      "epoch": 1.2151735962280326,
      "grad_norm": 15.182847023010254,
      "learning_rate": 1.8379768538362627e-05,
      "loss": 0.33,
      "step": 5670
    },
    {
      "epoch": 1.2173167595370766,
      "grad_norm": 14.793243408203125,
      "learning_rate": 1.83769109872839e-05,
      "loss": 0.8142,
      "step": 5680
    },
    {
      "epoch": 1.219459922846121,
      "grad_norm": 14.680902481079102,
      "learning_rate": 1.8374053436205172e-05,
      "loss": 0.9542,
      "step": 5690
    },
    {
      "epoch": 1.221603086155165,
      "grad_norm": 0.5468333959579468,
      "learning_rate": 1.8371195885126446e-05,
      "loss": 0.7768,
      "step": 5700
    },
    {
      "epoch": 1.223746249464209,
      "grad_norm": 0.5878452062606812,
      "learning_rate": 1.8368338334047723e-05,
      "loss": 0.4557,
      "step": 5710
    },
    {
      "epoch": 1.2258894127732534,
      "grad_norm": 29.774667739868164,
      "learning_rate": 1.8365480782968997e-05,
      "loss": 0.6252,
      "step": 5720
    },
    {
      "epoch": 1.2280325760822974,
      "grad_norm": 14.61589241027832,
      "learning_rate": 1.836262323189027e-05,
      "loss": 1.0784,
      "step": 5730
    },
    {
      "epoch": 1.2301757393913415,
      "grad_norm": 29.725229263305664,
      "learning_rate": 1.8359765680811545e-05,
      "loss": 1.0288,
      "step": 5740
    },
    {
      "epoch": 1.2323189027003858,
      "grad_norm": 0.701901912689209,
      "learning_rate": 1.835690812973282e-05,
      "loss": 0.5666,
      "step": 5750
    },
    {
      "epoch": 1.2344620660094299,
      "grad_norm": 14.728249549865723,
      "learning_rate": 1.8354050578654096e-05,
      "loss": 0.7028,
      "step": 5760
    },
    {
      "epoch": 1.2366052293184742,
      "grad_norm": 15.009405136108398,
      "learning_rate": 1.835119302757537e-05,
      "loss": 0.8303,
      "step": 5770
    },
    {
      "epoch": 1.2387483926275182,
      "grad_norm": 0.8975093364715576,
      "learning_rate": 1.8348335476496644e-05,
      "loss": 0.4075,
      "step": 5780
    },
    {
      "epoch": 1.2408915559365623,
      "grad_norm": 29.67933464050293,
      "learning_rate": 1.8345477925417918e-05,
      "loss": 0.4343,
      "step": 5790
    },
    {
      "epoch": 1.2430347192456066,
      "grad_norm": 15.383172988891602,
      "learning_rate": 1.8342620374339192e-05,
      "loss": 0.3214,
      "step": 5800
    },
    {
      "epoch": 1.2451778825546507,
      "grad_norm": 0.31894415616989136,
      "learning_rate": 1.833976282326047e-05,
      "loss": 0.3362,
      "step": 5810
    },
    {
      "epoch": 1.2473210458636947,
      "grad_norm": 0.29488325119018555,
      "learning_rate": 1.8336905272181743e-05,
      "loss": 0.4996,
      "step": 5820
    },
    {
      "epoch": 1.249464209172739,
      "grad_norm": 0.4340985119342804,
      "learning_rate": 1.8334047721103017e-05,
      "loss": 0.6749,
      "step": 5830
    },
    {
      "epoch": 1.251607372481783,
      "grad_norm": 15.103941917419434,
      "learning_rate": 1.833119017002429e-05,
      "loss": 0.6566,
      "step": 5840
    },
    {
      "epoch": 1.2537505357908272,
      "grad_norm": 0.4117104411125183,
      "learning_rate": 1.8328332618945565e-05,
      "loss": 0.638,
      "step": 5850
    },
    {
      "epoch": 1.2558936990998715,
      "grad_norm": 31.251953125,
      "learning_rate": 1.832547506786684e-05,
      "loss": 0.9583,
      "step": 5860
    },
    {
      "epoch": 1.2580368624089155,
      "grad_norm": 0.5607070326805115,
      "learning_rate": 1.8322617516788116e-05,
      "loss": 0.468,
      "step": 5870
    },
    {
      "epoch": 1.2601800257179598,
      "grad_norm": 15.245343208312988,
      "learning_rate": 1.831975996570939e-05,
      "loss": 0.8875,
      "step": 5880
    },
    {
      "epoch": 1.262323189027004,
      "grad_norm": 0.6578218340873718,
      "learning_rate": 1.8316902414630664e-05,
      "loss": 0.5728,
      "step": 5890
    },
    {
      "epoch": 1.264466352336048,
      "grad_norm": 0.6717219352722168,
      "learning_rate": 1.8314044863551938e-05,
      "loss": 0.9006,
      "step": 5900
    },
    {
      "epoch": 1.2666095156450923,
      "grad_norm": 0.5333312749862671,
      "learning_rate": 1.8311187312473212e-05,
      "loss": 0.4626,
      "step": 5910
    },
    {
      "epoch": 1.2687526789541363,
      "grad_norm": 14.624957084655762,
      "learning_rate": 1.8308329761394486e-05,
      "loss": 1.0795,
      "step": 5920
    },
    {
      "epoch": 1.2708958422631804,
      "grad_norm": 0.6826003789901733,
      "learning_rate": 1.830547221031576e-05,
      "loss": 0.4507,
      "step": 5930
    },
    {
      "epoch": 1.2730390055722247,
      "grad_norm": 14.45234489440918,
      "learning_rate": 1.8302614659237034e-05,
      "loss": 0.9891,
      "step": 5940
    },
    {
      "epoch": 1.2751821688812688,
      "grad_norm": 14.130871772766113,
      "learning_rate": 1.829975710815831e-05,
      "loss": 0.7941,
      "step": 5950
    },
    {
      "epoch": 1.2773253321903129,
      "grad_norm": 1.0692481994628906,
      "learning_rate": 1.8296899557079585e-05,
      "loss": 0.9059,
      "step": 5960
    },
    {
      "epoch": 1.2794684954993572,
      "grad_norm": 14.148715019226074,
      "learning_rate": 1.829404200600086e-05,
      "loss": 0.8789,
      "step": 5970
    },
    {
      "epoch": 1.2816116588084012,
      "grad_norm": 1.6823358535766602,
      "learning_rate": 1.8291184454922133e-05,
      "loss": 0.6075,
      "step": 5980
    },
    {
      "epoch": 1.2837548221174453,
      "grad_norm": 1.7559617757797241,
      "learning_rate": 1.8288326903843407e-05,
      "loss": 0.7879,
      "step": 5990
    },
    {
      "epoch": 1.2858979854264896,
      "grad_norm": 1.2155776023864746,
      "learning_rate": 1.828546935276468e-05,
      "loss": 0.6012,
      "step": 6000
    },
    {
      "epoch": 1.2880411487355337,
      "grad_norm": 14.343589782714844,
      "learning_rate": 1.8282611801685958e-05,
      "loss": 0.4138,
      "step": 6010
    },
    {
      "epoch": 1.2901843120445777,
      "grad_norm": 0.5264890789985657,
      "learning_rate": 1.8279754250607232e-05,
      "loss": 0.8592,
      "step": 6020
    },
    {
      "epoch": 1.292327475353622,
      "grad_norm": 1.087141990661621,
      "learning_rate": 1.8276896699528506e-05,
      "loss": 1.2528,
      "step": 6030
    },
    {
      "epoch": 1.294470638662666,
      "grad_norm": 1.5680112838745117,
      "learning_rate": 1.827403914844978e-05,
      "loss": 0.7282,
      "step": 6040
    },
    {
      "epoch": 1.2966138019717102,
      "grad_norm": 0.9810419678688049,
      "learning_rate": 1.8271181597371054e-05,
      "loss": 0.4113,
      "step": 6050
    },
    {
      "epoch": 1.2987569652807545,
      "grad_norm": 15.610544204711914,
      "learning_rate": 1.826832404629233e-05,
      "loss": 0.5693,
      "step": 6060
    },
    {
      "epoch": 1.3009001285897985,
      "grad_norm": 0.5133172273635864,
      "learning_rate": 1.8265466495213605e-05,
      "loss": 0.302,
      "step": 6070
    },
    {
      "epoch": 1.3030432918988426,
      "grad_norm": 0.5297370553016663,
      "learning_rate": 1.826260894413488e-05,
      "loss": 0.7709,
      "step": 6080
    },
    {
      "epoch": 1.305186455207887,
      "grad_norm": 0.5354008078575134,
      "learning_rate": 1.8259751393056153e-05,
      "loss": 0.7601,
      "step": 6090
    },
    {
      "epoch": 1.307329618516931,
      "grad_norm": 15.273737907409668,
      "learning_rate": 1.8256893841977427e-05,
      "loss": 0.4693,
      "step": 6100
    },
    {
      "epoch": 1.309472781825975,
      "grad_norm": 15.168397903442383,
      "learning_rate": 1.8254036290898704e-05,
      "loss": 0.8018,
      "step": 6110
    },
    {
      "epoch": 1.3116159451350193,
      "grad_norm": 0.4470352530479431,
      "learning_rate": 1.8251178739819974e-05,
      "loss": 0.3157,
      "step": 6120
    },
    {
      "epoch": 1.3137591084440634,
      "grad_norm": 0.5094112157821655,
      "learning_rate": 1.824832118874125e-05,
      "loss": 0.467,
      "step": 6130
    },
    {
      "epoch": 1.3159022717531075,
      "grad_norm": 0.48739010095596313,
      "learning_rate": 1.8245463637662522e-05,
      "loss": 0.7984,
      "step": 6140
    },
    {
      "epoch": 1.3180454350621518,
      "grad_norm": 16.608783721923828,
      "learning_rate": 1.82426060865838e-05,
      "loss": 0.6305,
      "step": 6150
    },
    {
      "epoch": 1.3201885983711958,
      "grad_norm": 0.4222832918167114,
      "learning_rate": 1.8239748535505073e-05,
      "loss": 0.3237,
      "step": 6160
    },
    {
      "epoch": 1.32233176168024,
      "grad_norm": 18.177845001220703,
      "learning_rate": 1.8236890984426347e-05,
      "loss": 0.6747,
      "step": 6170
    },
    {
      "epoch": 1.3244749249892842,
      "grad_norm": 0.32695236802101135,
      "learning_rate": 1.823403343334762e-05,
      "loss": 0.3308,
      "step": 6180
    },
    {
      "epoch": 1.3266180882983283,
      "grad_norm": 0.35595932602882385,
      "learning_rate": 1.8231175882268895e-05,
      "loss": 1.1634,
      "step": 6190
    },
    {
      "epoch": 1.3287612516073724,
      "grad_norm": 0.45168232917785645,
      "learning_rate": 1.8228318331190173e-05,
      "loss": 0.4731,
      "step": 6200
    },
    {
      "epoch": 1.3309044149164166,
      "grad_norm": 15.396681785583496,
      "learning_rate": 1.8225460780111447e-05,
      "loss": 0.4716,
      "step": 6210
    },
    {
      "epoch": 1.3330475782254607,
      "grad_norm": 0.36923474073410034,
      "learning_rate": 1.822260322903272e-05,
      "loss": 0.327,
      "step": 6220
    },
    {
      "epoch": 1.335190741534505,
      "grad_norm": 16.182077407836914,
      "learning_rate": 1.8219745677953994e-05,
      "loss": 0.966,
      "step": 6230
    },
    {
      "epoch": 1.337333904843549,
      "grad_norm": 0.5535143613815308,
      "learning_rate": 1.8216888126875268e-05,
      "loss": 0.7774,
      "step": 6240
    },
    {
      "epoch": 1.3394770681525932,
      "grad_norm": 0.5588995814323425,
      "learning_rate": 1.8214030575796546e-05,
      "loss": 0.453,
      "step": 6250
    },
    {
      "epoch": 1.3416202314616374,
      "grad_norm": 29.738311767578125,
      "learning_rate": 1.821117302471782e-05,
      "loss": 1.1982,
      "step": 6260
    },
    {
      "epoch": 1.3437633947706815,
      "grad_norm": 14.944512367248535,
      "learning_rate": 1.8208315473639093e-05,
      "loss": 0.6008,
      "step": 6270
    },
    {
      "epoch": 1.3459065580797258,
      "grad_norm": 0.5244857668876648,
      "learning_rate": 1.8205457922560367e-05,
      "loss": 0.6142,
      "step": 6280
    },
    {
      "epoch": 1.3480497213887699,
      "grad_norm": 0.5114423036575317,
      "learning_rate": 1.820260037148164e-05,
      "loss": 0.7368,
      "step": 6290
    },
    {
      "epoch": 1.350192884697814,
      "grad_norm": 0.6044869422912598,
      "learning_rate": 1.8199742820402915e-05,
      "loss": 1.0214,
      "step": 6300
    },
    {
      "epoch": 1.3523360480068582,
      "grad_norm": 15.201444625854492,
      "learning_rate": 1.8196885269324193e-05,
      "loss": 0.7129,
      "step": 6310
    },
    {
      "epoch": 1.3544792113159023,
      "grad_norm": 0.8580101132392883,
      "learning_rate": 1.8194027718245467e-05,
      "loss": 0.5566,
      "step": 6320
    },
    {
      "epoch": 1.3566223746249464,
      "grad_norm": 29.371824264526367,
      "learning_rate": 1.819117016716674e-05,
      "loss": 0.7309,
      "step": 6330
    },
    {
      "epoch": 1.3587655379339907,
      "grad_norm": 14.852556228637695,
      "learning_rate": 1.8188312616088014e-05,
      "loss": 0.9068,
      "step": 6340
    },
    {
      "epoch": 1.3609087012430348,
      "grad_norm": 16.180952072143555,
      "learning_rate": 1.8185455065009288e-05,
      "loss": 1.0015,
      "step": 6350
    },
    {
      "epoch": 1.3630518645520788,
      "grad_norm": 0.7692986726760864,
      "learning_rate": 1.8182597513930562e-05,
      "loss": 0.5668,
      "step": 6360
    },
    {
      "epoch": 1.3651950278611231,
      "grad_norm": 0.6616653800010681,
      "learning_rate": 1.8179739962851836e-05,
      "loss": 0.4198,
      "step": 6370
    },
    {
      "epoch": 1.3673381911701672,
      "grad_norm": 31.010042190551758,
      "learning_rate": 1.817688241177311e-05,
      "loss": 0.7731,
      "step": 6380
    },
    {
      "epoch": 1.3694813544792113,
      "grad_norm": 0.4290301203727722,
      "learning_rate": 1.8174024860694387e-05,
      "loss": 0.6253,
      "step": 6390
    },
    {
      "epoch": 1.3716245177882556,
      "grad_norm": 15.149547576904297,
      "learning_rate": 1.817116730961566e-05,
      "loss": 1.0838,
      "step": 6400
    },
    {
      "epoch": 1.3737676810972996,
      "grad_norm": 0.4790591895580292,
      "learning_rate": 1.8168309758536935e-05,
      "loss": 0.3155,
      "step": 6410
    },
    {
      "epoch": 1.3759108444063437,
      "grad_norm": 0.4033699333667755,
      "learning_rate": 1.816545220745821e-05,
      "loss": 0.1652,
      "step": 6420
    },
    {
      "epoch": 1.378054007715388,
      "grad_norm": 30.04236602783203,
      "learning_rate": 1.8162594656379483e-05,
      "loss": 0.6555,
      "step": 6430
    },
    {
      "epoch": 1.380197171024432,
      "grad_norm": 0.35190290212631226,
      "learning_rate": 1.815973710530076e-05,
      "loss": 0.498,
      "step": 6440
    },
    {
      "epoch": 1.3823403343334761,
      "grad_norm": 15.10527515411377,
      "learning_rate": 1.8156879554222034e-05,
      "loss": 0.1748,
      "step": 6450
    },
    {
      "epoch": 1.3844834976425204,
      "grad_norm": 0.26000121235847473,
      "learning_rate": 1.8154022003143308e-05,
      "loss": 0.1752,
      "step": 6460
    },
    {
      "epoch": 1.3866266609515645,
      "grad_norm": 0.28061050176620483,
      "learning_rate": 1.8151164452064582e-05,
      "loss": 0.8652,
      "step": 6470
    },
    {
      "epoch": 1.3887698242606086,
      "grad_norm": 0.291018009185791,
      "learning_rate": 1.8148306900985856e-05,
      "loss": 0.3474,
      "step": 6480
    },
    {
      "epoch": 1.3909129875696529,
      "grad_norm": 0.29135146737098694,
      "learning_rate": 1.814544934990713e-05,
      "loss": 0.6788,
      "step": 6490
    },
    {
      "epoch": 1.393056150878697,
      "grad_norm": 15.00800609588623,
      "learning_rate": 1.8142591798828407e-05,
      "loss": 0.6719,
      "step": 6500
    },
    {
      "epoch": 1.395199314187741,
      "grad_norm": 0.3802129328250885,
      "learning_rate": 1.813973424774968e-05,
      "loss": 0.9869,
      "step": 6510
    },
    {
      "epoch": 1.3973424774967853,
      "grad_norm": 15.193603515625,
      "learning_rate": 1.8136876696670955e-05,
      "loss": 0.9438,
      "step": 6520
    },
    {
      "epoch": 1.3994856408058294,
      "grad_norm": 0.5598987936973572,
      "learning_rate": 1.813401914559223e-05,
      "loss": 0.7644,
      "step": 6530
    },
    {
      "epoch": 1.4016288041148735,
      "grad_norm": 0.5591641068458557,
      "learning_rate": 1.8131161594513503e-05,
      "loss": 0.5934,
      "step": 6540
    },
    {
      "epoch": 1.4037719674239177,
      "grad_norm": 14.894002914428711,
      "learning_rate": 1.8128304043434777e-05,
      "loss": 0.8854,
      "step": 6550
    },
    {
      "epoch": 1.4059151307329618,
      "grad_norm": 0.5009369850158691,
      "learning_rate": 1.812544649235605e-05,
      "loss": 0.3062,
      "step": 6560
    },
    {
      "epoch": 1.4080582940420059,
      "grad_norm": 0.5337035059928894,
      "learning_rate": 1.8122588941277325e-05,
      "loss": 1.0553,
      "step": 6570
    },
    {
      "epoch": 1.4102014573510502,
      "grad_norm": 0.5418952703475952,
      "learning_rate": 1.8119731390198602e-05,
      "loss": 0.3091,
      "step": 6580
    },
    {
      "epoch": 1.4123446206600943,
      "grad_norm": 15.059221267700195,
      "learning_rate": 1.8116873839119876e-05,
      "loss": 0.1681,
      "step": 6590
    },
    {
      "epoch": 1.4144877839691383,
      "grad_norm": 14.710268020629883,
      "learning_rate": 1.811401628804115e-05,
      "loss": 0.9754,
      "step": 6600
    },
    {
      "epoch": 1.4166309472781826,
      "grad_norm": 14.791357040405273,
      "learning_rate": 1.8111158736962424e-05,
      "loss": 0.4845,
      "step": 6610
    },
    {
      "epoch": 1.4187741105872267,
      "grad_norm": 0.3888455629348755,
      "learning_rate": 1.8108301185883698e-05,
      "loss": 0.6424,
      "step": 6620
    },
    {
      "epoch": 1.4209172738962708,
      "grad_norm": 0.3665525019168854,
      "learning_rate": 1.8105443634804972e-05,
      "loss": 0.3256,
      "step": 6630
    },
    {
      "epoch": 1.423060437205315,
      "grad_norm": 0.44326141476631165,
      "learning_rate": 1.810258608372625e-05,
      "loss": 0.8015,
      "step": 6640
    },
    {
      "epoch": 1.4252036005143591,
      "grad_norm": 0.5678768754005432,
      "learning_rate": 1.8099728532647523e-05,
      "loss": 0.7555,
      "step": 6650
    },
    {
      "epoch": 1.4273467638234034,
      "grad_norm": 0.498605340719223,
      "learning_rate": 1.8096870981568797e-05,
      "loss": 0.3123,
      "step": 6660
    },
    {
      "epoch": 1.4294899271324475,
      "grad_norm": 14.817530632019043,
      "learning_rate": 1.809401343049007e-05,
      "loss": 0.4818,
      "step": 6670
    },
    {
      "epoch": 1.4316330904414916,
      "grad_norm": 0.43311041593551636,
      "learning_rate": 1.8091155879411345e-05,
      "loss": 0.7901,
      "step": 6680
    },
    {
      "epoch": 1.4337762537505359,
      "grad_norm": 0.45224466919898987,
      "learning_rate": 1.8088298328332622e-05,
      "loss": 0.163,
      "step": 6690
    },
    {
      "epoch": 1.43591941705958,
      "grad_norm": 0.39806535840034485,
      "learning_rate": 1.8085440777253896e-05,
      "loss": 0.6419,
      "step": 6700
    },
    {
      "epoch": 1.4380625803686242,
      "grad_norm": 0.40215378999710083,
      "learning_rate": 1.808258322617517e-05,
      "loss": 0.4792,
      "step": 6710
    },
    {
      "epoch": 1.4402057436776683,
      "grad_norm": 0.38618600368499756,
      "learning_rate": 1.8079725675096444e-05,
      "loss": 0.3243,
      "step": 6720
    },
    {
      "epoch": 1.4423489069867124,
      "grad_norm": 15.49643611907959,
      "learning_rate": 1.8076868124017718e-05,
      "loss": 0.6566,
      "step": 6730
    },
    {
      "epoch": 1.4444920702957567,
      "grad_norm": 0.4191928207874298,
      "learning_rate": 1.8074010572938995e-05,
      "loss": 0.6486,
      "step": 6740
    },
    {
      "epoch": 1.4466352336048007,
      "grad_norm": 0.4378778040409088,
      "learning_rate": 1.807115302186027e-05,
      "loss": 0.4792,
      "step": 6750
    },
    {
      "epoch": 1.4487783969138448,
      "grad_norm": 0.4883895516395569,
      "learning_rate": 1.8068295470781543e-05,
      "loss": 0.9327,
      "step": 6760
    },
    {
      "epoch": 1.450921560222889,
      "grad_norm": 0.47039276361465454,
      "learning_rate": 1.8065437919702814e-05,
      "loss": 0.4657,
      "step": 6770
    },
    {
      "epoch": 1.4530647235319332,
      "grad_norm": 0.47970056533813477,
      "learning_rate": 1.806258036862409e-05,
      "loss": 0.4678,
      "step": 6780
    },
    {
      "epoch": 1.4552078868409772,
      "grad_norm": 0.4789600372314453,
      "learning_rate": 1.8059722817545365e-05,
      "loss": 0.6215,
      "step": 6790
    },
    {
      "epoch": 1.4573510501500215,
      "grad_norm": 0.434519499540329,
      "learning_rate": 1.805686526646664e-05,
      "loss": 0.1676,
      "step": 6800
    },
    {
      "epoch": 1.4594942134590656,
      "grad_norm": 0.44325119256973267,
      "learning_rate": 1.8054007715387913e-05,
      "loss": 0.9372,
      "step": 6810
    },
    {
      "epoch": 1.4616373767681097,
      "grad_norm": 15.027899742126465,
      "learning_rate": 1.8051150164309187e-05,
      "loss": 0.9231,
      "step": 6820
    },
    {
      "epoch": 1.463780540077154,
      "grad_norm": 14.775461196899414,
      "learning_rate": 1.8048292613230464e-05,
      "loss": 0.7527,
      "step": 6830
    },
    {
      "epoch": 1.465923703386198,
      "grad_norm": 0.5707874298095703,
      "learning_rate": 1.8045435062151738e-05,
      "loss": 0.2977,
      "step": 6840
    },
    {
      "epoch": 1.4680668666952421,
      "grad_norm": 0.49173834919929504,
      "learning_rate": 1.8042577511073012e-05,
      "loss": 0.1621,
      "step": 6850
    },
    {
      "epoch": 1.4702100300042864,
      "grad_norm": 0.4193126857280731,
      "learning_rate": 1.8039719959994286e-05,
      "loss": 1.2427,
      "step": 6860
    },
    {
      "epoch": 1.4723531933133305,
      "grad_norm": 14.955408096313477,
      "learning_rate": 1.803686240891556e-05,
      "loss": 0.7892,
      "step": 6870
    },
    {
      "epoch": 1.4744963566223745,
      "grad_norm": 0.4467936158180237,
      "learning_rate": 1.8034004857836837e-05,
      "loss": 0.6255,
      "step": 6880
    },
    {
      "epoch": 1.4766395199314188,
      "grad_norm": 0.5303512811660767,
      "learning_rate": 1.803114730675811e-05,
      "loss": 0.9339,
      "step": 6890
    },
    {
      "epoch": 1.478782683240463,
      "grad_norm": 16.085617065429688,
      "learning_rate": 1.8028289755679385e-05,
      "loss": 0.3246,
      "step": 6900
    },
    {
      "epoch": 1.480925846549507,
      "grad_norm": 0.3462681472301483,
      "learning_rate": 1.802543220460066e-05,
      "loss": 0.0093,
      "step": 6910
    },
    {
      "epoch": 1.4830690098585513,
      "grad_norm": 14.755672454833984,
      "learning_rate": 1.8022574653521933e-05,
      "loss": 1.1539,
      "step": 6920
    },
    {
      "epoch": 1.4852121731675954,
      "grad_norm": 0.3677961230278015,
      "learning_rate": 1.8019717102443207e-05,
      "loss": 0.4913,
      "step": 6930
    },
    {
      "epoch": 1.4873553364766394,
      "grad_norm": 0.42669665813446045,
      "learning_rate": 1.8016859551364484e-05,
      "loss": 0.4854,
      "step": 6940
    },
    {
      "epoch": 1.4894984997856837,
      "grad_norm": 0.39860525727272034,
      "learning_rate": 1.8014002000285758e-05,
      "loss": 0.481,
      "step": 6950
    },
    {
      "epoch": 1.4916416630947278,
      "grad_norm": 0.42379361391067505,
      "learning_rate": 1.8011144449207032e-05,
      "loss": 0.3244,
      "step": 6960
    },
    {
      "epoch": 1.4937848264037719,
      "grad_norm": 30.384727478027344,
      "learning_rate": 1.8008286898128306e-05,
      "loss": 0.9522,
      "step": 6970
    },
    {
      "epoch": 1.4959279897128162,
      "grad_norm": 14.613707542419434,
      "learning_rate": 1.800542934704958e-05,
      "loss": 0.9185,
      "step": 6980
    },
    {
      "epoch": 1.4980711530218602,
      "grad_norm": 0.5734644532203674,
      "learning_rate": 1.8002571795970854e-05,
      "loss": 0.31,
      "step": 6990
    },
    {
      "epoch": 1.5002143163309043,
      "grad_norm": 15.212806701660156,
      "learning_rate": 1.7999714244892128e-05,
      "loss": 0.5982,
      "step": 7000
    },
    {
      "epoch": 1.5023574796399486,
      "grad_norm": 0.6187382340431213,
      "learning_rate": 1.79968566938134e-05,
      "loss": 1.173,
      "step": 7010
    },
    {
      "epoch": 1.5045006429489927,
      "grad_norm": 14.913849830627441,
      "learning_rate": 1.799399914273468e-05,
      "loss": 0.4358,
      "step": 7020
    },
    {
      "epoch": 1.5066438062580367,
      "grad_norm": 29.640613555908203,
      "learning_rate": 1.7991141591655953e-05,
      "loss": 0.7202,
      "step": 7030
    },
    {
      "epoch": 1.508786969567081,
      "grad_norm": 0.6256559491157532,
      "learning_rate": 1.7988284040577227e-05,
      "loss": 0.2932,
      "step": 7040
    },
    {
      "epoch": 1.5109301328761253,
      "grad_norm": 30.82657814025879,
      "learning_rate": 1.79854264894985e-05,
      "loss": 0.9343,
      "step": 7050
    },
    {
      "epoch": 1.5130732961851692,
      "grad_norm": 0.5522308349609375,
      "learning_rate": 1.7982568938419774e-05,
      "loss": 0.4655,
      "step": 7060
    },
    {
      "epoch": 1.5152164594942135,
      "grad_norm": 0.492671936750412,
      "learning_rate": 1.797971138734105e-05,
      "loss": 0.4504,
      "step": 7070
    },
    {
      "epoch": 1.5173596228032578,
      "grad_norm": 14.738795280456543,
      "learning_rate": 1.7976853836262326e-05,
      "loss": 0.6169,
      "step": 7080
    },
    {
      "epoch": 1.5195027861123016,
      "grad_norm": 0.46000850200653076,
      "learning_rate": 1.79739962851836e-05,
      "loss": 0.628,
      "step": 7090
    },
    {
      "epoch": 1.521645949421346,
      "grad_norm": 0.4855647087097168,
      "learning_rate": 1.7971138734104874e-05,
      "loss": 1.0777,
      "step": 7100
    },
    {
      "epoch": 1.5237891127303902,
      "grad_norm": 0.5828860402107239,
      "learning_rate": 1.7968281183026147e-05,
      "loss": 0.7531,
      "step": 7110
    },
    {
      "epoch": 1.525932276039434,
      "grad_norm": 0.5471866726875305,
      "learning_rate": 1.796542363194742e-05,
      "loss": 0.4456,
      "step": 7120
    },
    {
      "epoch": 1.5280754393484783,
      "grad_norm": 16.538419723510742,
      "learning_rate": 1.79625660808687e-05,
      "loss": 0.6101,
      "step": 7130
    },
    {
      "epoch": 1.5302186026575226,
      "grad_norm": 0.5089159607887268,
      "learning_rate": 1.7959708529789973e-05,
      "loss": 0.6173,
      "step": 7140
    },
    {
      "epoch": 1.5323617659665667,
      "grad_norm": 0.6068832874298096,
      "learning_rate": 1.7956850978711247e-05,
      "loss": 0.7528,
      "step": 7150
    },
    {
      "epoch": 1.5345049292756108,
      "grad_norm": 0.5299413204193115,
      "learning_rate": 1.795399342763252e-05,
      "loss": 0.1562,
      "step": 7160
    },
    {
      "epoch": 1.536648092584655,
      "grad_norm": 14.536843299865723,
      "learning_rate": 1.7951135876553794e-05,
      "loss": 1.344,
      "step": 7170
    },
    {
      "epoch": 1.5387912558936991,
      "grad_norm": 0.726876974105835,
      "learning_rate": 1.7948278325475072e-05,
      "loss": 0.4425,
      "step": 7180
    },
    {
      "epoch": 1.5409344192027432,
      "grad_norm": 15.374279022216797,
      "learning_rate": 1.7945420774396346e-05,
      "loss": 0.5761,
      "step": 7190
    },
    {
      "epoch": 1.5430775825117875,
      "grad_norm": 0.4412461519241333,
      "learning_rate": 1.7942563223317616e-05,
      "loss": 0.1585,
      "step": 7200
    },
    {
      "epoch": 1.5452207458208316,
      "grad_norm": 14.951462745666504,
      "learning_rate": 1.793970567223889e-05,
      "loss": 0.4811,
      "step": 7210
    },
    {
      "epoch": 1.5473639091298756,
      "grad_norm": 0.40604981780052185,
      "learning_rate": 1.7936848121160167e-05,
      "loss": 0.8129,
      "step": 7220
    },
    {
      "epoch": 1.54950707243892,
      "grad_norm": 14.657167434692383,
      "learning_rate": 1.793399057008144e-05,
      "loss": 1.24,
      "step": 7230
    },
    {
      "epoch": 1.551650235747964,
      "grad_norm": 14.767937660217285,
      "learning_rate": 1.7931133019002715e-05,
      "loss": 0.4588,
      "step": 7240
    },
    {
      "epoch": 1.553793399057008,
      "grad_norm": 0.5254853367805481,
      "learning_rate": 1.792827546792399e-05,
      "loss": 0.8998,
      "step": 7250
    },
    {
      "epoch": 1.5559365623660524,
      "grad_norm": 0.5017891526222229,
      "learning_rate": 1.7925417916845263e-05,
      "loss": 0.4581,
      "step": 7260
    },
    {
      "epoch": 1.5580797256750964,
      "grad_norm": 0.5149826407432556,
      "learning_rate": 1.792256036576654e-05,
      "loss": 0.7678,
      "step": 7270
    },
    {
      "epoch": 1.5602228889841405,
      "grad_norm": 29.59926986694336,
      "learning_rate": 1.7919702814687814e-05,
      "loss": 1.0396,
      "step": 7280
    },
    {
      "epoch": 1.5623660522931848,
      "grad_norm": 0.5432237386703491,
      "learning_rate": 1.791684526360909e-05,
      "loss": 0.6073,
      "step": 7290
    },
    {
      "epoch": 1.5645092156022289,
      "grad_norm": 14.854763984680176,
      "learning_rate": 1.7913987712530362e-05,
      "loss": 0.7279,
      "step": 7300
    },
    {
      "epoch": 1.566652378911273,
      "grad_norm": 14.57450008392334,
      "learning_rate": 1.7911130161451636e-05,
      "loss": 0.6097,
      "step": 7310
    },
    {
      "epoch": 1.5687955422203173,
      "grad_norm": 15.009625434875488,
      "learning_rate": 1.7908272610372914e-05,
      "loss": 0.6091,
      "step": 7320
    },
    {
      "epoch": 1.5709387055293613,
      "grad_norm": 0.5994583964347839,
      "learning_rate": 1.7905415059294187e-05,
      "loss": 0.7346,
      "step": 7330
    },
    {
      "epoch": 1.5730818688384054,
      "grad_norm": 0.5369939208030701,
      "learning_rate": 1.790255750821546e-05,
      "loss": 0.6053,
      "step": 7340
    },
    {
      "epoch": 1.5752250321474497,
      "grad_norm": 30.69940185546875,
      "learning_rate": 1.7899699957136735e-05,
      "loss": 0.8905,
      "step": 7350
    },
    {
      "epoch": 1.5773681954564938,
      "grad_norm": 0.5646873712539673,
      "learning_rate": 1.789684240605801e-05,
      "loss": 0.3024,
      "step": 7360
    },
    {
      "epoch": 1.5795113587655378,
      "grad_norm": 0.3804742693901062,
      "learning_rate": 1.7893984854979287e-05,
      "loss": 0.8027,
      "step": 7370
    },
    {
      "epoch": 1.5816545220745821,
      "grad_norm": 0.4910554885864258,
      "learning_rate": 1.789112730390056e-05,
      "loss": 0.791,
      "step": 7380
    },
    {
      "epoch": 1.5837976853836262,
      "grad_norm": 0.40362289547920227,
      "learning_rate": 1.7888269752821834e-05,
      "loss": 0.318,
      "step": 7390
    },
    {
      "epoch": 1.5859408486926703,
      "grad_norm": 0.410444438457489,
      "learning_rate": 1.788541220174311e-05,
      "loss": 0.6439,
      "step": 7400
    },
    {
      "epoch": 1.5880840120017146,
      "grad_norm": 0.5184396505355835,
      "learning_rate": 1.7882554650664382e-05,
      "loss": 0.4641,
      "step": 7410
    },
    {
      "epoch": 1.5902271753107586,
      "grad_norm": 14.912771224975586,
      "learning_rate": 1.7879697099585656e-05,
      "loss": 0.6176,
      "step": 7420
    },
    {
      "epoch": 1.5923703386198027,
      "grad_norm": 0.5560659766197205,
      "learning_rate": 1.787683954850693e-05,
      "loss": 0.9053,
      "step": 7430
    },
    {
      "epoch": 1.594513501928847,
      "grad_norm": 0.5614315867424011,
      "learning_rate": 1.7873981997428204e-05,
      "loss": 0.4498,
      "step": 7440
    },
    {
      "epoch": 1.5966566652378913,
      "grad_norm": 14.314477920532227,
      "learning_rate": 1.7871124446349478e-05,
      "loss": 1.4098,
      "step": 7450
    },
    {
      "epoch": 1.5987998285469351,
      "grad_norm": 0.9341049194335938,
      "learning_rate": 1.7868266895270755e-05,
      "loss": 0.4123,
      "step": 7460
    },
    {
      "epoch": 1.6009429918559794,
      "grad_norm": 0.6260021924972534,
      "learning_rate": 1.786540934419203e-05,
      "loss": 0.4204,
      "step": 7470
    },
    {
      "epoch": 1.6030861551650237,
      "grad_norm": 0.569232165813446,
      "learning_rate": 1.7862551793113303e-05,
      "loss": 0.7225,
      "step": 7480
    },
    {
      "epoch": 1.6052293184740676,
      "grad_norm": 0.843884289264679,
      "learning_rate": 1.7859694242034577e-05,
      "loss": 0.9745,
      "step": 7490
    },
    {
      "epoch": 1.6073724817831119,
      "grad_norm": 14.023890495300293,
      "learning_rate": 1.785683669095585e-05,
      "loss": 1.0117,
      "step": 7500
    },
    {
      "epoch": 1.6095156450921562,
      "grad_norm": 1.260824203491211,
      "learning_rate": 1.785397913987713e-05,
      "loss": 0.5098,
      "step": 7510
    },
    {
      "epoch": 1.6116588084012,
      "grad_norm": 15.379159927368164,
      "learning_rate": 1.7851121588798402e-05,
      "loss": 0.7494,
      "step": 7520
    },
    {
      "epoch": 1.6138019717102443,
      "grad_norm": 14.013586044311523,
      "learning_rate": 1.7848264037719676e-05,
      "loss": 0.6224,
      "step": 7530
    },
    {
      "epoch": 1.6159451350192886,
      "grad_norm": 1.2789499759674072,
      "learning_rate": 1.784540648664095e-05,
      "loss": 0.6219,
      "step": 7540
    },
    {
      "epoch": 1.6180882983283325,
      "grad_norm": 14.84382438659668,
      "learning_rate": 1.7842548935562224e-05,
      "loss": 0.5175,
      "step": 7550
    },
    {
      "epoch": 1.6202314616373767,
      "grad_norm": 15.701437950134277,
      "learning_rate": 1.7839691384483498e-05,
      "loss": 0.6808,
      "step": 7560
    },
    {
      "epoch": 1.622374624946421,
      "grad_norm": 14.332791328430176,
      "learning_rate": 1.7836833833404775e-05,
      "loss": 0.8031,
      "step": 7570
    },
    {
      "epoch": 1.6245177882554651,
      "grad_norm": 0.9528536796569824,
      "learning_rate": 1.783397628232605e-05,
      "loss": 0.9312,
      "step": 7580
    },
    {
      "epoch": 1.6266609515645092,
      "grad_norm": 0.6373676657676697,
      "learning_rate": 1.7831118731247323e-05,
      "loss": 0.573,
      "step": 7590
    },
    {
      "epoch": 1.6288041148735535,
      "grad_norm": 0.6460446715354919,
      "learning_rate": 1.7828261180168597e-05,
      "loss": 0.4394,
      "step": 7600
    },
    {
      "epoch": 1.6309472781825975,
      "grad_norm": 14.676901817321777,
      "learning_rate": 1.782540362908987e-05,
      "loss": 1.0137,
      "step": 7610
    },
    {
      "epoch": 1.6330904414916416,
      "grad_norm": 0.5813310742378235,
      "learning_rate": 1.782254607801115e-05,
      "loss": 0.7254,
      "step": 7620
    },
    {
      "epoch": 1.635233604800686,
      "grad_norm": 0.6545497179031372,
      "learning_rate": 1.781968852693242e-05,
      "loss": 0.5938,
      "step": 7630
    },
    {
      "epoch": 1.63737676810973,
      "grad_norm": 15.65234661102295,
      "learning_rate": 1.7816830975853693e-05,
      "loss": 0.8712,
      "step": 7640
    },
    {
      "epoch": 1.639519931418774,
      "grad_norm": 15.024123191833496,
      "learning_rate": 1.781397342477497e-05,
      "loss": 0.5696,
      "step": 7650
    },
    {
      "epoch": 1.6416630947278184,
      "grad_norm": 29.879770278930664,
      "learning_rate": 1.7811115873696244e-05,
      "loss": 0.7437,
      "step": 7660
    },
    {
      "epoch": 1.6438062580368624,
      "grad_norm": 0.514301061630249,
      "learning_rate": 1.7808258322617518e-05,
      "loss": 0.3076,
      "step": 7670
    },
    {
      "epoch": 1.6459494213459065,
      "grad_norm": 14.604448318481445,
      "learning_rate": 1.7805400771538792e-05,
      "loss": 0.7699,
      "step": 7680
    },
    {
      "epoch": 1.6480925846549508,
      "grad_norm": 0.4884628653526306,
      "learning_rate": 1.7802543220460066e-05,
      "loss": 0.4661,
      "step": 7690
    },
    {
      "epoch": 1.6502357479639949,
      "grad_norm": 0.47440120577812195,
      "learning_rate": 1.779968566938134e-05,
      "loss": 0.4666,
      "step": 7700
    },
    {
      "epoch": 1.652378911273039,
      "grad_norm": 14.69005298614502,
      "learning_rate": 1.7796828118302617e-05,
      "loss": 1.0569,
      "step": 7710
    },
    {
      "epoch": 1.6545220745820832,
      "grad_norm": 0.8148340582847595,
      "learning_rate": 1.779397056722389e-05,
      "loss": 1.0152,
      "step": 7720
    },
    {
      "epoch": 1.6566652378911273,
      "grad_norm": 0.9601537585258484,
      "learning_rate": 1.7791113016145165e-05,
      "loss": 0.8189,
      "step": 7730
    },
    {
      "epoch": 1.6588084012001714,
      "grad_norm": 0.9780295491218567,
      "learning_rate": 1.778825546506644e-05,
      "loss": 0.8046,
      "step": 7740
    },
    {
      "epoch": 1.6609515645092157,
      "grad_norm": 0.7282285094261169,
      "learning_rate": 1.7785397913987713e-05,
      "loss": 0.5536,
      "step": 7750
    },
    {
      "epoch": 1.6630947278182597,
      "grad_norm": 0.5796416401863098,
      "learning_rate": 1.778254036290899e-05,
      "loss": 0.444,
      "step": 7760
    },
    {
      "epoch": 1.6652378911273038,
      "grad_norm": 0.42552992701530457,
      "learning_rate": 1.7779682811830264e-05,
      "loss": 0.4603,
      "step": 7770
    },
    {
      "epoch": 1.667381054436348,
      "grad_norm": 0.3198639154434204,
      "learning_rate": 1.7776825260751538e-05,
      "loss": 0.4926,
      "step": 7780
    },
    {
      "epoch": 1.6695242177453922,
      "grad_norm": 15.380387306213379,
      "learning_rate": 1.7773967709672812e-05,
      "loss": 1.005,
      "step": 7790
    },
    {
      "epoch": 1.6716673810544362,
      "grad_norm": 14.705629348754883,
      "learning_rate": 1.7771110158594086e-05,
      "loss": 0.6518,
      "step": 7800
    },
    {
      "epoch": 1.6738105443634805,
      "grad_norm": 14.796027183532715,
      "learning_rate": 1.7768252607515363e-05,
      "loss": 0.4762,
      "step": 7810
    },
    {
      "epoch": 1.6759537076725246,
      "grad_norm": 0.5820617079734802,
      "learning_rate": 1.7765395056436637e-05,
      "loss": 0.772,
      "step": 7820
    },
    {
      "epoch": 1.6780968709815687,
      "grad_norm": 0.47248315811157227,
      "learning_rate": 1.776253750535791e-05,
      "loss": 0.3118,
      "step": 7830
    },
    {
      "epoch": 1.680240034290613,
      "grad_norm": 0.41091403365135193,
      "learning_rate": 1.775967995427918e-05,
      "loss": 0.326,
      "step": 7840
    },
    {
      "epoch": 1.682383197599657,
      "grad_norm": 0.3713844120502472,
      "learning_rate": 1.775682240320046e-05,
      "loss": 0.4895,
      "step": 7850
    },
    {
      "epoch": 1.6845263609087011,
      "grad_norm": 14.782734870910645,
      "learning_rate": 1.7753964852121733e-05,
      "loss": 0.4894,
      "step": 7860
    },
    {
      "epoch": 1.6866695242177454,
      "grad_norm": 0.3534139394760132,
      "learning_rate": 1.7751107301043007e-05,
      "loss": 0.008,
      "step": 7870
    },
    {
      "epoch": 1.6888126875267897,
      "grad_norm": 0.2948595881462097,
      "learning_rate": 1.774824974996428e-05,
      "loss": 0.5054,
      "step": 7880
    },
    {
      "epoch": 1.6909558508358336,
      "grad_norm": 0.27708742022514343,
      "learning_rate": 1.7745392198885555e-05,
      "loss": 0.1759,
      "step": 7890
    },
    {
      "epoch": 1.6930990141448778,
      "grad_norm": 0.2397858202457428,
      "learning_rate": 1.7742534647806832e-05,
      "loss": 0.5274,
      "step": 7900
    },
    {
      "epoch": 1.6952421774539221,
      "grad_norm": 0.2778799831867218,
      "learning_rate": 1.7739677096728106e-05,
      "loss": 0.6931,
      "step": 7910
    },
    {
      "epoch": 1.697385340762966,
      "grad_norm": 14.85171890258789,
      "learning_rate": 1.773681954564938e-05,
      "loss": 0.5128,
      "step": 7920
    },
    {
      "epoch": 1.6995285040720103,
      "grad_norm": 0.2927965521812439,
      "learning_rate": 1.7733961994570654e-05,
      "loss": 0.1831,
      "step": 7930
    },
    {
      "epoch": 1.7016716673810546,
      "grad_norm": 14.868435859680176,
      "learning_rate": 1.7731104443491928e-05,
      "loss": 0.6977,
      "step": 7940
    },
    {
      "epoch": 1.7038148306900984,
      "grad_norm": 15.128120422363281,
      "learning_rate": 1.7728246892413205e-05,
      "loss": 0.8475,
      "step": 7950
    },
    {
      "epoch": 1.7059579939991427,
      "grad_norm": 0.37751221656799316,
      "learning_rate": 1.772538934133448e-05,
      "loss": 0.6691,
      "step": 7960
    },
    {
      "epoch": 1.708101157308187,
      "grad_norm": 14.855533599853516,
      "learning_rate": 1.7722531790255753e-05,
      "loss": 0.3274,
      "step": 7970
    },
    {
      "epoch": 1.710244320617231,
      "grad_norm": 0.3359465003013611,
      "learning_rate": 1.7719674239177027e-05,
      "loss": 0.1703,
      "step": 7980
    },
    {
      "epoch": 1.7123874839262752,
      "grad_norm": 0.2844330966472626,
      "learning_rate": 1.77168166880983e-05,
      "loss": 0.346,
      "step": 7990
    },
    {
      "epoch": 1.7145306472353194,
      "grad_norm": 0.34086713194847107,
      "learning_rate": 1.7713959137019578e-05,
      "loss": 0.8427,
      "step": 8000
    },
    {
      "epoch": 1.7166738105443635,
      "grad_norm": 0.32197362184524536,
      "learning_rate": 1.7711101585940852e-05,
      "loss": 0.6592,
      "step": 8010
    },
    {
      "epoch": 1.7188169738534076,
      "grad_norm": 0.4688042998313904,
      "learning_rate": 1.7708244034862126e-05,
      "loss": 0.9535,
      "step": 8020
    },
    {
      "epoch": 1.7209601371624519,
      "grad_norm": 0.5366516709327698,
      "learning_rate": 1.77053864837834e-05,
      "loss": 0.4566,
      "step": 8030
    },
    {
      "epoch": 1.723103300471496,
      "grad_norm": 0.4030841886997223,
      "learning_rate": 1.7702528932704674e-05,
      "loss": 0.3262,
      "step": 8040
    },
    {
      "epoch": 1.72524646378054,
      "grad_norm": 0.42837557196617126,
      "learning_rate": 1.7699671381625948e-05,
      "loss": 0.7907,
      "step": 8050
    },
    {
      "epoch": 1.7273896270895843,
      "grad_norm": 0.388327032327652,
      "learning_rate": 1.769681383054722e-05,
      "loss": 0.3277,
      "step": 8060
    },
    {
      "epoch": 1.7295327903986284,
      "grad_norm": 15.307056427001953,
      "learning_rate": 1.7693956279468495e-05,
      "loss": 0.4977,
      "step": 8070
    },
    {
      "epoch": 1.7316759537076725,
      "grad_norm": 0.45592162013053894,
      "learning_rate": 1.769109872838977e-05,
      "loss": 1.2821,
      "step": 8080
    },
    {
      "epoch": 1.7338191170167168,
      "grad_norm": 0.49368125200271606,
      "learning_rate": 1.7688241177311047e-05,
      "loss": 0.1599,
      "step": 8090
    },
    {
      "epoch": 1.7359622803257608,
      "grad_norm": 0.37997767329216003,
      "learning_rate": 1.768538362623232e-05,
      "loss": 0.3206,
      "step": 8100
    },
    {
      "epoch": 1.738105443634805,
      "grad_norm": 16.074289321899414,
      "learning_rate": 1.7682526075153595e-05,
      "loss": 1.0768,
      "step": 8110
    },
    {
      "epoch": 1.7402486069438492,
      "grad_norm": 15.686217308044434,
      "learning_rate": 1.767966852407487e-05,
      "loss": 0.7159,
      "step": 8120
    },
    {
      "epoch": 1.7423917702528933,
      "grad_norm": 14.939306259155273,
      "learning_rate": 1.7676810972996142e-05,
      "loss": 0.8255,
      "step": 8130
    },
    {
      "epoch": 1.7445349335619373,
      "grad_norm": 1.0962464809417725,
      "learning_rate": 1.767395342191742e-05,
      "loss": 0.6624,
      "step": 8140
    },
    {
      "epoch": 1.7466780968709816,
      "grad_norm": 29.020233154296875,
      "learning_rate": 1.7671095870838694e-05,
      "loss": 1.1235,
      "step": 8150
    },
    {
      "epoch": 1.7488212601800257,
      "grad_norm": 13.939726829528809,
      "learning_rate": 1.7668238319759968e-05,
      "loss": 0.267,
      "step": 8160
    },
    {
      "epoch": 1.7509644234890698,
      "grad_norm": 0.8736385107040405,
      "learning_rate": 1.766538076868124e-05,
      "loss": 0.27,
      "step": 8170
    },
    {
      "epoch": 1.753107586798114,
      "grad_norm": 0.45686739683151245,
      "learning_rate": 1.7662523217602515e-05,
      "loss": 0.013,
      "step": 8180
    },
    {
      "epoch": 1.7552507501071581,
      "grad_norm": 0.43432098627090454,
      "learning_rate": 1.765966566652379e-05,
      "loss": 0.7941,
      "step": 8190
    },
    {
      "epoch": 1.7573939134162022,
      "grad_norm": 0.40398600697517395,
      "learning_rate": 1.7656808115445067e-05,
      "loss": 0.9366,
      "step": 8200
    },
    {
      "epoch": 1.7595370767252465,
      "grad_norm": 0.5104051232337952,
      "learning_rate": 1.765395056436634e-05,
      "loss": 1.2403,
      "step": 8210
    },
    {
      "epoch": 1.7616802400342906,
      "grad_norm": 14.488191604614258,
      "learning_rate": 1.7651093013287615e-05,
      "loss": 1.173,
      "step": 8220
    },
    {
      "epoch": 1.7638234033433347,
      "grad_norm": 0.834935188293457,
      "learning_rate": 1.764823546220889e-05,
      "loss": 0.4247,
      "step": 8230
    },
    {
      "epoch": 1.765966566652379,
      "grad_norm": 14.543158531188965,
      "learning_rate": 1.7645377911130162e-05,
      "loss": 0.5806,
      "step": 8240
    },
    {
      "epoch": 1.768109729961423,
      "grad_norm": 0.568925142288208,
      "learning_rate": 1.764252036005144e-05,
      "loss": 0.1593,
      "step": 8250
    },
    {
      "epoch": 1.770252893270467,
      "grad_norm": 44.80265808105469,
      "learning_rate": 1.7639662808972714e-05,
      "loss": 0.7733,
      "step": 8260
    },
    {
      "epoch": 1.7723960565795114,
      "grad_norm": 16.51927947998047,
      "learning_rate": 1.7636805257893984e-05,
      "loss": 0.9325,
      "step": 8270
    },
    {
      "epoch": 1.7745392198885555,
      "grad_norm": 0.5851259827613831,
      "learning_rate": 1.763394770681526e-05,
      "loss": 0.465,
      "step": 8280
    },
    {
      "epoch": 1.7766823831975995,
      "grad_norm": 0.7262325286865234,
      "learning_rate": 1.7631090155736535e-05,
      "loss": 0.5839,
      "step": 8290
    },
    {
      "epoch": 1.7788255465066438,
      "grad_norm": 0.5234789848327637,
      "learning_rate": 1.762823260465781e-05,
      "loss": 0.7376,
      "step": 8300
    },
    {
      "epoch": 1.7809687098156881,
      "grad_norm": 14.726615905761719,
      "learning_rate": 1.7625375053579083e-05,
      "loss": 0.921,
      "step": 8310
    },
    {
      "epoch": 1.783111873124732,
      "grad_norm": 0.4131854176521301,
      "learning_rate": 1.7622517502500357e-05,
      "loss": 0.0102,
      "step": 8320
    },
    {
      "epoch": 1.7852550364337763,
      "grad_norm": 14.859960556030273,
      "learning_rate": 1.761965995142163e-05,
      "loss": 0.8041,
      "step": 8330
    },
    {
      "epoch": 1.7873981997428205,
      "grad_norm": 14.767277717590332,
      "learning_rate": 1.761680240034291e-05,
      "loss": 0.9552,
      "step": 8340
    },
    {
      "epoch": 1.7895413630518644,
      "grad_norm": 15.931487083435059,
      "learning_rate": 1.7613944849264182e-05,
      "loss": 0.786,
      "step": 8350
    },
    {
      "epoch": 1.7916845263609087,
      "grad_norm": 29.605497360229492,
      "learning_rate": 1.7611087298185456e-05,
      "loss": 1.3082,
      "step": 8360
    },
    {
      "epoch": 1.793827689669953,
      "grad_norm": 0.7652633786201477,
      "learning_rate": 1.760822974710673e-05,
      "loss": 0.4295,
      "step": 8370
    },
    {
      "epoch": 1.7959708529789968,
      "grad_norm": 0.75636225938797,
      "learning_rate": 1.7605372196028004e-05,
      "loss": 0.7036,
      "step": 8380
    },
    {
      "epoch": 1.7981140162880411,
      "grad_norm": 0.6989886164665222,
      "learning_rate": 1.760251464494928e-05,
      "loss": 0.5702,
      "step": 8390
    },
    {
      "epoch": 1.8002571795970854,
      "grad_norm": 0.7002924084663391,
      "learning_rate": 1.7599657093870555e-05,
      "loss": 0.8689,
      "step": 8400
    },
    {
      "epoch": 1.8024003429061295,
      "grad_norm": 29.8996524810791,
      "learning_rate": 1.759679954279183e-05,
      "loss": 0.7021,
      "step": 8410
    },
    {
      "epoch": 1.8045435062151736,
      "grad_norm": 0.8107574582099915,
      "learning_rate": 1.7593941991713103e-05,
      "loss": 0.9456,
      "step": 8420
    },
    {
      "epoch": 1.8066866695242179,
      "grad_norm": 15.279382705688477,
      "learning_rate": 1.7591084440634377e-05,
      "loss": 0.7108,
      "step": 8430
    },
    {
      "epoch": 1.808829832833262,
      "grad_norm": 0.6668184995651245,
      "learning_rate": 1.7588226889555654e-05,
      "loss": 0.2993,
      "step": 8440
    },
    {
      "epoch": 1.810972996142306,
      "grad_norm": 15.182413101196289,
      "learning_rate": 1.758536933847693e-05,
      "loss": 0.7414,
      "step": 8450
    },
    {
      "epoch": 1.8131161594513503,
      "grad_norm": 0.6131008863449097,
      "learning_rate": 1.7582511787398202e-05,
      "loss": 0.5916,
      "step": 8460
    },
    {
      "epoch": 1.8152593227603944,
      "grad_norm": 15.08209228515625,
      "learning_rate": 1.7579654236319476e-05,
      "loss": 0.4541,
      "step": 8470
    },
    {
      "epoch": 1.8174024860694384,
      "grad_norm": 0.4270895719528198,
      "learning_rate": 1.757679668524075e-05,
      "loss": 0.6352,
      "step": 8480
    },
    {
      "epoch": 1.8195456493784827,
      "grad_norm": 0.5051339864730835,
      "learning_rate": 1.7573939134162024e-05,
      "loss": 0.9314,
      "step": 8490
    },
    {
      "epoch": 1.8216888126875268,
      "grad_norm": 0.4745473563671112,
      "learning_rate": 1.7571081583083298e-05,
      "loss": 0.3251,
      "step": 8500
    },
    {
      "epoch": 1.8238319759965709,
      "grad_norm": 0.5704062581062317,
      "learning_rate": 1.7568224032004572e-05,
      "loss": 0.9109,
      "step": 8510
    },
    {
      "epoch": 1.8259751393056152,
      "grad_norm": 0.4999794065952301,
      "learning_rate": 1.7565366480925846e-05,
      "loss": 0.3134,
      "step": 8520
    },
    {
      "epoch": 1.8281183026146592,
      "grad_norm": 16.94927215576172,
      "learning_rate": 1.7562508929847123e-05,
      "loss": 0.4695,
      "step": 8530
    },
    {
      "epoch": 1.8302614659237033,
      "grad_norm": 0.39578068256378174,
      "learning_rate": 1.7559651378768397e-05,
      "loss": 0.4698,
      "step": 8540
    },
    {
      "epoch": 1.8324046292327476,
      "grad_norm": 0.3734738230705261,
      "learning_rate": 1.755679382768967e-05,
      "loss": 0.8023,
      "step": 8550
    },
    {
      "epoch": 1.8345477925417917,
      "grad_norm": 14.899750709533691,
      "learning_rate": 1.7553936276610945e-05,
      "loss": 0.4914,
      "step": 8560
    },
    {
      "epoch": 1.8366909558508357,
      "grad_norm": 0.3561951816082001,
      "learning_rate": 1.755107872553222e-05,
      "loss": 0.6526,
      "step": 8570
    },
    {
      "epoch": 1.83883411915988,
      "grad_norm": 14.752140045166016,
      "learning_rate": 1.7548221174453496e-05,
      "loss": 0.6453,
      "step": 8580
    },
    {
      "epoch": 1.8409772824689241,
      "grad_norm": 0.4045369029045105,
      "learning_rate": 1.754536362337477e-05,
      "loss": 0.3222,
      "step": 8590
    },
    {
      "epoch": 1.8431204457779682,
      "grad_norm": 0.402188241481781,
      "learning_rate": 1.7542506072296044e-05,
      "loss": 0.4765,
      "step": 8600
    },
    {
      "epoch": 1.8452636090870125,
      "grad_norm": 14.977526664733887,
      "learning_rate": 1.7539648521217318e-05,
      "loss": 0.7968,
      "step": 8610
    },
    {
      "epoch": 1.8474067723960566,
      "grad_norm": 14.67702865600586,
      "learning_rate": 1.7536790970138592e-05,
      "loss": 0.7898,
      "step": 8620
    },
    {
      "epoch": 1.8495499357051006,
      "grad_norm": 14.618295669555664,
      "learning_rate": 1.7533933419059866e-05,
      "loss": 0.4678,
      "step": 8630
    },
    {
      "epoch": 1.851693099014145,
      "grad_norm": 0.5306098461151123,
      "learning_rate": 1.7531075867981143e-05,
      "loss": 0.6108,
      "step": 8640
    },
    {
      "epoch": 1.853836262323189,
      "grad_norm": 15.135780334472656,
      "learning_rate": 1.7528218316902417e-05,
      "loss": 1.0622,
      "step": 8650
    },
    {
      "epoch": 1.855979425632233,
      "grad_norm": 0.5741027593612671,
      "learning_rate": 1.752536076582369e-05,
      "loss": 0.4553,
      "step": 8660
    },
    {
      "epoch": 1.8581225889412774,
      "grad_norm": 0.5305792689323425,
      "learning_rate": 1.7522503214744965e-05,
      "loss": 0.7506,
      "step": 8670
    },
    {
      "epoch": 1.8602657522503214,
      "grad_norm": 14.655712127685547,
      "learning_rate": 1.751964566366624e-05,
      "loss": 0.3146,
      "step": 8680
    },
    {
      "epoch": 1.8624089155593655,
      "grad_norm": 14.66246223449707,
      "learning_rate": 1.7516788112587516e-05,
      "loss": 0.3206,
      "step": 8690
    },
    {
      "epoch": 1.8645520788684098,
      "grad_norm": 16.998947143554688,
      "learning_rate": 1.7513930561508787e-05,
      "loss": 0.9523,
      "step": 8700
    },
    {
      "epoch": 1.866695242177454,
      "grad_norm": 0.3863423466682434,
      "learning_rate": 1.751107301043006e-05,
      "loss": 0.009,
      "step": 8710
    },
    {
      "epoch": 1.868838405486498,
      "grad_norm": 0.32075393199920654,
      "learning_rate": 1.7508215459351338e-05,
      "loss": 0.3348,
      "step": 8720
    },
    {
      "epoch": 1.8709815687955422,
      "grad_norm": 0.3357234299182892,
      "learning_rate": 1.7505357908272612e-05,
      "loss": 0.8268,
      "step": 8730
    },
    {
      "epoch": 1.8731247321045865,
      "grad_norm": 0.3379156291484833,
      "learning_rate": 1.7502500357193886e-05,
      "loss": 0.6606,
      "step": 8740
    },
    {
      "epoch": 1.8752678954136304,
      "grad_norm": 0.3582061231136322,
      "learning_rate": 1.749964280611516e-05,
      "loss": 1.138,
      "step": 8750
    },
    {
      "epoch": 1.8774110587226747,
      "grad_norm": 0.42450374364852905,
      "learning_rate": 1.7496785255036434e-05,
      "loss": 0.3183,
      "step": 8760
    },
    {
      "epoch": 1.879554222031719,
      "grad_norm": 0.4100213348865509,
      "learning_rate": 1.7493927703957708e-05,
      "loss": 0.3204,
      "step": 8770
    },
    {
      "epoch": 1.8816973853407628,
      "grad_norm": 14.73009204864502,
      "learning_rate": 1.7491070152878985e-05,
      "loss": 1.252,
      "step": 8780
    },
    {
      "epoch": 1.883840548649807,
      "grad_norm": 0.44907405972480774,
      "learning_rate": 1.748821260180026e-05,
      "loss": 0.01,
      "step": 8790
    },
    {
      "epoch": 1.8859837119588514,
      "grad_norm": 14.6255521774292,
      "learning_rate": 1.7485355050721533e-05,
      "loss": 1.0857,
      "step": 8800
    },
    {
      "epoch": 1.8881268752678955,
      "grad_norm": 14.86916732788086,
      "learning_rate": 1.7482497499642807e-05,
      "loss": 0.6154,
      "step": 8810
    },
    {
      "epoch": 1.8902700385769395,
      "grad_norm": 0.5333459973335266,
      "learning_rate": 1.747963994856408e-05,
      "loss": 0.5981,
      "step": 8820
    },
    {
      "epoch": 1.8924132018859838,
      "grad_norm": 0.5735788345336914,
      "learning_rate": 1.7476782397485358e-05,
      "loss": 0.8821,
      "step": 8830
    },
    {
      "epoch": 1.894556365195028,
      "grad_norm": 14.416873931884766,
      "learning_rate": 1.7473924846406632e-05,
      "loss": 0.7299,
      "step": 8840
    },
    {
      "epoch": 1.896699528504072,
      "grad_norm": 0.6574510931968689,
      "learning_rate": 1.7471067295327906e-05,
      "loss": 0.5788,
      "step": 8850
    },
    {
      "epoch": 1.8988426918131163,
      "grad_norm": 0.5631979703903198,
      "learning_rate": 1.746820974424918e-05,
      "loss": 0.1586,
      "step": 8860
    },
    {
      "epoch": 1.9009858551221603,
      "grad_norm": 0.48699602484703064,
      "learning_rate": 1.7465352193170454e-05,
      "loss": 0.6147,
      "step": 8870
    },
    {
      "epoch": 1.9031290184312044,
      "grad_norm": 0.467776358127594,
      "learning_rate": 1.746249464209173e-05,
      "loss": 0.4619,
      "step": 8880
    },
    {
      "epoch": 1.9052721817402487,
      "grad_norm": 14.575786590576172,
      "learning_rate": 1.7459637091013005e-05,
      "loss": 1.5119,
      "step": 8890
    },
    {
      "epoch": 1.9074153450492928,
      "grad_norm": 0.6178780198097229,
      "learning_rate": 1.745677953993428e-05,
      "loss": 0.8845,
      "step": 8900
    },
    {
      "epoch": 1.9095585083583368,
      "grad_norm": 0.6736827492713928,
      "learning_rate": 1.745392198885555e-05,
      "loss": 0.4375,
      "step": 8910
    },
    {
      "epoch": 1.9117016716673811,
      "grad_norm": 0.5976489782333374,
      "learning_rate": 1.7451064437776827e-05,
      "loss": 0.5779,
      "step": 8920
    },
    {
      "epoch": 1.9138448349764252,
      "grad_norm": 0.5112444162368774,
      "learning_rate": 1.74482068866981e-05,
      "loss": 0.4586,
      "step": 8930
    },
    {
      "epoch": 1.9159879982854693,
      "grad_norm": 0.42350566387176514,
      "learning_rate": 1.7445349335619375e-05,
      "loss": 0.6244,
      "step": 8940
    },
    {
      "epoch": 1.9181311615945136,
      "grad_norm": 14.66403579711914,
      "learning_rate": 1.744249178454065e-05,
      "loss": 1.2568,
      "step": 8950
    },
    {
      "epoch": 1.9202743249035577,
      "grad_norm": 16.240915298461914,
      "learning_rate": 1.7439634233461922e-05,
      "loss": 0.3105,
      "step": 8960
    },
    {
      "epoch": 1.9224174882126017,
      "grad_norm": 0.45133331418037415,
      "learning_rate": 1.74367766823832e-05,
      "loss": 0.3145,
      "step": 8970
    },
    {
      "epoch": 1.924560651521646,
      "grad_norm": 0.4013252556324005,
      "learning_rate": 1.7433919131304474e-05,
      "loss": 0.1652,
      "step": 8980
    },
    {
      "epoch": 1.92670381483069,
      "grad_norm": 14.88019847869873,
      "learning_rate": 1.7431061580225748e-05,
      "loss": 0.9522,
      "step": 8990
    },
    {
      "epoch": 1.9288469781397342,
      "grad_norm": 29.69455337524414,
      "learning_rate": 1.742820402914702e-05,
      "loss": 0.7869,
      "step": 9000
    },
    {
      "epoch": 1.9309901414487785,
      "grad_norm": 14.60197639465332,
      "learning_rate": 1.7425346478068296e-05,
      "loss": 0.7748,
      "step": 9010
    },
    {
      "epoch": 1.9331333047578225,
      "grad_norm": 14.450699806213379,
      "learning_rate": 1.7422488926989573e-05,
      "loss": 1.1824,
      "step": 9020
    },
    {
      "epoch": 1.9352764680668666,
      "grad_norm": 0.7161566019058228,
      "learning_rate": 1.7419631375910847e-05,
      "loss": 0.5668,
      "step": 9030
    },
    {
      "epoch": 1.9374196313759109,
      "grad_norm": 14.439719200134277,
      "learning_rate": 1.741677382483212e-05,
      "loss": 0.5762,
      "step": 9040
    },
    {
      "epoch": 1.939562794684955,
      "grad_norm": 0.6515193581581116,
      "learning_rate": 1.7413916273753395e-05,
      "loss": 0.5728,
      "step": 9050
    },
    {
      "epoch": 1.941705957993999,
      "grad_norm": 0.5473116636276245,
      "learning_rate": 1.741105872267467e-05,
      "loss": 0.2979,
      "step": 9060
    },
    {
      "epoch": 1.9438491213030433,
      "grad_norm": 14.558714866638184,
      "learning_rate": 1.7408201171595946e-05,
      "loss": 0.6062,
      "step": 9070
    },
    {
      "epoch": 1.9459922846120874,
      "grad_norm": 0.4197186529636383,
      "learning_rate": 1.740534362051722e-05,
      "loss": 0.1673,
      "step": 9080
    },
    {
      "epoch": 1.9481354479211315,
      "grad_norm": 0.37318161129951477,
      "learning_rate": 1.7402486069438494e-05,
      "loss": 0.7878,
      "step": 9090
    },
    {
      "epoch": 1.9502786112301758,
      "grad_norm": 14.78538990020752,
      "learning_rate": 1.7399628518359768e-05,
      "loss": 0.6474,
      "step": 9100
    },
    {
      "epoch": 1.9524217745392198,
      "grad_norm": 14.644850730895996,
      "learning_rate": 1.739677096728104e-05,
      "loss": 0.6332,
      "step": 9110
    },
    {
      "epoch": 1.954564937848264,
      "grad_norm": 0.42244914174079895,
      "learning_rate": 1.7393913416202316e-05,
      "loss": 0.3248,
      "step": 9120
    },
    {
      "epoch": 1.9567081011573082,
      "grad_norm": 14.868046760559082,
      "learning_rate": 1.739105586512359e-05,
      "loss": 0.4723,
      "step": 9130
    },
    {
      "epoch": 1.9588512644663525,
      "grad_norm": 0.43108367919921875,
      "learning_rate": 1.7388198314044863e-05,
      "loss": 0.4964,
      "step": 9140
    },
    {
      "epoch": 1.9609944277753963,
      "grad_norm": 0.3441343307495117,
      "learning_rate": 1.7385340762966137e-05,
      "loss": 0.5002,
      "step": 9150
    },
    {
      "epoch": 1.9631375910844406,
      "grad_norm": 14.93820571899414,
      "learning_rate": 1.7382483211887415e-05,
      "loss": 0.647,
      "step": 9160
    },
    {
      "epoch": 1.965280754393485,
      "grad_norm": 14.856925964355469,
      "learning_rate": 1.737962566080869e-05,
      "loss": 0.4906,
      "step": 9170
    },
    {
      "epoch": 1.9674239177025288,
      "grad_norm": 0.5947893261909485,
      "learning_rate": 1.7376768109729962e-05,
      "loss": 1.0945,
      "step": 9180
    },
    {
      "epoch": 1.969567081011573,
      "grad_norm": 0.7112909555435181,
      "learning_rate": 1.7373910558651236e-05,
      "loss": 0.7261,
      "step": 9190
    },
    {
      "epoch": 1.9717102443206174,
      "grad_norm": 0.5410457849502563,
      "learning_rate": 1.737105300757251e-05,
      "loss": 0.1508,
      "step": 9200
    },
    {
      "epoch": 1.9738534076296612,
      "grad_norm": 15.576982498168945,
      "learning_rate": 1.7368195456493788e-05,
      "loss": 1.0397,
      "step": 9210
    },
    {
      "epoch": 1.9759965709387055,
      "grad_norm": 15.087874412536621,
      "learning_rate": 1.736533790541506e-05,
      "loss": 0.4481,
      "step": 9220
    },
    {
      "epoch": 1.9781397342477498,
      "grad_norm": 15.100568771362305,
      "learning_rate": 1.7362480354336335e-05,
      "loss": 0.4621,
      "step": 9230
    },
    {
      "epoch": 1.9802828975567939,
      "grad_norm": 0.4274992048740387,
      "learning_rate": 1.735962280325761e-05,
      "loss": 0.3116,
      "step": 9240
    },
    {
      "epoch": 1.982426060865838,
      "grad_norm": 0.353505939245224,
      "learning_rate": 1.7356765252178883e-05,
      "loss": 0.4887,
      "step": 9250
    },
    {
      "epoch": 1.9845692241748822,
      "grad_norm": 15.018338203430176,
      "learning_rate": 1.7353907701100157e-05,
      "loss": 1.1352,
      "step": 9260
    },
    {
      "epoch": 1.9867123874839263,
      "grad_norm": 14.775614738464355,
      "learning_rate": 1.7351050150021435e-05,
      "loss": 0.6261,
      "step": 9270
    },
    {
      "epoch": 1.9888555507929704,
      "grad_norm": 15.273102760314941,
      "learning_rate": 1.734819259894271e-05,
      "loss": 0.9242,
      "step": 9280
    },
    {
      "epoch": 1.9909987141020147,
      "grad_norm": 0.730392575263977,
      "learning_rate": 1.7345335047863982e-05,
      "loss": 1.1741,
      "step": 9290
    },
    {
      "epoch": 1.9931418774110587,
      "grad_norm": 14.758859634399414,
      "learning_rate": 1.7342477496785256e-05,
      "loss": 0.4245,
      "step": 9300
    },
    {
      "epoch": 1.9952850407201028,
      "grad_norm": 14.387619972229004,
      "learning_rate": 1.733961994570653e-05,
      "loss": 0.8354,
      "step": 9310
    },
    {
      "epoch": 1.9974282040291471,
      "grad_norm": 29.127317428588867,
      "learning_rate": 1.7336762394627808e-05,
      "loss": 0.9271,
      "step": 9320
    },
    {
      "epoch": 1.9995713673381912,
      "grad_norm": 14.500798225402832,
      "learning_rate": 1.733390484354908e-05,
      "loss": 0.9163,
      "step": 9330
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8676666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.5261979699134827,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 395.9818,
      "eval_samples_per_second": 7.576,
      "eval_steps_per_second": 2.525,
      "step": 9332
    },
    {
      "epoch": 2.0017145306472353,
      "grad_norm": 14.170350074768066,
      "learning_rate": 1.7331047292470352e-05,
      "loss": 0.5323,
      "step": 9340
    },
    {
      "epoch": 2.0038576939562796,
      "grad_norm": 0.7315813302993774,
      "learning_rate": 1.732818974139163e-05,
      "loss": 0.4254,
      "step": 9350
    },
    {
      "epoch": 2.0060008572653234,
      "grad_norm": 15.30521011352539,
      "learning_rate": 1.7325332190312903e-05,
      "loss": 0.4469,
      "step": 9360
    },
    {
      "epoch": 2.0081440205743677,
      "grad_norm": 0.4594772756099701,
      "learning_rate": 1.7322474639234177e-05,
      "loss": 0.1588,
      "step": 9370
    },
    {
      "epoch": 2.010287183883412,
      "grad_norm": 0.29668939113616943,
      "learning_rate": 1.731961708815545e-05,
      "loss": 0.3316,
      "step": 9380
    },
    {
      "epoch": 2.0124303471924563,
      "grad_norm": 0.3532557487487793,
      "learning_rate": 1.7316759537076725e-05,
      "loss": 0.6669,
      "step": 9390
    },
    {
      "epoch": 2.0145735105015,
      "grad_norm": 0.38191044330596924,
      "learning_rate": 1.7313901985998e-05,
      "loss": 0.4934,
      "step": 9400
    },
    {
      "epoch": 2.0167166738105444,
      "grad_norm": 0.41777631640434265,
      "learning_rate": 1.7311044434919276e-05,
      "loss": 0.643,
      "step": 9410
    },
    {
      "epoch": 2.0188598371195887,
      "grad_norm": 14.859551429748535,
      "learning_rate": 1.730818688384055e-05,
      "loss": 0.6628,
      "step": 9420
    },
    {
      "epoch": 2.0210030004286326,
      "grad_norm": 14.959527015686035,
      "learning_rate": 1.7305329332761824e-05,
      "loss": 0.515,
      "step": 9430
    },
    {
      "epoch": 2.023146163737677,
      "grad_norm": 0.33004069328308105,
      "learning_rate": 1.7302471781683098e-05,
      "loss": 0.8539,
      "step": 9440
    },
    {
      "epoch": 2.025289327046721,
      "grad_norm": 0.4284127354621887,
      "learning_rate": 1.7299614230604372e-05,
      "loss": 0.4831,
      "step": 9450
    },
    {
      "epoch": 2.027432490355765,
      "grad_norm": 0.35029739141464233,
      "learning_rate": 1.729675667952565e-05,
      "loss": 0.4917,
      "step": 9460
    },
    {
      "epoch": 2.0295756536648093,
      "grad_norm": 14.822473526000977,
      "learning_rate": 1.7293899128446923e-05,
      "loss": 0.8046,
      "step": 9470
    },
    {
      "epoch": 2.0317188169738536,
      "grad_norm": 15.035308837890625,
      "learning_rate": 1.7291041577368197e-05,
      "loss": 0.783,
      "step": 9480
    },
    {
      "epoch": 2.0338619802828974,
      "grad_norm": 14.390124320983887,
      "learning_rate": 1.728818402628947e-05,
      "loss": 0.5861,
      "step": 9490
    },
    {
      "epoch": 2.0360051435919417,
      "grad_norm": 0.5785946846008301,
      "learning_rate": 1.7285326475210745e-05,
      "loss": 0.4481,
      "step": 9500
    },
    {
      "epoch": 2.038148306900986,
      "grad_norm": 0.5545899868011475,
      "learning_rate": 1.7282468924132022e-05,
      "loss": 0.4438,
      "step": 9510
    },
    {
      "epoch": 2.04029147021003,
      "grad_norm": 16.806718826293945,
      "learning_rate": 1.7279611373053296e-05,
      "loss": 0.7788,
      "step": 9520
    },
    {
      "epoch": 2.042434633519074,
      "grad_norm": 0.4640388488769531,
      "learning_rate": 1.727675382197457e-05,
      "loss": 0.4959,
      "step": 9530
    },
    {
      "epoch": 2.0445777968281185,
      "grad_norm": 0.4129127562046051,
      "learning_rate": 1.7273896270895844e-05,
      "loss": 0.794,
      "step": 9540
    },
    {
      "epoch": 2.0467209601371623,
      "grad_norm": 0.3601485788822174,
      "learning_rate": 1.7271038719817118e-05,
      "loss": 0.9699,
      "step": 9550
    },
    {
      "epoch": 2.0488641234462066,
      "grad_norm": 0.5298388600349426,
      "learning_rate": 1.7268181168738392e-05,
      "loss": 0.4567,
      "step": 9560
    },
    {
      "epoch": 2.051007286755251,
      "grad_norm": 0.4505382776260376,
      "learning_rate": 1.7265323617659666e-05,
      "loss": 0.6396,
      "step": 9570
    },
    {
      "epoch": 2.0531504500642948,
      "grad_norm": 0.5223221182823181,
      "learning_rate": 1.726246606658094e-05,
      "loss": 0.7693,
      "step": 9580
    },
    {
      "epoch": 2.055293613373339,
      "grad_norm": 14.832755088806152,
      "learning_rate": 1.7259608515502214e-05,
      "loss": 0.7596,
      "step": 9590
    },
    {
      "epoch": 2.0574367766823833,
      "grad_norm": 30.103334426879883,
      "learning_rate": 1.725675096442349e-05,
      "loss": 0.6307,
      "step": 9600
    },
    {
      "epoch": 2.059579939991427,
      "grad_norm": 14.782407760620117,
      "learning_rate": 1.7253893413344765e-05,
      "loss": 0.9288,
      "step": 9610
    },
    {
      "epoch": 2.0617231033004715,
      "grad_norm": 0.6544215083122253,
      "learning_rate": 1.725103586226604e-05,
      "loss": 0.6077,
      "step": 9620
    },
    {
      "epoch": 2.0638662666095158,
      "grad_norm": 0.6413612961769104,
      "learning_rate": 1.7248178311187313e-05,
      "loss": 0.4437,
      "step": 9630
    },
    {
      "epoch": 2.0660094299185596,
      "grad_norm": 0.5178590416908264,
      "learning_rate": 1.7245320760108587e-05,
      "loss": 0.307,
      "step": 9640
    },
    {
      "epoch": 2.068152593227604,
      "grad_norm": 30.52540397644043,
      "learning_rate": 1.7242463209029864e-05,
      "loss": 1.0933,
      "step": 9650
    },
    {
      "epoch": 2.070295756536648,
      "grad_norm": 15.03737735748291,
      "learning_rate": 1.7239605657951138e-05,
      "loss": 0.502,
      "step": 9660
    },
    {
      "epoch": 2.072438919845692,
      "grad_norm": 0.5209364891052246,
      "learning_rate": 1.7236748106872412e-05,
      "loss": 0.6397,
      "step": 9670
    },
    {
      "epoch": 2.0745820831547364,
      "grad_norm": 0.5785747170448303,
      "learning_rate": 1.7233890555793686e-05,
      "loss": 0.8968,
      "step": 9680
    },
    {
      "epoch": 2.0767252464637806,
      "grad_norm": 0.6215443015098572,
      "learning_rate": 1.723103300471496e-05,
      "loss": 0.5958,
      "step": 9690
    },
    {
      "epoch": 2.0788684097728245,
      "grad_norm": 15.825352668762207,
      "learning_rate": 1.7228175453636237e-05,
      "loss": 0.7461,
      "step": 9700
    },
    {
      "epoch": 2.081011573081869,
      "grad_norm": 16.606412887573242,
      "learning_rate": 1.722531790255751e-05,
      "loss": 0.5944,
      "step": 9710
    },
    {
      "epoch": 2.083154736390913,
      "grad_norm": 0.6225503087043762,
      "learning_rate": 1.7222460351478785e-05,
      "loss": 0.573,
      "step": 9720
    },
    {
      "epoch": 2.085297899699957,
      "grad_norm": 0.3847590386867523,
      "learning_rate": 1.721960280040006e-05,
      "loss": 0.4706,
      "step": 9730
    },
    {
      "epoch": 2.0874410630090012,
      "grad_norm": 0.24992792308330536,
      "learning_rate": 1.7216745249321333e-05,
      "loss": 0.3427,
      "step": 9740
    },
    {
      "epoch": 2.0895842263180455,
      "grad_norm": 16.518491744995117,
      "learning_rate": 1.7213887698242607e-05,
      "loss": 0.5277,
      "step": 9750
    },
    {
      "epoch": 2.0917273896270894,
      "grad_norm": 0.3814893364906311,
      "learning_rate": 1.7211030147163884e-05,
      "loss": 0.6904,
      "step": 9760
    },
    {
      "epoch": 2.0938705529361337,
      "grad_norm": 0.3955221176147461,
      "learning_rate": 1.7208172596085155e-05,
      "loss": 0.8004,
      "step": 9770
    },
    {
      "epoch": 2.096013716245178,
      "grad_norm": 0.30451351404190063,
      "learning_rate": 1.720531504500643e-05,
      "loss": 0.5042,
      "step": 9780
    },
    {
      "epoch": 2.098156879554222,
      "grad_norm": 45.3336067199707,
      "learning_rate": 1.7202457493927706e-05,
      "loss": 0.862,
      "step": 9790
    },
    {
      "epoch": 2.100300042863266,
      "grad_norm": 15.073702812194824,
      "learning_rate": 1.719959994284898e-05,
      "loss": 0.8465,
      "step": 9800
    },
    {
      "epoch": 2.1024432061723104,
      "grad_norm": 0.44061532616615295,
      "learning_rate": 1.7196742391770254e-05,
      "loss": 0.6449,
      "step": 9810
    },
    {
      "epoch": 2.1045863694813547,
      "grad_norm": 0.3862210810184479,
      "learning_rate": 1.7193884840691528e-05,
      "loss": 0.324,
      "step": 9820
    },
    {
      "epoch": 2.1067295327903985,
      "grad_norm": 0.4451211392879486,
      "learning_rate": 1.71910272896128e-05,
      "loss": 0.7837,
      "step": 9830
    },
    {
      "epoch": 2.108872696099443,
      "grad_norm": 17.584592819213867,
      "learning_rate": 1.718816973853408e-05,
      "loss": 0.3157,
      "step": 9840
    },
    {
      "epoch": 2.111015859408487,
      "grad_norm": 0.23785175383090973,
      "learning_rate": 1.7185312187455353e-05,
      "loss": 0.5137,
      "step": 9850
    },
    {
      "epoch": 2.113159022717531,
      "grad_norm": 14.981602668762207,
      "learning_rate": 1.7182454636376627e-05,
      "loss": 0.803,
      "step": 9860
    },
    {
      "epoch": 2.1153021860265753,
      "grad_norm": 0.7596532106399536,
      "learning_rate": 1.71795970852979e-05,
      "loss": 0.7459,
      "step": 9870
    },
    {
      "epoch": 2.1174453493356196,
      "grad_norm": 0.4840448796749115,
      "learning_rate": 1.7176739534219175e-05,
      "loss": 0.3021,
      "step": 9880
    },
    {
      "epoch": 2.1195885126446634,
      "grad_norm": 15.543231010437012,
      "learning_rate": 1.717388198314045e-05,
      "loss": 0.7684,
      "step": 9890
    },
    {
      "epoch": 2.1217316759537077,
      "grad_norm": 0.543682873249054,
      "learning_rate": 1.7171024432061726e-05,
      "loss": 0.453,
      "step": 9900
    },
    {
      "epoch": 2.123874839262752,
      "grad_norm": 15.112324714660645,
      "learning_rate": 1.7168166880983e-05,
      "loss": 0.3343,
      "step": 9910
    },
    {
      "epoch": 2.126018002571796,
      "grad_norm": 31.037336349487305,
      "learning_rate": 1.7165309329904274e-05,
      "loss": 1.0073,
      "step": 9920
    },
    {
      "epoch": 2.12816116588084,
      "grad_norm": 31.31924057006836,
      "learning_rate": 1.7162451778825548e-05,
      "loss": 0.972,
      "step": 9930
    },
    {
      "epoch": 2.1303043291898844,
      "grad_norm": 0.6422595381736755,
      "learning_rate": 1.715959422774682e-05,
      "loss": 0.766,
      "step": 9940
    },
    {
      "epoch": 2.1324474924989283,
      "grad_norm": 15.153576850891113,
      "learning_rate": 1.71567366766681e-05,
      "loss": 0.3432,
      "step": 9950
    },
    {
      "epoch": 2.1345906558079726,
      "grad_norm": 0.37411361932754517,
      "learning_rate": 1.7153879125589373e-05,
      "loss": 0.6682,
      "step": 9960
    },
    {
      "epoch": 2.136733819117017,
      "grad_norm": 0.5076130032539368,
      "learning_rate": 1.7151021574510647e-05,
      "loss": 0.7713,
      "step": 9970
    },
    {
      "epoch": 2.1388769824260607,
      "grad_norm": 14.309333801269531,
      "learning_rate": 1.714816402343192e-05,
      "loss": 1.1604,
      "step": 9980
    },
    {
      "epoch": 2.141020145735105,
      "grad_norm": 0.7143166065216064,
      "learning_rate": 1.7145306472353195e-05,
      "loss": 0.0177,
      "step": 9990
    },
    {
      "epoch": 2.1431633090441493,
      "grad_norm": 31.311275482177734,
      "learning_rate": 1.714244892127447e-05,
      "loss": 0.7372,
      "step": 10000
    },
    {
      "epoch": 2.145306472353193,
      "grad_norm": 17.08043098449707,
      "learning_rate": 1.7139591370195743e-05,
      "loss": 0.9249,
      "step": 10010
    },
    {
      "epoch": 2.1474496356622375,
      "grad_norm": 0.6138358116149902,
      "learning_rate": 1.7136733819117016e-05,
      "loss": 0.6055,
      "step": 10020
    },
    {
      "epoch": 2.1495927989712817,
      "grad_norm": 0.5730047821998596,
      "learning_rate": 1.713387626803829e-05,
      "loss": 0.5925,
      "step": 10030
    },
    {
      "epoch": 2.1517359622803256,
      "grad_norm": 14.734573364257812,
      "learning_rate": 1.7131018716959568e-05,
      "loss": 0.8663,
      "step": 10040
    },
    {
      "epoch": 2.15387912558937,
      "grad_norm": 0.7362563014030457,
      "learning_rate": 1.712816116588084e-05,
      "loss": 0.5757,
      "step": 10050
    },
    {
      "epoch": 2.156022288898414,
      "grad_norm": 0.5433034896850586,
      "learning_rate": 1.7125303614802116e-05,
      "loss": 0.295,
      "step": 10060
    },
    {
      "epoch": 2.158165452207458,
      "grad_norm": 15.680695533752441,
      "learning_rate": 1.712244606372339e-05,
      "loss": 0.607,
      "step": 10070
    },
    {
      "epoch": 2.1603086155165023,
      "grad_norm": 0.5322872996330261,
      "learning_rate": 1.7119588512644663e-05,
      "loss": 0.7635,
      "step": 10080
    },
    {
      "epoch": 2.1624517788255466,
      "grad_norm": 14.577402114868164,
      "learning_rate": 1.711673096156594e-05,
      "loss": 0.8557,
      "step": 10090
    },
    {
      "epoch": 2.1645949421345905,
      "grad_norm": 0.848832368850708,
      "learning_rate": 1.7113873410487215e-05,
      "loss": 0.8288,
      "step": 10100
    },
    {
      "epoch": 2.1667381054436348,
      "grad_norm": 0.8091626763343811,
      "learning_rate": 1.711101585940849e-05,
      "loss": 0.5423,
      "step": 10110
    },
    {
      "epoch": 2.168881268752679,
      "grad_norm": 0.5736048221588135,
      "learning_rate": 1.7108158308329763e-05,
      "loss": 0.4416,
      "step": 10120
    },
    {
      "epoch": 2.171024432061723,
      "grad_norm": 0.531289279460907,
      "learning_rate": 1.7105300757251036e-05,
      "loss": 0.897,
      "step": 10130
    },
    {
      "epoch": 2.173167595370767,
      "grad_norm": 0.6209803223609924,
      "learning_rate": 1.7102443206172314e-05,
      "loss": 0.4596,
      "step": 10140
    },
    {
      "epoch": 2.1753107586798115,
      "grad_norm": 0.5472012162208557,
      "learning_rate": 1.7099585655093588e-05,
      "loss": 0.4525,
      "step": 10150
    },
    {
      "epoch": 2.1774539219888553,
      "grad_norm": 0.38124698400497437,
      "learning_rate": 1.709672810401486e-05,
      "loss": 0.3038,
      "step": 10160
    },
    {
      "epoch": 2.1795970852978996,
      "grad_norm": 15.456226348876953,
      "learning_rate": 1.7093870552936136e-05,
      "loss": 0.9523,
      "step": 10170
    },
    {
      "epoch": 2.181740248606944,
      "grad_norm": 0.6416675448417664,
      "learning_rate": 1.709101300185741e-05,
      "loss": 0.9014,
      "step": 10180
    },
    {
      "epoch": 2.1838834119159882,
      "grad_norm": 0.6569852232933044,
      "learning_rate": 1.7088155450778683e-05,
      "loss": 0.427,
      "step": 10190
    },
    {
      "epoch": 2.186026575225032,
      "grad_norm": 16.685243606567383,
      "learning_rate": 1.7085297899699957e-05,
      "loss": 0.7707,
      "step": 10200
    },
    {
      "epoch": 2.1881697385340764,
      "grad_norm": 1.0645571947097778,
      "learning_rate": 1.708244034862123e-05,
      "loss": 0.4272,
      "step": 10210
    },
    {
      "epoch": 2.1903129018431207,
      "grad_norm": 14.451460838317871,
      "learning_rate": 1.7079582797542505e-05,
      "loss": 0.2853,
      "step": 10220
    },
    {
      "epoch": 2.1924560651521645,
      "grad_norm": 16.075702667236328,
      "learning_rate": 1.7076725246463783e-05,
      "loss": 0.4729,
      "step": 10230
    },
    {
      "epoch": 2.194599228461209,
      "grad_norm": 43.83708953857422,
      "learning_rate": 1.7073867695385056e-05,
      "loss": 0.5621,
      "step": 10240
    },
    {
      "epoch": 2.196742391770253,
      "grad_norm": 0.27861717343330383,
      "learning_rate": 1.707101014430633e-05,
      "loss": 0.3473,
      "step": 10250
    },
    {
      "epoch": 2.198885555079297,
      "grad_norm": 15.171045303344727,
      "learning_rate": 1.7068152593227604e-05,
      "loss": 0.7035,
      "step": 10260
    },
    {
      "epoch": 2.2010287183883412,
      "grad_norm": 30.037715911865234,
      "learning_rate": 1.7065295042148878e-05,
      "loss": 0.6686,
      "step": 10270
    },
    {
      "epoch": 2.2031718816973855,
      "grad_norm": 15.096853256225586,
      "learning_rate": 1.7062437491070156e-05,
      "loss": 0.6552,
      "step": 10280
    },
    {
      "epoch": 2.2053150450064294,
      "grad_norm": 0.48334574699401855,
      "learning_rate": 1.705957993999143e-05,
      "loss": 0.4841,
      "step": 10290
    },
    {
      "epoch": 2.2074582083154737,
      "grad_norm": 14.951337814331055,
      "learning_rate": 1.7056722388912703e-05,
      "loss": 0.7913,
      "step": 10300
    },
    {
      "epoch": 2.209601371624518,
      "grad_norm": 14.910140991210938,
      "learning_rate": 1.7053864837833977e-05,
      "loss": 0.6283,
      "step": 10310
    },
    {
      "epoch": 2.211744534933562,
      "grad_norm": 0.48938649892807007,
      "learning_rate": 1.705100728675525e-05,
      "loss": 0.7692,
      "step": 10320
    },
    {
      "epoch": 2.213887698242606,
      "grad_norm": 0.5837118029594421,
      "learning_rate": 1.7048149735676525e-05,
      "loss": 0.6032,
      "step": 10330
    },
    {
      "epoch": 2.2160308615516504,
      "grad_norm": 29.626110076904297,
      "learning_rate": 1.7045292184597803e-05,
      "loss": 0.5834,
      "step": 10340
    },
    {
      "epoch": 2.2181740248606943,
      "grad_norm": 0.5698586106300354,
      "learning_rate": 1.7042434633519076e-05,
      "loss": 0.5986,
      "step": 10350
    },
    {
      "epoch": 2.2203171881697386,
      "grad_norm": 0.5386297702789307,
      "learning_rate": 1.703957708244035e-05,
      "loss": 1.0452,
      "step": 10360
    },
    {
      "epoch": 2.222460351478783,
      "grad_norm": 0.5269917845726013,
      "learning_rate": 1.7036719531361624e-05,
      "loss": 0.4701,
      "step": 10370
    },
    {
      "epoch": 2.2246035147878267,
      "grad_norm": 0.4428974688053131,
      "learning_rate": 1.7033861980282898e-05,
      "loss": 0.7742,
      "step": 10380
    },
    {
      "epoch": 2.226746678096871,
      "grad_norm": 0.49609705805778503,
      "learning_rate": 1.7031004429204176e-05,
      "loss": 0.3175,
      "step": 10390
    },
    {
      "epoch": 2.2288898414059153,
      "grad_norm": 14.92517375946045,
      "learning_rate": 1.702814687812545e-05,
      "loss": 0.1694,
      "step": 10400
    },
    {
      "epoch": 2.231033004714959,
      "grad_norm": 0.3146704137325287,
      "learning_rate": 1.7025289327046723e-05,
      "loss": 0.3432,
      "step": 10410
    },
    {
      "epoch": 2.2331761680240034,
      "grad_norm": 0.3238027095794678,
      "learning_rate": 1.7022431775967997e-05,
      "loss": 0.6815,
      "step": 10420
    },
    {
      "epoch": 2.2353193313330477,
      "grad_norm": 14.824750900268555,
      "learning_rate": 1.701957422488927e-05,
      "loss": 0.6653,
      "step": 10430
    },
    {
      "epoch": 2.2374624946420916,
      "grad_norm": 0.41562241315841675,
      "learning_rate": 1.7016716673810545e-05,
      "loss": 0.488,
      "step": 10440
    },
    {
      "epoch": 2.239605657951136,
      "grad_norm": 0.3685624897480011,
      "learning_rate": 1.701385912273182e-05,
      "loss": 0.1671,
      "step": 10450
    },
    {
      "epoch": 2.24174882126018,
      "grad_norm": 14.964441299438477,
      "learning_rate": 1.7011001571653093e-05,
      "loss": 0.6448,
      "step": 10460
    },
    {
      "epoch": 2.243891984569224,
      "grad_norm": 0.30436044931411743,
      "learning_rate": 1.7008144020574367e-05,
      "loss": 0.8236,
      "step": 10470
    },
    {
      "epoch": 2.2460351478782683,
      "grad_norm": 14.95036792755127,
      "learning_rate": 1.7005286469495644e-05,
      "loss": 0.8116,
      "step": 10480
    },
    {
      "epoch": 2.2481783111873126,
      "grad_norm": 0.5284432172775269,
      "learning_rate": 1.7002428918416918e-05,
      "loss": 0.7577,
      "step": 10490
    },
    {
      "epoch": 2.2503214744963564,
      "grad_norm": 14.685808181762695,
      "learning_rate": 1.6999571367338192e-05,
      "loss": 1.1684,
      "step": 10500
    },
    {
      "epoch": 2.2524646378054007,
      "grad_norm": 0.8629677891731262,
      "learning_rate": 1.6996713816259466e-05,
      "loss": 0.6595,
      "step": 10510
    },
    {
      "epoch": 2.254607801114445,
      "grad_norm": 0.4977506101131439,
      "learning_rate": 1.699385626518074e-05,
      "loss": 0.1577,
      "step": 10520
    },
    {
      "epoch": 2.256750964423489,
      "grad_norm": 0.4862208366394043,
      "learning_rate": 1.6990998714102017e-05,
      "loss": 0.761,
      "step": 10530
    },
    {
      "epoch": 2.258894127732533,
      "grad_norm": 0.34332093596458435,
      "learning_rate": 1.698814116302329e-05,
      "loss": 0.501,
      "step": 10540
    },
    {
      "epoch": 2.2610372910415775,
      "grad_norm": 0.3381403684616089,
      "learning_rate": 1.6985283611944565e-05,
      "loss": 0.3267,
      "step": 10550
    },
    {
      "epoch": 2.2631804543506213,
      "grad_norm": 14.771299362182617,
      "learning_rate": 1.698242606086584e-05,
      "loss": 0.8102,
      "step": 10560
    },
    {
      "epoch": 2.2653236176596656,
      "grad_norm": 0.4414994418621063,
      "learning_rate": 1.6979568509787113e-05,
      "loss": 1.2686,
      "step": 10570
    },
    {
      "epoch": 2.26746678096871,
      "grad_norm": 0.5863863229751587,
      "learning_rate": 1.697671095870839e-05,
      "loss": 0.3001,
      "step": 10580
    },
    {
      "epoch": 2.2696099442777538,
      "grad_norm": 0.6544229388237,
      "learning_rate": 1.6973853407629664e-05,
      "loss": 1.0362,
      "step": 10590
    },
    {
      "epoch": 2.271753107586798,
      "grad_norm": 0.6533812880516052,
      "learning_rate": 1.6970995856550938e-05,
      "loss": 1.1076,
      "step": 10600
    },
    {
      "epoch": 2.2738962708958423,
      "grad_norm": 0.6479247212409973,
      "learning_rate": 1.6968138305472212e-05,
      "loss": 0.2817,
      "step": 10610
    },
    {
      "epoch": 2.276039434204886,
      "grad_norm": 0.4175293743610382,
      "learning_rate": 1.6965280754393486e-05,
      "loss": 0.4566,
      "step": 10620
    },
    {
      "epoch": 2.2781825975139305,
      "grad_norm": 0.521201491355896,
      "learning_rate": 1.696242320331476e-05,
      "loss": 0.3111,
      "step": 10630
    },
    {
      "epoch": 2.280325760822975,
      "grad_norm": 0.29662150144577026,
      "learning_rate": 1.6959565652236034e-05,
      "loss": 0.7876,
      "step": 10640
    },
    {
      "epoch": 2.2824689241320186,
      "grad_norm": 0.31638631224632263,
      "learning_rate": 1.6956708101157308e-05,
      "loss": 0.6258,
      "step": 10650
    },
    {
      "epoch": 2.284612087441063,
      "grad_norm": 0.2735554575920105,
      "learning_rate": 1.6953850550078582e-05,
      "loss": 0.4392,
      "step": 10660
    },
    {
      "epoch": 2.286755250750107,
      "grad_norm": 0.4024924039840698,
      "learning_rate": 1.695099299899986e-05,
      "loss": 1.2748,
      "step": 10670
    },
    {
      "epoch": 2.288898414059151,
      "grad_norm": 0.5241432785987854,
      "learning_rate": 1.6948135447921133e-05,
      "loss": 0.4706,
      "step": 10680
    },
    {
      "epoch": 2.2910415773681954,
      "grad_norm": 0.5030534863471985,
      "learning_rate": 1.6945277896842407e-05,
      "loss": 0.2812,
      "step": 10690
    },
    {
      "epoch": 2.2931847406772397,
      "grad_norm": 0.32011258602142334,
      "learning_rate": 1.694242034576368e-05,
      "loss": 0.4547,
      "step": 10700
    },
    {
      "epoch": 2.295327903986284,
      "grad_norm": 15.134157180786133,
      "learning_rate": 1.6939562794684955e-05,
      "loss": 0.8195,
      "step": 10710
    },
    {
      "epoch": 2.297471067295328,
      "grad_norm": 16.26557159423828,
      "learning_rate": 1.6936705243606232e-05,
      "loss": 0.8748,
      "step": 10720
    },
    {
      "epoch": 2.299614230604372,
      "grad_norm": 14.60655403137207,
      "learning_rate": 1.6933847692527506e-05,
      "loss": 1.0011,
      "step": 10730
    },
    {
      "epoch": 2.3017573939134164,
      "grad_norm": 1.2404693365097046,
      "learning_rate": 1.693099014144878e-05,
      "loss": 0.4403,
      "step": 10740
    },
    {
      "epoch": 2.3039005572224602,
      "grad_norm": 0.7850785851478577,
      "learning_rate": 1.6928132590370054e-05,
      "loss": 0.5772,
      "step": 10750
    },
    {
      "epoch": 2.3060437205315045,
      "grad_norm": 14.356194496154785,
      "learning_rate": 1.6925275039291328e-05,
      "loss": 0.7872,
      "step": 10760
    },
    {
      "epoch": 2.308186883840549,
      "grad_norm": 15.445338249206543,
      "learning_rate": 1.6922417488212605e-05,
      "loss": 0.4179,
      "step": 10770
    },
    {
      "epoch": 2.3103300471495927,
      "grad_norm": 30.30423355102539,
      "learning_rate": 1.691955993713388e-05,
      "loss": 0.3078,
      "step": 10780
    },
    {
      "epoch": 2.312473210458637,
      "grad_norm": 0.36214694380760193,
      "learning_rate": 1.6916702386055153e-05,
      "loss": 0.4724,
      "step": 10790
    },
    {
      "epoch": 2.3146163737676813,
      "grad_norm": 0.2823711931705475,
      "learning_rate": 1.6913844834976427e-05,
      "loss": 0.4895,
      "step": 10800
    },
    {
      "epoch": 2.316759537076725,
      "grad_norm": 15.335806846618652,
      "learning_rate": 1.69109872838977e-05,
      "loss": 0.3407,
      "step": 10810
    },
    {
      "epoch": 2.3189027003857694,
      "grad_norm": 14.929638862609863,
      "learning_rate": 1.6908129732818975e-05,
      "loss": 0.4672,
      "step": 10820
    },
    {
      "epoch": 2.3210458636948137,
      "grad_norm": 0.36222535371780396,
      "learning_rate": 1.6905272181740252e-05,
      "loss": 0.8082,
      "step": 10830
    },
    {
      "epoch": 2.3231890270038575,
      "grad_norm": 0.3083784878253937,
      "learning_rate": 1.6902414630661526e-05,
      "loss": 0.3385,
      "step": 10840
    },
    {
      "epoch": 2.325332190312902,
      "grad_norm": 0.2985551357269287,
      "learning_rate": 1.6899557079582797e-05,
      "loss": 0.15,
      "step": 10850
    },
    {
      "epoch": 2.327475353621946,
      "grad_norm": 0.24819152057170868,
      "learning_rate": 1.6896699528504074e-05,
      "loss": 0.6611,
      "step": 10860
    },
    {
      "epoch": 2.32961851693099,
      "grad_norm": 15.105656623840332,
      "learning_rate": 1.6893841977425348e-05,
      "loss": 0.9466,
      "step": 10870
    },
    {
      "epoch": 2.3317616802400343,
      "grad_norm": 0.5136595368385315,
      "learning_rate": 1.6890984426346622e-05,
      "loss": 0.5051,
      "step": 10880
    },
    {
      "epoch": 2.3339048435490786,
      "grad_norm": 16.340940475463867,
      "learning_rate": 1.6888126875267896e-05,
      "loss": 0.7564,
      "step": 10890
    },
    {
      "epoch": 2.3360480068581224,
      "grad_norm": 30.212989807128906,
      "learning_rate": 1.688526932418917e-05,
      "loss": 0.7401,
      "step": 10900
    },
    {
      "epoch": 2.3381911701671667,
      "grad_norm": 17.80716323852539,
      "learning_rate": 1.6882411773110447e-05,
      "loss": 0.8452,
      "step": 10910
    },
    {
      "epoch": 2.340334333476211,
      "grad_norm": 1.0801807641983032,
      "learning_rate": 1.687955422203172e-05,
      "loss": 0.8761,
      "step": 10920
    },
    {
      "epoch": 2.342477496785255,
      "grad_norm": 35.49223327636719,
      "learning_rate": 1.6876696670952995e-05,
      "loss": 0.633,
      "step": 10930
    },
    {
      "epoch": 2.344620660094299,
      "grad_norm": 1.0431292057037354,
      "learning_rate": 1.687383911987427e-05,
      "loss": 0.2698,
      "step": 10940
    },
    {
      "epoch": 2.3467638234033434,
      "grad_norm": 29.846206665039062,
      "learning_rate": 1.6870981568795543e-05,
      "loss": 0.5711,
      "step": 10950
    },
    {
      "epoch": 2.3489069867123873,
      "grad_norm": 30.760534286499023,
      "learning_rate": 1.6868124017716817e-05,
      "loss": 1.1162,
      "step": 10960
    },
    {
      "epoch": 2.3510501500214316,
      "grad_norm": 15.049727439880371,
      "learning_rate": 1.6865266466638094e-05,
      "loss": 0.6204,
      "step": 10970
    },
    {
      "epoch": 2.353193313330476,
      "grad_norm": 0.5344589948654175,
      "learning_rate": 1.6862408915559368e-05,
      "loss": 0.2837,
      "step": 10980
    },
    {
      "epoch": 2.35533647663952,
      "grad_norm": 0.800629198551178,
      "learning_rate": 1.6859551364480642e-05,
      "loss": 0.869,
      "step": 10990
    },
    {
      "epoch": 2.357479639948564,
      "grad_norm": 28.882686614990234,
      "learning_rate": 1.6856693813401916e-05,
      "loss": 1.0697,
      "step": 11000
    },
    {
      "epoch": 2.3596228032576083,
      "grad_norm": 4.128833770751953,
      "learning_rate": 1.685383626232319e-05,
      "loss": 0.5336,
      "step": 11010
    },
    {
      "epoch": 2.3617659665666526,
      "grad_norm": 2.2451727390289307,
      "learning_rate": 1.6850978711244467e-05,
      "loss": 0.2343,
      "step": 11020
    },
    {
      "epoch": 2.3639091298756965,
      "grad_norm": 0.9790228605270386,
      "learning_rate": 1.684812116016574e-05,
      "loss": 0.2665,
      "step": 11030
    },
    {
      "epoch": 2.3660522931847408,
      "grad_norm": 15.029911041259766,
      "learning_rate": 1.6845263609087015e-05,
      "loss": 0.445,
      "step": 11040
    },
    {
      "epoch": 2.368195456493785,
      "grad_norm": 15.40539836883545,
      "learning_rate": 1.684240605800829e-05,
      "loss": 0.4696,
      "step": 11050
    },
    {
      "epoch": 2.370338619802829,
      "grad_norm": 0.4115562438964844,
      "learning_rate": 1.6839548506929563e-05,
      "loss": 0.5045,
      "step": 11060
    },
    {
      "epoch": 2.372481783111873,
      "grad_norm": 0.38230493664741516,
      "learning_rate": 1.6836690955850837e-05,
      "loss": 0.6338,
      "step": 11070
    },
    {
      "epoch": 2.3746249464209175,
      "grad_norm": 15.110050201416016,
      "learning_rate": 1.683383340477211e-05,
      "loss": 0.6313,
      "step": 11080
    },
    {
      "epoch": 2.3767681097299613,
      "grad_norm": 0.3284468650817871,
      "learning_rate": 1.6830975853693384e-05,
      "loss": 0.3217,
      "step": 11090
    },
    {
      "epoch": 2.3789112730390056,
      "grad_norm": 0.30724668502807617,
      "learning_rate": 1.682811830261466e-05,
      "loss": 0.5227,
      "step": 11100
    },
    {
      "epoch": 2.38105443634805,
      "grad_norm": 31.02971839904785,
      "learning_rate": 1.6825260751535936e-05,
      "loss": 1.1598,
      "step": 11110
    },
    {
      "epoch": 2.3831975996570938,
      "grad_norm": 18.557531356811523,
      "learning_rate": 1.682240320045721e-05,
      "loss": 1.1533,
      "step": 11120
    },
    {
      "epoch": 2.385340762966138,
      "grad_norm": 0.4659944772720337,
      "learning_rate": 1.6819545649378484e-05,
      "loss": 0.1611,
      "step": 11130
    },
    {
      "epoch": 2.3874839262751824,
      "grad_norm": 0.49265822768211365,
      "learning_rate": 1.6816688098299757e-05,
      "loss": 0.1567,
      "step": 11140
    },
    {
      "epoch": 2.389627089584226,
      "grad_norm": 15.079663276672363,
      "learning_rate": 1.681383054722103e-05,
      "loss": 0.4955,
      "step": 11150
    },
    {
      "epoch": 2.3917702528932705,
      "grad_norm": 0.2841522693634033,
      "learning_rate": 1.681097299614231e-05,
      "loss": 0.3458,
      "step": 11160
    },
    {
      "epoch": 2.393913416202315,
      "grad_norm": 0.26438772678375244,
      "learning_rate": 1.6808115445063583e-05,
      "loss": 0.6787,
      "step": 11170
    },
    {
      "epoch": 2.3960565795113586,
      "grad_norm": 0.29146450757980347,
      "learning_rate": 1.6805257893984857e-05,
      "loss": 0.6807,
      "step": 11180
    },
    {
      "epoch": 2.398199742820403,
      "grad_norm": 0.3918643891811371,
      "learning_rate": 1.680240034290613e-05,
      "loss": 0.9806,
      "step": 11190
    },
    {
      "epoch": 2.4003429061294472,
      "grad_norm": 0.4341770112514496,
      "learning_rate": 1.6799542791827404e-05,
      "loss": 0.9476,
      "step": 11200
    },
    {
      "epoch": 2.402486069438491,
      "grad_norm": 0.4800720512866974,
      "learning_rate": 1.6796685240748682e-05,
      "loss": 0.4605,
      "step": 11210
    },
    {
      "epoch": 2.4046292327475354,
      "grad_norm": 0.4252166450023651,
      "learning_rate": 1.6793827689669956e-05,
      "loss": 0.1635,
      "step": 11220
    },
    {
      "epoch": 2.4067723960565797,
      "grad_norm": 0.3219963014125824,
      "learning_rate": 1.679097013859123e-05,
      "loss": 0.1699,
      "step": 11230
    },
    {
      "epoch": 2.4089155593656235,
      "grad_norm": 0.30321869254112244,
      "learning_rate": 1.6788112587512503e-05,
      "loss": 0.5047,
      "step": 11240
    },
    {
      "epoch": 2.411058722674668,
      "grad_norm": 14.775243759155273,
      "learning_rate": 1.6785255036433777e-05,
      "loss": 1.0013,
      "step": 11250
    },
    {
      "epoch": 2.413201885983712,
      "grad_norm": 14.676817893981934,
      "learning_rate": 1.6782397485355055e-05,
      "loss": 1.1198,
      "step": 11260
    },
    {
      "epoch": 2.415345049292756,
      "grad_norm": 0.5544031858444214,
      "learning_rate": 1.677953993427633e-05,
      "loss": 0.6031,
      "step": 11270
    },
    {
      "epoch": 2.4174882126018002,
      "grad_norm": 0.7027775049209595,
      "learning_rate": 1.67766823831976e-05,
      "loss": 0.4577,
      "step": 11280
    },
    {
      "epoch": 2.4196313759108445,
      "grad_norm": 0.7601580023765564,
      "learning_rate": 1.6773824832118873e-05,
      "loss": 0.7109,
      "step": 11290
    },
    {
      "epoch": 2.4217745392198884,
      "grad_norm": 0.6573916077613831,
      "learning_rate": 1.677096728104015e-05,
      "loss": 0.1624,
      "step": 11300
    },
    {
      "epoch": 2.4239177025289327,
      "grad_norm": 0.5355845093727112,
      "learning_rate": 1.6768109729961424e-05,
      "loss": 0.7489,
      "step": 11310
    },
    {
      "epoch": 2.426060865837977,
      "grad_norm": 0.6468101143836975,
      "learning_rate": 1.6765252178882698e-05,
      "loss": 0.7719,
      "step": 11320
    },
    {
      "epoch": 2.428204029147021,
      "grad_norm": 14.584027290344238,
      "learning_rate": 1.6762394627803972e-05,
      "loss": 0.858,
      "step": 11330
    },
    {
      "epoch": 2.430347192456065,
      "grad_norm": 0.6265973448753357,
      "learning_rate": 1.6759537076725246e-05,
      "loss": 0.5848,
      "step": 11340
    },
    {
      "epoch": 2.4324903557651094,
      "grad_norm": 15.112320899963379,
      "learning_rate": 1.6756679525646523e-05,
      "loss": 0.4079,
      "step": 11350
    },
    {
      "epoch": 2.4346335190741533,
      "grad_norm": 29.55462074279785,
      "learning_rate": 1.6753821974567797e-05,
      "loss": 0.4725,
      "step": 11360
    },
    {
      "epoch": 2.4367766823831976,
      "grad_norm": 16.2159423828125,
      "learning_rate": 1.675096442348907e-05,
      "loss": 0.539,
      "step": 11370
    },
    {
      "epoch": 2.438919845692242,
      "grad_norm": 14.946588516235352,
      "learning_rate": 1.6748106872410345e-05,
      "loss": 0.7352,
      "step": 11380
    },
    {
      "epoch": 2.4410630090012857,
      "grad_norm": 15.272133827209473,
      "learning_rate": 1.674524932133162e-05,
      "loss": 0.7643,
      "step": 11390
    },
    {
      "epoch": 2.44320617231033,
      "grad_norm": 0.875811755657196,
      "learning_rate": 1.6742391770252897e-05,
      "loss": 0.7531,
      "step": 11400
    },
    {
      "epoch": 2.4453493356193743,
      "grad_norm": 0.8063347935676575,
      "learning_rate": 1.673953421917417e-05,
      "loss": 0.4306,
      "step": 11410
    },
    {
      "epoch": 2.447492498928418,
      "grad_norm": 0.44884827733039856,
      "learning_rate": 1.6736676668095444e-05,
      "loss": 0.254,
      "step": 11420
    },
    {
      "epoch": 2.4496356622374624,
      "grad_norm": 14.950733184814453,
      "learning_rate": 1.6733819117016718e-05,
      "loss": 0.4738,
      "step": 11430
    },
    {
      "epoch": 2.4517788255465067,
      "grad_norm": 14.842452049255371,
      "learning_rate": 1.6730961565937992e-05,
      "loss": 0.8139,
      "step": 11440
    },
    {
      "epoch": 2.4539219888555506,
      "grad_norm": 16.568702697753906,
      "learning_rate": 1.6728104014859266e-05,
      "loss": 0.4042,
      "step": 11450
    },
    {
      "epoch": 2.456065152164595,
      "grad_norm": 0.31170654296875,
      "learning_rate": 1.6725246463780543e-05,
      "loss": 0.1728,
      "step": 11460
    },
    {
      "epoch": 2.458208315473639,
      "grad_norm": 0.5862931609153748,
      "learning_rate": 1.6722388912701817e-05,
      "loss": 0.3017,
      "step": 11470
    },
    {
      "epoch": 2.460351478782683,
      "grad_norm": 0.3499930799007416,
      "learning_rate": 1.671953136162309e-05,
      "loss": 1.0886,
      "step": 11480
    },
    {
      "epoch": 2.4624946420917273,
      "grad_norm": 0.6986157894134521,
      "learning_rate": 1.6716673810544365e-05,
      "loss": 0.7593,
      "step": 11490
    },
    {
      "epoch": 2.4646378054007716,
      "grad_norm": 0.3117509186267853,
      "learning_rate": 1.671381625946564e-05,
      "loss": 0.3357,
      "step": 11500
    },
    {
      "epoch": 2.4667809687098154,
      "grad_norm": 0.3027786612510681,
      "learning_rate": 1.6710958708386913e-05,
      "loss": 0.5366,
      "step": 11510
    },
    {
      "epoch": 2.4689241320188597,
      "grad_norm": 0.6882714629173279,
      "learning_rate": 1.6708101157308187e-05,
      "loss": 0.9121,
      "step": 11520
    },
    {
      "epoch": 2.471067295327904,
      "grad_norm": 0.7082361578941345,
      "learning_rate": 1.670524360622946e-05,
      "loss": 1.0056,
      "step": 11530
    },
    {
      "epoch": 2.4732104586369483,
      "grad_norm": 0.7881935834884644,
      "learning_rate": 1.6702386055150738e-05,
      "loss": 0.5361,
      "step": 11540
    },
    {
      "epoch": 2.475353621945992,
      "grad_norm": 0.30543315410614014,
      "learning_rate": 1.6699528504072012e-05,
      "loss": 0.1319,
      "step": 11550
    },
    {
      "epoch": 2.4774967852550365,
      "grad_norm": 0.2670971751213074,
      "learning_rate": 1.6696670952993286e-05,
      "loss": 0.5164,
      "step": 11560
    },
    {
      "epoch": 2.4796399485640808,
      "grad_norm": 14.938395500183105,
      "learning_rate": 1.669381340191456e-05,
      "loss": 0.4762,
      "step": 11570
    },
    {
      "epoch": 2.4817831118731246,
      "grad_norm": 0.5156271457672119,
      "learning_rate": 1.6690955850835834e-05,
      "loss": 0.605,
      "step": 11580
    },
    {
      "epoch": 2.483926275182169,
      "grad_norm": 14.674254417419434,
      "learning_rate": 1.6688098299757108e-05,
      "loss": 0.7634,
      "step": 11590
    },
    {
      "epoch": 2.486069438491213,
      "grad_norm": 0.31982678174972534,
      "learning_rate": 1.6685240748678385e-05,
      "loss": 0.4627,
      "step": 11600
    },
    {
      "epoch": 2.488212601800257,
      "grad_norm": 14.510782241821289,
      "learning_rate": 1.668238319759966e-05,
      "loss": 0.4566,
      "step": 11610
    },
    {
      "epoch": 2.4903557651093013,
      "grad_norm": 0.17408183217048645,
      "learning_rate": 1.6679525646520933e-05,
      "loss": 0.1256,
      "step": 11620
    },
    {
      "epoch": 2.4924989284183456,
      "grad_norm": 0.0756017416715622,
      "learning_rate": 1.6676668095442207e-05,
      "loss": 0.5706,
      "step": 11630
    },
    {
      "epoch": 2.4946420917273895,
      "grad_norm": 0.06783474236726761,
      "learning_rate": 1.667381054436348e-05,
      "loss": 0.0057,
      "step": 11640
    },
    {
      "epoch": 2.496785255036434,
      "grad_norm": 15.518903732299805,
      "learning_rate": 1.6670952993284758e-05,
      "loss": 0.7959,
      "step": 11650
    },
    {
      "epoch": 2.498928418345478,
      "grad_norm": 0.0908135324716568,
      "learning_rate": 1.6668095442206032e-05,
      "loss": 0.6868,
      "step": 11660
    },
    {
      "epoch": 2.501071581654522,
      "grad_norm": 16.411209106445312,
      "learning_rate": 1.6665237891127306e-05,
      "loss": 0.653,
      "step": 11670
    },
    {
      "epoch": 2.503214744963566,
      "grad_norm": 16.798568725585938,
      "learning_rate": 1.666238034004858e-05,
      "loss": 0.6531,
      "step": 11680
    },
    {
      "epoch": 2.5053579082726105,
      "grad_norm": 0.33720463514328003,
      "learning_rate": 1.6659522788969854e-05,
      "loss": 0.2994,
      "step": 11690
    },
    {
      "epoch": 2.5075010715816544,
      "grad_norm": 30.655664443969727,
      "learning_rate": 1.665666523789113e-05,
      "loss": 1.1385,
      "step": 11700
    },
    {
      "epoch": 2.5096442348906987,
      "grad_norm": 0.9400365948677063,
      "learning_rate": 1.6653807686812402e-05,
      "loss": 0.2695,
      "step": 11710
    },
    {
      "epoch": 2.511787398199743,
      "grad_norm": 14.827738761901855,
      "learning_rate": 1.6650950135733676e-05,
      "loss": 0.4488,
      "step": 11720
    },
    {
      "epoch": 2.5139305615087872,
      "grad_norm": 0.7719284892082214,
      "learning_rate": 1.664809258465495e-05,
      "loss": 1.0387,
      "step": 11730
    },
    {
      "epoch": 2.516073724817831,
      "grad_norm": 0.34124135971069336,
      "learning_rate": 1.6645235033576227e-05,
      "loss": 0.5073,
      "step": 11740
    },
    {
      "epoch": 2.5182168881268754,
      "grad_norm": 0.36202186346054077,
      "learning_rate": 1.66423774824975e-05,
      "loss": 0.5367,
      "step": 11750
    },
    {
      "epoch": 2.5203600514359197,
      "grad_norm": 0.30398890376091003,
      "learning_rate": 1.6639519931418775e-05,
      "loss": 0.8801,
      "step": 11760
    },
    {
      "epoch": 2.5225032147449635,
      "grad_norm": 43.57715606689453,
      "learning_rate": 1.663666238034005e-05,
      "loss": 0.8499,
      "step": 11770
    },
    {
      "epoch": 2.524646378054008,
      "grad_norm": 15.465877532958984,
      "learning_rate": 1.6633804829261323e-05,
      "loss": 0.47,
      "step": 11780
    },
    {
      "epoch": 2.526789541363052,
      "grad_norm": 28.696557998657227,
      "learning_rate": 1.66309472781826e-05,
      "loss": 0.8876,
      "step": 11790
    },
    {
      "epoch": 2.528932704672096,
      "grad_norm": 45.910980224609375,
      "learning_rate": 1.6628089727103874e-05,
      "loss": 0.6678,
      "step": 11800
    },
    {
      "epoch": 2.5310758679811403,
      "grad_norm": 0.37700027227401733,
      "learning_rate": 1.6625232176025148e-05,
      "loss": 1.0329,
      "step": 11810
    },
    {
      "epoch": 2.5332190312901846,
      "grad_norm": 15.13520622253418,
      "learning_rate": 1.6622374624946422e-05,
      "loss": 0.6474,
      "step": 11820
    },
    {
      "epoch": 2.5353621945992284,
      "grad_norm": 16.029191970825195,
      "learning_rate": 1.6619517073867696e-05,
      "loss": 0.3515,
      "step": 11830
    },
    {
      "epoch": 2.5375053579082727,
      "grad_norm": 1.7706762552261353,
      "learning_rate": 1.6616659522788973e-05,
      "loss": 0.982,
      "step": 11840
    },
    {
      "epoch": 2.539648521217317,
      "grad_norm": 2.2019364833831787,
      "learning_rate": 1.6613801971710247e-05,
      "loss": 1.0814,
      "step": 11850
    },
    {
      "epoch": 2.541791684526361,
      "grad_norm": 1.1188082695007324,
      "learning_rate": 1.661094442063152e-05,
      "loss": 0.6173,
      "step": 11860
    },
    {
      "epoch": 2.543934847835405,
      "grad_norm": 13.751022338867188,
      "learning_rate": 1.6608086869552795e-05,
      "loss": 0.2602,
      "step": 11870
    },
    {
      "epoch": 2.5460780111444494,
      "grad_norm": 0.3135767877101898,
      "learning_rate": 1.660522931847407e-05,
      "loss": 0.1128,
      "step": 11880
    },
    {
      "epoch": 2.5482211744534933,
      "grad_norm": 0.26788684725761414,
      "learning_rate": 1.6602371767395343e-05,
      "loss": 0.5485,
      "step": 11890
    },
    {
      "epoch": 2.5503643377625376,
      "grad_norm": 0.7556714415550232,
      "learning_rate": 1.659951421631662e-05,
      "loss": 0.3262,
      "step": 11900
    },
    {
      "epoch": 2.552507501071582,
      "grad_norm": 19.853893280029297,
      "learning_rate": 1.6596656665237894e-05,
      "loss": 0.3452,
      "step": 11910
    },
    {
      "epoch": 2.5546506643806257,
      "grad_norm": 15.059954643249512,
      "learning_rate": 1.6593799114159164e-05,
      "loss": 0.6527,
      "step": 11920
    },
    {
      "epoch": 2.55679382768967,
      "grad_norm": 0.32535871863365173,
      "learning_rate": 1.6590941563080442e-05,
      "loss": 0.5144,
      "step": 11930
    },
    {
      "epoch": 2.5589369909987143,
      "grad_norm": 16.487009048461914,
      "learning_rate": 1.6588084012001716e-05,
      "loss": 0.6185,
      "step": 11940
    },
    {
      "epoch": 2.561080154307758,
      "grad_norm": 30.22410011291504,
      "learning_rate": 1.658522646092299e-05,
      "loss": 0.6353,
      "step": 11950
    },
    {
      "epoch": 2.5632233176168024,
      "grad_norm": 0.7246966361999512,
      "learning_rate": 1.6582368909844264e-05,
      "loss": 1.2105,
      "step": 11960
    },
    {
      "epoch": 2.5653664809258467,
      "grad_norm": 33.702423095703125,
      "learning_rate": 1.6579511358765538e-05,
      "loss": 0.4125,
      "step": 11970
    },
    {
      "epoch": 2.5675096442348906,
      "grad_norm": 16.115633010864258,
      "learning_rate": 1.6576653807686815e-05,
      "loss": 0.5777,
      "step": 11980
    },
    {
      "epoch": 2.569652807543935,
      "grad_norm": 15.087018966674805,
      "learning_rate": 1.657379625660809e-05,
      "loss": 0.6948,
      "step": 11990
    },
    {
      "epoch": 2.571795970852979,
      "grad_norm": 0.6790884733200073,
      "learning_rate": 1.6570938705529363e-05,
      "loss": 0.453,
      "step": 12000
    },
    {
      "epoch": 2.573939134162023,
      "grad_norm": 0.6953508853912354,
      "learning_rate": 1.6568081154450637e-05,
      "loss": 0.271,
      "step": 12010
    },
    {
      "epoch": 2.5760822974710673,
      "grad_norm": 0.680848240852356,
      "learning_rate": 1.656522360337191e-05,
      "loss": 0.6165,
      "step": 12020
    },
    {
      "epoch": 2.5782254607801116,
      "grad_norm": 0.17895367741584778,
      "learning_rate": 1.6562366052293184e-05,
      "loss": 0.4393,
      "step": 12030
    },
    {
      "epoch": 2.5803686240891555,
      "grad_norm": 0.28079095482826233,
      "learning_rate": 1.6559508501214462e-05,
      "loss": 0.4641,
      "step": 12040
    },
    {
      "epoch": 2.5825117873981998,
      "grad_norm": 16.10148048400879,
      "learning_rate": 1.6556650950135736e-05,
      "loss": 1.2678,
      "step": 12050
    },
    {
      "epoch": 2.584654950707244,
      "grad_norm": 0.4334862232208252,
      "learning_rate": 1.655379339905701e-05,
      "loss": 0.2927,
      "step": 12060
    },
    {
      "epoch": 2.586798114016288,
      "grad_norm": 14.982719421386719,
      "learning_rate": 1.6550935847978284e-05,
      "loss": 1.0677,
      "step": 12070
    },
    {
      "epoch": 2.588941277325332,
      "grad_norm": 0.4749464690685272,
      "learning_rate": 1.6548078296899558e-05,
      "loss": 1.1416,
      "step": 12080
    },
    {
      "epoch": 2.5910844406343765,
      "grad_norm": 16.101037979125977,
      "learning_rate": 1.6545220745820835e-05,
      "loss": 0.5515,
      "step": 12090
    },
    {
      "epoch": 2.5932276039434203,
      "grad_norm": 0.6563510298728943,
      "learning_rate": 1.654236319474211e-05,
      "loss": 0.494,
      "step": 12100
    },
    {
      "epoch": 2.5953707672524646,
      "grad_norm": 0.6338124871253967,
      "learning_rate": 1.6539505643663383e-05,
      "loss": 0.6612,
      "step": 12110
    },
    {
      "epoch": 2.597513930561509,
      "grad_norm": 15.459325790405273,
      "learning_rate": 1.6536648092584657e-05,
      "loss": 0.9943,
      "step": 12120
    },
    {
      "epoch": 2.5996570938705528,
      "grad_norm": 0.6598973274230957,
      "learning_rate": 1.653379054150593e-05,
      "loss": 0.524,
      "step": 12130
    },
    {
      "epoch": 2.601800257179597,
      "grad_norm": 14.078262329101562,
      "learning_rate": 1.6530932990427204e-05,
      "loss": 0.6154,
      "step": 12140
    },
    {
      "epoch": 2.6039434204886414,
      "grad_norm": 30.087095260620117,
      "learning_rate": 1.652807543934848e-05,
      "loss": 0.4646,
      "step": 12150
    },
    {
      "epoch": 2.606086583797685,
      "grad_norm": 8.98912525177002,
      "learning_rate": 1.6525217888269752e-05,
      "loss": 0.7069,
      "step": 12160
    },
    {
      "epoch": 2.6082297471067295,
      "grad_norm": 15.499136924743652,
      "learning_rate": 1.6522360337191026e-05,
      "loss": 0.5729,
      "step": 12170
    },
    {
      "epoch": 2.610372910415774,
      "grad_norm": 0.3618684709072113,
      "learning_rate": 1.6519502786112304e-05,
      "loss": 0.453,
      "step": 12180
    },
    {
      "epoch": 2.6125160737248176,
      "grad_norm": 0.7401379942893982,
      "learning_rate": 1.6516645235033577e-05,
      "loss": 0.5147,
      "step": 12190
    },
    {
      "epoch": 2.614659237033862,
      "grad_norm": 0.3594028949737549,
      "learning_rate": 1.651378768395485e-05,
      "loss": 0.5048,
      "step": 12200
    },
    {
      "epoch": 2.6168024003429062,
      "grad_norm": 55.26193618774414,
      "learning_rate": 1.6510930132876125e-05,
      "loss": 0.9576,
      "step": 12210
    },
    {
      "epoch": 2.61894556365195,
      "grad_norm": 0.4800446629524231,
      "learning_rate": 1.65080725817974e-05,
      "loss": 0.4437,
      "step": 12220
    },
    {
      "epoch": 2.6210887269609944,
      "grad_norm": 0.4710058271884918,
      "learning_rate": 1.6505215030718677e-05,
      "loss": 0.7462,
      "step": 12230
    },
    {
      "epoch": 2.6232318902700387,
      "grad_norm": 0.5073912739753723,
      "learning_rate": 1.650235747963995e-05,
      "loss": 0.5285,
      "step": 12240
    },
    {
      "epoch": 2.6253750535790825,
      "grad_norm": 0.525599479675293,
      "learning_rate": 1.6499499928561224e-05,
      "loss": 0.3183,
      "step": 12250
    },
    {
      "epoch": 2.627518216888127,
      "grad_norm": 14.916919708251953,
      "learning_rate": 1.64966423774825e-05,
      "loss": 0.5797,
      "step": 12260
    },
    {
      "epoch": 2.629661380197171,
      "grad_norm": 0.4740760326385498,
      "learning_rate": 1.6493784826403772e-05,
      "loss": 0.4265,
      "step": 12270
    },
    {
      "epoch": 2.631804543506215,
      "grad_norm": 1.1899234056472778,
      "learning_rate": 1.649092727532505e-05,
      "loss": 0.8539,
      "step": 12280
    },
    {
      "epoch": 2.6339477068152592,
      "grad_norm": 0.9144578576087952,
      "learning_rate": 1.6488069724246324e-05,
      "loss": 0.8201,
      "step": 12290
    },
    {
      "epoch": 2.6360908701243035,
      "grad_norm": 0.40598195791244507,
      "learning_rate": 1.6485212173167597e-05,
      "loss": 0.1256,
      "step": 12300
    },
    {
      "epoch": 2.6382340334333474,
      "grad_norm": 0.39599254727363586,
      "learning_rate": 1.648235462208887e-05,
      "loss": 0.5483,
      "step": 12310
    },
    {
      "epoch": 2.6403771967423917,
      "grad_norm": 0.3426834046840668,
      "learning_rate": 1.6479497071010145e-05,
      "loss": 0.6109,
      "step": 12320
    },
    {
      "epoch": 2.642520360051436,
      "grad_norm": 15.009767532348633,
      "learning_rate": 1.6476639519931423e-05,
      "loss": 0.3415,
      "step": 12330
    },
    {
      "epoch": 2.64466352336048,
      "grad_norm": 0.9097475409507751,
      "learning_rate": 1.6473781968852697e-05,
      "loss": 0.3439,
      "step": 12340
    },
    {
      "epoch": 2.646806686669524,
      "grad_norm": 15.329994201660156,
      "learning_rate": 1.6470924417773967e-05,
      "loss": 0.958,
      "step": 12350
    },
    {
      "epoch": 2.6489498499785684,
      "grad_norm": 14.200325965881348,
      "learning_rate": 1.646806686669524e-05,
      "loss": 0.4113,
      "step": 12360
    },
    {
      "epoch": 2.6510930132876123,
      "grad_norm": 0.32124069333076477,
      "learning_rate": 1.646520931561652e-05,
      "loss": 0.461,
      "step": 12370
    },
    {
      "epoch": 2.6532361765966566,
      "grad_norm": 0.303661584854126,
      "learning_rate": 1.6462351764537792e-05,
      "loss": 0.4116,
      "step": 12380
    },
    {
      "epoch": 2.655379339905701,
      "grad_norm": 15.079812049865723,
      "learning_rate": 1.6459494213459066e-05,
      "loss": 0.3519,
      "step": 12390
    },
    {
      "epoch": 2.6575225032147447,
      "grad_norm": 14.987679481506348,
      "learning_rate": 1.645663666238034e-05,
      "loss": 0.5321,
      "step": 12400
    },
    {
      "epoch": 2.659665666523789,
      "grad_norm": 30.679420471191406,
      "learning_rate": 1.6453779111301614e-05,
      "loss": 0.6067,
      "step": 12410
    },
    {
      "epoch": 2.6618088298328333,
      "grad_norm": 0.5958520770072937,
      "learning_rate": 1.645092156022289e-05,
      "loss": 0.828,
      "step": 12420
    },
    {
      "epoch": 2.663951993141877,
      "grad_norm": 16.109066009521484,
      "learning_rate": 1.6448064009144165e-05,
      "loss": 0.7565,
      "step": 12430
    },
    {
      "epoch": 2.6660951564509214,
      "grad_norm": 15.035196304321289,
      "learning_rate": 1.644520645806544e-05,
      "loss": 0.4504,
      "step": 12440
    },
    {
      "epoch": 2.6682383197599657,
      "grad_norm": 14.436039924621582,
      "learning_rate": 1.6442348906986713e-05,
      "loss": 0.7415,
      "step": 12450
    },
    {
      "epoch": 2.67038148306901,
      "grad_norm": 14.018715858459473,
      "learning_rate": 1.6439491355907987e-05,
      "loss": 0.4929,
      "step": 12460
    },
    {
      "epoch": 2.672524646378054,
      "grad_norm": 0.3873598873615265,
      "learning_rate": 1.6436633804829264e-05,
      "loss": 0.5365,
      "step": 12470
    },
    {
      "epoch": 2.674667809687098,
      "grad_norm": 0.31453561782836914,
      "learning_rate": 1.643377625375054e-05,
      "loss": 0.4379,
      "step": 12480
    },
    {
      "epoch": 2.6768109729961425,
      "grad_norm": 16.749656677246094,
      "learning_rate": 1.6430918702671812e-05,
      "loss": 0.532,
      "step": 12490
    },
    {
      "epoch": 2.6789541363051863,
      "grad_norm": 0.8777009844779968,
      "learning_rate": 1.6428061151593086e-05,
      "loss": 0.6174,
      "step": 12500
    },
    {
      "epoch": 2.6810972996142306,
      "grad_norm": 0.3159029483795166,
      "learning_rate": 1.642520360051436e-05,
      "loss": 0.2926,
      "step": 12510
    },
    {
      "epoch": 2.683240462923275,
      "grad_norm": 15.882587432861328,
      "learning_rate": 1.6422346049435634e-05,
      "loss": 0.6375,
      "step": 12520
    },
    {
      "epoch": 2.6853836262323187,
      "grad_norm": 0.32939204573631287,
      "learning_rate": 1.641948849835691e-05,
      "loss": 0.6582,
      "step": 12530
    },
    {
      "epoch": 2.687526789541363,
      "grad_norm": 0.2700428068637848,
      "learning_rate": 1.6416630947278185e-05,
      "loss": 0.509,
      "step": 12540
    },
    {
      "epoch": 2.6896699528504073,
      "grad_norm": 0.2710934579372406,
      "learning_rate": 1.641377339619946e-05,
      "loss": 0.6889,
      "step": 12550
    },
    {
      "epoch": 2.6918131161594516,
      "grad_norm": 0.3060287535190582,
      "learning_rate": 1.6410915845120733e-05,
      "loss": 0.8729,
      "step": 12560
    },
    {
      "epoch": 2.6939562794684955,
      "grad_norm": 15.784402847290039,
      "learning_rate": 1.6408058294042007e-05,
      "loss": 0.8272,
      "step": 12570
    },
    {
      "epoch": 2.6960994427775398,
      "grad_norm": 0.413420706987381,
      "learning_rate": 1.640520074296328e-05,
      "loss": 0.5929,
      "step": 12580
    },
    {
      "epoch": 2.698242606086584,
      "grad_norm": 0.40933695435523987,
      "learning_rate": 1.6402343191884555e-05,
      "loss": 0.7902,
      "step": 12590
    },
    {
      "epoch": 2.700385769395628,
      "grad_norm": 0.4647010564804077,
      "learning_rate": 1.639948564080583e-05,
      "loss": 0.6256,
      "step": 12600
    },
    {
      "epoch": 2.702528932704672,
      "grad_norm": 0.5276395082473755,
      "learning_rate": 1.6396628089727106e-05,
      "loss": 0.6078,
      "step": 12610
    },
    {
      "epoch": 2.7046720960137165,
      "grad_norm": 0.47124016284942627,
      "learning_rate": 1.639377053864838e-05,
      "loss": 0.4081,
      "step": 12620
    },
    {
      "epoch": 2.7068152593227603,
      "grad_norm": 0.3785841464996338,
      "learning_rate": 1.6390912987569654e-05,
      "loss": 0.3263,
      "step": 12630
    },
    {
      "epoch": 2.7089584226318046,
      "grad_norm": 14.714275360107422,
      "learning_rate": 1.6388055436490928e-05,
      "loss": 0.4512,
      "step": 12640
    },
    {
      "epoch": 2.711101585940849,
      "grad_norm": 15.364367485046387,
      "learning_rate": 1.6385197885412202e-05,
      "loss": 1.019,
      "step": 12650
    },
    {
      "epoch": 2.713244749249893,
      "grad_norm": 0.4000648558139801,
      "learning_rate": 1.6382340334333476e-05,
      "loss": 0.4851,
      "step": 12660
    },
    {
      "epoch": 2.715387912558937,
      "grad_norm": 29.7266902923584,
      "learning_rate": 1.6379482783254753e-05,
      "loss": 0.6314,
      "step": 12670
    },
    {
      "epoch": 2.7175310758679814,
      "grad_norm": 0.41348809003829956,
      "learning_rate": 1.6376625232176027e-05,
      "loss": 0.3161,
      "step": 12680
    },
    {
      "epoch": 2.719674239177025,
      "grad_norm": 0.35372406244277954,
      "learning_rate": 1.63737676810973e-05,
      "loss": 0.5126,
      "step": 12690
    },
    {
      "epoch": 2.7218174024860695,
      "grad_norm": 0.37942007184028625,
      "learning_rate": 1.6370910130018575e-05,
      "loss": 0.426,
      "step": 12700
    },
    {
      "epoch": 2.723960565795114,
      "grad_norm": 14.851960182189941,
      "learning_rate": 1.636805257893985e-05,
      "loss": 0.8643,
      "step": 12710
    },
    {
      "epoch": 2.7261037291041577,
      "grad_norm": 0.36211296916007996,
      "learning_rate": 1.6365195027861126e-05,
      "loss": 0.8145,
      "step": 12720
    },
    {
      "epoch": 2.728246892413202,
      "grad_norm": 16.410526275634766,
      "learning_rate": 1.63623374767824e-05,
      "loss": 1.0832,
      "step": 12730
    },
    {
      "epoch": 2.7303900557222462,
      "grad_norm": 0.6269435882568359,
      "learning_rate": 1.6359479925703674e-05,
      "loss": 0.6916,
      "step": 12740
    },
    {
      "epoch": 2.73253321903129,
      "grad_norm": 28.227821350097656,
      "learning_rate": 1.6356622374624948e-05,
      "loss": 0.584,
      "step": 12750
    },
    {
      "epoch": 2.7346763823403344,
      "grad_norm": 0.6551559567451477,
      "learning_rate": 1.6353764823546222e-05,
      "loss": 0.5336,
      "step": 12760
    },
    {
      "epoch": 2.7368195456493787,
      "grad_norm": 15.206975936889648,
      "learning_rate": 1.63509072724675e-05,
      "loss": 0.4595,
      "step": 12770
    },
    {
      "epoch": 2.7389627089584225,
      "grad_norm": 0.45041289925575256,
      "learning_rate": 1.634804972138877e-05,
      "loss": 0.426,
      "step": 12780
    },
    {
      "epoch": 2.741105872267467,
      "grad_norm": 29.77650260925293,
      "learning_rate": 1.6345192170310044e-05,
      "loss": 0.8894,
      "step": 12790
    },
    {
      "epoch": 2.743249035576511,
      "grad_norm": 0.4529705047607422,
      "learning_rate": 1.6342334619231318e-05,
      "loss": 0.633,
      "step": 12800
    },
    {
      "epoch": 2.745392198885555,
      "grad_norm": 0.5038024187088013,
      "learning_rate": 1.6339477068152595e-05,
      "loss": 0.8426,
      "step": 12810
    },
    {
      "epoch": 2.7475353621945993,
      "grad_norm": 15.001253128051758,
      "learning_rate": 1.633661951707387e-05,
      "loss": 0.9505,
      "step": 12820
    },
    {
      "epoch": 2.7496785255036436,
      "grad_norm": 0.6738041043281555,
      "learning_rate": 1.6333761965995143e-05,
      "loss": 0.3974,
      "step": 12830
    },
    {
      "epoch": 2.7518216888126874,
      "grad_norm": 14.29434585571289,
      "learning_rate": 1.6330904414916417e-05,
      "loss": 0.4759,
      "step": 12840
    },
    {
      "epoch": 2.7539648521217317,
      "grad_norm": 0.4976409673690796,
      "learning_rate": 1.632804686383769e-05,
      "loss": 0.6073,
      "step": 12850
    },
    {
      "epoch": 2.756108015430776,
      "grad_norm": 14.485566139221191,
      "learning_rate": 1.6325189312758968e-05,
      "loss": 0.674,
      "step": 12860
    },
    {
      "epoch": 2.75825117873982,
      "grad_norm": 0.43925604224205017,
      "learning_rate": 1.6322331761680242e-05,
      "loss": 0.7369,
      "step": 12870
    },
    {
      "epoch": 2.760394342048864,
      "grad_norm": 15.133960723876953,
      "learning_rate": 1.6319474210601516e-05,
      "loss": 0.9972,
      "step": 12880
    },
    {
      "epoch": 2.7625375053579084,
      "grad_norm": 14.732636451721191,
      "learning_rate": 1.631661665952279e-05,
      "loss": 0.8501,
      "step": 12890
    },
    {
      "epoch": 2.7646806686669523,
      "grad_norm": 14.529596328735352,
      "learning_rate": 1.6313759108444064e-05,
      "loss": 0.6749,
      "step": 12900
    },
    {
      "epoch": 2.7668238319759966,
      "grad_norm": 0.6130014657974243,
      "learning_rate": 1.631090155736534e-05,
      "loss": 0.1186,
      "step": 12910
    },
    {
      "epoch": 2.768966995285041,
      "grad_norm": 16.022930145263672,
      "learning_rate": 1.6308044006286615e-05,
      "loss": 1.0294,
      "step": 12920
    },
    {
      "epoch": 2.7711101585940847,
      "grad_norm": 0.6684538722038269,
      "learning_rate": 1.630518645520789e-05,
      "loss": 0.6967,
      "step": 12930
    },
    {
      "epoch": 2.773253321903129,
      "grad_norm": 0.6701202988624573,
      "learning_rate": 1.6302328904129163e-05,
      "loss": 0.4048,
      "step": 12940
    },
    {
      "epoch": 2.7753964852121733,
      "grad_norm": 0.8873630166053772,
      "learning_rate": 1.6299471353050437e-05,
      "loss": 0.7218,
      "step": 12950
    },
    {
      "epoch": 2.777539648521217,
      "grad_norm": 0.4946395754814148,
      "learning_rate": 1.6296613801971714e-05,
      "loss": 0.8847,
      "step": 12960
    },
    {
      "epoch": 2.7796828118302614,
      "grad_norm": 0.7541233897209167,
      "learning_rate": 1.6293756250892988e-05,
      "loss": 0.5879,
      "step": 12970
    },
    {
      "epoch": 2.7818259751393057,
      "grad_norm": 0.6518023610115051,
      "learning_rate": 1.6290898699814262e-05,
      "loss": 0.4435,
      "step": 12980
    },
    {
      "epoch": 2.7839691384483496,
      "grad_norm": 0.5261711478233337,
      "learning_rate": 1.6288041148735532e-05,
      "loss": 0.2944,
      "step": 12990
    },
    {
      "epoch": 2.786112301757394,
      "grad_norm": 0.37594956159591675,
      "learning_rate": 1.628518359765681e-05,
      "loss": 0.7629,
      "step": 13000
    },
    {
      "epoch": 2.788255465066438,
      "grad_norm": 0.36944901943206787,
      "learning_rate": 1.6282326046578084e-05,
      "loss": 0.1421,
      "step": 13010
    },
    {
      "epoch": 2.790398628375482,
      "grad_norm": 0.2728171646595001,
      "learning_rate": 1.6279468495499358e-05,
      "loss": 0.4734,
      "step": 13020
    },
    {
      "epoch": 2.7925417916845263,
      "grad_norm": 0.4719139635562897,
      "learning_rate": 1.627661094442063e-05,
      "loss": 0.4792,
      "step": 13030
    },
    {
      "epoch": 2.7946849549935706,
      "grad_norm": 0.2861407399177551,
      "learning_rate": 1.6273753393341905e-05,
      "loss": 0.8034,
      "step": 13040
    },
    {
      "epoch": 2.7968281183026145,
      "grad_norm": 0.23565658926963806,
      "learning_rate": 1.6270895842263183e-05,
      "loss": 0.1391,
      "step": 13050
    },
    {
      "epoch": 2.7989712816116588,
      "grad_norm": 0.19927462935447693,
      "learning_rate": 1.6268038291184457e-05,
      "loss": 0.6588,
      "step": 13060
    },
    {
      "epoch": 2.801114444920703,
      "grad_norm": 15.100865364074707,
      "learning_rate": 1.626518074010573e-05,
      "loss": 1.2726,
      "step": 13070
    },
    {
      "epoch": 2.803257608229747,
      "grad_norm": 0.28823497891426086,
      "learning_rate": 1.6262323189027005e-05,
      "loss": 0.5178,
      "step": 13080
    },
    {
      "epoch": 2.805400771538791,
      "grad_norm": 0.5907468795776367,
      "learning_rate": 1.625946563794828e-05,
      "loss": 0.3025,
      "step": 13090
    },
    {
      "epoch": 2.8075439348478355,
      "grad_norm": 0.6291534304618835,
      "learning_rate": 1.6256608086869556e-05,
      "loss": 0.9234,
      "step": 13100
    },
    {
      "epoch": 2.8096870981568793,
      "grad_norm": 0.3259512484073639,
      "learning_rate": 1.625375053579083e-05,
      "loss": 0.1704,
      "step": 13110
    },
    {
      "epoch": 2.8118302614659236,
      "grad_norm": 0.33601003885269165,
      "learning_rate": 1.6250892984712104e-05,
      "loss": 0.5108,
      "step": 13120
    },
    {
      "epoch": 2.813973424774968,
      "grad_norm": 15.542263984680176,
      "learning_rate": 1.6248035433633378e-05,
      "loss": 0.8651,
      "step": 13130
    },
    {
      "epoch": 2.8161165880840118,
      "grad_norm": 0.993851363658905,
      "learning_rate": 1.624517788255465e-05,
      "loss": 1.0344,
      "step": 13140
    },
    {
      "epoch": 2.818259751393056,
      "grad_norm": 28.75887107849121,
      "learning_rate": 1.6242320331475925e-05,
      "loss": 0.9285,
      "step": 13150
    },
    {
      "epoch": 2.8204029147021004,
      "grad_norm": 13.981000900268555,
      "learning_rate": 1.6239462780397203e-05,
      "loss": 1.0542,
      "step": 13160
    },
    {
      "epoch": 2.822546078011144,
      "grad_norm": 0.9069790244102478,
      "learning_rate": 1.6236605229318477e-05,
      "loss": 0.5006,
      "step": 13170
    },
    {
      "epoch": 2.8246892413201885,
      "grad_norm": 0.7081723809242249,
      "learning_rate": 1.623374767823975e-05,
      "loss": 0.4463,
      "step": 13180
    },
    {
      "epoch": 2.826832404629233,
      "grad_norm": 0.5737764835357666,
      "learning_rate": 1.6230890127161025e-05,
      "loss": 0.3818,
      "step": 13190
    },
    {
      "epoch": 2.8289755679382766,
      "grad_norm": 14.193607330322266,
      "learning_rate": 1.62280325760823e-05,
      "loss": 0.4176,
      "step": 13200
    },
    {
      "epoch": 2.831118731247321,
      "grad_norm": 13.360898971557617,
      "learning_rate": 1.6225175025003572e-05,
      "loss": 0.3589,
      "step": 13210
    },
    {
      "epoch": 2.8332618945563652,
      "grad_norm": 17.818199157714844,
      "learning_rate": 1.6222317473924846e-05,
      "loss": 0.4584,
      "step": 13220
    },
    {
      "epoch": 2.835405057865409,
      "grad_norm": 29.910490036010742,
      "learning_rate": 1.621945992284612e-05,
      "loss": 0.9705,
      "step": 13230
    },
    {
      "epoch": 2.8375482211744534,
      "grad_norm": 0.2943834364414215,
      "learning_rate": 1.6216602371767398e-05,
      "loss": 0.6905,
      "step": 13240
    },
    {
      "epoch": 2.8396913844834977,
      "grad_norm": 30.792003631591797,
      "learning_rate": 1.621374482068867e-05,
      "loss": 1.3117,
      "step": 13250
    },
    {
      "epoch": 2.8418345477925415,
      "grad_norm": 0.8172852993011475,
      "learning_rate": 1.6210887269609945e-05,
      "loss": 0.4793,
      "step": 13260
    },
    {
      "epoch": 2.843977711101586,
      "grad_norm": 15.2092866897583,
      "learning_rate": 1.620802971853122e-05,
      "loss": 0.872,
      "step": 13270
    },
    {
      "epoch": 2.84612087441063,
      "grad_norm": 15.544856071472168,
      "learning_rate": 1.6205172167452493e-05,
      "loss": 0.266,
      "step": 13280
    },
    {
      "epoch": 2.8482640377196744,
      "grad_norm": 0.31423288583755493,
      "learning_rate": 1.6202314616373767e-05,
      "loss": 0.2324,
      "step": 13290
    },
    {
      "epoch": 2.8504072010287183,
      "grad_norm": 0.29901304841041565,
      "learning_rate": 1.6199457065295045e-05,
      "loss": 0.6729,
      "step": 13300
    },
    {
      "epoch": 2.8525503643377625,
      "grad_norm": 0.27058547735214233,
      "learning_rate": 1.619659951421632e-05,
      "loss": 0.5544,
      "step": 13310
    },
    {
      "epoch": 2.854693527646807,
      "grad_norm": 0.4705244302749634,
      "learning_rate": 1.6193741963137592e-05,
      "loss": 0.7016,
      "step": 13320
    },
    {
      "epoch": 2.8568366909558507,
      "grad_norm": 16.52926254272461,
      "learning_rate": 1.6190884412058866e-05,
      "loss": 0.6177,
      "step": 13330
    },
    {
      "epoch": 2.858979854264895,
      "grad_norm": 0.35433605313301086,
      "learning_rate": 1.618802686098014e-05,
      "loss": 0.706,
      "step": 13340
    },
    {
      "epoch": 2.8611230175739393,
      "grad_norm": 0.433529794216156,
      "learning_rate": 1.6185169309901418e-05,
      "loss": 0.4864,
      "step": 13350
    },
    {
      "epoch": 2.863266180882983,
      "grad_norm": 0.9613271951675415,
      "learning_rate": 1.618231175882269e-05,
      "loss": 0.8936,
      "step": 13360
    },
    {
      "epoch": 2.8654093441920274,
      "grad_norm": 0.5121837258338928,
      "learning_rate": 1.6179454207743965e-05,
      "loss": 0.6054,
      "step": 13370
    },
    {
      "epoch": 2.8675525075010717,
      "grad_norm": 0.6755224466323853,
      "learning_rate": 1.617659665666524e-05,
      "loss": 0.2595,
      "step": 13380
    },
    {
      "epoch": 2.869695670810116,
      "grad_norm": 0.4077709913253784,
      "learning_rate": 1.6173739105586513e-05,
      "loss": 0.0097,
      "step": 13390
    },
    {
      "epoch": 2.87183883411916,
      "grad_norm": 0.21450193226337433,
      "learning_rate": 1.617088155450779e-05,
      "loss": 0.6757,
      "step": 13400
    },
    {
      "epoch": 2.873981997428204,
      "grad_norm": 0.27593857049942017,
      "learning_rate": 1.6168024003429065e-05,
      "loss": 0.8212,
      "step": 13410
    },
    {
      "epoch": 2.8761251607372484,
      "grad_norm": 0.36713382601737976,
      "learning_rate": 1.6165166452350335e-05,
      "loss": 0.9714,
      "step": 13420
    },
    {
      "epoch": 2.8782683240462923,
      "grad_norm": 16.579574584960938,
      "learning_rate": 1.616230890127161e-05,
      "loss": 0.8958,
      "step": 13430
    },
    {
      "epoch": 2.8804114873553366,
      "grad_norm": 1.3613461256027222,
      "learning_rate": 1.6159451350192886e-05,
      "loss": 0.9768,
      "step": 13440
    },
    {
      "epoch": 2.882554650664381,
      "grad_norm": 14.61347770690918,
      "learning_rate": 1.615659379911416e-05,
      "loss": 0.3938,
      "step": 13450
    },
    {
      "epoch": 2.8846978139734247,
      "grad_norm": 0.5666264891624451,
      "learning_rate": 1.6153736248035434e-05,
      "loss": 0.2739,
      "step": 13460
    },
    {
      "epoch": 2.886840977282469,
      "grad_norm": 15.499170303344727,
      "learning_rate": 1.6150878696956708e-05,
      "loss": 0.891,
      "step": 13470
    },
    {
      "epoch": 2.8889841405915133,
      "grad_norm": 0.6557052135467529,
      "learning_rate": 1.6148021145877982e-05,
      "loss": 1.0016,
      "step": 13480
    },
    {
      "epoch": 2.891127303900557,
      "grad_norm": 1.0434867143630981,
      "learning_rate": 1.614516359479926e-05,
      "loss": 0.302,
      "step": 13490
    },
    {
      "epoch": 2.8932704672096015,
      "grad_norm": 0.5343202948570251,
      "learning_rate": 1.6142306043720533e-05,
      "loss": 0.3744,
      "step": 13500
    },
    {
      "epoch": 2.8954136305186458,
      "grad_norm": 0.572679877281189,
      "learning_rate": 1.6139448492641807e-05,
      "loss": 0.1295,
      "step": 13510
    },
    {
      "epoch": 2.8975567938276896,
      "grad_norm": 17.661685943603516,
      "learning_rate": 1.613659094156308e-05,
      "loss": 0.7094,
      "step": 13520
    },
    {
      "epoch": 2.899699957136734,
      "grad_norm": 0.6052391529083252,
      "learning_rate": 1.6133733390484355e-05,
      "loss": 0.4781,
      "step": 13530
    },
    {
      "epoch": 2.901843120445778,
      "grad_norm": 0.5353337526321411,
      "learning_rate": 1.6130875839405632e-05,
      "loss": 0.4187,
      "step": 13540
    },
    {
      "epoch": 2.903986283754822,
      "grad_norm": 0.16241392493247986,
      "learning_rate": 1.6128018288326906e-05,
      "loss": 0.3138,
      "step": 13550
    },
    {
      "epoch": 2.9061294470638663,
      "grad_norm": 0.18294139206409454,
      "learning_rate": 1.612516073724818e-05,
      "loss": 0.7319,
      "step": 13560
    },
    {
      "epoch": 2.9082726103729106,
      "grad_norm": 0.2846488356590271,
      "learning_rate": 1.6122303186169454e-05,
      "loss": 1.0231,
      "step": 13570
    },
    {
      "epoch": 2.9104157736819545,
      "grad_norm": 0.24510182440280914,
      "learning_rate": 1.6119445635090728e-05,
      "loss": 0.3562,
      "step": 13580
    },
    {
      "epoch": 2.9125589369909988,
      "grad_norm": 16.71953582763672,
      "learning_rate": 1.6116588084012005e-05,
      "loss": 0.2974,
      "step": 13590
    },
    {
      "epoch": 2.914702100300043,
      "grad_norm": 15.1852445602417,
      "learning_rate": 1.611373053293328e-05,
      "loss": 0.9777,
      "step": 13600
    },
    {
      "epoch": 2.916845263609087,
      "grad_norm": 0.32981303334236145,
      "learning_rate": 1.6110872981854553e-05,
      "loss": 0.5086,
      "step": 13610
    },
    {
      "epoch": 2.918988426918131,
      "grad_norm": 18.061870574951172,
      "learning_rate": 1.6108015430775827e-05,
      "loss": 1.2148,
      "step": 13620
    },
    {
      "epoch": 2.9211315902271755,
      "grad_norm": 0.46236661076545715,
      "learning_rate": 1.61051578796971e-05,
      "loss": 0.7427,
      "step": 13630
    },
    {
      "epoch": 2.9232747535362194,
      "grad_norm": 29.88557243347168,
      "learning_rate": 1.6102300328618375e-05,
      "loss": 1.1766,
      "step": 13640
    },
    {
      "epoch": 2.9254179168452636,
      "grad_norm": 1.211241364479065,
      "learning_rate": 1.609944277753965e-05,
      "loss": 0.5466,
      "step": 13650
    },
    {
      "epoch": 2.927561080154308,
      "grad_norm": 15.11159896850586,
      "learning_rate": 1.6096585226460923e-05,
      "loss": 0.171,
      "step": 13660
    },
    {
      "epoch": 2.929704243463352,
      "grad_norm": 0.4500582814216614,
      "learning_rate": 1.6093727675382197e-05,
      "loss": 0.62,
      "step": 13670
    },
    {
      "epoch": 2.931847406772396,
      "grad_norm": 15.22383975982666,
      "learning_rate": 1.6090870124303474e-05,
      "loss": 0.452,
      "step": 13680
    },
    {
      "epoch": 2.9339905700814404,
      "grad_norm": 0.5229515433311462,
      "learning_rate": 1.6088012573224748e-05,
      "loss": 0.4728,
      "step": 13690
    },
    {
      "epoch": 2.9361337333904842,
      "grad_norm": 0.6114881038665771,
      "learning_rate": 1.6085155022146022e-05,
      "loss": 0.2657,
      "step": 13700
    },
    {
      "epoch": 2.9382768966995285,
      "grad_norm": 31.33542251586914,
      "learning_rate": 1.6082297471067296e-05,
      "loss": 0.7399,
      "step": 13710
    },
    {
      "epoch": 2.940420060008573,
      "grad_norm": 15.958436965942383,
      "learning_rate": 1.607943991998857e-05,
      "loss": 0.9997,
      "step": 13720
    },
    {
      "epoch": 2.9425632233176167,
      "grad_norm": 0.3197183310985565,
      "learning_rate": 1.6076582368909847e-05,
      "loss": 0.1615,
      "step": 13730
    },
    {
      "epoch": 2.944706386626661,
      "grad_norm": 0.3552459478378296,
      "learning_rate": 1.607372481783112e-05,
      "loss": 0.78,
      "step": 13740
    },
    {
      "epoch": 2.9468495499357052,
      "grad_norm": 15.633621215820312,
      "learning_rate": 1.6070867266752395e-05,
      "loss": 0.6813,
      "step": 13750
    },
    {
      "epoch": 2.948992713244749,
      "grad_norm": 0.4154983460903168,
      "learning_rate": 1.606800971567367e-05,
      "loss": 0.6041,
      "step": 13760
    },
    {
      "epoch": 2.9511358765537934,
      "grad_norm": 0.5596655607223511,
      "learning_rate": 1.6065152164594943e-05,
      "loss": 1.0776,
      "step": 13770
    },
    {
      "epoch": 2.9532790398628377,
      "grad_norm": 0.7733805179595947,
      "learning_rate": 1.6062294613516217e-05,
      "loss": 0.7725,
      "step": 13780
    },
    {
      "epoch": 2.9554222031718815,
      "grad_norm": 13.999866485595703,
      "learning_rate": 1.6059437062437494e-05,
      "loss": 0.4917,
      "step": 13790
    },
    {
      "epoch": 2.957565366480926,
      "grad_norm": 14.634765625,
      "learning_rate": 1.6056579511358768e-05,
      "loss": 0.6135,
      "step": 13800
    },
    {
      "epoch": 2.95970852978997,
      "grad_norm": 15.866615295410156,
      "learning_rate": 1.6053721960280042e-05,
      "loss": 0.9308,
      "step": 13810
    },
    {
      "epoch": 2.961851693099014,
      "grad_norm": 30.469785690307617,
      "learning_rate": 1.6050864409201316e-05,
      "loss": 1.1154,
      "step": 13820
    },
    {
      "epoch": 2.9639948564080583,
      "grad_norm": 0.7330518364906311,
      "learning_rate": 1.604800685812259e-05,
      "loss": 0.5695,
      "step": 13830
    },
    {
      "epoch": 2.9661380197171026,
      "grad_norm": 1.1781808137893677,
      "learning_rate": 1.6045149307043867e-05,
      "loss": 0.3226,
      "step": 13840
    },
    {
      "epoch": 2.9682811830261464,
      "grad_norm": 1.046651005744934,
      "learning_rate": 1.6042291755965138e-05,
      "loss": 0.261,
      "step": 13850
    },
    {
      "epoch": 2.9704243463351907,
      "grad_norm": 0.7996301651000977,
      "learning_rate": 1.603943420488641e-05,
      "loss": 0.7576,
      "step": 13860
    },
    {
      "epoch": 2.972567509644235,
      "grad_norm": 0.6463091969490051,
      "learning_rate": 1.603657665380769e-05,
      "loss": 0.4412,
      "step": 13870
    },
    {
      "epoch": 2.974710672953279,
      "grad_norm": 0.5093632936477661,
      "learning_rate": 1.6033719102728963e-05,
      "loss": 0.7448,
      "step": 13880
    },
    {
      "epoch": 2.976853836262323,
      "grad_norm": 14.96170425415039,
      "learning_rate": 1.6030861551650237e-05,
      "loss": 0.6293,
      "step": 13890
    },
    {
      "epoch": 2.9789969995713674,
      "grad_norm": 0.30777958035469055,
      "learning_rate": 1.602800400057151e-05,
      "loss": 0.4217,
      "step": 13900
    },
    {
      "epoch": 2.9811401628804113,
      "grad_norm": 0.8074300289154053,
      "learning_rate": 1.6025146449492785e-05,
      "loss": 1.1925,
      "step": 13910
    },
    {
      "epoch": 2.9832833261894556,
      "grad_norm": 0.4846109449863434,
      "learning_rate": 1.602228889841406e-05,
      "loss": 0.8446,
      "step": 13920
    },
    {
      "epoch": 2.9854264894985,
      "grad_norm": 0.467703253030777,
      "learning_rate": 1.6019431347335336e-05,
      "loss": 0.1228,
      "step": 13930
    },
    {
      "epoch": 2.9875696528075437,
      "grad_norm": 0.5165654420852661,
      "learning_rate": 1.601657379625661e-05,
      "loss": 0.9251,
      "step": 13940
    },
    {
      "epoch": 2.989712816116588,
      "grad_norm": 15.895454406738281,
      "learning_rate": 1.6013716245177884e-05,
      "loss": 0.9309,
      "step": 13950
    },
    {
      "epoch": 2.9918559794256323,
      "grad_norm": 0.7328779697418213,
      "learning_rate": 1.6010858694099158e-05,
      "loss": 0.3457,
      "step": 13960
    },
    {
      "epoch": 2.993999142734676,
      "grad_norm": 0.6930330991744995,
      "learning_rate": 1.600800114302043e-05,
      "loss": 0.4702,
      "step": 13970
    },
    {
      "epoch": 2.9961423060437204,
      "grad_norm": 28.11105728149414,
      "learning_rate": 1.600514359194171e-05,
      "loss": 0.943,
      "step": 13980
    },
    {
      "epoch": 2.9982854693527647,
      "grad_norm": 0.5717800259590149,
      "learning_rate": 1.6002286040862983e-05,
      "loss": 0.1004,
      "step": 13990
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.8676666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.540880560874939,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 397.7225,
      "eval_samples_per_second": 7.543,
      "eval_steps_per_second": 2.514,
      "step": 13998
    },
    {
      "epoch": 3.000428632661809,
      "grad_norm": 2.392373561859131,
      "learning_rate": 1.5999428489784257e-05,
      "loss": 0.4086,
      "step": 14000
    },
    {
      "epoch": 3.002571795970853,
      "grad_norm": 15.238775253295898,
      "learning_rate": 1.599657093870553e-05,
      "loss": 1.1259,
      "step": 14010
    },
    {
      "epoch": 3.004714959279897,
      "grad_norm": 0.47744616866111755,
      "learning_rate": 1.5993713387626805e-05,
      "loss": 0.5678,
      "step": 14020
    },
    {
      "epoch": 3.0068581225889415,
      "grad_norm": 0.31365010142326355,
      "learning_rate": 1.5990855836548082e-05,
      "loss": 0.1907,
      "step": 14030
    },
    {
      "epoch": 3.0090012858979853,
      "grad_norm": 16.783172607421875,
      "learning_rate": 1.5987998285469356e-05,
      "loss": 0.2982,
      "step": 14040
    },
    {
      "epoch": 3.0111444492070296,
      "grad_norm": 15.597428321838379,
      "learning_rate": 1.598514073439063e-05,
      "loss": 0.5938,
      "step": 14050
    },
    {
      "epoch": 3.013287612516074,
      "grad_norm": 16.20595359802246,
      "learning_rate": 1.5982283183311904e-05,
      "loss": 0.3042,
      "step": 14060
    },
    {
      "epoch": 3.0154307758251178,
      "grad_norm": 16.712793350219727,
      "learning_rate": 1.5979425632233178e-05,
      "loss": 1.0512,
      "step": 14070
    },
    {
      "epoch": 3.017573939134162,
      "grad_norm": 0.36078524589538574,
      "learning_rate": 1.597656808115445e-05,
      "loss": 0.7339,
      "step": 14080
    },
    {
      "epoch": 3.0197171024432063,
      "grad_norm": 0.5281307101249695,
      "learning_rate": 1.5973710530075726e-05,
      "loss": 0.694,
      "step": 14090
    },
    {
      "epoch": 3.02186026575225,
      "grad_norm": 29.285470962524414,
      "learning_rate": 1.5970852978997e-05,
      "loss": 0.6749,
      "step": 14100
    },
    {
      "epoch": 3.0240034290612945,
      "grad_norm": 0.8442820906639099,
      "learning_rate": 1.5967995427918273e-05,
      "loss": 0.3217,
      "step": 14110
    },
    {
      "epoch": 3.026146592370339,
      "grad_norm": 0.4620945155620575,
      "learning_rate": 1.596513787683955e-05,
      "loss": 0.8936,
      "step": 14120
    },
    {
      "epoch": 3.0282897556793826,
      "grad_norm": 17.31917953491211,
      "learning_rate": 1.5962280325760825e-05,
      "loss": 0.3963,
      "step": 14130
    },
    {
      "epoch": 3.030432918988427,
      "grad_norm": 0.3484063744544983,
      "learning_rate": 1.59594227746821e-05,
      "loss": 0.4501,
      "step": 14140
    },
    {
      "epoch": 3.032576082297471,
      "grad_norm": 15.091978073120117,
      "learning_rate": 1.5956565223603372e-05,
      "loss": 1.0165,
      "step": 14150
    },
    {
      "epoch": 3.034719245606515,
      "grad_norm": 0.5936940312385559,
      "learning_rate": 1.5953707672524646e-05,
      "loss": 0.4121,
      "step": 14160
    },
    {
      "epoch": 3.0368624089155594,
      "grad_norm": 0.5813977122306824,
      "learning_rate": 1.5950850121445924e-05,
      "loss": 0.6173,
      "step": 14170
    },
    {
      "epoch": 3.0390055722246037,
      "grad_norm": 14.23444938659668,
      "learning_rate": 1.5947992570367198e-05,
      "loss": 0.1744,
      "step": 14180
    },
    {
      "epoch": 3.0411487355336475,
      "grad_norm": 2.1596579551696777,
      "learning_rate": 1.594513501928847e-05,
      "loss": 0.3891,
      "step": 14190
    },
    {
      "epoch": 3.043291898842692,
      "grad_norm": 0.25931352376937866,
      "learning_rate": 1.5942277468209746e-05,
      "loss": 0.2873,
      "step": 14200
    },
    {
      "epoch": 3.045435062151736,
      "grad_norm": 0.5416253805160522,
      "learning_rate": 1.593941991713102e-05,
      "loss": 0.3012,
      "step": 14210
    },
    {
      "epoch": 3.04757822546078,
      "grad_norm": 0.5407453179359436,
      "learning_rate": 1.5936562366052293e-05,
      "loss": 0.4273,
      "step": 14220
    },
    {
      "epoch": 3.0497213887698242,
      "grad_norm": 14.633268356323242,
      "learning_rate": 1.593370481497357e-05,
      "loss": 1.1441,
      "step": 14230
    },
    {
      "epoch": 3.0518645520788685,
      "grad_norm": 14.825822830200195,
      "learning_rate": 1.5930847263894845e-05,
      "loss": 0.8191,
      "step": 14240
    },
    {
      "epoch": 3.0540077153879124,
      "grad_norm": 0.3187660872936249,
      "learning_rate": 1.592798971281612e-05,
      "loss": 0.4547,
      "step": 14250
    },
    {
      "epoch": 3.0561508786969567,
      "grad_norm": 1.4839708805084229,
      "learning_rate": 1.5925132161737392e-05,
      "loss": 0.9339,
      "step": 14260
    },
    {
      "epoch": 3.058294042006001,
      "grad_norm": 15.321879386901855,
      "learning_rate": 1.5922274610658666e-05,
      "loss": 1.0528,
      "step": 14270
    },
    {
      "epoch": 3.060437205315045,
      "grad_norm": 15.139251708984375,
      "learning_rate": 1.591941705957994e-05,
      "loss": 0.7213,
      "step": 14280
    },
    {
      "epoch": 3.062580368624089,
      "grad_norm": 0.6266856789588928,
      "learning_rate": 1.5916559508501214e-05,
      "loss": 0.2589,
      "step": 14290
    },
    {
      "epoch": 3.0647235319331334,
      "grad_norm": 1.555458426475525,
      "learning_rate": 1.5913701957422488e-05,
      "loss": 0.7985,
      "step": 14300
    },
    {
      "epoch": 3.0668666952421773,
      "grad_norm": 14.718420028686523,
      "learning_rate": 1.5910844406343765e-05,
      "loss": 0.5458,
      "step": 14310
    },
    {
      "epoch": 3.0690098585512215,
      "grad_norm": 0.4555428624153137,
      "learning_rate": 1.590798685526504e-05,
      "loss": 0.3139,
      "step": 14320
    },
    {
      "epoch": 3.071153021860266,
      "grad_norm": 15.472119331359863,
      "learning_rate": 1.5905129304186313e-05,
      "loss": 0.1739,
      "step": 14330
    },
    {
      "epoch": 3.0732961851693097,
      "grad_norm": 0.34186312556266785,
      "learning_rate": 1.5902271753107587e-05,
      "loss": 0.6894,
      "step": 14340
    },
    {
      "epoch": 3.075439348478354,
      "grad_norm": 0.3218187689781189,
      "learning_rate": 1.589941420202886e-05,
      "loss": 0.5308,
      "step": 14350
    },
    {
      "epoch": 3.0775825117873983,
      "grad_norm": 14.60909366607666,
      "learning_rate": 1.5896556650950135e-05,
      "loss": 0.4714,
      "step": 14360
    },
    {
      "epoch": 3.0797256750964426,
      "grad_norm": 0.38973790407180786,
      "learning_rate": 1.5893699099871412e-05,
      "loss": 0.7725,
      "step": 14370
    },
    {
      "epoch": 3.0818688384054864,
      "grad_norm": 14.677974700927734,
      "learning_rate": 1.5890841548792686e-05,
      "loss": 0.7479,
      "step": 14380
    },
    {
      "epoch": 3.0840120017145307,
      "grad_norm": 0.5150374174118042,
      "learning_rate": 1.588798399771396e-05,
      "loss": 0.3218,
      "step": 14390
    },
    {
      "epoch": 3.086155165023575,
      "grad_norm": 0.46514442563056946,
      "learning_rate": 1.5885126446635234e-05,
      "loss": 0.5744,
      "step": 14400
    },
    {
      "epoch": 3.088298328332619,
      "grad_norm": 0.47867098450660706,
      "learning_rate": 1.5882268895556508e-05,
      "loss": 1.0084,
      "step": 14410
    },
    {
      "epoch": 3.090441491641663,
      "grad_norm": 1.085805058479309,
      "learning_rate": 1.5879411344477785e-05,
      "loss": 0.7727,
      "step": 14420
    },
    {
      "epoch": 3.0925846549507074,
      "grad_norm": 1.0152899026870728,
      "learning_rate": 1.587655379339906e-05,
      "loss": 0.6254,
      "step": 14430
    },
    {
      "epoch": 3.0947278182597513,
      "grad_norm": 14.48593807220459,
      "learning_rate": 1.5873696242320333e-05,
      "loss": 0.6892,
      "step": 14440
    },
    {
      "epoch": 3.0968709815687956,
      "grad_norm": 0.5036051273345947,
      "learning_rate": 1.5870838691241607e-05,
      "loss": 0.4244,
      "step": 14450
    },
    {
      "epoch": 3.09901414487784,
      "grad_norm": 15.697649002075195,
      "learning_rate": 1.586798114016288e-05,
      "loss": 0.8012,
      "step": 14460
    },
    {
      "epoch": 3.1011573081868837,
      "grad_norm": 0.5350624918937683,
      "learning_rate": 1.586512358908416e-05,
      "loss": 0.7227,
      "step": 14470
    },
    {
      "epoch": 3.103300471495928,
      "grad_norm": 14.837505340576172,
      "learning_rate": 1.5862266038005432e-05,
      "loss": 0.7248,
      "step": 14480
    },
    {
      "epoch": 3.1054436348049723,
      "grad_norm": 14.491016387939453,
      "learning_rate": 1.5859408486926706e-05,
      "loss": 0.5713,
      "step": 14490
    },
    {
      "epoch": 3.107586798114016,
      "grad_norm": 0.9057543277740479,
      "learning_rate": 1.5856550935847977e-05,
      "loss": 0.4857,
      "step": 14500
    },
    {
      "epoch": 3.1097299614230605,
      "grad_norm": 13.838873863220215,
      "learning_rate": 1.5853693384769254e-05,
      "loss": 0.9342,
      "step": 14510
    },
    {
      "epoch": 3.1118731247321048,
      "grad_norm": 14.806522369384766,
      "learning_rate": 1.5850835833690528e-05,
      "loss": 0.5561,
      "step": 14520
    },
    {
      "epoch": 3.1140162880411486,
      "grad_norm": 13.975794792175293,
      "learning_rate": 1.5847978282611802e-05,
      "loss": 0.5493,
      "step": 14530
    },
    {
      "epoch": 3.116159451350193,
      "grad_norm": 1.1077739000320435,
      "learning_rate": 1.5845120731533076e-05,
      "loss": 0.3611,
      "step": 14540
    },
    {
      "epoch": 3.118302614659237,
      "grad_norm": 0.9108960628509521,
      "learning_rate": 1.584226318045435e-05,
      "loss": 0.0177,
      "step": 14550
    },
    {
      "epoch": 3.120445777968281,
      "grad_norm": 15.354255676269531,
      "learning_rate": 1.5839405629375627e-05,
      "loss": 0.6144,
      "step": 14560
    },
    {
      "epoch": 3.1225889412773253,
      "grad_norm": 0.30635353922843933,
      "learning_rate": 1.58365480782969e-05,
      "loss": 0.6693,
      "step": 14570
    },
    {
      "epoch": 3.1247321045863696,
      "grad_norm": 1.2635438442230225,
      "learning_rate": 1.5833690527218175e-05,
      "loss": 0.8061,
      "step": 14580
    },
    {
      "epoch": 3.1268752678954135,
      "grad_norm": 0.3742527961730957,
      "learning_rate": 1.583083297613945e-05,
      "loss": 0.176,
      "step": 14590
    },
    {
      "epoch": 3.1290184312044578,
      "grad_norm": 28.749832153320312,
      "learning_rate": 1.5827975425060723e-05,
      "loss": 0.5733,
      "step": 14600
    },
    {
      "epoch": 3.131161594513502,
      "grad_norm": 1.3693276643753052,
      "learning_rate": 1.5825117873982e-05,
      "loss": 0.444,
      "step": 14610
    },
    {
      "epoch": 3.133304757822546,
      "grad_norm": 1.0529448986053467,
      "learning_rate": 1.5822260322903274e-05,
      "loss": 0.6527,
      "step": 14620
    },
    {
      "epoch": 3.13544792113159,
      "grad_norm": 0.2659367322921753,
      "learning_rate": 1.5819402771824548e-05,
      "loss": 0.3239,
      "step": 14630
    },
    {
      "epoch": 3.1375910844406345,
      "grad_norm": 15.28457260131836,
      "learning_rate": 1.5816545220745822e-05,
      "loss": 0.4736,
      "step": 14640
    },
    {
      "epoch": 3.1397342477496784,
      "grad_norm": 0.22328314185142517,
      "learning_rate": 1.5813687669667096e-05,
      "loss": 0.8275,
      "step": 14650
    },
    {
      "epoch": 3.1418774110587226,
      "grad_norm": 0.26675060391426086,
      "learning_rate": 1.5810830118588373e-05,
      "loss": 0.8887,
      "step": 14660
    },
    {
      "epoch": 3.144020574367767,
      "grad_norm": 0.8317065238952637,
      "learning_rate": 1.5807972567509647e-05,
      "loss": 0.569,
      "step": 14670
    },
    {
      "epoch": 3.146163737676811,
      "grad_norm": 15.200064659118652,
      "learning_rate": 1.580511501643092e-05,
      "loss": 0.4662,
      "step": 14680
    },
    {
      "epoch": 3.148306900985855,
      "grad_norm": 0.28278255462646484,
      "learning_rate": 1.5802257465352195e-05,
      "loss": 0.2927,
      "step": 14690
    },
    {
      "epoch": 3.1504500642948994,
      "grad_norm": 14.760164260864258,
      "learning_rate": 1.579939991427347e-05,
      "loss": 1.2915,
      "step": 14700
    },
    {
      "epoch": 3.1525932276039432,
      "grad_norm": 0.34979405999183655,
      "learning_rate": 1.5796542363194743e-05,
      "loss": 0.2869,
      "step": 14710
    },
    {
      "epoch": 3.1547363909129875,
      "grad_norm": 0.7406586408615112,
      "learning_rate": 1.5793684812116017e-05,
      "loss": 0.7771,
      "step": 14720
    },
    {
      "epoch": 3.156879554222032,
      "grad_norm": 0.39015883207321167,
      "learning_rate": 1.579082726103729e-05,
      "loss": 0.4005,
      "step": 14730
    },
    {
      "epoch": 3.1590227175310757,
      "grad_norm": 0.31895098090171814,
      "learning_rate": 1.5787969709958565e-05,
      "loss": 0.4023,
      "step": 14740
    },
    {
      "epoch": 3.16116588084012,
      "grad_norm": 0.3076379597187042,
      "learning_rate": 1.5785112158879842e-05,
      "loss": 0.5406,
      "step": 14750
    },
    {
      "epoch": 3.1633090441491643,
      "grad_norm": 0.25645771622657776,
      "learning_rate": 1.5782254607801116e-05,
      "loss": 0.4054,
      "step": 14760
    },
    {
      "epoch": 3.165452207458208,
      "grad_norm": 14.703703880310059,
      "learning_rate": 1.577939705672239e-05,
      "loss": 0.2863,
      "step": 14770
    },
    {
      "epoch": 3.1675953707672524,
      "grad_norm": 0.19050122797489166,
      "learning_rate": 1.5776539505643664e-05,
      "loss": 0.1286,
      "step": 14780
    },
    {
      "epoch": 3.1697385340762967,
      "grad_norm": 0.18556861579418182,
      "learning_rate": 1.5773681954564938e-05,
      "loss": 1.05,
      "step": 14790
    },
    {
      "epoch": 3.1718816973853405,
      "grad_norm": 0.23729103803634644,
      "learning_rate": 1.5770824403486215e-05,
      "loss": 0.7885,
      "step": 14800
    },
    {
      "epoch": 3.174024860694385,
      "grad_norm": 0.2646065056324005,
      "learning_rate": 1.576796685240749e-05,
      "loss": 0.5924,
      "step": 14810
    },
    {
      "epoch": 3.176168024003429,
      "grad_norm": 0.2727105915546417,
      "learning_rate": 1.5765109301328763e-05,
      "loss": 0.9289,
      "step": 14820
    },
    {
      "epoch": 3.1783111873124734,
      "grad_norm": 14.891201972961426,
      "learning_rate": 1.5762251750250037e-05,
      "loss": 0.8359,
      "step": 14830
    },
    {
      "epoch": 3.1804543506215173,
      "grad_norm": 0.9149184226989746,
      "learning_rate": 1.575939419917131e-05,
      "loss": 0.8638,
      "step": 14840
    },
    {
      "epoch": 3.1825975139305616,
      "grad_norm": 1.2237203121185303,
      "learning_rate": 1.5756536648092585e-05,
      "loss": 0.8933,
      "step": 14850
    },
    {
      "epoch": 3.184740677239606,
      "grad_norm": 14.788005828857422,
      "learning_rate": 1.5753679097013862e-05,
      "loss": 0.5087,
      "step": 14860
    },
    {
      "epoch": 3.1868838405486497,
      "grad_norm": 1.6454322338104248,
      "learning_rate": 1.5750821545935136e-05,
      "loss": 0.8947,
      "step": 14870
    },
    {
      "epoch": 3.189027003857694,
      "grad_norm": 16.62606430053711,
      "learning_rate": 1.574796399485641e-05,
      "loss": 0.4595,
      "step": 14880
    },
    {
      "epoch": 3.1911701671667383,
      "grad_norm": 0.6988953948020935,
      "learning_rate": 1.5745106443777684e-05,
      "loss": 0.6388,
      "step": 14890
    },
    {
      "epoch": 3.193313330475782,
      "grad_norm": 1.7203257083892822,
      "learning_rate": 1.5742248892698958e-05,
      "loss": 0.4582,
      "step": 14900
    },
    {
      "epoch": 3.1954564937848264,
      "grad_norm": 14.126873970031738,
      "learning_rate": 1.5739391341620235e-05,
      "loss": 0.4931,
      "step": 14910
    },
    {
      "epoch": 3.1975996570938707,
      "grad_norm": 0.39445993304252625,
      "learning_rate": 1.573653379054151e-05,
      "loss": 0.3287,
      "step": 14920
    },
    {
      "epoch": 3.1997428204029146,
      "grad_norm": 0.4125913679599762,
      "learning_rate": 1.573367623946278e-05,
      "loss": 0.6784,
      "step": 14930
    },
    {
      "epoch": 3.201885983711959,
      "grad_norm": 0.4171045422554016,
      "learning_rate": 1.5730818688384057e-05,
      "loss": 0.4274,
      "step": 14940
    },
    {
      "epoch": 3.204029147021003,
      "grad_norm": 14.833334922790527,
      "learning_rate": 1.572796113730533e-05,
      "loss": 0.3354,
      "step": 14950
    },
    {
      "epoch": 3.206172310330047,
      "grad_norm": 0.8757534623146057,
      "learning_rate": 1.5725103586226605e-05,
      "loss": 0.7565,
      "step": 14960
    },
    {
      "epoch": 3.2083154736390913,
      "grad_norm": 1.6542091369628906,
      "learning_rate": 1.572224603514788e-05,
      "loss": 0.5842,
      "step": 14970
    },
    {
      "epoch": 3.2104586369481356,
      "grad_norm": 29.999460220336914,
      "learning_rate": 1.5719388484069153e-05,
      "loss": 0.5076,
      "step": 14980
    },
    {
      "epoch": 3.2126018002571795,
      "grad_norm": 0.31121721863746643,
      "learning_rate": 1.5716530932990426e-05,
      "loss": 0.4421,
      "step": 14990
    },
    {
      "epoch": 3.2147449635662237,
      "grad_norm": 13.491003036499023,
      "learning_rate": 1.5713673381911704e-05,
      "loss": 0.7818,
      "step": 15000
    },
    {
      "epoch": 3.216888126875268,
      "grad_norm": 14.784445762634277,
      "learning_rate": 1.5710815830832978e-05,
      "loss": 0.6013,
      "step": 15010
    },
    {
      "epoch": 3.219031290184312,
      "grad_norm": 0.43858975172042847,
      "learning_rate": 1.570795827975425e-05,
      "loss": 0.4506,
      "step": 15020
    },
    {
      "epoch": 3.221174453493356,
      "grad_norm": 0.4180285930633545,
      "learning_rate": 1.5705100728675526e-05,
      "loss": 0.322,
      "step": 15030
    },
    {
      "epoch": 3.2233176168024005,
      "grad_norm": 14.010608673095703,
      "learning_rate": 1.57022431775968e-05,
      "loss": 0.2718,
      "step": 15040
    },
    {
      "epoch": 3.2254607801114443,
      "grad_norm": 15.055150985717773,
      "learning_rate": 1.5699385626518077e-05,
      "loss": 0.6496,
      "step": 15050
    },
    {
      "epoch": 3.2276039434204886,
      "grad_norm": 0.3191692531108856,
      "learning_rate": 1.569652807543935e-05,
      "loss": 0.5512,
      "step": 15060
    },
    {
      "epoch": 3.229747106729533,
      "grad_norm": 0.9176828861236572,
      "learning_rate": 1.5693670524360625e-05,
      "loss": 0.5196,
      "step": 15070
    },
    {
      "epoch": 3.2318902700385768,
      "grad_norm": 0.3019673526287079,
      "learning_rate": 1.56908129732819e-05,
      "loss": 0.8312,
      "step": 15080
    },
    {
      "epoch": 3.234033433347621,
      "grad_norm": 0.3114328980445862,
      "learning_rate": 1.5687955422203173e-05,
      "loss": 0.6073,
      "step": 15090
    },
    {
      "epoch": 3.2361765966566653,
      "grad_norm": 14.676494598388672,
      "learning_rate": 1.568509787112445e-05,
      "loss": 1.0873,
      "step": 15100
    },
    {
      "epoch": 3.238319759965709,
      "grad_norm": 0.556890606880188,
      "learning_rate": 1.5682240320045724e-05,
      "loss": 0.7312,
      "step": 15110
    },
    {
      "epoch": 3.2404629232747535,
      "grad_norm": 0.5466943979263306,
      "learning_rate": 1.5679382768966998e-05,
      "loss": 0.3121,
      "step": 15120
    },
    {
      "epoch": 3.242606086583798,
      "grad_norm": 0.5326868891716003,
      "learning_rate": 1.567652521788827e-05,
      "loss": 0.71,
      "step": 15130
    },
    {
      "epoch": 3.2447492498928416,
      "grad_norm": 0.5199065208435059,
      "learning_rate": 1.5673667666809546e-05,
      "loss": 0.3178,
      "step": 15140
    },
    {
      "epoch": 3.246892413201886,
      "grad_norm": 15.633077621459961,
      "learning_rate": 1.567081011573082e-05,
      "loss": 0.9214,
      "step": 15150
    },
    {
      "epoch": 3.2490355765109302,
      "grad_norm": 14.929306030273438,
      "learning_rate": 1.5667952564652093e-05,
      "loss": 0.1625,
      "step": 15160
    },
    {
      "epoch": 3.2511787398199745,
      "grad_norm": 0.8208510279655457,
      "learning_rate": 1.5665095013573367e-05,
      "loss": 0.3162,
      "step": 15170
    },
    {
      "epoch": 3.2533219031290184,
      "grad_norm": 29.881635665893555,
      "learning_rate": 1.566223746249464e-05,
      "loss": 0.7121,
      "step": 15180
    },
    {
      "epoch": 3.2554650664380627,
      "grad_norm": 0.3857991099357605,
      "learning_rate": 1.565937991141592e-05,
      "loss": 0.7733,
      "step": 15190
    },
    {
      "epoch": 3.257608229747107,
      "grad_norm": 0.6897079348564148,
      "learning_rate": 1.5656522360337193e-05,
      "loss": 0.2409,
      "step": 15200
    },
    {
      "epoch": 3.259751393056151,
      "grad_norm": 0.7051547169685364,
      "learning_rate": 1.5653664809258466e-05,
      "loss": 0.9415,
      "step": 15210
    },
    {
      "epoch": 3.261894556365195,
      "grad_norm": 0.4006263017654419,
      "learning_rate": 1.565080725817974e-05,
      "loss": 0.5762,
      "step": 15220
    },
    {
      "epoch": 3.2640377196742394,
      "grad_norm": 16.15677261352539,
      "learning_rate": 1.5647949707101014e-05,
      "loss": 0.484,
      "step": 15230
    },
    {
      "epoch": 3.2661808829832832,
      "grad_norm": 0.37766242027282715,
      "learning_rate": 1.564509215602229e-05,
      "loss": 0.8704,
      "step": 15240
    },
    {
      "epoch": 3.2683240462923275,
      "grad_norm": 0.7237155437469482,
      "learning_rate": 1.5642234604943566e-05,
      "loss": 0.1642,
      "step": 15250
    },
    {
      "epoch": 3.270467209601372,
      "grad_norm": 0.41305017471313477,
      "learning_rate": 1.563937705386484e-05,
      "loss": 0.7845,
      "step": 15260
    },
    {
      "epoch": 3.2726103729104157,
      "grad_norm": 14.852704048156738,
      "learning_rate": 1.5636519502786113e-05,
      "loss": 0.8321,
      "step": 15270
    },
    {
      "epoch": 3.27475353621946,
      "grad_norm": 30.41853141784668,
      "learning_rate": 1.5633661951707387e-05,
      "loss": 1.4811,
      "step": 15280
    },
    {
      "epoch": 3.2768966995285043,
      "grad_norm": 14.358107566833496,
      "learning_rate": 1.5630804400628665e-05,
      "loss": 0.4615,
      "step": 15290
    },
    {
      "epoch": 3.279039862837548,
      "grad_norm": 14.16463851928711,
      "learning_rate": 1.562794684954994e-05,
      "loss": 0.9925,
      "step": 15300
    },
    {
      "epoch": 3.2811830261465924,
      "grad_norm": 14.815409660339355,
      "learning_rate": 1.5625089298471213e-05,
      "loss": 0.5473,
      "step": 15310
    },
    {
      "epoch": 3.2833261894556367,
      "grad_norm": 14.770438194274902,
      "learning_rate": 1.5622231747392486e-05,
      "loss": 0.7862,
      "step": 15320
    },
    {
      "epoch": 3.2854693527646806,
      "grad_norm": 15.908544540405273,
      "learning_rate": 1.561937419631376e-05,
      "loss": 0.5025,
      "step": 15330
    },
    {
      "epoch": 3.287612516073725,
      "grad_norm": 14.95740032196045,
      "learning_rate": 1.5616516645235034e-05,
      "loss": 0.4102,
      "step": 15340
    },
    {
      "epoch": 3.289755679382769,
      "grad_norm": 0.47852542996406555,
      "learning_rate": 1.561365909415631e-05,
      "loss": 0.4706,
      "step": 15350
    },
    {
      "epoch": 3.291898842691813,
      "grad_norm": 15.384275436401367,
      "learning_rate": 1.5610801543077582e-05,
      "loss": 0.9235,
      "step": 15360
    },
    {
      "epoch": 3.2940420060008573,
      "grad_norm": 14.997539520263672,
      "learning_rate": 1.5607943991998856e-05,
      "loss": 0.6674,
      "step": 15370
    },
    {
      "epoch": 3.2961851693099016,
      "grad_norm": 0.48616471886634827,
      "learning_rate": 1.5605086440920133e-05,
      "loss": 0.423,
      "step": 15380
    },
    {
      "epoch": 3.2983283326189454,
      "grad_norm": 1.0027836561203003,
      "learning_rate": 1.5602228889841407e-05,
      "loss": 0.9761,
      "step": 15390
    },
    {
      "epoch": 3.3004714959279897,
      "grad_norm": 1.1748507022857666,
      "learning_rate": 1.559937133876268e-05,
      "loss": 0.955,
      "step": 15400
    },
    {
      "epoch": 3.302614659237034,
      "grad_norm": 0.6275419592857361,
      "learning_rate": 1.5596513787683955e-05,
      "loss": 0.592,
      "step": 15410
    },
    {
      "epoch": 3.304757822546078,
      "grad_norm": 1.3133171796798706,
      "learning_rate": 1.559365623660523e-05,
      "loss": 0.3948,
      "step": 15420
    },
    {
      "epoch": 3.306900985855122,
      "grad_norm": 0.5686473250389099,
      "learning_rate": 1.5590798685526506e-05,
      "loss": 0.1529,
      "step": 15430
    },
    {
      "epoch": 3.3090441491641664,
      "grad_norm": 0.5549002885818481,
      "learning_rate": 1.558794113444778e-05,
      "loss": 1.2476,
      "step": 15440
    },
    {
      "epoch": 3.3111873124732103,
      "grad_norm": 1.342654824256897,
      "learning_rate": 1.5585083583369054e-05,
      "loss": 0.8384,
      "step": 15450
    },
    {
      "epoch": 3.3133304757822546,
      "grad_norm": 14.750570297241211,
      "learning_rate": 1.5582226032290328e-05,
      "loss": 0.9878,
      "step": 15460
    },
    {
      "epoch": 3.315473639091299,
      "grad_norm": 0.7993636727333069,
      "learning_rate": 1.5579368481211602e-05,
      "loss": 0.5259,
      "step": 15470
    },
    {
      "epoch": 3.3176168024003427,
      "grad_norm": 14.226755142211914,
      "learning_rate": 1.5576510930132876e-05,
      "loss": 0.627,
      "step": 15480
    },
    {
      "epoch": 3.319759965709387,
      "grad_norm": 0.6794288158416748,
      "learning_rate": 1.5573653379054153e-05,
      "loss": 0.2977,
      "step": 15490
    },
    {
      "epoch": 3.3219031290184313,
      "grad_norm": 15.378082275390625,
      "learning_rate": 1.5570795827975427e-05,
      "loss": 0.605,
      "step": 15500
    },
    {
      "epoch": 3.324046292327475,
      "grad_norm": 0.5130974054336548,
      "learning_rate": 1.55679382768967e-05,
      "loss": 0.1648,
      "step": 15510
    },
    {
      "epoch": 3.3261894556365195,
      "grad_norm": 1.767077088356018,
      "learning_rate": 1.5565080725817975e-05,
      "loss": 0.4294,
      "step": 15520
    },
    {
      "epoch": 3.3283326189455638,
      "grad_norm": 14.95400333404541,
      "learning_rate": 1.556222317473925e-05,
      "loss": 0.331,
      "step": 15530
    },
    {
      "epoch": 3.3304757822546076,
      "grad_norm": 0.3403143286705017,
      "learning_rate": 1.5559365623660526e-05,
      "loss": 0.6132,
      "step": 15540
    },
    {
      "epoch": 3.332618945563652,
      "grad_norm": 0.3227512240409851,
      "learning_rate": 1.55565080725818e-05,
      "loss": 0.1308,
      "step": 15550
    },
    {
      "epoch": 3.334762108872696,
      "grad_norm": 15.563799858093262,
      "learning_rate": 1.5553650521503074e-05,
      "loss": 0.4314,
      "step": 15560
    },
    {
      "epoch": 3.33690527218174,
      "grad_norm": 0.2520974278450012,
      "learning_rate": 1.5550792970424348e-05,
      "loss": 0.1795,
      "step": 15570
    },
    {
      "epoch": 3.3390484354907843,
      "grad_norm": 29.258546829223633,
      "learning_rate": 1.5547935419345622e-05,
      "loss": 0.6115,
      "step": 15580
    },
    {
      "epoch": 3.3411915987998286,
      "grad_norm": 0.2189193218946457,
      "learning_rate": 1.5545077868266896e-05,
      "loss": 0.4855,
      "step": 15590
    },
    {
      "epoch": 3.3433347621088725,
      "grad_norm": 0.21251259744167328,
      "learning_rate": 1.554222031718817e-05,
      "loss": 0.315,
      "step": 15600
    },
    {
      "epoch": 3.3454779254179168,
      "grad_norm": 0.2121354043483734,
      "learning_rate": 1.5539362766109444e-05,
      "loss": 1.2215,
      "step": 15610
    },
    {
      "epoch": 3.347621088726961,
      "grad_norm": 0.268757164478302,
      "learning_rate": 1.5536505215030718e-05,
      "loss": 1.1351,
      "step": 15620
    },
    {
      "epoch": 3.349764252036005,
      "grad_norm": 0.6311765313148499,
      "learning_rate": 1.5533647663951995e-05,
      "loss": 1.0741,
      "step": 15630
    },
    {
      "epoch": 3.351907415345049,
      "grad_norm": 0.7245866060256958,
      "learning_rate": 1.553079011287327e-05,
      "loss": 0.326,
      "step": 15640
    },
    {
      "epoch": 3.3540505786540935,
      "grad_norm": 0.3445708155632019,
      "learning_rate": 1.5527932561794543e-05,
      "loss": 0.2873,
      "step": 15650
    },
    {
      "epoch": 3.3561937419631374,
      "grad_norm": 0.6932288408279419,
      "learning_rate": 1.5525075010715817e-05,
      "loss": 0.8817,
      "step": 15660
    },
    {
      "epoch": 3.3583369052721816,
      "grad_norm": 0.654730498790741,
      "learning_rate": 1.552221745963709e-05,
      "loss": 0.4756,
      "step": 15670
    },
    {
      "epoch": 3.360480068581226,
      "grad_norm": 0.6993421912193298,
      "learning_rate": 1.5519359908558368e-05,
      "loss": 0.4801,
      "step": 15680
    },
    {
      "epoch": 3.36262323189027,
      "grad_norm": 14.908812522888184,
      "learning_rate": 1.5516502357479642e-05,
      "loss": 1.2113,
      "step": 15690
    },
    {
      "epoch": 3.364766395199314,
      "grad_norm": 15.296146392822266,
      "learning_rate": 1.5513644806400916e-05,
      "loss": 0.5527,
      "step": 15700
    },
    {
      "epoch": 3.3669095585083584,
      "grad_norm": 0.7655794620513916,
      "learning_rate": 1.551078725532219e-05,
      "loss": 0.4593,
      "step": 15710
    },
    {
      "epoch": 3.3690527218174027,
      "grad_norm": 0.7603694796562195,
      "learning_rate": 1.5507929704243464e-05,
      "loss": 0.9396,
      "step": 15720
    },
    {
      "epoch": 3.3711958851264465,
      "grad_norm": 0.48288777470588684,
      "learning_rate": 1.550507215316474e-05,
      "loss": 0.2878,
      "step": 15730
    },
    {
      "epoch": 3.373339048435491,
      "grad_norm": 15.36806583404541,
      "learning_rate": 1.5502214602086015e-05,
      "loss": 0.8707,
      "step": 15740
    },
    {
      "epoch": 3.375482211744535,
      "grad_norm": 1.3599752187728882,
      "learning_rate": 1.549935705100729e-05,
      "loss": 0.3024,
      "step": 15750
    },
    {
      "epoch": 3.377625375053579,
      "grad_norm": 17.413522720336914,
      "learning_rate": 1.5496499499928563e-05,
      "loss": 0.5693,
      "step": 15760
    },
    {
      "epoch": 3.3797685383626233,
      "grad_norm": 0.4138510227203369,
      "learning_rate": 1.5493641948849837e-05,
      "loss": 0.6374,
      "step": 15770
    },
    {
      "epoch": 3.3819117016716675,
      "grad_norm": 14.539131164550781,
      "learning_rate": 1.549078439777111e-05,
      "loss": 0.4317,
      "step": 15780
    },
    {
      "epoch": 3.3840548649807114,
      "grad_norm": 16.052640914916992,
      "learning_rate": 1.5487926846692385e-05,
      "loss": 0.995,
      "step": 15790
    },
    {
      "epoch": 3.3861980282897557,
      "grad_norm": 0.5962913036346436,
      "learning_rate": 1.548506929561366e-05,
      "loss": 0.5798,
      "step": 15800
    },
    {
      "epoch": 3.3883411915988,
      "grad_norm": 0.45843014121055603,
      "learning_rate": 1.5482211744534933e-05,
      "loss": 0.3459,
      "step": 15810
    },
    {
      "epoch": 3.390484354907844,
      "grad_norm": 0.7909585237503052,
      "learning_rate": 1.547935419345621e-05,
      "loss": 0.6404,
      "step": 15820
    },
    {
      "epoch": 3.392627518216888,
      "grad_norm": 14.861896514892578,
      "learning_rate": 1.5476496642377484e-05,
      "loss": 0.4279,
      "step": 15830
    },
    {
      "epoch": 3.3947706815259324,
      "grad_norm": 15.17066478729248,
      "learning_rate": 1.5473639091298758e-05,
      "loss": 0.4246,
      "step": 15840
    },
    {
      "epoch": 3.3969138448349763,
      "grad_norm": 0.3369661569595337,
      "learning_rate": 1.5470781540220032e-05,
      "loss": 1.1334,
      "step": 15850
    },
    {
      "epoch": 3.3990570081440206,
      "grad_norm": 0.42698168754577637,
      "learning_rate": 1.5467923989141306e-05,
      "loss": 0.6458,
      "step": 15860
    },
    {
      "epoch": 3.401200171453065,
      "grad_norm": 0.4175739586353302,
      "learning_rate": 1.5465066438062583e-05,
      "loss": 0.7478,
      "step": 15870
    },
    {
      "epoch": 3.4033433347621087,
      "grad_norm": 0.9720137119293213,
      "learning_rate": 1.5462208886983857e-05,
      "loss": 0.5932,
      "step": 15880
    },
    {
      "epoch": 3.405486498071153,
      "grad_norm": 0.4466516673564911,
      "learning_rate": 1.545935133590513e-05,
      "loss": 0.5946,
      "step": 15890
    },
    {
      "epoch": 3.4076296613801973,
      "grad_norm": 0.4085019528865814,
      "learning_rate": 1.5456493784826405e-05,
      "loss": 0.6288,
      "step": 15900
    },
    {
      "epoch": 3.409772824689241,
      "grad_norm": 15.72304916381836,
      "learning_rate": 1.545363623374768e-05,
      "loss": 0.4214,
      "step": 15910
    },
    {
      "epoch": 3.4119159879982854,
      "grad_norm": 0.4609857499599457,
      "learning_rate": 1.5450778682668953e-05,
      "loss": 1.2009,
      "step": 15920
    },
    {
      "epoch": 3.4140591513073297,
      "grad_norm": 0.45206451416015625,
      "learning_rate": 1.544792113159023e-05,
      "loss": 0.5849,
      "step": 15930
    },
    {
      "epoch": 3.4162023146163736,
      "grad_norm": 1.0508005619049072,
      "learning_rate": 1.5445063580511504e-05,
      "loss": 0.7442,
      "step": 15940
    },
    {
      "epoch": 3.418345477925418,
      "grad_norm": 15.024187088012695,
      "learning_rate": 1.5442206029432778e-05,
      "loss": 0.7852,
      "step": 15950
    },
    {
      "epoch": 3.420488641234462,
      "grad_norm": 0.7608440518379211,
      "learning_rate": 1.5439348478354052e-05,
      "loss": 0.8958,
      "step": 15960
    },
    {
      "epoch": 3.4226318045435065,
      "grad_norm": 0.43767526745796204,
      "learning_rate": 1.5436490927275326e-05,
      "loss": 0.3803,
      "step": 15970
    },
    {
      "epoch": 3.4247749678525503,
      "grad_norm": 0.5914907455444336,
      "learning_rate": 1.5433633376196603e-05,
      "loss": 0.5853,
      "step": 15980
    },
    {
      "epoch": 3.4269181311615946,
      "grad_norm": 1.1691722869873047,
      "learning_rate": 1.5430775825117877e-05,
      "loss": 0.403,
      "step": 15990
    },
    {
      "epoch": 3.429061294470639,
      "grad_norm": 1.0093353986740112,
      "learning_rate": 1.5427918274039147e-05,
      "loss": 0.6519,
      "step": 16000
    },
    {
      "epoch": 3.4312044577796827,
      "grad_norm": 14.80035400390625,
      "learning_rate": 1.5425060722960425e-05,
      "loss": 1.3599,
      "step": 16010
    },
    {
      "epoch": 3.433347621088727,
      "grad_norm": 1.0657813549041748,
      "learning_rate": 1.54222031718817e-05,
      "loss": 0.5193,
      "step": 16020
    },
    {
      "epoch": 3.4354907843977713,
      "grad_norm": 0.5085251331329346,
      "learning_rate": 1.5419345620802973e-05,
      "loss": 0.2633,
      "step": 16030
    },
    {
      "epoch": 3.437633947706815,
      "grad_norm": 0.46429193019866943,
      "learning_rate": 1.5416488069724247e-05,
      "loss": 0.4213,
      "step": 16040
    },
    {
      "epoch": 3.4397771110158595,
      "grad_norm": 0.4064007103443146,
      "learning_rate": 1.541363051864552e-05,
      "loss": 0.2865,
      "step": 16050
    },
    {
      "epoch": 3.4419202743249038,
      "grad_norm": 15.164243698120117,
      "learning_rate": 1.5410772967566794e-05,
      "loss": 0.4295,
      "step": 16060
    },
    {
      "epoch": 3.4440634376339476,
      "grad_norm": 0.2906246781349182,
      "learning_rate": 1.5407915416488072e-05,
      "loss": 0.3498,
      "step": 16070
    },
    {
      "epoch": 3.446206600942992,
      "grad_norm": 29.77643394470215,
      "learning_rate": 1.5405057865409346e-05,
      "loss": 0.5174,
      "step": 16080
    },
    {
      "epoch": 3.448349764252036,
      "grad_norm": 16.423343658447266,
      "learning_rate": 1.540220031433062e-05,
      "loss": 1.1312,
      "step": 16090
    },
    {
      "epoch": 3.45049292756108,
      "grad_norm": 0.33120933175086975,
      "learning_rate": 1.5399342763251894e-05,
      "loss": 0.4871,
      "step": 16100
    },
    {
      "epoch": 3.4526360908701244,
      "grad_norm": 2.89994740486145,
      "learning_rate": 1.5396485212173167e-05,
      "loss": 0.8714,
      "step": 16110
    },
    {
      "epoch": 3.4547792541791686,
      "grad_norm": 16.001480102539062,
      "learning_rate": 1.5393627661094445e-05,
      "loss": 0.5195,
      "step": 16120
    },
    {
      "epoch": 3.4569224174882125,
      "grad_norm": 16.302040100097656,
      "learning_rate": 1.539077011001572e-05,
      "loss": 0.7031,
      "step": 16130
    },
    {
      "epoch": 3.459065580797257,
      "grad_norm": 26.043519973754883,
      "learning_rate": 1.5387912558936993e-05,
      "loss": 0.7715,
      "step": 16140
    },
    {
      "epoch": 3.461208744106301,
      "grad_norm": 0.9266358613967896,
      "learning_rate": 1.5385055007858267e-05,
      "loss": 0.6598,
      "step": 16150
    },
    {
      "epoch": 3.463351907415345,
      "grad_norm": 0.1894688606262207,
      "learning_rate": 1.538219745677954e-05,
      "loss": 0.4429,
      "step": 16160
    },
    {
      "epoch": 3.4654950707243892,
      "grad_norm": 0.1388895958662033,
      "learning_rate": 1.5379339905700818e-05,
      "loss": 0.3423,
      "step": 16170
    },
    {
      "epoch": 3.4676382340334335,
      "grad_norm": 16.198972702026367,
      "learning_rate": 1.5376482354622092e-05,
      "loss": 0.9649,
      "step": 16180
    },
    {
      "epoch": 3.4697813973424774,
      "grad_norm": 0.260286420583725,
      "learning_rate": 1.5373624803543366e-05,
      "loss": 0.3428,
      "step": 16190
    },
    {
      "epoch": 3.4719245606515217,
      "grad_norm": 0.5505477786064148,
      "learning_rate": 1.537076725246464e-05,
      "loss": 0.5368,
      "step": 16200
    },
    {
      "epoch": 3.474067723960566,
      "grad_norm": 0.19361400604248047,
      "learning_rate": 1.5367909701385914e-05,
      "loss": 0.7788,
      "step": 16210
    },
    {
      "epoch": 3.47621088726961,
      "grad_norm": 0.35021987557411194,
      "learning_rate": 1.5365052150307187e-05,
      "loss": 0.571,
      "step": 16220
    },
    {
      "epoch": 3.478354050578654,
      "grad_norm": 0.14949320256710052,
      "learning_rate": 1.536219459922846e-05,
      "loss": 0.1913,
      "step": 16230
    },
    {
      "epoch": 3.4804972138876984,
      "grad_norm": 26.744014739990234,
      "learning_rate": 1.5359337048149735e-05,
      "loss": 0.7785,
      "step": 16240
    },
    {
      "epoch": 3.4826403771967422,
      "grad_norm": 0.7242850661277771,
      "learning_rate": 1.535647949707101e-05,
      "loss": 0.7686,
      "step": 16250
    },
    {
      "epoch": 3.4847835405057865,
      "grad_norm": 0.30861783027648926,
      "learning_rate": 1.5353621945992287e-05,
      "loss": 0.5034,
      "step": 16260
    },
    {
      "epoch": 3.486926703814831,
      "grad_norm": 31.853544235229492,
      "learning_rate": 1.535076439491356e-05,
      "loss": 0.8551,
      "step": 16270
    },
    {
      "epoch": 3.4890698671238747,
      "grad_norm": 15.093530654907227,
      "learning_rate": 1.5347906843834834e-05,
      "loss": 0.8984,
      "step": 16280
    },
    {
      "epoch": 3.491213030432919,
      "grad_norm": 0.38238808512687683,
      "learning_rate": 1.534504929275611e-05,
      "loss": 0.1153,
      "step": 16290
    },
    {
      "epoch": 3.4933561937419633,
      "grad_norm": 0.09748242050409317,
      "learning_rate": 1.5342191741677382e-05,
      "loss": 0.3446,
      "step": 16300
    },
    {
      "epoch": 3.495499357051007,
      "grad_norm": 0.7985101938247681,
      "learning_rate": 1.533933419059866e-05,
      "loss": 0.6755,
      "step": 16310
    },
    {
      "epoch": 3.4976425203600514,
      "grad_norm": 29.018325805664062,
      "learning_rate": 1.5336476639519933e-05,
      "loss": 0.7803,
      "step": 16320
    },
    {
      "epoch": 3.4997856836690957,
      "grad_norm": 1.0374865531921387,
      "learning_rate": 1.5333619088441207e-05,
      "loss": 0.9165,
      "step": 16330
    },
    {
      "epoch": 3.5019288469781396,
      "grad_norm": 0.33447080850601196,
      "learning_rate": 1.533076153736248e-05,
      "loss": 0.5822,
      "step": 16340
    },
    {
      "epoch": 3.504072010287184,
      "grad_norm": 24.870372772216797,
      "learning_rate": 1.5327903986283755e-05,
      "loss": 1.1523,
      "step": 16350
    },
    {
      "epoch": 3.506215173596228,
      "grad_norm": 0.4806704819202423,
      "learning_rate": 1.5325046435205033e-05,
      "loss": 0.446,
      "step": 16360
    },
    {
      "epoch": 3.508358336905272,
      "grad_norm": 0.14084693789482117,
      "learning_rate": 1.5322188884126307e-05,
      "loss": 0.1913,
      "step": 16370
    },
    {
      "epoch": 3.5105015002143163,
      "grad_norm": 0.1594693511724472,
      "learning_rate": 1.531933133304758e-05,
      "loss": 0.9869,
      "step": 16380
    },
    {
      "epoch": 3.5126446635233606,
      "grad_norm": 1.2643507719039917,
      "learning_rate": 1.5316473781968854e-05,
      "loss": 0.8096,
      "step": 16390
    },
    {
      "epoch": 3.5147878268324044,
      "grad_norm": 0.4806472063064575,
      "learning_rate": 1.531361623089013e-05,
      "loss": 0.4291,
      "step": 16400
    },
    {
      "epoch": 3.5169309901414487,
      "grad_norm": 0.16550439596176147,
      "learning_rate": 1.5310758679811402e-05,
      "loss": 1.0336,
      "step": 16410
    },
    {
      "epoch": 3.519074153450493,
      "grad_norm": 47.432159423828125,
      "learning_rate": 1.530790112873268e-05,
      "loss": 0.7795,
      "step": 16420
    },
    {
      "epoch": 3.521217316759537,
      "grad_norm": 0.5659666061401367,
      "learning_rate": 1.530504357765395e-05,
      "loss": 1.0005,
      "step": 16430
    },
    {
      "epoch": 3.523360480068581,
      "grad_norm": 51.17749786376953,
      "learning_rate": 1.5302186026575224e-05,
      "loss": 0.8106,
      "step": 16440
    },
    {
      "epoch": 3.5255036433776255,
      "grad_norm": 0.3269764184951782,
      "learning_rate": 1.52993284754965e-05,
      "loss": 0.6258,
      "step": 16450
    },
    {
      "epoch": 3.5276468066866693,
      "grad_norm": 0.09652099013328552,
      "learning_rate": 1.5296470924417775e-05,
      "loss": 0.6642,
      "step": 16460
    },
    {
      "epoch": 3.5297899699957136,
      "grad_norm": 0.7022560238838196,
      "learning_rate": 1.529361337333905e-05,
      "loss": 0.7564,
      "step": 16470
    },
    {
      "epoch": 3.531933133304758,
      "grad_norm": 14.797810554504395,
      "learning_rate": 1.5290755822260323e-05,
      "loss": 0.6637,
      "step": 16480
    },
    {
      "epoch": 3.5340762966138017,
      "grad_norm": 0.3856394588947296,
      "learning_rate": 1.5287898271181597e-05,
      "loss": 1.1282,
      "step": 16490
    },
    {
      "epoch": 3.536219459922846,
      "grad_norm": 0.6420933604240417,
      "learning_rate": 1.5285040720102874e-05,
      "loss": 0.8439,
      "step": 16500
    },
    {
      "epoch": 3.5383626232318903,
      "grad_norm": 15.361089706420898,
      "learning_rate": 1.5282183169024148e-05,
      "loss": 0.581,
      "step": 16510
    },
    {
      "epoch": 3.540505786540934,
      "grad_norm": 29.07198715209961,
      "learning_rate": 1.5279325617945422e-05,
      "loss": 0.9388,
      "step": 16520
    },
    {
      "epoch": 3.5426489498499785,
      "grad_norm": 27.37764549255371,
      "learning_rate": 1.5276468066866696e-05,
      "loss": 0.7563,
      "step": 16530
    },
    {
      "epoch": 3.5447921131590228,
      "grad_norm": 0.35391902923583984,
      "learning_rate": 1.527361051578797e-05,
      "loss": 1.0073,
      "step": 16540
    },
    {
      "epoch": 3.5469352764680666,
      "grad_norm": 0.4220559895038605,
      "learning_rate": 1.5270752964709244e-05,
      "loss": 0.6515,
      "step": 16550
    },
    {
      "epoch": 3.549078439777111,
      "grad_norm": 0.3098180592060089,
      "learning_rate": 1.526789541363052e-05,
      "loss": 0.2786,
      "step": 16560
    },
    {
      "epoch": 3.551221603086155,
      "grad_norm": 0.2167489379644394,
      "learning_rate": 1.5265037862551795e-05,
      "loss": 0.4896,
      "step": 16570
    },
    {
      "epoch": 3.553364766395199,
      "grad_norm": 28.754884719848633,
      "learning_rate": 1.526218031147307e-05,
      "loss": 1.4003,
      "step": 16580
    },
    {
      "epoch": 3.5555079297042433,
      "grad_norm": 1.1027365922927856,
      "learning_rate": 1.5259322760394343e-05,
      "loss": 0.3516,
      "step": 16590
    },
    {
      "epoch": 3.5576510930132876,
      "grad_norm": 24.501522064208984,
      "learning_rate": 1.5256465209315619e-05,
      "loss": 0.3186,
      "step": 16600
    },
    {
      "epoch": 3.5597942563223315,
      "grad_norm": 0.7044582366943359,
      "learning_rate": 1.5253607658236893e-05,
      "loss": 1.2267,
      "step": 16610
    },
    {
      "epoch": 3.561937419631376,
      "grad_norm": 0.43617165088653564,
      "learning_rate": 1.5250750107158168e-05,
      "loss": 1.3207,
      "step": 16620
    },
    {
      "epoch": 3.56408058294042,
      "grad_norm": 0.5824494957923889,
      "learning_rate": 1.5247892556079442e-05,
      "loss": 0.3589,
      "step": 16630
    },
    {
      "epoch": 3.5662237462494644,
      "grad_norm": 20.51740264892578,
      "learning_rate": 1.5245035005000714e-05,
      "loss": 0.5774,
      "step": 16640
    },
    {
      "epoch": 3.568366909558508,
      "grad_norm": 0.26241666078567505,
      "learning_rate": 1.5242177453921988e-05,
      "loss": 0.5445,
      "step": 16650
    },
    {
      "epoch": 3.5705100728675525,
      "grad_norm": 0.24330860376358032,
      "learning_rate": 1.5239319902843264e-05,
      "loss": 0.7788,
      "step": 16660
    },
    {
      "epoch": 3.572653236176597,
      "grad_norm": 0.3948068618774414,
      "learning_rate": 1.5236462351764538e-05,
      "loss": 0.7731,
      "step": 16670
    },
    {
      "epoch": 3.5747963994856407,
      "grad_norm": 0.8180784583091736,
      "learning_rate": 1.5233604800685814e-05,
      "loss": 0.9928,
      "step": 16680
    },
    {
      "epoch": 3.576939562794685,
      "grad_norm": 0.544679582118988,
      "learning_rate": 1.5230747249607087e-05,
      "loss": 1.194,
      "step": 16690
    },
    {
      "epoch": 3.5790827261037292,
      "grad_norm": 47.94928741455078,
      "learning_rate": 1.5227889698528361e-05,
      "loss": 1.2805,
      "step": 16700
    },
    {
      "epoch": 3.581225889412773,
      "grad_norm": 27.627042770385742,
      "learning_rate": 1.5225032147449637e-05,
      "loss": 0.5061,
      "step": 16710
    },
    {
      "epoch": 3.5833690527218174,
      "grad_norm": 0.4117688834667206,
      "learning_rate": 1.5222174596370911e-05,
      "loss": 0.9127,
      "step": 16720
    },
    {
      "epoch": 3.5855122160308617,
      "grad_norm": 0.33703866600990295,
      "learning_rate": 1.5219317045292185e-05,
      "loss": 0.7324,
      "step": 16730
    },
    {
      "epoch": 3.587655379339906,
      "grad_norm": 0.5378496646881104,
      "learning_rate": 1.521645949421346e-05,
      "loss": 0.4494,
      "step": 16740
    },
    {
      "epoch": 3.58979854264895,
      "grad_norm": 0.23220568895339966,
      "learning_rate": 1.5213601943134734e-05,
      "loss": 0.5083,
      "step": 16750
    },
    {
      "epoch": 3.591941705957994,
      "grad_norm": 40.82501220703125,
      "learning_rate": 1.521074439205601e-05,
      "loss": 0.5379,
      "step": 16760
    },
    {
      "epoch": 3.5940848692670384,
      "grad_norm": 26.379594802856445,
      "learning_rate": 1.5207886840977284e-05,
      "loss": 0.8046,
      "step": 16770
    },
    {
      "epoch": 3.5962280325760823,
      "grad_norm": 0.45288169384002686,
      "learning_rate": 1.5205029289898558e-05,
      "loss": 1.0135,
      "step": 16780
    },
    {
      "epoch": 3.5983711958851265,
      "grad_norm": 0.6898131370544434,
      "learning_rate": 1.5202171738819834e-05,
      "loss": 0.9803,
      "step": 16790
    },
    {
      "epoch": 3.600514359194171,
      "grad_norm": 23.42213249206543,
      "learning_rate": 1.5199314187741107e-05,
      "loss": 0.4735,
      "step": 16800
    },
    {
      "epoch": 3.6026575225032147,
      "grad_norm": 25.027376174926758,
      "learning_rate": 1.5196456636662383e-05,
      "loss": 0.7281,
      "step": 16810
    },
    {
      "epoch": 3.604800685812259,
      "grad_norm": 0.30550432205200195,
      "learning_rate": 1.5193599085583657e-05,
      "loss": 0.6164,
      "step": 16820
    },
    {
      "epoch": 3.6069438491213033,
      "grad_norm": 0.4921688437461853,
      "learning_rate": 1.5190741534504931e-05,
      "loss": 0.9132,
      "step": 16830
    },
    {
      "epoch": 3.609087012430347,
      "grad_norm": 0.19549311697483063,
      "learning_rate": 1.5187883983426207e-05,
      "loss": 0.0073,
      "step": 16840
    },
    {
      "epoch": 3.6112301757393914,
      "grad_norm": 0.1652940958738327,
      "learning_rate": 1.518502643234748e-05,
      "loss": 0.7256,
      "step": 16850
    },
    {
      "epoch": 3.6133733390484357,
      "grad_norm": 0.14185471832752228,
      "learning_rate": 1.5182168881268753e-05,
      "loss": 0.1939,
      "step": 16860
    },
    {
      "epoch": 3.6155165023574796,
      "grad_norm": 0.18743963539600372,
      "learning_rate": 1.5179311330190027e-05,
      "loss": 0.7848,
      "step": 16870
    },
    {
      "epoch": 3.617659665666524,
      "grad_norm": 25.655353546142578,
      "learning_rate": 1.5176453779111302e-05,
      "loss": 0.7676,
      "step": 16880
    },
    {
      "epoch": 3.619802828975568,
      "grad_norm": 0.471149742603302,
      "learning_rate": 1.5173596228032576e-05,
      "loss": 0.6733,
      "step": 16890
    },
    {
      "epoch": 3.621945992284612,
      "grad_norm": 0.34849685430526733,
      "learning_rate": 1.5170738676953852e-05,
      "loss": 0.3425,
      "step": 16900
    },
    {
      "epoch": 3.6240891555936563,
      "grad_norm": 43.160179138183594,
      "learning_rate": 1.5167881125875126e-05,
      "loss": 0.9404,
      "step": 16910
    },
    {
      "epoch": 3.6262323189027006,
      "grad_norm": 0.27290183305740356,
      "learning_rate": 1.51650235747964e-05,
      "loss": 0.3913,
      "step": 16920
    },
    {
      "epoch": 3.6283754822117444,
      "grad_norm": 0.17012155055999756,
      "learning_rate": 1.5162166023717675e-05,
      "loss": 0.7482,
      "step": 16930
    },
    {
      "epoch": 3.6305186455207887,
      "grad_norm": 0.2554929256439209,
      "learning_rate": 1.515930847263895e-05,
      "loss": 0.1862,
      "step": 16940
    },
    {
      "epoch": 3.632661808829833,
      "grad_norm": 0.26677316427230835,
      "learning_rate": 1.5156450921560225e-05,
      "loss": 1.1006,
      "step": 16950
    },
    {
      "epoch": 3.634804972138877,
      "grad_norm": 0.2565387487411499,
      "learning_rate": 1.5153593370481499e-05,
      "loss": 0.3337,
      "step": 16960
    },
    {
      "epoch": 3.636948135447921,
      "grad_norm": 0.4553534686565399,
      "learning_rate": 1.5150735819402773e-05,
      "loss": 1.7519,
      "step": 16970
    },
    {
      "epoch": 3.6390912987569655,
      "grad_norm": 25.556671142578125,
      "learning_rate": 1.5147878268324048e-05,
      "loss": 0.5765,
      "step": 16980
    },
    {
      "epoch": 3.6412344620660093,
      "grad_norm": 0.9694514274597168,
      "learning_rate": 1.5145020717245322e-05,
      "loss": 0.4534,
      "step": 16990
    },
    {
      "epoch": 3.6433776253750536,
      "grad_norm": 0.5164216160774231,
      "learning_rate": 1.5142163166166596e-05,
      "loss": 0.7879,
      "step": 17000
    },
    {
      "epoch": 3.645520788684098,
      "grad_norm": 0.5468558669090271,
      "learning_rate": 1.5139305615087872e-05,
      "loss": 0.4902,
      "step": 17010
    },
    {
      "epoch": 3.6476639519931418,
      "grad_norm": 24.356653213500977,
      "learning_rate": 1.5136448064009146e-05,
      "loss": 1.0518,
      "step": 17020
    },
    {
      "epoch": 3.649807115302186,
      "grad_norm": 0.25558459758758545,
      "learning_rate": 1.5133590512930421e-05,
      "loss": 0.1373,
      "step": 17030
    },
    {
      "epoch": 3.6519502786112303,
      "grad_norm": 25.72112464904785,
      "learning_rate": 1.5130732961851695e-05,
      "loss": 0.3839,
      "step": 17040
    },
    {
      "epoch": 3.654093441920274,
      "grad_norm": 0.30424270033836365,
      "learning_rate": 1.512787541077297e-05,
      "loss": 1.2449,
      "step": 17050
    },
    {
      "epoch": 3.6562366052293185,
      "grad_norm": 26.52252197265625,
      "learning_rate": 1.5125017859694245e-05,
      "loss": 1.2779,
      "step": 17060
    },
    {
      "epoch": 3.6583797685383628,
      "grad_norm": 28.350204467773438,
      "learning_rate": 1.5122160308615517e-05,
      "loss": 0.8769,
      "step": 17070
    },
    {
      "epoch": 3.6605229318474066,
      "grad_norm": 1.09300696849823,
      "learning_rate": 1.5119302757536791e-05,
      "loss": 0.431,
      "step": 17080
    },
    {
      "epoch": 3.662666095156451,
      "grad_norm": 0.23603908717632294,
      "learning_rate": 1.5116445206458067e-05,
      "loss": 0.329,
      "step": 17090
    },
    {
      "epoch": 3.664809258465495,
      "grad_norm": 28.213592529296875,
      "learning_rate": 1.511358765537934e-05,
      "loss": 0.4395,
      "step": 17100
    },
    {
      "epoch": 3.666952421774539,
      "grad_norm": 26.04107093811035,
      "learning_rate": 1.5110730104300614e-05,
      "loss": 0.8349,
      "step": 17110
    },
    {
      "epoch": 3.6690955850835834,
      "grad_norm": 0.23505799472332,
      "learning_rate": 1.510787255322189e-05,
      "loss": 0.7246,
      "step": 17120
    },
    {
      "epoch": 3.6712387483926276,
      "grad_norm": 0.26607537269592285,
      "learning_rate": 1.5105015002143164e-05,
      "loss": 0.3682,
      "step": 17130
    },
    {
      "epoch": 3.6733819117016715,
      "grad_norm": 0.2830560505390167,
      "learning_rate": 1.5102157451064438e-05,
      "loss": 0.5721,
      "step": 17140
    },
    {
      "epoch": 3.675525075010716,
      "grad_norm": 0.3104899525642395,
      "learning_rate": 1.5099299899985714e-05,
      "loss": 0.173,
      "step": 17150
    },
    {
      "epoch": 3.67766823831976,
      "grad_norm": 0.14458224177360535,
      "learning_rate": 1.5096442348906988e-05,
      "loss": 0.3301,
      "step": 17160
    },
    {
      "epoch": 3.679811401628804,
      "grad_norm": 0.12859196960926056,
      "learning_rate": 1.5093584797828263e-05,
      "loss": 0.4047,
      "step": 17170
    },
    {
      "epoch": 3.6819545649378482,
      "grad_norm": 0.5017804503440857,
      "learning_rate": 1.5090727246749537e-05,
      "loss": 0.5648,
      "step": 17180
    },
    {
      "epoch": 3.6840977282468925,
      "grad_norm": 0.24176186323165894,
      "learning_rate": 1.5087869695670811e-05,
      "loss": 0.6038,
      "step": 17190
    },
    {
      "epoch": 3.6862408915559364,
      "grad_norm": 24.284957885742188,
      "learning_rate": 1.5085012144592087e-05,
      "loss": 0.605,
      "step": 17200
    },
    {
      "epoch": 3.6883840548649807,
      "grad_norm": 0.20666444301605225,
      "learning_rate": 1.508215459351336e-05,
      "loss": 0.3667,
      "step": 17210
    },
    {
      "epoch": 3.690527218174025,
      "grad_norm": 0.3214746415615082,
      "learning_rate": 1.5079297042434634e-05,
      "loss": 1.2379,
      "step": 17220
    },
    {
      "epoch": 3.692670381483069,
      "grad_norm": 0.6992329359054565,
      "learning_rate": 1.507643949135591e-05,
      "loss": 0.2905,
      "step": 17230
    },
    {
      "epoch": 3.694813544792113,
      "grad_norm": 24.7388858795166,
      "learning_rate": 1.5073581940277184e-05,
      "loss": 1.0269,
      "step": 17240
    },
    {
      "epoch": 3.6969567081011574,
      "grad_norm": 21.93272590637207,
      "learning_rate": 1.507072438919846e-05,
      "loss": 1.4336,
      "step": 17250
    },
    {
      "epoch": 3.6990998714102012,
      "grad_norm": 21.521089553833008,
      "learning_rate": 1.5067866838119734e-05,
      "loss": 0.2092,
      "step": 17260
    },
    {
      "epoch": 3.7012430347192455,
      "grad_norm": 0.11622651666402817,
      "learning_rate": 1.5065009287041008e-05,
      "loss": 0.4016,
      "step": 17270
    },
    {
      "epoch": 3.70338619802829,
      "grad_norm": 20.771215438842773,
      "learning_rate": 1.5062151735962283e-05,
      "loss": 0.2202,
      "step": 17280
    },
    {
      "epoch": 3.7055293613373337,
      "grad_norm": 0.08457136154174805,
      "learning_rate": 1.5059294184883555e-05,
      "loss": 0.4361,
      "step": 17290
    },
    {
      "epoch": 3.707672524646378,
      "grad_norm": 0.14623212814331055,
      "learning_rate": 1.505643663380483e-05,
      "loss": 0.4254,
      "step": 17300
    },
    {
      "epoch": 3.7098156879554223,
      "grad_norm": 0.18580669164657593,
      "learning_rate": 1.5053579082726105e-05,
      "loss": 0.9626,
      "step": 17310
    },
    {
      "epoch": 3.711958851264466,
      "grad_norm": 0.7351292967796326,
      "learning_rate": 1.5050721531647379e-05,
      "loss": 0.7545,
      "step": 17320
    },
    {
      "epoch": 3.7141020145735104,
      "grad_norm": 0.869238555431366,
      "learning_rate": 1.5047863980568653e-05,
      "loss": 0.741,
      "step": 17330
    },
    {
      "epoch": 3.7162451778825547,
      "grad_norm": 0.7819499969482422,
      "learning_rate": 1.5045006429489928e-05,
      "loss": 0.5448,
      "step": 17340
    },
    {
      "epoch": 3.7183883411915986,
      "grad_norm": 0.39873257279396057,
      "learning_rate": 1.5042148878411202e-05,
      "loss": 0.4648,
      "step": 17350
    },
    {
      "epoch": 3.720531504500643,
      "grad_norm": 0.20419424772262573,
      "learning_rate": 1.5039291327332476e-05,
      "loss": 0.3497,
      "step": 17360
    },
    {
      "epoch": 3.722674667809687,
      "grad_norm": 0.13699012994766235,
      "learning_rate": 1.5036433776253752e-05,
      "loss": 0.1927,
      "step": 17370
    },
    {
      "epoch": 3.724817831118731,
      "grad_norm": 19.014406204223633,
      "learning_rate": 1.5033576225175026e-05,
      "loss": 0.9344,
      "step": 17380
    },
    {
      "epoch": 3.7269609944277753,
      "grad_norm": 16.94844627380371,
      "learning_rate": 1.5030718674096301e-05,
      "loss": 0.3741,
      "step": 17390
    },
    {
      "epoch": 3.7291041577368196,
      "grad_norm": 0.37424135208129883,
      "learning_rate": 1.5027861123017575e-05,
      "loss": 0.6864,
      "step": 17400
    },
    {
      "epoch": 3.7312473210458634,
      "grad_norm": 17.95694351196289,
      "learning_rate": 1.502500357193885e-05,
      "loss": 0.9444,
      "step": 17410
    },
    {
      "epoch": 3.7333904843549077,
      "grad_norm": 0.4052906036376953,
      "learning_rate": 1.5022146020860125e-05,
      "loss": 0.3274,
      "step": 17420
    },
    {
      "epoch": 3.735533647663952,
      "grad_norm": 0.3174594044685364,
      "learning_rate": 1.5019288469781399e-05,
      "loss": 0.8691,
      "step": 17430
    },
    {
      "epoch": 3.737676810972996,
      "grad_norm": 17.915983200073242,
      "learning_rate": 1.5016430918702673e-05,
      "loss": 0.8227,
      "step": 17440
    },
    {
      "epoch": 3.73981997428204,
      "grad_norm": 0.6251537799835205,
      "learning_rate": 1.5013573367623948e-05,
      "loss": 0.4831,
      "step": 17450
    },
    {
      "epoch": 3.7419631375910845,
      "grad_norm": 32.67329025268555,
      "learning_rate": 1.5010715816545222e-05,
      "loss": 0.672,
      "step": 17460
    },
    {
      "epoch": 3.7441063009001287,
      "grad_norm": 15.362391471862793,
      "learning_rate": 1.5007858265466498e-05,
      "loss": 0.6579,
      "step": 17470
    },
    {
      "epoch": 3.7462494642091726,
      "grad_norm": 0.49050477147102356,
      "learning_rate": 1.5005000714387772e-05,
      "loss": 0.9742,
      "step": 17480
    },
    {
      "epoch": 3.748392627518217,
      "grad_norm": 15.686385154724121,
      "learning_rate": 1.5002143163309046e-05,
      "loss": 0.4607,
      "step": 17490
    },
    {
      "epoch": 3.750535790827261,
      "grad_norm": 0.400287389755249,
      "learning_rate": 1.4999285612230318e-05,
      "loss": 0.4521,
      "step": 17500
    },
    {
      "epoch": 3.752678954136305,
      "grad_norm": 0.37359023094177246,
      "learning_rate": 1.4996428061151594e-05,
      "loss": 0.4794,
      "step": 17510
    },
    {
      "epoch": 3.7548221174453493,
      "grad_norm": 0.3593962788581848,
      "learning_rate": 1.4993570510072868e-05,
      "loss": 0.4994,
      "step": 17520
    },
    {
      "epoch": 3.7569652807543936,
      "grad_norm": 0.4291652739048004,
      "learning_rate": 1.4990712958994143e-05,
      "loss": 0.6562,
      "step": 17530
    },
    {
      "epoch": 3.7591084440634375,
      "grad_norm": 0.38163912296295166,
      "learning_rate": 1.4987855407915417e-05,
      "loss": 0.4862,
      "step": 17540
    },
    {
      "epoch": 3.7612516073724818,
      "grad_norm": 0.34507450461387634,
      "learning_rate": 1.4984997856836691e-05,
      "loss": 0.0082,
      "step": 17550
    },
    {
      "epoch": 3.763394770681526,
      "grad_norm": 0.2364489883184433,
      "learning_rate": 1.4982140305757967e-05,
      "loss": 0.1745,
      "step": 17560
    },
    {
      "epoch": 3.7655379339905704,
      "grad_norm": 0.20896382629871368,
      "learning_rate": 1.497928275467924e-05,
      "loss": 0.706,
      "step": 17570
    },
    {
      "epoch": 3.767681097299614,
      "grad_norm": 0.26756200194358826,
      "learning_rate": 1.4976425203600515e-05,
      "loss": 0.8727,
      "step": 17580
    },
    {
      "epoch": 3.7698242606086585,
      "grad_norm": 0.4011026620864868,
      "learning_rate": 1.497356765252179e-05,
      "loss": 1.0068,
      "step": 17590
    },
    {
      "epoch": 3.771967423917703,
      "grad_norm": 15.2356595993042,
      "learning_rate": 1.4970710101443064e-05,
      "loss": 0.9198,
      "step": 17600
    },
    {
      "epoch": 3.7741105872267466,
      "grad_norm": 0.7215611338615417,
      "learning_rate": 1.496785255036434e-05,
      "loss": 0.8729,
      "step": 17610
    },
    {
      "epoch": 3.776253750535791,
      "grad_norm": 15.197895050048828,
      "learning_rate": 1.4964994999285614e-05,
      "loss": 0.6993,
      "step": 17620
    },
    {
      "epoch": 3.7783969138448352,
      "grad_norm": 14.784025192260742,
      "learning_rate": 1.4962137448206888e-05,
      "loss": 0.6899,
      "step": 17630
    },
    {
      "epoch": 3.780540077153879,
      "grad_norm": 15.20535659790039,
      "learning_rate": 1.4959279897128163e-05,
      "loss": 0.2903,
      "step": 17640
    },
    {
      "epoch": 3.7826832404629234,
      "grad_norm": 0.642299473285675,
      "learning_rate": 1.4956422346049437e-05,
      "loss": 0.7323,
      "step": 17650
    },
    {
      "epoch": 3.7848264037719677,
      "grad_norm": 30.093456268310547,
      "learning_rate": 1.4953564794970713e-05,
      "loss": 0.857,
      "step": 17660
    },
    {
      "epoch": 3.7869695670810115,
      "grad_norm": 15.382423400878906,
      "learning_rate": 1.4950707243891987e-05,
      "loss": 0.5798,
      "step": 17670
    },
    {
      "epoch": 3.789112730390056,
      "grad_norm": 14.861058235168457,
      "learning_rate": 1.494784969281326e-05,
      "loss": 0.7207,
      "step": 17680
    },
    {
      "epoch": 3.7912558936991,
      "grad_norm": 15.718493461608887,
      "learning_rate": 1.4944992141734536e-05,
      "loss": 0.8461,
      "step": 17690
    },
    {
      "epoch": 3.793399057008144,
      "grad_norm": 15.822867393493652,
      "learning_rate": 1.494213459065581e-05,
      "loss": 0.686,
      "step": 17700
    },
    {
      "epoch": 3.7955422203171882,
      "grad_norm": 0.8300597667694092,
      "learning_rate": 1.4939277039577084e-05,
      "loss": 0.823,
      "step": 17710
    },
    {
      "epoch": 3.7976853836262325,
      "grad_norm": 0.8059408068656921,
      "learning_rate": 1.4936419488498356e-05,
      "loss": 0.2989,
      "step": 17720
    },
    {
      "epoch": 3.7998285469352764,
      "grad_norm": 0.6291930675506592,
      "learning_rate": 1.4933561937419632e-05,
      "loss": 0.2984,
      "step": 17730
    },
    {
      "epoch": 3.8019717102443207,
      "grad_norm": 0.4844498038291931,
      "learning_rate": 1.4930704386340906e-05,
      "loss": 0.7666,
      "step": 17740
    },
    {
      "epoch": 3.804114873553365,
      "grad_norm": 15.20286750793457,
      "learning_rate": 1.4927846835262181e-05,
      "loss": 0.7775,
      "step": 17750
    },
    {
      "epoch": 3.806258036862409,
      "grad_norm": 0.47189825773239136,
      "learning_rate": 1.4924989284183455e-05,
      "loss": 0.318,
      "step": 17760
    },
    {
      "epoch": 3.808401200171453,
      "grad_norm": 0.39647090435028076,
      "learning_rate": 1.492213173310473e-05,
      "loss": 0.3225,
      "step": 17770
    },
    {
      "epoch": 3.8105443634804974,
      "grad_norm": 0.3585710823535919,
      "learning_rate": 1.4919274182026005e-05,
      "loss": 0.3267,
      "step": 17780
    },
    {
      "epoch": 3.8126875267895413,
      "grad_norm": 0.3718299865722656,
      "learning_rate": 1.4916416630947279e-05,
      "loss": 0.4975,
      "step": 17790
    },
    {
      "epoch": 3.8148306900985856,
      "grad_norm": 0.3271718919277191,
      "learning_rate": 1.4913559079868554e-05,
      "loss": 0.3301,
      "step": 17800
    },
    {
      "epoch": 3.81697385340763,
      "grad_norm": 15.137473106384277,
      "learning_rate": 1.4910701528789828e-05,
      "loss": 0.498,
      "step": 17810
    },
    {
      "epoch": 3.8191170167166737,
      "grad_norm": 0.33253154158592224,
      "learning_rate": 1.4907843977711102e-05,
      "loss": 0.343,
      "step": 17820
    },
    {
      "epoch": 3.821260180025718,
      "grad_norm": 0.32007765769958496,
      "learning_rate": 1.4904986426632378e-05,
      "loss": 0.5108,
      "step": 17830
    },
    {
      "epoch": 3.8234033433347623,
      "grad_norm": 0.29488736391067505,
      "learning_rate": 1.4902128875553652e-05,
      "loss": 0.5121,
      "step": 17840
    },
    {
      "epoch": 3.825546506643806,
      "grad_norm": 0.32107654213905334,
      "learning_rate": 1.4899271324474926e-05,
      "loss": 1.0109,
      "step": 17850
    },
    {
      "epoch": 3.8276896699528504,
      "grad_norm": 0.3679601550102234,
      "learning_rate": 1.4896413773396201e-05,
      "loss": 0.3325,
      "step": 17860
    },
    {
      "epoch": 3.8298328332618947,
      "grad_norm": 0.3214395344257355,
      "learning_rate": 1.4893556222317475e-05,
      "loss": 0.6595,
      "step": 17870
    },
    {
      "epoch": 3.8319759965709386,
      "grad_norm": 0.3361361026763916,
      "learning_rate": 1.4890698671238751e-05,
      "loss": 0.1615,
      "step": 17880
    },
    {
      "epoch": 3.834119159879983,
      "grad_norm": 15.608548164367676,
      "learning_rate": 1.4887841120160025e-05,
      "loss": 0.5065,
      "step": 17890
    },
    {
      "epoch": 3.836262323189027,
      "grad_norm": 0.30855029821395874,
      "learning_rate": 1.4884983569081299e-05,
      "loss": 1.1781,
      "step": 17900
    },
    {
      "epoch": 3.838405486498071,
      "grad_norm": 32.032958984375,
      "learning_rate": 1.4882126018002574e-05,
      "loss": 0.8315,
      "step": 17910
    },
    {
      "epoch": 3.8405486498071153,
      "grad_norm": 16.153501510620117,
      "learning_rate": 1.4879268466923848e-05,
      "loss": 0.4874,
      "step": 17920
    },
    {
      "epoch": 3.8426918131161596,
      "grad_norm": 16.132848739624023,
      "learning_rate": 1.487641091584512e-05,
      "loss": 0.942,
      "step": 17930
    },
    {
      "epoch": 3.8448349764252034,
      "grad_norm": 0.4958658516407013,
      "learning_rate": 1.4873553364766396e-05,
      "loss": 0.3109,
      "step": 17940
    },
    {
      "epoch": 3.8469781397342477,
      "grad_norm": 17.77452278137207,
      "learning_rate": 1.487069581368767e-05,
      "loss": 0.915,
      "step": 17950
    },
    {
      "epoch": 3.849121303043292,
      "grad_norm": 16.96474266052246,
      "learning_rate": 1.4867838262608944e-05,
      "loss": 0.4394,
      "step": 17960
    },
    {
      "epoch": 3.851264466352336,
      "grad_norm": 0.6265007853507996,
      "learning_rate": 1.486498071153022e-05,
      "loss": 0.443,
      "step": 17970
    },
    {
      "epoch": 3.85340762966138,
      "grad_norm": 0.5856717824935913,
      "learning_rate": 1.4862123160451494e-05,
      "loss": 0.6019,
      "step": 17980
    },
    {
      "epoch": 3.8555507929704245,
      "grad_norm": 17.158226013183594,
      "learning_rate": 1.4859265609372768e-05,
      "loss": 0.4696,
      "step": 17990
    },
    {
      "epoch": 3.8576939562794683,
      "grad_norm": 15.352254867553711,
      "learning_rate": 1.4856408058294043e-05,
      "loss": 0.7888,
      "step": 18000
    },
    {
      "epoch": 3.8598371195885126,
      "grad_norm": 14.990509033203125,
      "learning_rate": 1.4853550507215317e-05,
      "loss": 0.4726,
      "step": 18010
    },
    {
      "epoch": 3.861980282897557,
      "grad_norm": 0.37419652938842773,
      "learning_rate": 1.4850692956136593e-05,
      "loss": 0.1643,
      "step": 18020
    },
    {
      "epoch": 3.8641234462066008,
      "grad_norm": 0.3582794964313507,
      "learning_rate": 1.4847835405057867e-05,
      "loss": 0.3223,
      "step": 18030
    },
    {
      "epoch": 3.866266609515645,
      "grad_norm": 0.34034815430641174,
      "learning_rate": 1.484497785397914e-05,
      "loss": 0.6666,
      "step": 18040
    },
    {
      "epoch": 3.8684097728246893,
      "grad_norm": 30.011272430419922,
      "learning_rate": 1.4842120302900416e-05,
      "loss": 0.9738,
      "step": 18050
    },
    {
      "epoch": 3.870552936133733,
      "grad_norm": 0.49172505736351013,
      "learning_rate": 1.483926275182169e-05,
      "loss": 0.7774,
      "step": 18060
    },
    {
      "epoch": 3.8726960994427775,
      "grad_norm": 0.5047164559364319,
      "learning_rate": 1.4836405200742964e-05,
      "loss": 0.4535,
      "step": 18070
    },
    {
      "epoch": 3.8748392627518218,
      "grad_norm": 0.5106288194656372,
      "learning_rate": 1.483354764966424e-05,
      "loss": 0.3034,
      "step": 18080
    },
    {
      "epoch": 3.8769824260608656,
      "grad_norm": 15.225110054016113,
      "learning_rate": 1.4830690098585514e-05,
      "loss": 0.463,
      "step": 18090
    },
    {
      "epoch": 3.87912558936991,
      "grad_norm": 15.258953094482422,
      "learning_rate": 1.482783254750679e-05,
      "loss": 1.076,
      "step": 18100
    },
    {
      "epoch": 3.881268752678954,
      "grad_norm": 29.541831970214844,
      "learning_rate": 1.4824974996428063e-05,
      "loss": 0.8749,
      "step": 18110
    },
    {
      "epoch": 3.883411915987998,
      "grad_norm": 0.6933501362800598,
      "learning_rate": 1.4822117445349337e-05,
      "loss": 0.575,
      "step": 18120
    },
    {
      "epoch": 3.8855550792970424,
      "grad_norm": 15.159832954406738,
      "learning_rate": 1.4819259894270613e-05,
      "loss": 0.7375,
      "step": 18130
    },
    {
      "epoch": 3.8876982426060867,
      "grad_norm": 0.6016374826431274,
      "learning_rate": 1.4816402343191887e-05,
      "loss": 0.1596,
      "step": 18140
    },
    {
      "epoch": 3.8898414059151305,
      "grad_norm": 14.692681312561035,
      "learning_rate": 1.4813544792113159e-05,
      "loss": 0.4729,
      "step": 18150
    },
    {
      "epoch": 3.891984569224175,
      "grad_norm": 0.46121692657470703,
      "learning_rate": 1.4810687241034435e-05,
      "loss": 0.7865,
      "step": 18160
    },
    {
      "epoch": 3.894127732533219,
      "grad_norm": 29.831974029541016,
      "learning_rate": 1.4807829689955708e-05,
      "loss": 0.9198,
      "step": 18170
    },
    {
      "epoch": 3.896270895842263,
      "grad_norm": 15.25552749633789,
      "learning_rate": 1.4804972138876982e-05,
      "loss": 0.8968,
      "step": 18180
    },
    {
      "epoch": 3.8984140591513072,
      "grad_norm": 0.7516806125640869,
      "learning_rate": 1.4802114587798258e-05,
      "loss": 0.4533,
      "step": 18190
    },
    {
      "epoch": 3.9005572224603515,
      "grad_norm": 0.6687058806419373,
      "learning_rate": 1.4799257036719532e-05,
      "loss": 0.5718,
      "step": 18200
    },
    {
      "epoch": 3.9027003857693954,
      "grad_norm": 0.5744920969009399,
      "learning_rate": 1.4796399485640806e-05,
      "loss": 0.585,
      "step": 18210
    },
    {
      "epoch": 3.9048435490784397,
      "grad_norm": 0.6430491209030151,
      "learning_rate": 1.4793541934562082e-05,
      "loss": 0.586,
      "step": 18220
    },
    {
      "epoch": 3.906986712387484,
      "grad_norm": 16.009296417236328,
      "learning_rate": 1.4790684383483355e-05,
      "loss": 0.5915,
      "step": 18230
    },
    {
      "epoch": 3.909129875696528,
      "grad_norm": 0.5429756045341492,
      "learning_rate": 1.4787826832404631e-05,
      "loss": 0.6098,
      "step": 18240
    },
    {
      "epoch": 3.911273039005572,
      "grad_norm": 14.747625350952148,
      "learning_rate": 1.4784969281325905e-05,
      "loss": 0.4675,
      "step": 18250
    },
    {
      "epoch": 3.9134162023146164,
      "grad_norm": 0.4850307106971741,
      "learning_rate": 1.4782111730247179e-05,
      "loss": 0.6209,
      "step": 18260
    },
    {
      "epoch": 3.9155593656236602,
      "grad_norm": 14.722479820251465,
      "learning_rate": 1.4779254179168455e-05,
      "loss": 0.786,
      "step": 18270
    },
    {
      "epoch": 3.9177025289327045,
      "grad_norm": 0.463718444108963,
      "learning_rate": 1.4776396628089728e-05,
      "loss": 0.3178,
      "step": 18280
    },
    {
      "epoch": 3.919845692241749,
      "grad_norm": 0.4651043117046356,
      "learning_rate": 1.4773539077011002e-05,
      "loss": 0.9411,
      "step": 18290
    },
    {
      "epoch": 3.921988855550793,
      "grad_norm": 0.6266505122184753,
      "learning_rate": 1.4770681525932278e-05,
      "loss": 1.1999,
      "step": 18300
    },
    {
      "epoch": 3.924132018859837,
      "grad_norm": 0.7862668037414551,
      "learning_rate": 1.4767823974853552e-05,
      "loss": 0.4309,
      "step": 18310
    },
    {
      "epoch": 3.9262751821688813,
      "grad_norm": 14.765913009643555,
      "learning_rate": 1.4764966423774828e-05,
      "loss": 0.5651,
      "step": 18320
    },
    {
      "epoch": 3.9284183454779256,
      "grad_norm": 0.6657131314277649,
      "learning_rate": 1.4762108872696101e-05,
      "loss": 0.8509,
      "step": 18330
    },
    {
      "epoch": 3.9305615087869694,
      "grad_norm": 14.436700820922852,
      "learning_rate": 1.4759251321617375e-05,
      "loss": 0.8344,
      "step": 18340
    },
    {
      "epoch": 3.9327046720960137,
      "grad_norm": 0.9131069779396057,
      "learning_rate": 1.4756393770538651e-05,
      "loss": 0.5468,
      "step": 18350
    },
    {
      "epoch": 3.934847835405058,
      "grad_norm": 1.106161117553711,
      "learning_rate": 1.4753536219459923e-05,
      "loss": 1.0386,
      "step": 18360
    },
    {
      "epoch": 3.936990998714102,
      "grad_norm": 1.1463898420333862,
      "learning_rate": 1.4750678668381197e-05,
      "loss": 0.3847,
      "step": 18370
    },
    {
      "epoch": 3.939134162023146,
      "grad_norm": 15.426803588867188,
      "learning_rate": 1.4747821117302473e-05,
      "loss": 0.6859,
      "step": 18380
    },
    {
      "epoch": 3.9412773253321904,
      "grad_norm": 0.8236062526702881,
      "learning_rate": 1.4744963566223747e-05,
      "loss": 0.8254,
      "step": 18390
    },
    {
      "epoch": 3.9434204886412347,
      "grad_norm": 14.661813735961914,
      "learning_rate": 1.474210601514502e-05,
      "loss": 0.9384,
      "step": 18400
    },
    {
      "epoch": 3.9455636519502786,
      "grad_norm": 14.542082786560059,
      "learning_rate": 1.4739248464066296e-05,
      "loss": 0.4101,
      "step": 18410
    },
    {
      "epoch": 3.947706815259323,
      "grad_norm": 15.092669486999512,
      "learning_rate": 1.473639091298757e-05,
      "loss": 0.4151,
      "step": 18420
    },
    {
      "epoch": 3.949849978568367,
      "grad_norm": 0.6502286791801453,
      "learning_rate": 1.4733533361908844e-05,
      "loss": 0.1556,
      "step": 18430
    },
    {
      "epoch": 3.951993141877411,
      "grad_norm": 30.821229934692383,
      "learning_rate": 1.473067581083012e-05,
      "loss": 0.9069,
      "step": 18440
    },
    {
      "epoch": 3.9541363051864553,
      "grad_norm": 30.278104782104492,
      "learning_rate": 1.4727818259751394e-05,
      "loss": 1.1852,
      "step": 18450
    },
    {
      "epoch": 3.9562794684954996,
      "grad_norm": 0.7542192935943604,
      "learning_rate": 1.472496070867267e-05,
      "loss": 0.7062,
      "step": 18460
    },
    {
      "epoch": 3.9584226318045435,
      "grad_norm": 0.6162928342819214,
      "learning_rate": 1.4722103157593943e-05,
      "loss": 0.1563,
      "step": 18470
    },
    {
      "epoch": 3.9605657951135878,
      "grad_norm": 0.5775378346443176,
      "learning_rate": 1.4719245606515217e-05,
      "loss": 0.7356,
      "step": 18480
    },
    {
      "epoch": 3.962708958422632,
      "grad_norm": 14.54152774810791,
      "learning_rate": 1.4716388055436493e-05,
      "loss": 0.6005,
      "step": 18490
    },
    {
      "epoch": 3.964852121731676,
      "grad_norm": 0.5829058289527893,
      "learning_rate": 1.4713530504357767e-05,
      "loss": 0.454,
      "step": 18500
    },
    {
      "epoch": 3.96699528504072,
      "grad_norm": 17.542360305786133,
      "learning_rate": 1.4710672953279042e-05,
      "loss": 0.6223,
      "step": 18510
    },
    {
      "epoch": 3.9691384483497645,
      "grad_norm": 15.096410751342773,
      "learning_rate": 1.4707815402200316e-05,
      "loss": 0.4623,
      "step": 18520
    },
    {
      "epoch": 3.9712816116588083,
      "grad_norm": 0.5730249285697937,
      "learning_rate": 1.470495785112159e-05,
      "loss": 0.6187,
      "step": 18530
    },
    {
      "epoch": 3.9734247749678526,
      "grad_norm": 30.5485782623291,
      "learning_rate": 1.4702100300042866e-05,
      "loss": 1.2087,
      "step": 18540
    },
    {
      "epoch": 3.975567938276897,
      "grad_norm": 15.647767066955566,
      "learning_rate": 1.469924274896414e-05,
      "loss": 0.5949,
      "step": 18550
    },
    {
      "epoch": 3.9777111015859408,
      "grad_norm": 15.08694076538086,
      "learning_rate": 1.4696385197885414e-05,
      "loss": 0.3089,
      "step": 18560
    },
    {
      "epoch": 3.979854264894985,
      "grad_norm": 0.5297611951828003,
      "learning_rate": 1.469352764680669e-05,
      "loss": 0.4635,
      "step": 18570
    },
    {
      "epoch": 3.9819974282040294,
      "grad_norm": 0.4013197720050812,
      "learning_rate": 1.4690670095727962e-05,
      "loss": 0.1679,
      "step": 18580
    },
    {
      "epoch": 3.984140591513073,
      "grad_norm": 0.38399529457092285,
      "learning_rate": 1.4687812544649235e-05,
      "loss": 0.644,
      "step": 18590
    },
    {
      "epoch": 3.9862837548221175,
      "grad_norm": 0.43322211503982544,
      "learning_rate": 1.4684954993570511e-05,
      "loss": 1.1206,
      "step": 18600
    },
    {
      "epoch": 3.988426918131162,
      "grad_norm": 0.456045538187027,
      "learning_rate": 1.4682097442491785e-05,
      "loss": 0.6279,
      "step": 18610
    },
    {
      "epoch": 3.9905700814402056,
      "grad_norm": 0.4684552848339081,
      "learning_rate": 1.4679239891413059e-05,
      "loss": 0.4685,
      "step": 18620
    },
    {
      "epoch": 3.99271324474925,
      "grad_norm": 0.4900135099887848,
      "learning_rate": 1.4676382340334335e-05,
      "loss": 1.0711,
      "step": 18630
    },
    {
      "epoch": 3.9948564080582942,
      "grad_norm": 0.5667321681976318,
      "learning_rate": 1.4673524789255609e-05,
      "loss": 0.7525,
      "step": 18640
    },
    {
      "epoch": 3.996999571367338,
      "grad_norm": 0.566409707069397,
      "learning_rate": 1.4670667238176884e-05,
      "loss": 0.7457,
      "step": 18650
    },
    {
      "epoch": 3.9991427346763824,
      "grad_norm": 0.5456860661506653,
      "learning_rate": 1.4667809687098158e-05,
      "loss": 0.6003,
      "step": 18660
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.8676666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.5955194234848022,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 397.3199,
      "eval_samples_per_second": 7.551,
      "eval_steps_per_second": 2.517,
      "step": 18664
    },
    {
      "epoch": 4.001285897985427,
      "grad_norm": 14.623750686645508,
      "learning_rate": 1.4664952136019432e-05,
      "loss": 0.6008,
      "step": 18670
    },
    {
      "epoch": 4.0034290612944705,
      "grad_norm": 14.803803443908691,
      "learning_rate": 1.4662094584940708e-05,
      "loss": 0.7478,
      "step": 18680
    },
    {
      "epoch": 4.005572224603514,
      "grad_norm": 0.5514478087425232,
      "learning_rate": 1.4659237033861982e-05,
      "loss": 0.1567,
      "step": 18690
    },
    {
      "epoch": 4.007715387912559,
      "grad_norm": 0.5482776165008545,
      "learning_rate": 1.4656379482783255e-05,
      "loss": 0.4507,
      "step": 18700
    },
    {
      "epoch": 4.009858551221603,
      "grad_norm": 0.5167407393455505,
      "learning_rate": 1.4653521931704531e-05,
      "loss": 0.7503,
      "step": 18710
    },
    {
      "epoch": 4.012001714530647,
      "grad_norm": 15.040021896362305,
      "learning_rate": 1.4650664380625805e-05,
      "loss": 0.606,
      "step": 18720
    },
    {
      "epoch": 4.0141448778396915,
      "grad_norm": 0.583371102809906,
      "learning_rate": 1.464780682954708e-05,
      "loss": 0.4521,
      "step": 18730
    },
    {
      "epoch": 4.016288041148735,
      "grad_norm": 14.538261413574219,
      "learning_rate": 1.4644949278468355e-05,
      "loss": 0.5915,
      "step": 18740
    },
    {
      "epoch": 4.01843120445778,
      "grad_norm": 14.535694122314453,
      "learning_rate": 1.4642091727389629e-05,
      "loss": 1.0295,
      "step": 18750
    },
    {
      "epoch": 4.020574367766824,
      "grad_norm": 0.6955288052558899,
      "learning_rate": 1.4639234176310904e-05,
      "loss": 0.5809,
      "step": 18760
    },
    {
      "epoch": 4.022717531075868,
      "grad_norm": 14.756061553955078,
      "learning_rate": 1.4636376625232178e-05,
      "loss": 0.5715,
      "step": 18770
    },
    {
      "epoch": 4.024860694384913,
      "grad_norm": 14.702451705932617,
      "learning_rate": 1.4633519074153452e-05,
      "loss": 0.5783,
      "step": 18780
    },
    {
      "epoch": 4.027003857693956,
      "grad_norm": 0.6476909518241882,
      "learning_rate": 1.4630661523074726e-05,
      "loss": 0.5835,
      "step": 18790
    },
    {
      "epoch": 4.029147021003,
      "grad_norm": 29.44179916381836,
      "learning_rate": 1.4627803971996e-05,
      "loss": 0.8685,
      "step": 18800
    },
    {
      "epoch": 4.031290184312045,
      "grad_norm": 14.627503395080566,
      "learning_rate": 1.4624946420917274e-05,
      "loss": 0.7242,
      "step": 18810
    },
    {
      "epoch": 4.033433347621089,
      "grad_norm": 0.6439825892448425,
      "learning_rate": 1.462208886983855e-05,
      "loss": 0.4415,
      "step": 18820
    },
    {
      "epoch": 4.035576510930133,
      "grad_norm": 0.612409770488739,
      "learning_rate": 1.4619231318759823e-05,
      "loss": 0.4425,
      "step": 18830
    },
    {
      "epoch": 4.037719674239177,
      "grad_norm": 29.68379783630371,
      "learning_rate": 1.4616373767681097e-05,
      "loss": 0.8779,
      "step": 18840
    },
    {
      "epoch": 4.039862837548221,
      "grad_norm": 0.5780922174453735,
      "learning_rate": 1.4613516216602373e-05,
      "loss": 0.5944,
      "step": 18850
    },
    {
      "epoch": 4.042006000857265,
      "grad_norm": 0.6350778341293335,
      "learning_rate": 1.4610658665523647e-05,
      "loss": 0.8735,
      "step": 18860
    },
    {
      "epoch": 4.04414916416631,
      "grad_norm": 14.437865257263184,
      "learning_rate": 1.4607801114444922e-05,
      "loss": 0.9801,
      "step": 18870
    },
    {
      "epoch": 4.046292327475354,
      "grad_norm": 0.910353422164917,
      "learning_rate": 1.4604943563366196e-05,
      "loss": 0.8212,
      "step": 18880
    },
    {
      "epoch": 4.048435490784398,
      "grad_norm": 0.8729599118232727,
      "learning_rate": 1.460208601228747e-05,
      "loss": 0.4114,
      "step": 18890
    },
    {
      "epoch": 4.050578654093442,
      "grad_norm": 0.8389845490455627,
      "learning_rate": 1.4599228461208746e-05,
      "loss": 0.5435,
      "step": 18900
    },
    {
      "epoch": 4.052721817402486,
      "grad_norm": 0.7540870308876038,
      "learning_rate": 1.459637091013002e-05,
      "loss": 0.8473,
      "step": 18910
    },
    {
      "epoch": 4.05486498071153,
      "grad_norm": 0.7386357188224792,
      "learning_rate": 1.4593513359051294e-05,
      "loss": 0.2909,
      "step": 18920
    },
    {
      "epoch": 4.057008144020575,
      "grad_norm": 0.6641172766685486,
      "learning_rate": 1.459065580797257e-05,
      "loss": 0.7223,
      "step": 18930
    },
    {
      "epoch": 4.059151307329619,
      "grad_norm": 0.7049168944358826,
      "learning_rate": 1.4587798256893843e-05,
      "loss": 1.1264,
      "step": 18940
    },
    {
      "epoch": 4.0612944706386624,
      "grad_norm": 0.7316290736198425,
      "learning_rate": 1.4584940705815119e-05,
      "loss": 0.2919,
      "step": 18950
    },
    {
      "epoch": 4.063437633947707,
      "grad_norm": 0.6565561294555664,
      "learning_rate": 1.4582083154736393e-05,
      "loss": 0.5741,
      "step": 18960
    },
    {
      "epoch": 4.065580797256751,
      "grad_norm": 0.6225579977035522,
      "learning_rate": 1.4579225603657667e-05,
      "loss": 0.4363,
      "step": 18970
    },
    {
      "epoch": 4.067723960565795,
      "grad_norm": 0.5118199586868286,
      "learning_rate": 1.4576368052578942e-05,
      "loss": 0.307,
      "step": 18980
    },
    {
      "epoch": 4.06986712387484,
      "grad_norm": 0.501221239566803,
      "learning_rate": 1.4573510501500216e-05,
      "loss": 0.6055,
      "step": 18990
    },
    {
      "epoch": 4.0720102871838835,
      "grad_norm": 0.47949615120887756,
      "learning_rate": 1.457065295042149e-05,
      "loss": 0.909,
      "step": 19000
    },
    {
      "epoch": 4.074153450492927,
      "grad_norm": 0.545778751373291,
      "learning_rate": 1.4567795399342764e-05,
      "loss": 1.0553,
      "step": 19010
    },
    {
      "epoch": 4.076296613801972,
      "grad_norm": 14.512401580810547,
      "learning_rate": 1.4564937848264038e-05,
      "loss": 0.5868,
      "step": 19020
    },
    {
      "epoch": 4.078439777111016,
      "grad_norm": 0.6894742846488953,
      "learning_rate": 1.4562080297185312e-05,
      "loss": 0.8632,
      "step": 19030
    },
    {
      "epoch": 4.08058294042006,
      "grad_norm": 14.375349998474121,
      "learning_rate": 1.4559222746106588e-05,
      "loss": 0.8356,
      "step": 19040
    },
    {
      "epoch": 4.0827261037291045,
      "grad_norm": 14.566534042358398,
      "learning_rate": 1.4556365195027862e-05,
      "loss": 0.8405,
      "step": 19050
    },
    {
      "epoch": 4.084869267038148,
      "grad_norm": 0.6857842803001404,
      "learning_rate": 1.4553507643949136e-05,
      "loss": 0.565,
      "step": 19060
    },
    {
      "epoch": 4.087012430347192,
      "grad_norm": 0.8645095229148865,
      "learning_rate": 1.4550650092870411e-05,
      "loss": 1.2589,
      "step": 19070
    },
    {
      "epoch": 4.089155593656237,
      "grad_norm": 0.7708257436752319,
      "learning_rate": 1.4547792541791685e-05,
      "loss": 0.4197,
      "step": 19080
    },
    {
      "epoch": 4.091298756965281,
      "grad_norm": 0.685671329498291,
      "learning_rate": 1.454493499071296e-05,
      "loss": 0.5642,
      "step": 19090
    },
    {
      "epoch": 4.093441920274325,
      "grad_norm": 0.6933147311210632,
      "learning_rate": 1.4542077439634235e-05,
      "loss": 0.7079,
      "step": 19100
    },
    {
      "epoch": 4.095585083583369,
      "grad_norm": 0.620220959186554,
      "learning_rate": 1.4539219888555509e-05,
      "loss": 0.2959,
      "step": 19110
    },
    {
      "epoch": 4.097728246892413,
      "grad_norm": 0.5931236147880554,
      "learning_rate": 1.4536362337476784e-05,
      "loss": 0.4428,
      "step": 19120
    },
    {
      "epoch": 4.099871410201457,
      "grad_norm": 14.584933280944824,
      "learning_rate": 1.4533504786398058e-05,
      "loss": 0.4479,
      "step": 19130
    },
    {
      "epoch": 4.102014573510502,
      "grad_norm": 29.993539810180664,
      "learning_rate": 1.4530647235319332e-05,
      "loss": 0.8976,
      "step": 19140
    },
    {
      "epoch": 4.104157736819546,
      "grad_norm": 29.734041213989258,
      "learning_rate": 1.4527789684240608e-05,
      "loss": 0.5978,
      "step": 19150
    },
    {
      "epoch": 4.1063009001285895,
      "grad_norm": 0.562401294708252,
      "learning_rate": 1.4524932133161882e-05,
      "loss": 0.747,
      "step": 19160
    },
    {
      "epoch": 4.108444063437634,
      "grad_norm": 14.539604187011719,
      "learning_rate": 1.4522074582083157e-05,
      "loss": 0.8873,
      "step": 19170
    },
    {
      "epoch": 4.110587226746678,
      "grad_norm": 0.6373733878135681,
      "learning_rate": 1.4519217031004431e-05,
      "loss": 0.5902,
      "step": 19180
    },
    {
      "epoch": 4.112730390055722,
      "grad_norm": 0.6345820426940918,
      "learning_rate": 1.4516359479925705e-05,
      "loss": 0.5751,
      "step": 19190
    },
    {
      "epoch": 4.114873553364767,
      "grad_norm": 29.706249237060547,
      "learning_rate": 1.451350192884698e-05,
      "loss": 1.1624,
      "step": 19200
    },
    {
      "epoch": 4.1170167166738105,
      "grad_norm": 0.6052966713905334,
      "learning_rate": 1.4510644377768255e-05,
      "loss": 0.4388,
      "step": 19210
    },
    {
      "epoch": 4.119159879982854,
      "grad_norm": 0.6694036722183228,
      "learning_rate": 1.4507786826689527e-05,
      "loss": 0.7236,
      "step": 19220
    },
    {
      "epoch": 4.121303043291899,
      "grad_norm": 0.5390774607658386,
      "learning_rate": 1.4504929275610802e-05,
      "loss": 0.1588,
      "step": 19230
    },
    {
      "epoch": 4.123446206600943,
      "grad_norm": 0.4869096279144287,
      "learning_rate": 1.4502071724532076e-05,
      "loss": 0.462,
      "step": 19240
    },
    {
      "epoch": 4.125589369909987,
      "grad_norm": 0.49474409222602844,
      "learning_rate": 1.449921417345335e-05,
      "loss": 0.9148,
      "step": 19250
    },
    {
      "epoch": 4.1277325332190316,
      "grad_norm": 0.5651565790176392,
      "learning_rate": 1.4496356622374626e-05,
      "loss": 1.0464,
      "step": 19260
    },
    {
      "epoch": 4.129875696528075,
      "grad_norm": 0.6093942523002625,
      "learning_rate": 1.44934990712959e-05,
      "loss": 0.7373,
      "step": 19270
    },
    {
      "epoch": 4.132018859837119,
      "grad_norm": 0.6239624619483948,
      "learning_rate": 1.4490641520217174e-05,
      "loss": 0.7298,
      "step": 19280
    },
    {
      "epoch": 4.134162023146164,
      "grad_norm": 14.477228164672852,
      "learning_rate": 1.448778396913845e-05,
      "loss": 0.4404,
      "step": 19290
    },
    {
      "epoch": 4.136305186455208,
      "grad_norm": 0.5798612236976624,
      "learning_rate": 1.4484926418059723e-05,
      "loss": 0.157,
      "step": 19300
    },
    {
      "epoch": 4.138448349764252,
      "grad_norm": 0.5091378092765808,
      "learning_rate": 1.4482068866980999e-05,
      "loss": 0.4567,
      "step": 19310
    },
    {
      "epoch": 4.140591513073296,
      "grad_norm": 0.5011678338050842,
      "learning_rate": 1.4479211315902273e-05,
      "loss": 1.0567,
      "step": 19320
    },
    {
      "epoch": 4.14273467638234,
      "grad_norm": 15.626975059509277,
      "learning_rate": 1.4476353764823547e-05,
      "loss": 0.6045,
      "step": 19330
    },
    {
      "epoch": 4.144877839691384,
      "grad_norm": 0.5491816997528076,
      "learning_rate": 1.4473496213744822e-05,
      "loss": 0.4489,
      "step": 19340
    },
    {
      "epoch": 4.147021003000429,
      "grad_norm": 0.5006697773933411,
      "learning_rate": 1.4470638662666096e-05,
      "loss": 0.0122,
      "step": 19350
    },
    {
      "epoch": 4.149164166309473,
      "grad_norm": 14.601643562316895,
      "learning_rate": 1.4467781111587372e-05,
      "loss": 1.0644,
      "step": 19360
    },
    {
      "epoch": 4.151307329618517,
      "grad_norm": 14.528858184814453,
      "learning_rate": 1.4464923560508646e-05,
      "loss": 0.6031,
      "step": 19370
    },
    {
      "epoch": 4.153450492927561,
      "grad_norm": 0.5285123586654663,
      "learning_rate": 1.446206600942992e-05,
      "loss": 0.4599,
      "step": 19380
    },
    {
      "epoch": 4.155593656236605,
      "grad_norm": 0.5275326371192932,
      "learning_rate": 1.4459208458351195e-05,
      "loss": 0.7553,
      "step": 19390
    },
    {
      "epoch": 4.157736819545649,
      "grad_norm": 14.727141380310059,
      "learning_rate": 1.445635090727247e-05,
      "loss": 0.6028,
      "step": 19400
    },
    {
      "epoch": 4.159879982854694,
      "grad_norm": 29.51881217956543,
      "learning_rate": 1.4453493356193743e-05,
      "loss": 1.3237,
      "step": 19410
    },
    {
      "epoch": 4.162023146163738,
      "grad_norm": 0.6702279448509216,
      "learning_rate": 1.4450635805115019e-05,
      "loss": 0.5878,
      "step": 19420
    },
    {
      "epoch": 4.164166309472781,
      "grad_norm": 0.6620425581932068,
      "learning_rate": 1.4447778254036291e-05,
      "loss": 0.7206,
      "step": 19430
    },
    {
      "epoch": 4.166309472781826,
      "grad_norm": 0.6524767875671387,
      "learning_rate": 1.4444920702957565e-05,
      "loss": 0.7242,
      "step": 19440
    },
    {
      "epoch": 4.16845263609087,
      "grad_norm": 0.6207259297370911,
      "learning_rate": 1.444206315187884e-05,
      "loss": 0.2959,
      "step": 19450
    },
    {
      "epoch": 4.170595799399914,
      "grad_norm": 14.657526969909668,
      "learning_rate": 1.4439205600800115e-05,
      "loss": 0.596,
      "step": 19460
    },
    {
      "epoch": 4.172738962708959,
      "grad_norm": 0.5380248427391052,
      "learning_rate": 1.4436348049721389e-05,
      "loss": 0.7482,
      "step": 19470
    },
    {
      "epoch": 4.1748821260180025,
      "grad_norm": 0.6289853453636169,
      "learning_rate": 1.4433490498642664e-05,
      "loss": 0.5986,
      "step": 19480
    },
    {
      "epoch": 4.177025289327046,
      "grad_norm": 0.6299093961715698,
      "learning_rate": 1.4430632947563938e-05,
      "loss": 0.7219,
      "step": 19490
    },
    {
      "epoch": 4.179168452636091,
      "grad_norm": 0.5404736995697021,
      "learning_rate": 1.4427775396485214e-05,
      "loss": 0.3043,
      "step": 19500
    },
    {
      "epoch": 4.181311615945135,
      "grad_norm": 0.5415958166122437,
      "learning_rate": 1.4424917845406488e-05,
      "loss": 0.7504,
      "step": 19510
    },
    {
      "epoch": 4.183454779254179,
      "grad_norm": 0.5017533302307129,
      "learning_rate": 1.4422060294327762e-05,
      "loss": 0.4556,
      "step": 19520
    },
    {
      "epoch": 4.1855979425632235,
      "grad_norm": 0.4497118890285492,
      "learning_rate": 1.4419202743249037e-05,
      "loss": 0.3112,
      "step": 19530
    },
    {
      "epoch": 4.187741105872267,
      "grad_norm": 0.44995880126953125,
      "learning_rate": 1.4416345192170311e-05,
      "loss": 0.9377,
      "step": 19540
    },
    {
      "epoch": 4.189884269181311,
      "grad_norm": 14.584393501281738,
      "learning_rate": 1.4413487641091585e-05,
      "loss": 0.7592,
      "step": 19550
    },
    {
      "epoch": 4.192027432490356,
      "grad_norm": 14.683505058288574,
      "learning_rate": 1.441063009001286e-05,
      "loss": 0.6102,
      "step": 19560
    },
    {
      "epoch": 4.1941705957994,
      "grad_norm": 14.623652458190918,
      "learning_rate": 1.4407772538934135e-05,
      "loss": 0.3095,
      "step": 19570
    },
    {
      "epoch": 4.196313759108444,
      "grad_norm": 0.5371366739273071,
      "learning_rate": 1.440491498785541e-05,
      "loss": 0.6095,
      "step": 19580
    },
    {
      "epoch": 4.198456922417488,
      "grad_norm": 0.48790857195854187,
      "learning_rate": 1.4402057436776684e-05,
      "loss": 0.0111,
      "step": 19590
    },
    {
      "epoch": 4.200600085726532,
      "grad_norm": 0.37964364886283875,
      "learning_rate": 1.4399199885697958e-05,
      "loss": 0.4763,
      "step": 19600
    },
    {
      "epoch": 4.202743249035576,
      "grad_norm": 14.685091018676758,
      "learning_rate": 1.4396342334619234e-05,
      "loss": 0.9763,
      "step": 19610
    },
    {
      "epoch": 4.204886412344621,
      "grad_norm": 0.41334620118141174,
      "learning_rate": 1.4393484783540508e-05,
      "loss": 0.64,
      "step": 19620
    },
    {
      "epoch": 4.207029575653665,
      "grad_norm": 0.4407466650009155,
      "learning_rate": 1.4390627232461782e-05,
      "loss": 0.3189,
      "step": 19630
    },
    {
      "epoch": 4.209172738962709,
      "grad_norm": 0.43600937724113464,
      "learning_rate": 1.4387769681383057e-05,
      "loss": 0.4727,
      "step": 19640
    },
    {
      "epoch": 4.211315902271753,
      "grad_norm": 0.4500173330307007,
      "learning_rate": 1.438491213030433e-05,
      "loss": 0.7752,
      "step": 19650
    },
    {
      "epoch": 4.213459065580797,
      "grad_norm": 15.896896362304688,
      "learning_rate": 1.4382054579225603e-05,
      "loss": 0.9415,
      "step": 19660
    },
    {
      "epoch": 4.215602228889842,
      "grad_norm": 0.5087045431137085,
      "learning_rate": 1.4379197028146879e-05,
      "loss": 0.6191,
      "step": 19670
    },
    {
      "epoch": 4.217745392198886,
      "grad_norm": 0.5198479890823364,
      "learning_rate": 1.4376339477068153e-05,
      "loss": 0.465,
      "step": 19680
    },
    {
      "epoch": 4.2198885555079295,
      "grad_norm": 0.4985314607620239,
      "learning_rate": 1.4373481925989427e-05,
      "loss": 0.6112,
      "step": 19690
    },
    {
      "epoch": 4.222031718816974,
      "grad_norm": 0.49901726841926575,
      "learning_rate": 1.4370624374910703e-05,
      "loss": 0.4621,
      "step": 19700
    },
    {
      "epoch": 4.224174882126018,
      "grad_norm": 0.4624170958995819,
      "learning_rate": 1.4367766823831976e-05,
      "loss": 0.3127,
      "step": 19710
    },
    {
      "epoch": 4.226318045435062,
      "grad_norm": 0.4436253607273102,
      "learning_rate": 1.4364909272753252e-05,
      "loss": 0.626,
      "step": 19720
    },
    {
      "epoch": 4.228461208744107,
      "grad_norm": 0.4430484473705292,
      "learning_rate": 1.4362051721674526e-05,
      "loss": 1.0833,
      "step": 19730
    },
    {
      "epoch": 4.2306043720531505,
      "grad_norm": 14.611135482788086,
      "learning_rate": 1.43591941705958e-05,
      "loss": 0.469,
      "step": 19740
    },
    {
      "epoch": 4.232747535362194,
      "grad_norm": 0.479047566652298,
      "learning_rate": 1.4356336619517076e-05,
      "loss": 0.6232,
      "step": 19750
    },
    {
      "epoch": 4.234890698671239,
      "grad_norm": 0.42462974786758423,
      "learning_rate": 1.435347906843835e-05,
      "loss": 0.1649,
      "step": 19760
    },
    {
      "epoch": 4.237033861980283,
      "grad_norm": 14.737691879272461,
      "learning_rate": 1.4350621517359623e-05,
      "loss": 0.6295,
      "step": 19770
    },
    {
      "epoch": 4.239177025289327,
      "grad_norm": 0.4019985496997833,
      "learning_rate": 1.4347763966280899e-05,
      "loss": 0.6358,
      "step": 19780
    },
    {
      "epoch": 4.241320188598372,
      "grad_norm": 14.665092468261719,
      "learning_rate": 1.4344906415202173e-05,
      "loss": 0.7877,
      "step": 19790
    },
    {
      "epoch": 4.243463351907415,
      "grad_norm": 0.42741066217422485,
      "learning_rate": 1.4342048864123449e-05,
      "loss": 0.32,
      "step": 19800
    },
    {
      "epoch": 4.245606515216459,
      "grad_norm": 0.4488667845726013,
      "learning_rate": 1.4339191313044723e-05,
      "loss": 0.7815,
      "step": 19810
    },
    {
      "epoch": 4.247749678525504,
      "grad_norm": 0.46654948592185974,
      "learning_rate": 1.4336333761965996e-05,
      "loss": 0.9283,
      "step": 19820
    },
    {
      "epoch": 4.249892841834548,
      "grad_norm": 0.4674835503101349,
      "learning_rate": 1.4333476210887272e-05,
      "loss": 0.615,
      "step": 19830
    },
    {
      "epoch": 4.252036005143592,
      "grad_norm": 0.48937854170799255,
      "learning_rate": 1.4330618659808546e-05,
      "loss": 0.7728,
      "step": 19840
    },
    {
      "epoch": 4.254179168452636,
      "grad_norm": 0.4766487181186676,
      "learning_rate": 1.432776110872982e-05,
      "loss": 0.615,
      "step": 19850
    },
    {
      "epoch": 4.25632233176168,
      "grad_norm": 0.5462271571159363,
      "learning_rate": 1.4324903557651094e-05,
      "loss": 0.7579,
      "step": 19860
    },
    {
      "epoch": 4.258465495070724,
      "grad_norm": 0.47323375940322876,
      "learning_rate": 1.4322046006572368e-05,
      "loss": 0.1606,
      "step": 19870
    },
    {
      "epoch": 4.260608658379769,
      "grad_norm": 0.4591849148273468,
      "learning_rate": 1.4319188455493642e-05,
      "loss": 0.9347,
      "step": 19880
    },
    {
      "epoch": 4.262751821688813,
      "grad_norm": 0.479665607213974,
      "learning_rate": 1.4316330904414917e-05,
      "loss": 0.3154,
      "step": 19890
    },
    {
      "epoch": 4.264894984997857,
      "grad_norm": 14.781761169433594,
      "learning_rate": 1.4313473353336191e-05,
      "loss": 0.9414,
      "step": 19900
    },
    {
      "epoch": 4.267038148306901,
      "grad_norm": 14.628820419311523,
      "learning_rate": 1.4310615802257465e-05,
      "loss": 0.9428,
      "step": 19910
    },
    {
      "epoch": 4.269181311615945,
      "grad_norm": 0.4666427671909332,
      "learning_rate": 1.430775825117874e-05,
      "loss": 0.4698,
      "step": 19920
    },
    {
      "epoch": 4.271324474924989,
      "grad_norm": 0.46250858902931213,
      "learning_rate": 1.4304900700100015e-05,
      "loss": 0.623,
      "step": 19930
    },
    {
      "epoch": 4.273467638234034,
      "grad_norm": 0.43236997723579407,
      "learning_rate": 1.430204314902129e-05,
      "loss": 0.1673,
      "step": 19940
    },
    {
      "epoch": 4.275610801543078,
      "grad_norm": 0.40204736590385437,
      "learning_rate": 1.4299185597942564e-05,
      "loss": 0.7901,
      "step": 19950
    },
    {
      "epoch": 4.2777539648521214,
      "grad_norm": 0.3912244141101837,
      "learning_rate": 1.4296328046863838e-05,
      "loss": 0.4799,
      "step": 19960
    },
    {
      "epoch": 4.279897128161166,
      "grad_norm": 0.4098372459411621,
      "learning_rate": 1.4293470495785114e-05,
      "loss": 0.6348,
      "step": 19970
    },
    {
      "epoch": 4.28204029147021,
      "grad_norm": 0.3988271653652191,
      "learning_rate": 1.4290612944706388e-05,
      "loss": 0.7941,
      "step": 19980
    },
    {
      "epoch": 4.284183454779254,
      "grad_norm": 14.782342910766602,
      "learning_rate": 1.4287755393627662e-05,
      "loss": 1.7082,
      "step": 19990
    },
    {
      "epoch": 4.286326618088299,
      "grad_norm": 29.684492111206055,
      "learning_rate": 1.4284897842548937e-05,
      "loss": 0.9056,
      "step": 20000
    },
    {
      "epoch": 4.2884697813973425,
      "grad_norm": 0.7139643430709839,
      "learning_rate": 1.4282040291470211e-05,
      "loss": 0.8637,
      "step": 20010
    },
    {
      "epoch": 4.290612944706386,
      "grad_norm": 0.7916011810302734,
      "learning_rate": 1.4279182740391487e-05,
      "loss": 0.6922,
      "step": 20020
    },
    {
      "epoch": 4.292756108015431,
      "grad_norm": 0.8063873648643494,
      "learning_rate": 1.427632518931276e-05,
      "loss": 0.5564,
      "step": 20030
    },
    {
      "epoch": 4.294899271324475,
      "grad_norm": 14.313135147094727,
      "learning_rate": 1.4273467638234035e-05,
      "loss": 0.6887,
      "step": 20040
    },
    {
      "epoch": 4.297042434633519,
      "grad_norm": 14.384387969970703,
      "learning_rate": 1.427061008715531e-05,
      "loss": 0.9554,
      "step": 20050
    },
    {
      "epoch": 4.2991855979425635,
      "grad_norm": 14.576394081115723,
      "learning_rate": 1.4267752536076584e-05,
      "loss": 0.6895,
      "step": 20060
    },
    {
      "epoch": 4.301328761251607,
      "grad_norm": 0.7280464768409729,
      "learning_rate": 1.426489498499786e-05,
      "loss": 0.2864,
      "step": 20070
    },
    {
      "epoch": 4.303471924560651,
      "grad_norm": 14.453989028930664,
      "learning_rate": 1.4262037433919132e-05,
      "loss": 0.7218,
      "step": 20080
    },
    {
      "epoch": 4.305615087869696,
      "grad_norm": 0.6012696027755737,
      "learning_rate": 1.4259179882840406e-05,
      "loss": 0.4423,
      "step": 20090
    },
    {
      "epoch": 4.30775825117874,
      "grad_norm": 0.5198104381561279,
      "learning_rate": 1.425632233176168e-05,
      "loss": 0.3098,
      "step": 20100
    },
    {
      "epoch": 4.309901414487784,
      "grad_norm": 14.60145378112793,
      "learning_rate": 1.4253464780682956e-05,
      "loss": 1.0709,
      "step": 20110
    },
    {
      "epoch": 4.312044577796828,
      "grad_norm": 0.5354089736938477,
      "learning_rate": 1.425060722960423e-05,
      "loss": 0.7621,
      "step": 20120
    },
    {
      "epoch": 4.314187741105872,
      "grad_norm": 0.5132692456245422,
      "learning_rate": 1.4247749678525503e-05,
      "loss": 0.605,
      "step": 20130
    },
    {
      "epoch": 4.316330904414916,
      "grad_norm": 0.4953005611896515,
      "learning_rate": 1.4244892127446779e-05,
      "loss": 0.455,
      "step": 20140
    },
    {
      "epoch": 4.318474067723961,
      "grad_norm": 0.5262302160263062,
      "learning_rate": 1.4242034576368053e-05,
      "loss": 0.4618,
      "step": 20150
    },
    {
      "epoch": 4.320617231033005,
      "grad_norm": 0.48748740553855896,
      "learning_rate": 1.4239177025289329e-05,
      "loss": 0.9115,
      "step": 20160
    },
    {
      "epoch": 4.3227603943420485,
      "grad_norm": 15.123340606689453,
      "learning_rate": 1.4236319474210603e-05,
      "loss": 0.4591,
      "step": 20170
    },
    {
      "epoch": 4.324903557651093,
      "grad_norm": 0.48828408122062683,
      "learning_rate": 1.4233461923131876e-05,
      "loss": 0.306,
      "step": 20180
    },
    {
      "epoch": 4.327046720960137,
      "grad_norm": 0.41693949699401855,
      "learning_rate": 1.4230604372053152e-05,
      "loss": 0.326,
      "step": 20190
    },
    {
      "epoch": 4.329189884269181,
      "grad_norm": 15.866291046142578,
      "learning_rate": 1.4227746820974426e-05,
      "loss": 0.8073,
      "step": 20200
    },
    {
      "epoch": 4.331333047578226,
      "grad_norm": 15.569411277770996,
      "learning_rate": 1.4224889269895702e-05,
      "loss": 0.8061,
      "step": 20210
    },
    {
      "epoch": 4.3334762108872695,
      "grad_norm": 15.385330200195312,
      "learning_rate": 1.4222031718816976e-05,
      "loss": 0.1669,
      "step": 20220
    },
    {
      "epoch": 4.335619374196313,
      "grad_norm": 14.982068061828613,
      "learning_rate": 1.421917416773825e-05,
      "loss": 0.4915,
      "step": 20230
    },
    {
      "epoch": 4.337762537505358,
      "grad_norm": 29.92150115966797,
      "learning_rate": 1.4216316616659525e-05,
      "loss": 0.66,
      "step": 20240
    },
    {
      "epoch": 4.339905700814402,
      "grad_norm": 0.3475842773914337,
      "learning_rate": 1.4213459065580799e-05,
      "loss": 0.3316,
      "step": 20250
    },
    {
      "epoch": 4.342048864123446,
      "grad_norm": 14.951302528381348,
      "learning_rate": 1.4210601514502073e-05,
      "loss": 0.3433,
      "step": 20260
    },
    {
      "epoch": 4.3441920274324906,
      "grad_norm": 0.33832278847694397,
      "learning_rate": 1.4207743963423349e-05,
      "loss": 0.666,
      "step": 20270
    },
    {
      "epoch": 4.346335190741534,
      "grad_norm": 14.88437557220459,
      "learning_rate": 1.4204886412344623e-05,
      "loss": 0.6635,
      "step": 20280
    },
    {
      "epoch": 4.348478354050578,
      "grad_norm": 15.079795837402344,
      "learning_rate": 1.4202028861265895e-05,
      "loss": 0.6595,
      "step": 20290
    },
    {
      "epoch": 4.350621517359623,
      "grad_norm": 0.38630473613739014,
      "learning_rate": 1.419917131018717e-05,
      "loss": 0.651,
      "step": 20300
    },
    {
      "epoch": 4.352764680668667,
      "grad_norm": 14.866222381591797,
      "learning_rate": 1.4196313759108444e-05,
      "loss": 0.6394,
      "step": 20310
    },
    {
      "epoch": 4.354907843977711,
      "grad_norm": 15.077783584594727,
      "learning_rate": 1.4193456208029718e-05,
      "loss": 0.4817,
      "step": 20320
    },
    {
      "epoch": 4.357051007286755,
      "grad_norm": 0.4441262483596802,
      "learning_rate": 1.4190598656950994e-05,
      "loss": 0.7856,
      "step": 20330
    },
    {
      "epoch": 4.359194170595799,
      "grad_norm": 0.5086964964866638,
      "learning_rate": 1.4187741105872268e-05,
      "loss": 0.9248,
      "step": 20340
    },
    {
      "epoch": 4.361337333904844,
      "grad_norm": 0.5493067502975464,
      "learning_rate": 1.4184883554793543e-05,
      "loss": 0.3059,
      "step": 20350
    },
    {
      "epoch": 4.363480497213888,
      "grad_norm": 0.4106012284755707,
      "learning_rate": 1.4182026003714817e-05,
      "loss": 0.0107,
      "step": 20360
    },
    {
      "epoch": 4.365623660522932,
      "grad_norm": 0.39933159947395325,
      "learning_rate": 1.4179168452636091e-05,
      "loss": 0.4715,
      "step": 20370
    },
    {
      "epoch": 4.3677668238319765,
      "grad_norm": 0.38398754596710205,
      "learning_rate": 1.4176310901557367e-05,
      "loss": 0.7968,
      "step": 20380
    },
    {
      "epoch": 4.36990998714102,
      "grad_norm": 0.37314021587371826,
      "learning_rate": 1.417345335047864e-05,
      "loss": 0.3247,
      "step": 20390
    },
    {
      "epoch": 4.372053150450064,
      "grad_norm": 0.35286906361579895,
      "learning_rate": 1.4170595799399915e-05,
      "loss": 0.4908,
      "step": 20400
    },
    {
      "epoch": 4.374196313759109,
      "grad_norm": 0.32822614908218384,
      "learning_rate": 1.416773824832119e-05,
      "loss": 0.6596,
      "step": 20410
    },
    {
      "epoch": 4.376339477068153,
      "grad_norm": 14.766605377197266,
      "learning_rate": 1.4164880697242464e-05,
      "loss": 0.6573,
      "step": 20420
    },
    {
      "epoch": 4.378482640377197,
      "grad_norm": 0.3686932325363159,
      "learning_rate": 1.416202314616374e-05,
      "loss": 0.1662,
      "step": 20430
    },
    {
      "epoch": 4.380625803686241,
      "grad_norm": 15.048480033874512,
      "learning_rate": 1.4159165595085014e-05,
      "loss": 1.1387,
      "step": 20440
    },
    {
      "epoch": 4.382768966995285,
      "grad_norm": 0.4559856951236725,
      "learning_rate": 1.4156308044006288e-05,
      "loss": 0.9573,
      "step": 20450
    },
    {
      "epoch": 4.384912130304329,
      "grad_norm": 14.621529579162598,
      "learning_rate": 1.4153450492927563e-05,
      "loss": 0.7676,
      "step": 20460
    },
    {
      "epoch": 4.387055293613374,
      "grad_norm": 0.5412061214447021,
      "learning_rate": 1.4150592941848837e-05,
      "loss": 0.4583,
      "step": 20470
    },
    {
      "epoch": 4.389198456922418,
      "grad_norm": 0.5412653684616089,
      "learning_rate": 1.4147735390770111e-05,
      "loss": 0.6011,
      "step": 20480
    },
    {
      "epoch": 4.3913416202314615,
      "grad_norm": 0.4784945547580719,
      "learning_rate": 1.4144877839691387e-05,
      "loss": 0.3125,
      "step": 20490
    },
    {
      "epoch": 4.393484783540506,
      "grad_norm": 0.48723104596138,
      "learning_rate": 1.414202028861266e-05,
      "loss": 0.4614,
      "step": 20500
    },
    {
      "epoch": 4.39562794684955,
      "grad_norm": 0.45055174827575684,
      "learning_rate": 1.4139162737533933e-05,
      "loss": 0.4665,
      "step": 20510
    },
    {
      "epoch": 4.397771110158594,
      "grad_norm": 0.39260753989219666,
      "learning_rate": 1.4136305186455209e-05,
      "loss": 0.3221,
      "step": 20520
    },
    {
      "epoch": 4.399914273467639,
      "grad_norm": 0.3815874755382538,
      "learning_rate": 1.4133447635376483e-05,
      "loss": 0.4815,
      "step": 20530
    },
    {
      "epoch": 4.4020574367766825,
      "grad_norm": 0.3752288818359375,
      "learning_rate": 1.4130590084297757e-05,
      "loss": 0.8125,
      "step": 20540
    },
    {
      "epoch": 4.404200600085726,
      "grad_norm": 14.742246627807617,
      "learning_rate": 1.4127732533219032e-05,
      "loss": 0.6532,
      "step": 20550
    },
    {
      "epoch": 4.406343763394771,
      "grad_norm": 14.69150161743164,
      "learning_rate": 1.4124874982140306e-05,
      "loss": 0.6444,
      "step": 20560
    },
    {
      "epoch": 4.408486926703815,
      "grad_norm": 0.43368539214134216,
      "learning_rate": 1.4122017431061582e-05,
      "loss": 0.3192,
      "step": 20570
    },
    {
      "epoch": 4.410630090012859,
      "grad_norm": 0.40148839354515076,
      "learning_rate": 1.4119159879982856e-05,
      "loss": 0.1646,
      "step": 20580
    },
    {
      "epoch": 4.4127732533219035,
      "grad_norm": 0.41963818669319153,
      "learning_rate": 1.411630232890413e-05,
      "loss": 0.6375,
      "step": 20590
    },
    {
      "epoch": 4.414916416630947,
      "grad_norm": 0.45240074396133423,
      "learning_rate": 1.4113444777825405e-05,
      "loss": 0.48,
      "step": 20600
    },
    {
      "epoch": 4.417059579939991,
      "grad_norm": 0.45740029215812683,
      "learning_rate": 1.4110587226746679e-05,
      "loss": 0.6277,
      "step": 20610
    },
    {
      "epoch": 4.419202743249036,
      "grad_norm": 14.747572898864746,
      "learning_rate": 1.4107729675667953e-05,
      "loss": 0.6269,
      "step": 20620
    },
    {
      "epoch": 4.42134590655808,
      "grad_norm": 0.43039265275001526,
      "learning_rate": 1.4104872124589229e-05,
      "loss": 0.4758,
      "step": 20630
    },
    {
      "epoch": 4.423489069867124,
      "grad_norm": 0.4404614269733429,
      "learning_rate": 1.4102014573510503e-05,
      "loss": 0.4774,
      "step": 20640
    },
    {
      "epoch": 4.425632233176168,
      "grad_norm": 14.708064079284668,
      "learning_rate": 1.4099157022431778e-05,
      "loss": 0.6354,
      "step": 20650
    },
    {
      "epoch": 4.427775396485212,
      "grad_norm": 14.783659934997559,
      "learning_rate": 1.4096299471353052e-05,
      "loss": 0.1662,
      "step": 20660
    },
    {
      "epoch": 4.429918559794256,
      "grad_norm": 0.3538438379764557,
      "learning_rate": 1.4093441920274326e-05,
      "loss": 0.327,
      "step": 20670
    },
    {
      "epoch": 4.432061723103301,
      "grad_norm": 0.3465706706047058,
      "learning_rate": 1.4090584369195602e-05,
      "loss": 0.6572,
      "step": 20680
    },
    {
      "epoch": 4.434204886412345,
      "grad_norm": 0.3457849621772766,
      "learning_rate": 1.4087726818116876e-05,
      "loss": 0.1688,
      "step": 20690
    },
    {
      "epoch": 4.4363480497213885,
      "grad_norm": 0.30654051899909973,
      "learning_rate": 1.408486926703815e-05,
      "loss": 0.3348,
      "step": 20700
    },
    {
      "epoch": 4.438491213030433,
      "grad_norm": 0.3163018524646759,
      "learning_rate": 1.4082011715959425e-05,
      "loss": 0.5021,
      "step": 20710
    },
    {
      "epoch": 4.440634376339477,
      "grad_norm": 14.786462783813477,
      "learning_rate": 1.4079154164880697e-05,
      "loss": 0.5091,
      "step": 20720
    },
    {
      "epoch": 4.442777539648521,
      "grad_norm": 14.802547454833984,
      "learning_rate": 1.4076296613801971e-05,
      "loss": 0.507,
      "step": 20730
    },
    {
      "epoch": 4.444920702957566,
      "grad_norm": 29.79139518737793,
      "learning_rate": 1.4073439062723247e-05,
      "loss": 0.5027,
      "step": 20740
    },
    {
      "epoch": 4.4470638662666095,
      "grad_norm": 0.31220805644989014,
      "learning_rate": 1.4070581511644521e-05,
      "loss": 0.5045,
      "step": 20750
    },
    {
      "epoch": 4.449207029575653,
      "grad_norm": 0.3362140357494354,
      "learning_rate": 1.4067723960565795e-05,
      "loss": 0.4978,
      "step": 20760
    },
    {
      "epoch": 4.451350192884698,
      "grad_norm": 0.3113587498664856,
      "learning_rate": 1.406486640948707e-05,
      "loss": 0.5053,
      "step": 20770
    },
    {
      "epoch": 4.453493356193742,
      "grad_norm": 15.438114166259766,
      "learning_rate": 1.4062008858408344e-05,
      "loss": 0.6755,
      "step": 20780
    },
    {
      "epoch": 4.455636519502786,
      "grad_norm": 14.749068260192871,
      "learning_rate": 1.405915130732962e-05,
      "loss": 0.3358,
      "step": 20790
    },
    {
      "epoch": 4.457779682811831,
      "grad_norm": 0.3282279074192047,
      "learning_rate": 1.4056293756250894e-05,
      "loss": 0.5006,
      "step": 20800
    },
    {
      "epoch": 4.459922846120874,
      "grad_norm": 0.32837995886802673,
      "learning_rate": 1.4053436205172168e-05,
      "loss": 0.1726,
      "step": 20810
    },
    {
      "epoch": 4.462066009429918,
      "grad_norm": 0.34608370065689087,
      "learning_rate": 1.4050578654093443e-05,
      "loss": 0.6652,
      "step": 20820
    },
    {
      "epoch": 4.464209172738963,
      "grad_norm": 0.36836525797843933,
      "learning_rate": 1.4047721103014717e-05,
      "loss": 0.9881,
      "step": 20830
    },
    {
      "epoch": 4.466352336048007,
      "grad_norm": 14.710982322692871,
      "learning_rate": 1.4044863551935991e-05,
      "loss": 0.6465,
      "step": 20840
    },
    {
      "epoch": 4.468495499357051,
      "grad_norm": 14.757226943969727,
      "learning_rate": 1.4042006000857267e-05,
      "loss": 0.7902,
      "step": 20850
    },
    {
      "epoch": 4.470638662666095,
      "grad_norm": 0.4756525158882141,
      "learning_rate": 1.4039148449778541e-05,
      "loss": 0.626,
      "step": 20860
    },
    {
      "epoch": 4.472781825975139,
      "grad_norm": 14.621920585632324,
      "learning_rate": 1.4036290898699816e-05,
      "loss": 0.7655,
      "step": 20870
    },
    {
      "epoch": 4.474924989284183,
      "grad_norm": 14.849593162536621,
      "learning_rate": 1.403343334762109e-05,
      "loss": 0.4612,
      "step": 20880
    },
    {
      "epoch": 4.477068152593228,
      "grad_norm": 14.931017875671387,
      "learning_rate": 1.4030575796542364e-05,
      "loss": 0.751,
      "step": 20890
    },
    {
      "epoch": 4.479211315902272,
      "grad_norm": 14.453805923461914,
      "learning_rate": 1.402771824546364e-05,
      "loss": 0.8827,
      "step": 20900
    },
    {
      "epoch": 4.481354479211316,
      "grad_norm": 0.6421511769294739,
      "learning_rate": 1.4024860694384914e-05,
      "loss": 0.156,
      "step": 20910
    },
    {
      "epoch": 4.48349764252036,
      "grad_norm": 0.5892648100852966,
      "learning_rate": 1.402200314330619e-05,
      "loss": 0.5845,
      "step": 20920
    },
    {
      "epoch": 4.485640805829404,
      "grad_norm": 14.696450233459473,
      "learning_rate": 1.4019145592227463e-05,
      "loss": 0.5998,
      "step": 20930
    },
    {
      "epoch": 4.487783969138448,
      "grad_norm": 0.5281637907028198,
      "learning_rate": 1.4016288041148736e-05,
      "loss": 0.6024,
      "step": 20940
    },
    {
      "epoch": 4.489927132447493,
      "grad_norm": 0.5697649121284485,
      "learning_rate": 1.401343049007001e-05,
      "loss": 0.8949,
      "step": 20950
    },
    {
      "epoch": 4.492070295756537,
      "grad_norm": 0.6100568175315857,
      "learning_rate": 1.4010572938991285e-05,
      "loss": 0.7352,
      "step": 20960
    },
    {
      "epoch": 4.4942134590655805,
      "grad_norm": 0.5814668536186218,
      "learning_rate": 1.400771538791256e-05,
      "loss": 0.2989,
      "step": 20970
    },
    {
      "epoch": 4.496356622374625,
      "grad_norm": 0.4987444579601288,
      "learning_rate": 1.4004857836833833e-05,
      "loss": 0.4619,
      "step": 20980
    },
    {
      "epoch": 4.498499785683669,
      "grad_norm": 14.715404510498047,
      "learning_rate": 1.4002000285755109e-05,
      "loss": 0.6134,
      "step": 20990
    },
    {
      "epoch": 4.500642948992713,
      "grad_norm": 14.593908309936523,
      "learning_rate": 1.3999142734676383e-05,
      "loss": 0.7653,
      "step": 21000
    },
    {
      "epoch": 4.502786112301758,
      "grad_norm": 0.4885092079639435,
      "learning_rate": 1.3996285183597658e-05,
      "loss": 0.4635,
      "step": 21010
    },
    {
      "epoch": 4.5049292756108015,
      "grad_norm": 0.5379514098167419,
      "learning_rate": 1.3993427632518932e-05,
      "loss": 0.6118,
      "step": 21020
    },
    {
      "epoch": 4.507072438919845,
      "grad_norm": 14.581574440002441,
      "learning_rate": 1.3990570081440206e-05,
      "loss": 0.6048,
      "step": 21030
    },
    {
      "epoch": 4.50921560222889,
      "grad_norm": 0.5114963054656982,
      "learning_rate": 1.3987712530361482e-05,
      "loss": 0.6113,
      "step": 21040
    },
    {
      "epoch": 4.511358765537934,
      "grad_norm": 14.598546981811523,
      "learning_rate": 1.3984854979282756e-05,
      "loss": 0.615,
      "step": 21050
    },
    {
      "epoch": 4.513501928846978,
      "grad_norm": 14.710912704467773,
      "learning_rate": 1.3981997428204031e-05,
      "loss": 0.47,
      "step": 21060
    },
    {
      "epoch": 4.5156450921560225,
      "grad_norm": 29.574024200439453,
      "learning_rate": 1.3979139877125305e-05,
      "loss": 1.0805,
      "step": 21070
    },
    {
      "epoch": 4.517788255465066,
      "grad_norm": 0.5160897970199585,
      "learning_rate": 1.397628232604658e-05,
      "loss": 0.4624,
      "step": 21080
    },
    {
      "epoch": 4.51993141877411,
      "grad_norm": 0.47995486855506897,
      "learning_rate": 1.3973424774967855e-05,
      "loss": 0.1615,
      "step": 21090
    },
    {
      "epoch": 4.522074582083155,
      "grad_norm": 0.4250534772872925,
      "learning_rate": 1.3970567223889129e-05,
      "loss": 0.4694,
      "step": 21100
    },
    {
      "epoch": 4.524217745392199,
      "grad_norm": 14.662919998168945,
      "learning_rate": 1.3967709672810403e-05,
      "loss": 0.9464,
      "step": 21110
    },
    {
      "epoch": 4.526360908701243,
      "grad_norm": 0.4338521957397461,
      "learning_rate": 1.3964852121731678e-05,
      "loss": 0.4756,
      "step": 21120
    },
    {
      "epoch": 4.528504072010287,
      "grad_norm": 0.4364486336708069,
      "learning_rate": 1.3961994570652952e-05,
      "loss": 0.3192,
      "step": 21130
    },
    {
      "epoch": 4.530647235319331,
      "grad_norm": 0.4215332865715027,
      "learning_rate": 1.3959137019574228e-05,
      "loss": 0.4752,
      "step": 21140
    },
    {
      "epoch": 4.532790398628375,
      "grad_norm": 14.75291633605957,
      "learning_rate": 1.39562794684955e-05,
      "loss": 0.6312,
      "step": 21150
    },
    {
      "epoch": 4.53493356193742,
      "grad_norm": 0.4627974033355713,
      "learning_rate": 1.3953421917416774e-05,
      "loss": 0.7859,
      "step": 21160
    },
    {
      "epoch": 4.537076725246464,
      "grad_norm": 14.574386596679688,
      "learning_rate": 1.3950564366338048e-05,
      "loss": 0.7644,
      "step": 21170
    },
    {
      "epoch": 4.5392198885555075,
      "grad_norm": 0.5320401191711426,
      "learning_rate": 1.3947706815259324e-05,
      "loss": 0.6046,
      "step": 21180
    },
    {
      "epoch": 4.541363051864552,
      "grad_norm": 14.492154121398926,
      "learning_rate": 1.3944849264180597e-05,
      "loss": 1.0369,
      "step": 21190
    },
    {
      "epoch": 4.543506215173596,
      "grad_norm": 14.45183277130127,
      "learning_rate": 1.3941991713101873e-05,
      "loss": 1.0209,
      "step": 21200
    },
    {
      "epoch": 4.54564937848264,
      "grad_norm": 14.3888521194458,
      "learning_rate": 1.3939134162023147e-05,
      "loss": 0.5767,
      "step": 21210
    },
    {
      "epoch": 4.547792541791685,
      "grad_norm": 14.363483428955078,
      "learning_rate": 1.3936276610944421e-05,
      "loss": 1.1235,
      "step": 21220
    },
    {
      "epoch": 4.5499357051007285,
      "grad_norm": 14.313480377197266,
      "learning_rate": 1.3933419059865697e-05,
      "loss": 0.8252,
      "step": 21230
    },
    {
      "epoch": 4.552078868409772,
      "grad_norm": 0.9041764736175537,
      "learning_rate": 1.393056150878697e-05,
      "loss": 0.5448,
      "step": 21240
    },
    {
      "epoch": 4.554222031718817,
      "grad_norm": 14.707558631896973,
      "learning_rate": 1.3927703957708244e-05,
      "loss": 0.6749,
      "step": 21250
    },
    {
      "epoch": 4.556365195027861,
      "grad_norm": 0.8365404605865479,
      "learning_rate": 1.392484640662952e-05,
      "loss": 0.1507,
      "step": 21260
    },
    {
      "epoch": 4.558508358336905,
      "grad_norm": 0.7148638367652893,
      "learning_rate": 1.3921988855550794e-05,
      "loss": 0.6947,
      "step": 21270
    },
    {
      "epoch": 4.56065152164595,
      "grad_norm": 0.685729444026947,
      "learning_rate": 1.391913130447207e-05,
      "loss": 0.429,
      "step": 21280
    },
    {
      "epoch": 4.562794684954993,
      "grad_norm": 0.6018653512001038,
      "learning_rate": 1.3916273753393344e-05,
      "loss": 0.1562,
      "step": 21290
    },
    {
      "epoch": 4.564937848264037,
      "grad_norm": 0.5461319088935852,
      "learning_rate": 1.3913416202314617e-05,
      "loss": 0.7447,
      "step": 21300
    },
    {
      "epoch": 4.567081011573082,
      "grad_norm": 14.501152038574219,
      "learning_rate": 1.3910558651235893e-05,
      "loss": 0.7497,
      "step": 21310
    },
    {
      "epoch": 4.569224174882126,
      "grad_norm": 0.5435627698898315,
      "learning_rate": 1.3907701100157167e-05,
      "loss": 0.3007,
      "step": 21320
    },
    {
      "epoch": 4.57136733819117,
      "grad_norm": 0.5296000242233276,
      "learning_rate": 1.3904843549078441e-05,
      "loss": 0.5995,
      "step": 21330
    },
    {
      "epoch": 4.573510501500214,
      "grad_norm": 0.4908999800682068,
      "learning_rate": 1.3901985997999717e-05,
      "loss": 0.1621,
      "step": 21340
    },
    {
      "epoch": 4.575653664809258,
      "grad_norm": 0.4241320788860321,
      "learning_rate": 1.389912844692099e-05,
      "loss": 0.633,
      "step": 21350
    },
    {
      "epoch": 4.577796828118302,
      "grad_norm": 14.644058227539062,
      "learning_rate": 1.3896270895842266e-05,
      "loss": 0.7804,
      "step": 21360
    },
    {
      "epoch": 4.579939991427347,
      "grad_norm": 14.627281188964844,
      "learning_rate": 1.3893413344763538e-05,
      "loss": 0.3172,
      "step": 21370
    },
    {
      "epoch": 4.582083154736391,
      "grad_norm": 0.4446621239185333,
      "learning_rate": 1.3890555793684812e-05,
      "loss": 0.316,
      "step": 21380
    },
    {
      "epoch": 4.584226318045435,
      "grad_norm": 14.64829158782959,
      "learning_rate": 1.3887698242606086e-05,
      "loss": 0.7786,
      "step": 21390
    },
    {
      "epoch": 4.586369481354479,
      "grad_norm": 14.870598793029785,
      "learning_rate": 1.3884840691527362e-05,
      "loss": 0.7661,
      "step": 21400
    },
    {
      "epoch": 4.588512644663523,
      "grad_norm": 0.514318585395813,
      "learning_rate": 1.3881983140448636e-05,
      "loss": 0.3136,
      "step": 21410
    },
    {
      "epoch": 4.590655807972568,
      "grad_norm": 0.5053170919418335,
      "learning_rate": 1.3879125589369911e-05,
      "loss": 0.6139,
      "step": 21420
    },
    {
      "epoch": 4.592798971281612,
      "grad_norm": 14.625831604003906,
      "learning_rate": 1.3876268038291185e-05,
      "loss": 0.6187,
      "step": 21430
    },
    {
      "epoch": 4.594942134590656,
      "grad_norm": 0.4587504267692566,
      "learning_rate": 1.387341048721246e-05,
      "loss": 0.4681,
      "step": 21440
    },
    {
      "epoch": 4.5970852978997,
      "grad_norm": 14.613224983215332,
      "learning_rate": 1.3870552936133735e-05,
      "loss": 0.6256,
      "step": 21450
    },
    {
      "epoch": 4.599228461208744,
      "grad_norm": 0.46250173449516296,
      "learning_rate": 1.3867695385055009e-05,
      "loss": 0.314,
      "step": 21460
    },
    {
      "epoch": 4.601371624517788,
      "grad_norm": 29.61140251159668,
      "learning_rate": 1.3864837833976283e-05,
      "loss": 0.6213,
      "step": 21470
    },
    {
      "epoch": 4.603514787826833,
      "grad_norm": 0.46864330768585205,
      "learning_rate": 1.3861980282897558e-05,
      "loss": 0.4721,
      "step": 21480
    },
    {
      "epoch": 4.605657951135877,
      "grad_norm": 14.56573486328125,
      "learning_rate": 1.3859122731818832e-05,
      "loss": 1.0749,
      "step": 21490
    },
    {
      "epoch": 4.6078011144449205,
      "grad_norm": 14.57636833190918,
      "learning_rate": 1.3856265180740108e-05,
      "loss": 0.3098,
      "step": 21500
    },
    {
      "epoch": 4.609944277753965,
      "grad_norm": 0.4681561589241028,
      "learning_rate": 1.3853407629661382e-05,
      "loss": 0.4622,
      "step": 21510
    },
    {
      "epoch": 4.612087441063009,
      "grad_norm": 14.599711418151855,
      "learning_rate": 1.3850550078582656e-05,
      "loss": 0.6138,
      "step": 21520
    },
    {
      "epoch": 4.614230604372053,
      "grad_norm": 0.5324180722236633,
      "learning_rate": 1.3847692527503931e-05,
      "loss": 0.7629,
      "step": 21530
    },
    {
      "epoch": 4.616373767681098,
      "grad_norm": 0.5919254422187805,
      "learning_rate": 1.3844834976425205e-05,
      "loss": 1.0384,
      "step": 21540
    },
    {
      "epoch": 4.6185169309901415,
      "grad_norm": 14.634505271911621,
      "learning_rate": 1.384197742534648e-05,
      "loss": 0.5831,
      "step": 21550
    },
    {
      "epoch": 4.620660094299185,
      "grad_norm": 0.6972379088401794,
      "learning_rate": 1.3839119874267755e-05,
      "loss": 0.9931,
      "step": 21560
    },
    {
      "epoch": 4.62280325760823,
      "grad_norm": 0.7262974977493286,
      "learning_rate": 1.3836262323189029e-05,
      "loss": 0.429,
      "step": 21570
    },
    {
      "epoch": 4.624946420917274,
      "grad_norm": 0.6892795562744141,
      "learning_rate": 1.3833404772110301e-05,
      "loss": 0.5714,
      "step": 21580
    },
    {
      "epoch": 4.627089584226318,
      "grad_norm": 0.6961425542831421,
      "learning_rate": 1.3830547221031577e-05,
      "loss": 0.7107,
      "step": 21590
    },
    {
      "epoch": 4.6292327475353625,
      "grad_norm": 14.511183738708496,
      "learning_rate": 1.382768966995285e-05,
      "loss": 0.5745,
      "step": 21600
    },
    {
      "epoch": 4.631375910844406,
      "grad_norm": 0.6932379603385925,
      "learning_rate": 1.3824832118874124e-05,
      "loss": 0.994,
      "step": 21610
    },
    {
      "epoch": 4.63351907415345,
      "grad_norm": 14.410748481750488,
      "learning_rate": 1.38219745677954e-05,
      "loss": 0.7153,
      "step": 21620
    },
    {
      "epoch": 4.635662237462495,
      "grad_norm": 0.6914400458335876,
      "learning_rate": 1.3819117016716674e-05,
      "loss": 0.5696,
      "step": 21630
    },
    {
      "epoch": 4.637805400771539,
      "grad_norm": 0.6779896020889282,
      "learning_rate": 1.381625946563795e-05,
      "loss": 0.5714,
      "step": 21640
    },
    {
      "epoch": 4.639948564080583,
      "grad_norm": 14.467804908752441,
      "learning_rate": 1.3813401914559224e-05,
      "loss": 0.5849,
      "step": 21650
    },
    {
      "epoch": 4.642091727389627,
      "grad_norm": 0.573423445224762,
      "learning_rate": 1.3810544363480497e-05,
      "loss": 0.4481,
      "step": 21660
    },
    {
      "epoch": 4.644234890698671,
      "grad_norm": 14.575844764709473,
      "learning_rate": 1.3807686812401773e-05,
      "loss": 0.3111,
      "step": 21670
    },
    {
      "epoch": 4.646378054007715,
      "grad_norm": 0.5006624460220337,
      "learning_rate": 1.3804829261323047e-05,
      "loss": 0.615,
      "step": 21680
    },
    {
      "epoch": 4.64852121731676,
      "grad_norm": 0.50527423620224,
      "learning_rate": 1.3801971710244321e-05,
      "loss": 0.7609,
      "step": 21690
    },
    {
      "epoch": 4.650664380625804,
      "grad_norm": 0.5633516907691956,
      "learning_rate": 1.3799114159165597e-05,
      "loss": 0.7481,
      "step": 21700
    },
    {
      "epoch": 4.6528075439348475,
      "grad_norm": 14.49834156036377,
      "learning_rate": 1.379625660808687e-05,
      "loss": 0.3048,
      "step": 21710
    },
    {
      "epoch": 4.654950707243892,
      "grad_norm": 0.5276634693145752,
      "learning_rate": 1.3793399057008146e-05,
      "loss": 0.3052,
      "step": 21720
    },
    {
      "epoch": 4.657093870552936,
      "grad_norm": 29.564207077026367,
      "learning_rate": 1.379054150592942e-05,
      "loss": 1.0338,
      "step": 21730
    },
    {
      "epoch": 4.65923703386198,
      "grad_norm": 0.5911640524864197,
      "learning_rate": 1.3787683954850694e-05,
      "loss": 0.1571,
      "step": 21740
    },
    {
      "epoch": 4.661380197171025,
      "grad_norm": 14.517904281616211,
      "learning_rate": 1.378482640377197e-05,
      "loss": 0.8825,
      "step": 21750
    },
    {
      "epoch": 4.6635233604800685,
      "grad_norm": 0.5833656787872314,
      "learning_rate": 1.3781968852693244e-05,
      "loss": 0.5958,
      "step": 21760
    },
    {
      "epoch": 4.665666523789112,
      "grad_norm": 0.537855327129364,
      "learning_rate": 1.377911130161452e-05,
      "loss": 0.1578,
      "step": 21770
    },
    {
      "epoch": 4.667809687098157,
      "grad_norm": 0.5223677754402161,
      "learning_rate": 1.3776253750535793e-05,
      "loss": 0.7547,
      "step": 21780
    },
    {
      "epoch": 4.669952850407201,
      "grad_norm": 14.588188171386719,
      "learning_rate": 1.3773396199457067e-05,
      "loss": 0.3138,
      "step": 21790
    },
    {
      "epoch": 4.672096013716245,
      "grad_norm": 14.650501251220703,
      "learning_rate": 1.377053864837834e-05,
      "loss": 0.32,
      "step": 21800
    },
    {
      "epoch": 4.67423917702529,
      "grad_norm": 0.433893620967865,
      "learning_rate": 1.3767681097299615e-05,
      "loss": 1.095,
      "step": 21810
    },
    {
      "epoch": 4.676382340334333,
      "grad_norm": 0.4462992548942566,
      "learning_rate": 1.3764823546220889e-05,
      "loss": 0.7805,
      "step": 21820
    },
    {
      "epoch": 4.678525503643377,
      "grad_norm": 14.686089515686035,
      "learning_rate": 1.3761965995142163e-05,
      "loss": 0.468,
      "step": 21830
    },
    {
      "epoch": 4.680668666952422,
      "grad_norm": 0.46256327629089355,
      "learning_rate": 1.3759108444063438e-05,
      "loss": 0.3129,
      "step": 21840
    },
    {
      "epoch": 4.682811830261466,
      "grad_norm": 0.44443053007125854,
      "learning_rate": 1.3756250892984712e-05,
      "loss": 0.3186,
      "step": 21850
    },
    {
      "epoch": 4.68495499357051,
      "grad_norm": 0.40681809186935425,
      "learning_rate": 1.3753393341905988e-05,
      "loss": 0.164,
      "step": 21860
    },
    {
      "epoch": 4.6870981568795544,
      "grad_norm": 14.705763816833496,
      "learning_rate": 1.3750535790827262e-05,
      "loss": 0.8098,
      "step": 21870
    },
    {
      "epoch": 4.689241320188598,
      "grad_norm": 0.3571743667125702,
      "learning_rate": 1.3747678239748536e-05,
      "loss": 0.17,
      "step": 21880
    },
    {
      "epoch": 4.691384483497642,
      "grad_norm": 0.3573712706565857,
      "learning_rate": 1.3744820688669811e-05,
      "loss": 0.8149,
      "step": 21890
    },
    {
      "epoch": 4.693527646806687,
      "grad_norm": 14.954985618591309,
      "learning_rate": 1.3741963137591085e-05,
      "loss": 0.3334,
      "step": 21900
    },
    {
      "epoch": 4.695670810115731,
      "grad_norm": 0.38632965087890625,
      "learning_rate": 1.3739105586512361e-05,
      "loss": 1.1381,
      "step": 21910
    },
    {
      "epoch": 4.697813973424775,
      "grad_norm": 0.4260095953941345,
      "learning_rate": 1.3736248035433635e-05,
      "loss": 0.6363,
      "step": 21920
    },
    {
      "epoch": 4.699957136733819,
      "grad_norm": 0.44783031940460205,
      "learning_rate": 1.3733390484354909e-05,
      "loss": 0.4778,
      "step": 21930
    },
    {
      "epoch": 4.702100300042863,
      "grad_norm": 14.64297103881836,
      "learning_rate": 1.3730532933276184e-05,
      "loss": 0.4713,
      "step": 21940
    },
    {
      "epoch": 4.704243463351908,
      "grad_norm": 0.46772128343582153,
      "learning_rate": 1.3727675382197458e-05,
      "loss": 1.0802,
      "step": 21950
    },
    {
      "epoch": 4.706386626660952,
      "grad_norm": 15.739021301269531,
      "learning_rate": 1.3724817831118732e-05,
      "loss": 1.0454,
      "step": 21960
    },
    {
      "epoch": 4.708529789969996,
      "grad_norm": 0.7109333276748657,
      "learning_rate": 1.3721960280040008e-05,
      "loss": 0.5555,
      "step": 21970
    },
    {
      "epoch": 4.71067295327904,
      "grad_norm": 14.446173667907715,
      "learning_rate": 1.3719102728961282e-05,
      "loss": 0.4409,
      "step": 21980
    },
    {
      "epoch": 4.712816116588084,
      "grad_norm": 29.5864200592041,
      "learning_rate": 1.3716245177882557e-05,
      "loss": 0.8783,
      "step": 21990
    },
    {
      "epoch": 4.714959279897128,
      "grad_norm": 14.478428840637207,
      "learning_rate": 1.3713387626803831e-05,
      "loss": 1.2978,
      "step": 22000
    },
    {
      "epoch": 4.717102443206173,
      "grad_norm": 0.8036484718322754,
      "learning_rate": 1.3710530075725104e-05,
      "loss": 0.5603,
      "step": 22010
    },
    {
      "epoch": 4.719245606515217,
      "grad_norm": 15.093605995178223,
      "learning_rate": 1.3707672524646378e-05,
      "loss": 0.5461,
      "step": 22020
    },
    {
      "epoch": 4.7213887698242605,
      "grad_norm": 15.115094184875488,
      "learning_rate": 1.3704814973567653e-05,
      "loss": 0.4215,
      "step": 22030
    },
    {
      "epoch": 4.723531933133305,
      "grad_norm": 0.6888526678085327,
      "learning_rate": 1.3701957422488927e-05,
      "loss": 0.2929,
      "step": 22040
    },
    {
      "epoch": 4.725675096442349,
      "grad_norm": 0.6895535588264465,
      "learning_rate": 1.3699099871410203e-05,
      "loss": 0.9984,
      "step": 22050
    },
    {
      "epoch": 4.727818259751393,
      "grad_norm": 0.6093377470970154,
      "learning_rate": 1.3696242320331477e-05,
      "loss": 0.7186,
      "step": 22060
    },
    {
      "epoch": 4.729961423060438,
      "grad_norm": 0.5970238447189331,
      "learning_rate": 1.369338476925275e-05,
      "loss": 0.4544,
      "step": 22070
    },
    {
      "epoch": 4.7321045863694815,
      "grad_norm": 0.6182493567466736,
      "learning_rate": 1.3690527218174026e-05,
      "loss": 0.8755,
      "step": 22080
    },
    {
      "epoch": 4.734247749678525,
      "grad_norm": 0.682060718536377,
      "learning_rate": 1.36876696670953e-05,
      "loss": 1.5597,
      "step": 22090
    },
    {
      "epoch": 4.73639091298757,
      "grad_norm": 0.7391844987869263,
      "learning_rate": 1.3684812116016574e-05,
      "loss": 0.6976,
      "step": 22100
    },
    {
      "epoch": 4.738534076296614,
      "grad_norm": 44.41645431518555,
      "learning_rate": 1.368195456493785e-05,
      "loss": 1.0967,
      "step": 22110
    },
    {
      "epoch": 4.740677239605658,
      "grad_norm": 0.8111132979393005,
      "learning_rate": 1.3679097013859124e-05,
      "loss": 0.4185,
      "step": 22120
    },
    {
      "epoch": 4.7428204029147025,
      "grad_norm": 0.6839402914047241,
      "learning_rate": 1.36762394627804e-05,
      "loss": 0.4307,
      "step": 22130
    },
    {
      "epoch": 4.744963566223746,
      "grad_norm": 0.6362501978874207,
      "learning_rate": 1.3673381911701673e-05,
      "loss": 0.2947,
      "step": 22140
    },
    {
      "epoch": 4.74710672953279,
      "grad_norm": 14.700157165527344,
      "learning_rate": 1.3670524360622947e-05,
      "loss": 0.4608,
      "step": 22150
    },
    {
      "epoch": 4.749249892841835,
      "grad_norm": 0.5022764801979065,
      "learning_rate": 1.3667666809544223e-05,
      "loss": 0.4641,
      "step": 22160
    },
    {
      "epoch": 4.751393056150879,
      "grad_norm": 0.502592146396637,
      "learning_rate": 1.3664809258465497e-05,
      "loss": 1.2174,
      "step": 22170
    },
    {
      "epoch": 4.753536219459923,
      "grad_norm": 14.684226989746094,
      "learning_rate": 1.366195170738677e-05,
      "loss": 0.6072,
      "step": 22180
    },
    {
      "epoch": 4.755679382768967,
      "grad_norm": 0.5290525555610657,
      "learning_rate": 1.3659094156308046e-05,
      "loss": 0.1609,
      "step": 22190
    },
    {
      "epoch": 4.757822546078011,
      "grad_norm": 14.523731231689453,
      "learning_rate": 1.365623660522932e-05,
      "loss": 1.0512,
      "step": 22200
    },
    {
      "epoch": 4.759965709387055,
      "grad_norm": 0.6175076365470886,
      "learning_rate": 1.3653379054150596e-05,
      "loss": 1.1816,
      "step": 22210
    },
    {
      "epoch": 4.7621088726961,
      "grad_norm": 0.711618185043335,
      "learning_rate": 1.365052150307187e-05,
      "loss": 0.9998,
      "step": 22220
    },
    {
      "epoch": 4.764252036005144,
      "grad_norm": 0.7603440880775452,
      "learning_rate": 1.3647663951993142e-05,
      "loss": 0.5613,
      "step": 22230
    },
    {
      "epoch": 4.7663951993141875,
      "grad_norm": 0.7278482913970947,
      "learning_rate": 1.3644806400914416e-05,
      "loss": 0.4267,
      "step": 22240
    },
    {
      "epoch": 4.768538362623232,
      "grad_norm": 14.377676963806152,
      "learning_rate": 1.3641948849835691e-05,
      "loss": 0.5688,
      "step": 22250
    },
    {
      "epoch": 4.770681525932276,
      "grad_norm": 0.7102711200714111,
      "learning_rate": 1.3639091298756965e-05,
      "loss": 0.8489,
      "step": 22260
    },
    {
      "epoch": 4.77282468924132,
      "grad_norm": 0.7547155022621155,
      "learning_rate": 1.3636233747678241e-05,
      "loss": 0.5625,
      "step": 22270
    },
    {
      "epoch": 4.774967852550365,
      "grad_norm": 0.6899420022964478,
      "learning_rate": 1.3633376196599515e-05,
      "loss": 0.1519,
      "step": 22280
    },
    {
      "epoch": 4.777111015859409,
      "grad_norm": 14.586664199829102,
      "learning_rate": 1.3630518645520789e-05,
      "loss": 0.7187,
      "step": 22290
    },
    {
      "epoch": 4.779254179168452,
      "grad_norm": 29.84796142578125,
      "learning_rate": 1.3627661094442064e-05,
      "loss": 0.7368,
      "step": 22300
    },
    {
      "epoch": 4.781397342477497,
      "grad_norm": 0.5899540185928345,
      "learning_rate": 1.3624803543363338e-05,
      "loss": 0.4482,
      "step": 22310
    },
    {
      "epoch": 4.783540505786541,
      "grad_norm": 29.80326271057129,
      "learning_rate": 1.3621945992284612e-05,
      "loss": 0.7369,
      "step": 22320
    },
    {
      "epoch": 4.785683669095585,
      "grad_norm": 14.498822212219238,
      "learning_rate": 1.3619088441205888e-05,
      "loss": 0.8808,
      "step": 22330
    },
    {
      "epoch": 4.78782683240463,
      "grad_norm": 0.5847281217575073,
      "learning_rate": 1.3616230890127162e-05,
      "loss": 0.3015,
      "step": 22340
    },
    {
      "epoch": 4.789969995713673,
      "grad_norm": 14.484420776367188,
      "learning_rate": 1.3613373339048438e-05,
      "loss": 0.7376,
      "step": 22350
    },
    {
      "epoch": 4.792113159022717,
      "grad_norm": 14.557849884033203,
      "learning_rate": 1.3610515787969711e-05,
      "loss": 0.3055,
      "step": 22360
    },
    {
      "epoch": 4.794256322331762,
      "grad_norm": 0.5015215873718262,
      "learning_rate": 1.3607658236890985e-05,
      "loss": 0.6017,
      "step": 22370
    },
    {
      "epoch": 4.796399485640806,
      "grad_norm": 0.40852266550064087,
      "learning_rate": 1.3604800685812261e-05,
      "loss": 0.6215,
      "step": 22380
    },
    {
      "epoch": 4.79854264894985,
      "grad_norm": 14.672266006469727,
      "learning_rate": 1.3601943134733535e-05,
      "loss": 1.1101,
      "step": 22390
    },
    {
      "epoch": 4.8006858122588945,
      "grad_norm": 29.670534133911133,
      "learning_rate": 1.359908558365481e-05,
      "loss": 0.9292,
      "step": 22400
    },
    {
      "epoch": 4.802828975567938,
      "grad_norm": 0.5227175951004028,
      "learning_rate": 1.3596228032576084e-05,
      "loss": 0.6133,
      "step": 22410
    },
    {
      "epoch": 4.804972138876982,
      "grad_norm": 14.524301528930664,
      "learning_rate": 1.3593370481497358e-05,
      "loss": 0.6,
      "step": 22420
    },
    {
      "epoch": 4.807115302186027,
      "grad_norm": 0.5306116938591003,
      "learning_rate": 1.3590512930418634e-05,
      "loss": 0.453,
      "step": 22430
    },
    {
      "epoch": 4.809258465495071,
      "grad_norm": 0.5229335427284241,
      "learning_rate": 1.3587655379339906e-05,
      "loss": 0.7554,
      "step": 22440
    },
    {
      "epoch": 4.811401628804115,
      "grad_norm": 14.614027976989746,
      "learning_rate": 1.358479782826118e-05,
      "loss": 0.8998,
      "step": 22450
    },
    {
      "epoch": 4.813544792113159,
      "grad_norm": 16.238239288330078,
      "learning_rate": 1.3581940277182454e-05,
      "loss": 0.3077,
      "step": 22460
    },
    {
      "epoch": 4.815687955422203,
      "grad_norm": 0.5017499327659607,
      "learning_rate": 1.357908272610373e-05,
      "loss": 0.457,
      "step": 22470
    },
    {
      "epoch": 4.817831118731247,
      "grad_norm": 0.44929569959640503,
      "learning_rate": 1.3576225175025004e-05,
      "loss": 0.1594,
      "step": 22480
    },
    {
      "epoch": 4.819974282040292,
      "grad_norm": 14.716353416442871,
      "learning_rate": 1.357336762394628e-05,
      "loss": 0.3213,
      "step": 22490
    },
    {
      "epoch": 4.822117445349336,
      "grad_norm": 0.3608156144618988,
      "learning_rate": 1.3570510072867553e-05,
      "loss": 0.965,
      "step": 22500
    },
    {
      "epoch": 4.8242606086583795,
      "grad_norm": 14.808328628540039,
      "learning_rate": 1.3567652521788827e-05,
      "loss": 0.6477,
      "step": 22510
    },
    {
      "epoch": 4.826403771967424,
      "grad_norm": 0.466059148311615,
      "learning_rate": 1.3564794970710103e-05,
      "loss": 0.4836,
      "step": 22520
    },
    {
      "epoch": 4.828546935276468,
      "grad_norm": 15.060062408447266,
      "learning_rate": 1.3561937419631377e-05,
      "loss": 1.5294,
      "step": 22530
    },
    {
      "epoch": 4.830690098585512,
      "grad_norm": 14.59538459777832,
      "learning_rate": 1.3559079868552652e-05,
      "loss": 0.7518,
      "step": 22540
    },
    {
      "epoch": 4.832833261894557,
      "grad_norm": 0.5998349189758301,
      "learning_rate": 1.3556222317473926e-05,
      "loss": 0.887,
      "step": 22550
    },
    {
      "epoch": 4.8349764252036005,
      "grad_norm": 14.919600486755371,
      "learning_rate": 1.35533647663952e-05,
      "loss": 0.4463,
      "step": 22560
    },
    {
      "epoch": 4.837119588512644,
      "grad_norm": 14.569451332092285,
      "learning_rate": 1.3550507215316476e-05,
      "loss": 0.9036,
      "step": 22570
    },
    {
      "epoch": 4.839262751821689,
      "grad_norm": 0.6564289331436157,
      "learning_rate": 1.354764966423775e-05,
      "loss": 0.7226,
      "step": 22580
    },
    {
      "epoch": 4.841405915130733,
      "grad_norm": 0.5699265003204346,
      "learning_rate": 1.3544792113159024e-05,
      "loss": 0.4397,
      "step": 22590
    },
    {
      "epoch": 4.843549078439777,
      "grad_norm": 14.650811195373535,
      "learning_rate": 1.35419345620803e-05,
      "loss": 0.3093,
      "step": 22600
    },
    {
      "epoch": 4.8456922417488215,
      "grad_norm": 0.3596480190753937,
      "learning_rate": 1.3539077011001573e-05,
      "loss": 0.6393,
      "step": 22610
    },
    {
      "epoch": 4.847835405057865,
      "grad_norm": 0.3632870614528656,
      "learning_rate": 1.3536219459922849e-05,
      "loss": 0.3325,
      "step": 22620
    },
    {
      "epoch": 4.849978568366909,
      "grad_norm": 0.37466737627983093,
      "learning_rate": 1.3533361908844123e-05,
      "loss": 0.4927,
      "step": 22630
    },
    {
      "epoch": 4.852121731675954,
      "grad_norm": 0.381757527589798,
      "learning_rate": 1.3530504357765397e-05,
      "loss": 0.9736,
      "step": 22640
    },
    {
      "epoch": 4.854264894984998,
      "grad_norm": 0.38935786485671997,
      "learning_rate": 1.3527646806686672e-05,
      "loss": 0.4818,
      "step": 22650
    },
    {
      "epoch": 4.856408058294042,
      "grad_norm": 0.4042237401008606,
      "learning_rate": 1.3524789255607945e-05,
      "loss": 0.8075,
      "step": 22660
    },
    {
      "epoch": 4.858551221603086,
      "grad_norm": 0.42132726311683655,
      "learning_rate": 1.3521931704529218e-05,
      "loss": 0.9448,
      "step": 22670
    },
    {
      "epoch": 4.86069438491213,
      "grad_norm": 14.601326942443848,
      "learning_rate": 1.3519074153450494e-05,
      "loss": 0.7765,
      "step": 22680
    },
    {
      "epoch": 4.862837548221174,
      "grad_norm": 0.5628464818000793,
      "learning_rate": 1.3516216602371768e-05,
      "loss": 0.6046,
      "step": 22690
    },
    {
      "epoch": 4.864980711530219,
      "grad_norm": 30.46561622619629,
      "learning_rate": 1.3513359051293042e-05,
      "loss": 1.0345,
      "step": 22700
    },
    {
      "epoch": 4.867123874839263,
      "grad_norm": 0.6037053465843201,
      "learning_rate": 1.3510501500214318e-05,
      "loss": 0.1556,
      "step": 22710
    },
    {
      "epoch": 4.8692670381483065,
      "grad_norm": 0.4989672005176544,
      "learning_rate": 1.3507643949135591e-05,
      "loss": 0.4604,
      "step": 22720
    },
    {
      "epoch": 4.871410201457351,
      "grad_norm": 0.47788166999816895,
      "learning_rate": 1.3504786398056865e-05,
      "loss": 0.4607,
      "step": 22730
    },
    {
      "epoch": 4.873553364766395,
      "grad_norm": 0.5130394101142883,
      "learning_rate": 1.3501928846978141e-05,
      "loss": 0.7613,
      "step": 22740
    },
    {
      "epoch": 4.875696528075439,
      "grad_norm": 14.57981014251709,
      "learning_rate": 1.3499071295899415e-05,
      "loss": 0.9006,
      "step": 22750
    },
    {
      "epoch": 4.877839691384484,
      "grad_norm": 0.621016800403595,
      "learning_rate": 1.349621374482069e-05,
      "loss": 0.8839,
      "step": 22760
    },
    {
      "epoch": 4.8799828546935275,
      "grad_norm": 14.397504806518555,
      "learning_rate": 1.3493356193741965e-05,
      "loss": 0.8611,
      "step": 22770
    },
    {
      "epoch": 4.882126018002571,
      "grad_norm": 0.6819153428077698,
      "learning_rate": 1.3490498642663238e-05,
      "loss": 0.2929,
      "step": 22780
    },
    {
      "epoch": 4.884269181311616,
      "grad_norm": 14.405669212341309,
      "learning_rate": 1.3487641091584514e-05,
      "loss": 0.7158,
      "step": 22790
    },
    {
      "epoch": 4.88641234462066,
      "grad_norm": 0.5295364856719971,
      "learning_rate": 1.3484783540505788e-05,
      "loss": 0.302,
      "step": 22800
    },
    {
      "epoch": 4.888555507929704,
      "grad_norm": 0.5674290657043457,
      "learning_rate": 1.3481925989427062e-05,
      "loss": 0.5913,
      "step": 22810
    },
    {
      "epoch": 4.890698671238749,
      "grad_norm": 14.49930191040039,
      "learning_rate": 1.3479068438348338e-05,
      "loss": 1.0305,
      "step": 22820
    },
    {
      "epoch": 4.892841834547792,
      "grad_norm": 14.314427375793457,
      "learning_rate": 1.3476210887269611e-05,
      "loss": 0.7136,
      "step": 22830
    },
    {
      "epoch": 4.894984997856836,
      "grad_norm": 0.8077274560928345,
      "learning_rate": 1.3473353336190887e-05,
      "loss": 0.8296,
      "step": 22840
    },
    {
      "epoch": 4.897128161165881,
      "grad_norm": 0.9546632170677185,
      "learning_rate": 1.3470495785112161e-05,
      "loss": 1.0669,
      "step": 22850
    },
    {
      "epoch": 4.899271324474925,
      "grad_norm": 29.774681091308594,
      "learning_rate": 1.3467638234033435e-05,
      "loss": 1.0266,
      "step": 22860
    },
    {
      "epoch": 4.901414487783969,
      "grad_norm": 29.26804542541504,
      "learning_rate": 1.3464780682954707e-05,
      "loss": 0.8418,
      "step": 22870
    },
    {
      "epoch": 4.9035576510930134,
      "grad_norm": 13.595296859741211,
      "learning_rate": 1.3461923131875983e-05,
      "loss": 0.8123,
      "step": 22880
    },
    {
      "epoch": 4.905700814402057,
      "grad_norm": 13.010095596313477,
      "learning_rate": 1.3459065580797257e-05,
      "loss": 0.6752,
      "step": 22890
    },
    {
      "epoch": 4.907843977711101,
      "grad_norm": 2.1673617362976074,
      "learning_rate": 1.3456208029718532e-05,
      "loss": 0.4451,
      "step": 22900
    },
    {
      "epoch": 4.909987141020146,
      "grad_norm": 2.1501007080078125,
      "learning_rate": 1.3453350478639806e-05,
      "loss": 0.5493,
      "step": 22910
    },
    {
      "epoch": 4.91213030432919,
      "grad_norm": 1.6832497119903564,
      "learning_rate": 1.345049292756108e-05,
      "loss": 0.3631,
      "step": 22920
    },
    {
      "epoch": 4.914273467638234,
      "grad_norm": 13.922856330871582,
      "learning_rate": 1.3447635376482356e-05,
      "loss": 0.6213,
      "step": 22930
    },
    {
      "epoch": 4.916416630947278,
      "grad_norm": 14.09716796875,
      "learning_rate": 1.344477782540363e-05,
      "loss": 0.7724,
      "step": 22940
    },
    {
      "epoch": 4.918559794256322,
      "grad_norm": 1.082787036895752,
      "learning_rate": 1.3441920274324904e-05,
      "loss": 0.6444,
      "step": 22950
    },
    {
      "epoch": 4.920702957565366,
      "grad_norm": 0.996224582195282,
      "learning_rate": 1.343906272324618e-05,
      "loss": 0.3988,
      "step": 22960
    },
    {
      "epoch": 4.922846120874411,
      "grad_norm": 29.20822525024414,
      "learning_rate": 1.3436205172167453e-05,
      "loss": 1.0463,
      "step": 22970
    },
    {
      "epoch": 4.924989284183455,
      "grad_norm": 14.165602684020996,
      "learning_rate": 1.3433347621088729e-05,
      "loss": 0.6658,
      "step": 22980
    },
    {
      "epoch": 4.9271324474924985,
      "grad_norm": 0.8504778146743774,
      "learning_rate": 1.3430490070010003e-05,
      "loss": 0.5416,
      "step": 22990
    },
    {
      "epoch": 4.929275610801543,
      "grad_norm": 0.8244238495826721,
      "learning_rate": 1.3427632518931277e-05,
      "loss": 0.4168,
      "step": 23000
    },
    {
      "epoch": 4.931418774110587,
      "grad_norm": 0.8013492226600647,
      "learning_rate": 1.3424774967852552e-05,
      "loss": 1.0926,
      "step": 23010
    },
    {
      "epoch": 4.933561937419631,
      "grad_norm": 0.8226027488708496,
      "learning_rate": 1.3421917416773826e-05,
      "loss": 0.824,
      "step": 23020
    },
    {
      "epoch": 4.935705100728676,
      "grad_norm": 14.308148384094238,
      "learning_rate": 1.34190598656951e-05,
      "loss": 0.6768,
      "step": 23030
    },
    {
      "epoch": 4.9378482640377195,
      "grad_norm": 0.7840240597724915,
      "learning_rate": 1.3416202314616376e-05,
      "loss": 0.1513,
      "step": 23040
    },
    {
      "epoch": 4.939991427346763,
      "grad_norm": 14.454208374023438,
      "learning_rate": 1.341334476353765e-05,
      "loss": 0.1577,
      "step": 23050
    },
    {
      "epoch": 4.942134590655808,
      "grad_norm": 0.5608558058738708,
      "learning_rate": 1.3410487212458925e-05,
      "loss": 0.5874,
      "step": 23060
    },
    {
      "epoch": 4.944277753964852,
      "grad_norm": 14.61961555480957,
      "learning_rate": 1.34076296613802e-05,
      "loss": 0.4516,
      "step": 23070
    },
    {
      "epoch": 4.946420917273897,
      "grad_norm": 0.5061869025230408,
      "learning_rate": 1.3404772110301472e-05,
      "loss": 0.4577,
      "step": 23080
    },
    {
      "epoch": 4.9485640805829405,
      "grad_norm": 14.69095230102539,
      "learning_rate": 1.3401914559222745e-05,
      "loss": 0.4646,
      "step": 23090
    },
    {
      "epoch": 4.950707243891984,
      "grad_norm": 0.46861445903778076,
      "learning_rate": 1.3399057008144021e-05,
      "loss": 0.3134,
      "step": 23100
    },
    {
      "epoch": 4.952850407201029,
      "grad_norm": 14.673861503601074,
      "learning_rate": 1.3396199457065295e-05,
      "loss": 0.3164,
      "step": 23110
    },
    {
      "epoch": 4.954993570510073,
      "grad_norm": 0.42603960633277893,
      "learning_rate": 1.339334190598657e-05,
      "loss": 0.7858,
      "step": 23120
    },
    {
      "epoch": 4.957136733819117,
      "grad_norm": 0.41424670815467834,
      "learning_rate": 1.3390484354907845e-05,
      "loss": 0.3241,
      "step": 23130
    },
    {
      "epoch": 4.9592798971281615,
      "grad_norm": 0.40631332993507385,
      "learning_rate": 1.3387626803829118e-05,
      "loss": 0.6386,
      "step": 23140
    },
    {
      "epoch": 4.961423060437205,
      "grad_norm": 0.3889378309249878,
      "learning_rate": 1.3384769252750394e-05,
      "loss": 0.3246,
      "step": 23150
    },
    {
      "epoch": 4.963566223746249,
      "grad_norm": 0.39618274569511414,
      "learning_rate": 1.3381911701671668e-05,
      "loss": 0.6417,
      "step": 23160
    },
    {
      "epoch": 4.965709387055294,
      "grad_norm": 14.730783462524414,
      "learning_rate": 1.3379054150592942e-05,
      "loss": 0.6399,
      "step": 23170
    },
    {
      "epoch": 4.967852550364338,
      "grad_norm": 29.665664672851562,
      "learning_rate": 1.3376196599514218e-05,
      "loss": 0.6412,
      "step": 23180
    },
    {
      "epoch": 4.969995713673382,
      "grad_norm": 0.3860332667827606,
      "learning_rate": 1.3373339048435492e-05,
      "loss": 0.1659,
      "step": 23190
    },
    {
      "epoch": 4.972138876982426,
      "grad_norm": 0.3957259953022003,
      "learning_rate": 1.3370481497356767e-05,
      "loss": 0.7973,
      "step": 23200
    },
    {
      "epoch": 4.97428204029147,
      "grad_norm": 0.40223056077957153,
      "learning_rate": 1.3367623946278041e-05,
      "loss": 0.3234,
      "step": 23210
    },
    {
      "epoch": 4.976425203600514,
      "grad_norm": 0.4126593768596649,
      "learning_rate": 1.3364766395199315e-05,
      "loss": 0.9539,
      "step": 23220
    },
    {
      "epoch": 4.978568366909559,
      "grad_norm": 0.43606022000312805,
      "learning_rate": 1.336190884412059e-05,
      "loss": 0.3196,
      "step": 23230
    },
    {
      "epoch": 4.980711530218603,
      "grad_norm": 0.4610309898853302,
      "learning_rate": 1.3359051293041865e-05,
      "loss": 1.4008,
      "step": 23240
    },
    {
      "epoch": 4.9828546935276465,
      "grad_norm": 29.530611038208008,
      "learning_rate": 1.335619374196314e-05,
      "loss": 0.9178,
      "step": 23250
    },
    {
      "epoch": 4.984997856836691,
      "grad_norm": 0.5217078328132629,
      "learning_rate": 1.3353336190884414e-05,
      "loss": 0.3087,
      "step": 23260
    },
    {
      "epoch": 4.987141020145735,
      "grad_norm": 0.5320985913276672,
      "learning_rate": 1.3350478639805688e-05,
      "loss": 0.7522,
      "step": 23270
    },
    {
      "epoch": 4.989284183454779,
      "grad_norm": 0.5686067342758179,
      "learning_rate": 1.3347621088726964e-05,
      "loss": 0.6001,
      "step": 23280
    },
    {
      "epoch": 4.991427346763824,
      "grad_norm": 0.5801846385002136,
      "learning_rate": 1.3344763537648238e-05,
      "loss": 0.8835,
      "step": 23290
    },
    {
      "epoch": 4.993570510072868,
      "grad_norm": 0.5802127718925476,
      "learning_rate": 1.334190598656951e-05,
      "loss": 0.4448,
      "step": 23300
    },
    {
      "epoch": 4.995713673381911,
      "grad_norm": 0.5433357954025269,
      "learning_rate": 1.3339048435490784e-05,
      "loss": 0.3036,
      "step": 23310
    },
    {
      "epoch": 4.997856836690956,
      "grad_norm": 0.4886610507965088,
      "learning_rate": 1.333619088441206e-05,
      "loss": 0.1609,
      "step": 23320
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.44737088680267334,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 1.0858,
      "step": 23330
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.8676666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.6170293688774109,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 396.5049,
      "eval_samples_per_second": 7.566,
      "eval_steps_per_second": 2.522,
      "step": 23330
    },
    {
      "epoch": 5.002143163309044,
      "grad_norm": 0.4828394055366516,
      "learning_rate": 1.3330475782254609e-05,
      "loss": 1.0821,
      "step": 23340
    },
    {
      "epoch": 5.004286326618089,
      "grad_norm": 0.49457210302352905,
      "learning_rate": 1.3327618231175883e-05,
      "loss": 0.3107,
      "step": 23350
    },
    {
      "epoch": 5.006429489927132,
      "grad_norm": 0.49619126319885254,
      "learning_rate": 1.3324760680097157e-05,
      "loss": 0.3115,
      "step": 23360
    },
    {
      "epoch": 5.008572653236176,
      "grad_norm": 14.55933666229248,
      "learning_rate": 1.3321903129018432e-05,
      "loss": 1.0652,
      "step": 23370
    },
    {
      "epoch": 5.010715816545221,
      "grad_norm": 0.5254421234130859,
      "learning_rate": 1.3319045577939706e-05,
      "loss": 0.4561,
      "step": 23380
    },
    {
      "epoch": 5.012858979854265,
      "grad_norm": 14.557207107543945,
      "learning_rate": 1.3316188026860982e-05,
      "loss": 0.4549,
      "step": 23390
    },
    {
      "epoch": 5.015002143163309,
      "grad_norm": 14.542459487915039,
      "learning_rate": 1.3313330475782256e-05,
      "loss": 0.7464,
      "step": 23400
    },
    {
      "epoch": 5.0171453064723535,
      "grad_norm": 0.5874727964401245,
      "learning_rate": 1.331047292470353e-05,
      "loss": 1.1726,
      "step": 23410
    },
    {
      "epoch": 5.019288469781397,
      "grad_norm": 14.698453903198242,
      "learning_rate": 1.3307615373624805e-05,
      "loss": 0.4478,
      "step": 23420
    },
    {
      "epoch": 5.021431633090441,
      "grad_norm": 14.458744049072266,
      "learning_rate": 1.330475782254608e-05,
      "loss": 0.8687,
      "step": 23430
    },
    {
      "epoch": 5.023574796399486,
      "grad_norm": 0.6360600590705872,
      "learning_rate": 1.3301900271467353e-05,
      "loss": 0.1557,
      "step": 23440
    },
    {
      "epoch": 5.02571795970853,
      "grad_norm": 14.459975242614746,
      "learning_rate": 1.3299042720388629e-05,
      "loss": 0.7251,
      "step": 23450
    },
    {
      "epoch": 5.027861123017574,
      "grad_norm": 14.466989517211914,
      "learning_rate": 1.3296185169309903e-05,
      "loss": 0.8699,
      "step": 23460
    },
    {
      "epoch": 5.030004286326618,
      "grad_norm": 0.6036055088043213,
      "learning_rate": 1.3293327618231178e-05,
      "loss": 1.1593,
      "step": 23470
    },
    {
      "epoch": 5.032147449635662,
      "grad_norm": 0.6217136383056641,
      "learning_rate": 1.3290470067152452e-05,
      "loss": 0.5885,
      "step": 23480
    },
    {
      "epoch": 5.034290612944706,
      "grad_norm": 14.490045547485352,
      "learning_rate": 1.3287612516073726e-05,
      "loss": 0.8689,
      "step": 23490
    },
    {
      "epoch": 5.036433776253751,
      "grad_norm": 0.7085395455360413,
      "learning_rate": 1.3284754964995002e-05,
      "loss": 1.0038,
      "step": 23500
    },
    {
      "epoch": 5.038576939562795,
      "grad_norm": 0.6959117650985718,
      "learning_rate": 1.3281897413916274e-05,
      "loss": 0.4283,
      "step": 23510
    },
    {
      "epoch": 5.0407201028718385,
      "grad_norm": 0.6789565682411194,
      "learning_rate": 1.3279039862837548e-05,
      "loss": 0.2902,
      "step": 23520
    },
    {
      "epoch": 5.042863266180883,
      "grad_norm": 14.353048324584961,
      "learning_rate": 1.3276182311758824e-05,
      "loss": 0.995,
      "step": 23530
    },
    {
      "epoch": 5.045006429489927,
      "grad_norm": 14.608176231384277,
      "learning_rate": 1.3273324760680098e-05,
      "loss": 0.1568,
      "step": 23540
    },
    {
      "epoch": 5.047149592798971,
      "grad_norm": 14.523810386657715,
      "learning_rate": 1.3270467209601372e-05,
      "loss": 0.1614,
      "step": 23550
    },
    {
      "epoch": 5.049292756108016,
      "grad_norm": 0.49814537167549133,
      "learning_rate": 1.3267609658522647e-05,
      "loss": 0.4613,
      "step": 23560
    },
    {
      "epoch": 5.0514359194170595,
      "grad_norm": 0.521513044834137,
      "learning_rate": 1.3264752107443921e-05,
      "loss": 0.7617,
      "step": 23570
    },
    {
      "epoch": 5.053579082726103,
      "grad_norm": 0.5278438329696655,
      "learning_rate": 1.3261894556365195e-05,
      "loss": 0.4591,
      "step": 23580
    },
    {
      "epoch": 5.055722246035148,
      "grad_norm": 14.810043334960938,
      "learning_rate": 1.325903700528647e-05,
      "loss": 0.4602,
      "step": 23590
    },
    {
      "epoch": 5.057865409344192,
      "grad_norm": 14.587279319763184,
      "learning_rate": 1.3256179454207745e-05,
      "loss": 1.0725,
      "step": 23600
    },
    {
      "epoch": 5.060008572653236,
      "grad_norm": 0.5020041465759277,
      "learning_rate": 1.325332190312902e-05,
      "loss": 0.6122,
      "step": 23610
    },
    {
      "epoch": 5.0621517359622805,
      "grad_norm": 0.5171518325805664,
      "learning_rate": 1.3250464352050294e-05,
      "loss": 0.4602,
      "step": 23620
    },
    {
      "epoch": 5.064294899271324,
      "grad_norm": 0.48589834570884705,
      "learning_rate": 1.3247606800971568e-05,
      "loss": 0.3135,
      "step": 23630
    },
    {
      "epoch": 5.066438062580368,
      "grad_norm": 14.56032943725586,
      "learning_rate": 1.3244749249892844e-05,
      "loss": 1.2136,
      "step": 23640
    },
    {
      "epoch": 5.068581225889413,
      "grad_norm": 0.5540680885314941,
      "learning_rate": 1.3241891698814118e-05,
      "loss": 0.4536,
      "step": 23650
    },
    {
      "epoch": 5.070724389198457,
      "grad_norm": 0.5561660528182983,
      "learning_rate": 1.3239034147735392e-05,
      "loss": 0.595,
      "step": 23660
    },
    {
      "epoch": 5.072867552507501,
      "grad_norm": 14.49600887298584,
      "learning_rate": 1.3236176596656667e-05,
      "loss": 1.0351,
      "step": 23670
    },
    {
      "epoch": 5.075010715816545,
      "grad_norm": 0.5581320524215698,
      "learning_rate": 1.3233319045577941e-05,
      "loss": 0.3081,
      "step": 23680
    },
    {
      "epoch": 5.077153879125589,
      "grad_norm": 0.5775865316390991,
      "learning_rate": 1.3230461494499217e-05,
      "loss": 0.6089,
      "step": 23690
    },
    {
      "epoch": 5.079297042434633,
      "grad_norm": 0.5090070962905884,
      "learning_rate": 1.322760394342049e-05,
      "loss": 0.3081,
      "step": 23700
    },
    {
      "epoch": 5.081440205743678,
      "grad_norm": 0.465035080909729,
      "learning_rate": 1.3224746392341765e-05,
      "loss": 0.4641,
      "step": 23710
    },
    {
      "epoch": 5.083583369052722,
      "grad_norm": 14.688894271850586,
      "learning_rate": 1.322188884126304e-05,
      "loss": 0.6206,
      "step": 23720
    },
    {
      "epoch": 5.085726532361766,
      "grad_norm": 0.46917399764060974,
      "learning_rate": 1.3219031290184312e-05,
      "loss": 0.4664,
      "step": 23730
    },
    {
      "epoch": 5.08786969567081,
      "grad_norm": 0.47613516449928284,
      "learning_rate": 1.3216173739105586e-05,
      "loss": 0.9297,
      "step": 23740
    },
    {
      "epoch": 5.090012858979854,
      "grad_norm": 14.598689079284668,
      "learning_rate": 1.3213316188026862e-05,
      "loss": 0.4641,
      "step": 23750
    },
    {
      "epoch": 5.092156022288899,
      "grad_norm": 0.491784930229187,
      "learning_rate": 1.3210458636948136e-05,
      "loss": 0.7803,
      "step": 23760
    },
    {
      "epoch": 5.094299185597943,
      "grad_norm": 14.5477933883667,
      "learning_rate": 1.320760108586941e-05,
      "loss": 1.0521,
      "step": 23770
    },
    {
      "epoch": 5.0964423489069866,
      "grad_norm": 0.6361358165740967,
      "learning_rate": 1.3204743534790685e-05,
      "loss": 0.7384,
      "step": 23780
    },
    {
      "epoch": 5.098585512216031,
      "grad_norm": 0.6143564581871033,
      "learning_rate": 1.320188598371196e-05,
      "loss": 0.4419,
      "step": 23790
    },
    {
      "epoch": 5.100728675525075,
      "grad_norm": 0.6059980988502502,
      "learning_rate": 1.3199028432633233e-05,
      "loss": 0.4421,
      "step": 23800
    },
    {
      "epoch": 5.102871838834119,
      "grad_norm": 0.5962865352630615,
      "learning_rate": 1.3196170881554509e-05,
      "loss": 0.7346,
      "step": 23810
    },
    {
      "epoch": 5.105015002143164,
      "grad_norm": 0.6008406281471252,
      "learning_rate": 1.3193313330475783e-05,
      "loss": 0.4432,
      "step": 23820
    },
    {
      "epoch": 5.107158165452208,
      "grad_norm": 0.5546503663063049,
      "learning_rate": 1.3190455779397059e-05,
      "loss": 0.5932,
      "step": 23830
    },
    {
      "epoch": 5.109301328761251,
      "grad_norm": 14.521197319030762,
      "learning_rate": 1.3187598228318332e-05,
      "loss": 0.8968,
      "step": 23840
    },
    {
      "epoch": 5.111444492070296,
      "grad_norm": 14.483182907104492,
      "learning_rate": 1.3184740677239606e-05,
      "loss": 0.7401,
      "step": 23850
    },
    {
      "epoch": 5.11358765537934,
      "grad_norm": 0.5876826643943787,
      "learning_rate": 1.3181883126160882e-05,
      "loss": 0.1569,
      "step": 23860
    },
    {
      "epoch": 5.115730818688384,
      "grad_norm": 14.96709156036377,
      "learning_rate": 1.3179025575082156e-05,
      "loss": 1.0221,
      "step": 23870
    },
    {
      "epoch": 5.117873981997429,
      "grad_norm": 0.7401845455169678,
      "learning_rate": 1.317616802400343e-05,
      "loss": 0.9919,
      "step": 23880
    },
    {
      "epoch": 5.1200171453064725,
      "grad_norm": 14.400074005126953,
      "learning_rate": 1.3173310472924705e-05,
      "loss": 0.8252,
      "step": 23890
    },
    {
      "epoch": 5.122160308615516,
      "grad_norm": 0.8237456679344177,
      "learning_rate": 1.317045292184598e-05,
      "loss": 0.4185,
      "step": 23900
    },
    {
      "epoch": 5.124303471924561,
      "grad_norm": 0.708595335483551,
      "learning_rate": 1.3167595370767255e-05,
      "loss": 0.562,
      "step": 23910
    },
    {
      "epoch": 5.126446635233605,
      "grad_norm": 14.404577255249023,
      "learning_rate": 1.3164737819688529e-05,
      "loss": 0.9897,
      "step": 23920
    },
    {
      "epoch": 5.128589798542649,
      "grad_norm": 14.364924430847168,
      "learning_rate": 1.3161880268609803e-05,
      "loss": 0.293,
      "step": 23930
    },
    {
      "epoch": 5.1307329618516935,
      "grad_norm": 14.559449195861816,
      "learning_rate": 1.3159022717531075e-05,
      "loss": 0.4403,
      "step": 23940
    },
    {
      "epoch": 5.132876125160737,
      "grad_norm": 0.5978811383247375,
      "learning_rate": 1.315616516645235e-05,
      "loss": 0.4416,
      "step": 23950
    },
    {
      "epoch": 5.135019288469781,
      "grad_norm": 0.6206507086753845,
      "learning_rate": 1.3153307615373625e-05,
      "loss": 1.0168,
      "step": 23960
    },
    {
      "epoch": 5.137162451778826,
      "grad_norm": 0.646366536617279,
      "learning_rate": 1.31504500642949e-05,
      "loss": 0.8686,
      "step": 23970
    },
    {
      "epoch": 5.13930561508787,
      "grad_norm": 14.496865272521973,
      "learning_rate": 1.3147592513216174e-05,
      "loss": 0.3006,
      "step": 23980
    },
    {
      "epoch": 5.141448778396914,
      "grad_norm": 14.552776336669922,
      "learning_rate": 1.3144734962137448e-05,
      "loss": 0.5911,
      "step": 23990
    },
    {
      "epoch": 5.143591941705958,
      "grad_norm": 14.498770713806152,
      "learning_rate": 1.3141877411058724e-05,
      "loss": 0.7422,
      "step": 24000
    },
    {
      "epoch": 5.145735105015002,
      "grad_norm": 14.892971992492676,
      "learning_rate": 1.3139019859979998e-05,
      "loss": 0.7349,
      "step": 24010
    },
    {
      "epoch": 5.147878268324046,
      "grad_norm": 0.577602207660675,
      "learning_rate": 1.3136162308901272e-05,
      "loss": 0.3041,
      "step": 24020
    },
    {
      "epoch": 5.150021431633091,
      "grad_norm": 0.5496249794960022,
      "learning_rate": 1.3133304757822547e-05,
      "loss": 0.3037,
      "step": 24030
    },
    {
      "epoch": 5.152164594942135,
      "grad_norm": 0.4910311698913574,
      "learning_rate": 1.3130447206743821e-05,
      "loss": 0.3083,
      "step": 24040
    },
    {
      "epoch": 5.1543077582511785,
      "grad_norm": 0.4160290062427521,
      "learning_rate": 1.3127589655665097e-05,
      "loss": 0.3144,
      "step": 24050
    },
    {
      "epoch": 5.156450921560223,
      "grad_norm": 0.35457107424736023,
      "learning_rate": 1.312473210458637e-05,
      "loss": 0.6443,
      "step": 24060
    },
    {
      "epoch": 5.158594084869267,
      "grad_norm": 0.3455672860145569,
      "learning_rate": 1.3121874553507645e-05,
      "loss": 0.17,
      "step": 24070
    },
    {
      "epoch": 5.160737248178311,
      "grad_norm": 14.749273300170898,
      "learning_rate": 1.311901700242892e-05,
      "loss": 0.173,
      "step": 24080
    },
    {
      "epoch": 5.162880411487356,
      "grad_norm": 0.31086280941963196,
      "learning_rate": 1.3116159451350194e-05,
      "loss": 0.5035,
      "step": 24090
    },
    {
      "epoch": 5.1650235747963995,
      "grad_norm": 0.3218977749347687,
      "learning_rate": 1.311330190027147e-05,
      "loss": 0.504,
      "step": 24100
    },
    {
      "epoch": 5.167166738105443,
      "grad_norm": 14.739553451538086,
      "learning_rate": 1.3110444349192744e-05,
      "loss": 0.9923,
      "step": 24110
    },
    {
      "epoch": 5.169309901414488,
      "grad_norm": 0.3631444275379181,
      "learning_rate": 1.3107586798114018e-05,
      "loss": 0.979,
      "step": 24120
    },
    {
      "epoch": 5.171453064723532,
      "grad_norm": 0.40329709649086,
      "learning_rate": 1.3104729247035293e-05,
      "loss": 0.4867,
      "step": 24130
    },
    {
      "epoch": 5.173596228032576,
      "grad_norm": 0.40907230973243713,
      "learning_rate": 1.3101871695956567e-05,
      "loss": 0.7939,
      "step": 24140
    },
    {
      "epoch": 5.1757393913416205,
      "grad_norm": 0.4519693851470947,
      "learning_rate": 1.3099014144877841e-05,
      "loss": 0.7894,
      "step": 24150
    },
    {
      "epoch": 5.177882554650664,
      "grad_norm": 0.4049909710884094,
      "learning_rate": 1.3096156593799113e-05,
      "loss": 0.3236,
      "step": 24160
    },
    {
      "epoch": 5.180025717959708,
      "grad_norm": 0.46174871921539307,
      "learning_rate": 1.3093299042720389e-05,
      "loss": 0.6317,
      "step": 24170
    },
    {
      "epoch": 5.182168881268753,
      "grad_norm": 0.43707701563835144,
      "learning_rate": 1.3090441491641663e-05,
      "loss": 0.6284,
      "step": 24180
    },
    {
      "epoch": 5.184312044577797,
      "grad_norm": 14.679944038391113,
      "learning_rate": 1.3087583940562939e-05,
      "loss": 0.6408,
      "step": 24190
    },
    {
      "epoch": 5.186455207886841,
      "grad_norm": 0.3901295065879822,
      "learning_rate": 1.3084726389484212e-05,
      "loss": 0.6477,
      "step": 24200
    },
    {
      "epoch": 5.188598371195885,
      "grad_norm": 15.160308837890625,
      "learning_rate": 1.3081868838405486e-05,
      "loss": 0.4859,
      "step": 24210
    },
    {
      "epoch": 5.190741534504929,
      "grad_norm": 0.39791202545166016,
      "learning_rate": 1.3079011287326762e-05,
      "loss": 0.7942,
      "step": 24220
    },
    {
      "epoch": 5.192884697813973,
      "grad_norm": 0.42396917939186096,
      "learning_rate": 1.3076153736248036e-05,
      "loss": 0.6354,
      "step": 24230
    },
    {
      "epoch": 5.195027861123018,
      "grad_norm": 0.4600389003753662,
      "learning_rate": 1.3073296185169312e-05,
      "loss": 0.3197,
      "step": 24240
    },
    {
      "epoch": 5.197171024432062,
      "grad_norm": 0.4164564907550812,
      "learning_rate": 1.3070438634090586e-05,
      "loss": 0.6323,
      "step": 24250
    },
    {
      "epoch": 5.1993141877411055,
      "grad_norm": 14.670175552368164,
      "learning_rate": 1.306758108301186e-05,
      "loss": 0.4793,
      "step": 24260
    },
    {
      "epoch": 5.20145735105015,
      "grad_norm": 14.751251220703125,
      "learning_rate": 1.3064723531933135e-05,
      "loss": 0.9465,
      "step": 24270
    },
    {
      "epoch": 5.203600514359194,
      "grad_norm": 30.022197723388672,
      "learning_rate": 1.3061865980854409e-05,
      "loss": 0.4682,
      "step": 24280
    },
    {
      "epoch": 5.205743677668238,
      "grad_norm": 14.784943580627441,
      "learning_rate": 1.3059008429775683e-05,
      "loss": 0.7732,
      "step": 24290
    },
    {
      "epoch": 5.207886840977283,
      "grad_norm": 0.556810200214386,
      "learning_rate": 1.3056150878696959e-05,
      "loss": 0.756,
      "step": 24300
    },
    {
      "epoch": 5.210030004286327,
      "grad_norm": 0.5728524923324585,
      "learning_rate": 1.3053293327618232e-05,
      "loss": 0.5982,
      "step": 24310
    },
    {
      "epoch": 5.21217316759537,
      "grad_norm": 14.553404808044434,
      "learning_rate": 1.3050435776539508e-05,
      "loss": 0.3057,
      "step": 24320
    },
    {
      "epoch": 5.214316330904415,
      "grad_norm": 0.5297481417655945,
      "learning_rate": 1.3047578225460782e-05,
      "loss": 0.4588,
      "step": 24330
    },
    {
      "epoch": 5.216459494213459,
      "grad_norm": 14.594277381896973,
      "learning_rate": 1.3044720674382056e-05,
      "loss": 0.3112,
      "step": 24340
    },
    {
      "epoch": 5.218602657522503,
      "grad_norm": 0.5108607411384583,
      "learning_rate": 1.3041863123303332e-05,
      "loss": 1.0622,
      "step": 24350
    },
    {
      "epoch": 5.220745820831548,
      "grad_norm": 29.631216049194336,
      "learning_rate": 1.3039005572224606e-05,
      "loss": 0.6047,
      "step": 24360
    },
    {
      "epoch": 5.222888984140591,
      "grad_norm": 0.5541254878044128,
      "learning_rate": 1.3036148021145878e-05,
      "loss": 0.7502,
      "step": 24370
    },
    {
      "epoch": 5.225032147449635,
      "grad_norm": 14.511136054992676,
      "learning_rate": 1.3033290470067153e-05,
      "loss": 0.5985,
      "step": 24380
    },
    {
      "epoch": 5.22717531075868,
      "grad_norm": 0.5913118720054626,
      "learning_rate": 1.3030432918988427e-05,
      "loss": 0.592,
      "step": 24390
    },
    {
      "epoch": 5.229318474067724,
      "grad_norm": 0.4996304214000702,
      "learning_rate": 1.3027575367909701e-05,
      "loss": 0.3054,
      "step": 24400
    },
    {
      "epoch": 5.231461637376768,
      "grad_norm": 0.5187912583351135,
      "learning_rate": 1.3024717816830977e-05,
      "loss": 0.6076,
      "step": 24410
    },
    {
      "epoch": 5.2336048006858125,
      "grad_norm": 0.4961469769477844,
      "learning_rate": 1.302186026575225e-05,
      "loss": 0.1599,
      "step": 24420
    },
    {
      "epoch": 5.235747963994856,
      "grad_norm": 14.60942554473877,
      "learning_rate": 1.3019002714673525e-05,
      "loss": 0.7653,
      "step": 24430
    },
    {
      "epoch": 5.2378911273039,
      "grad_norm": 29.52726936340332,
      "learning_rate": 1.30161451635948e-05,
      "loss": 0.9106,
      "step": 24440
    },
    {
      "epoch": 5.240034290612945,
      "grad_norm": 14.549187660217285,
      "learning_rate": 1.3013287612516074e-05,
      "loss": 0.6046,
      "step": 24450
    },
    {
      "epoch": 5.242177453921989,
      "grad_norm": 0.5268747210502625,
      "learning_rate": 1.301043006143735e-05,
      "loss": 0.6015,
      "step": 24460
    },
    {
      "epoch": 5.244320617231033,
      "grad_norm": 14.536609649658203,
      "learning_rate": 1.3007572510358624e-05,
      "loss": 0.6004,
      "step": 24470
    },
    {
      "epoch": 5.246463780540077,
      "grad_norm": 0.5722495913505554,
      "learning_rate": 1.3004714959279898e-05,
      "loss": 0.5975,
      "step": 24480
    },
    {
      "epoch": 5.248606943849121,
      "grad_norm": 0.5473899245262146,
      "learning_rate": 1.3001857408201173e-05,
      "loss": 0.4486,
      "step": 24490
    },
    {
      "epoch": 5.250750107158165,
      "grad_norm": 14.546011924743652,
      "learning_rate": 1.2998999857122447e-05,
      "loss": 0.8897,
      "step": 24500
    },
    {
      "epoch": 5.25289327046721,
      "grad_norm": 14.587100982666016,
      "learning_rate": 1.2996142306043721e-05,
      "loss": 0.454,
      "step": 24510
    },
    {
      "epoch": 5.255036433776254,
      "grad_norm": 0.5867013931274414,
      "learning_rate": 1.2993284754964997e-05,
      "loss": 0.4484,
      "step": 24520
    },
    {
      "epoch": 5.2571795970852975,
      "grad_norm": 0.5730960369110107,
      "learning_rate": 1.299042720388627e-05,
      "loss": 0.4493,
      "step": 24530
    },
    {
      "epoch": 5.259322760394342,
      "grad_norm": 0.5593664646148682,
      "learning_rate": 1.2987569652807546e-05,
      "loss": 0.4478,
      "step": 24540
    },
    {
      "epoch": 5.261465923703386,
      "grad_norm": 29.497379302978516,
      "learning_rate": 1.298471210172882e-05,
      "loss": 1.1757,
      "step": 24550
    },
    {
      "epoch": 5.26360908701243,
      "grad_norm": 14.487911224365234,
      "learning_rate": 1.2981854550650094e-05,
      "loss": 0.5895,
      "step": 24560
    },
    {
      "epoch": 5.265752250321475,
      "grad_norm": 0.6304792761802673,
      "learning_rate": 1.297899699957137e-05,
      "loss": 0.5868,
      "step": 24570
    },
    {
      "epoch": 5.2678954136305185,
      "grad_norm": 14.712479591369629,
      "learning_rate": 1.2976139448492644e-05,
      "loss": 0.5869,
      "step": 24580
    },
    {
      "epoch": 5.270038576939562,
      "grad_norm": 14.57126235961914,
      "learning_rate": 1.2973281897413916e-05,
      "loss": 0.4392,
      "step": 24590
    },
    {
      "epoch": 5.272181740248607,
      "grad_norm": 14.511947631835938,
      "learning_rate": 1.2970424346335192e-05,
      "loss": 0.5845,
      "step": 24600
    },
    {
      "epoch": 5.274324903557651,
      "grad_norm": 0.5822210311889648,
      "learning_rate": 1.2967566795256466e-05,
      "loss": 0.3053,
      "step": 24610
    },
    {
      "epoch": 5.276468066866695,
      "grad_norm": 0.5259159803390503,
      "learning_rate": 1.296470924417774e-05,
      "loss": 0.0123,
      "step": 24620
    },
    {
      "epoch": 5.2786112301757395,
      "grad_norm": 0.4799727499485016,
      "learning_rate": 1.2961851693099015e-05,
      "loss": 0.6159,
      "step": 24630
    },
    {
      "epoch": 5.280754393484783,
      "grad_norm": 14.7151460647583,
      "learning_rate": 1.2958994142020289e-05,
      "loss": 0.4741,
      "step": 24640
    },
    {
      "epoch": 5.282897556793827,
      "grad_norm": 29.68515968322754,
      "learning_rate": 1.2956136590941563e-05,
      "loss": 0.9477,
      "step": 24650
    },
    {
      "epoch": 5.285040720102872,
      "grad_norm": 29.596696853637695,
      "learning_rate": 1.2953279039862839e-05,
      "loss": 1.0896,
      "step": 24660
    },
    {
      "epoch": 5.287183883411916,
      "grad_norm": 14.592583656311035,
      "learning_rate": 1.2950421488784113e-05,
      "loss": 0.6174,
      "step": 24670
    },
    {
      "epoch": 5.2893270467209605,
      "grad_norm": 29.638486862182617,
      "learning_rate": 1.2947563937705388e-05,
      "loss": 0.6138,
      "step": 24680
    },
    {
      "epoch": 5.291470210030004,
      "grad_norm": 0.485198438167572,
      "learning_rate": 1.2944706386626662e-05,
      "loss": 0.4639,
      "step": 24690
    },
    {
      "epoch": 5.293613373339048,
      "grad_norm": 14.629899024963379,
      "learning_rate": 1.2941848835547936e-05,
      "loss": 1.0612,
      "step": 24700
    },
    {
      "epoch": 5.295756536648093,
      "grad_norm": 14.521575927734375,
      "learning_rate": 1.2938991284469212e-05,
      "loss": 0.7504,
      "step": 24710
    },
    {
      "epoch": 5.297899699957137,
      "grad_norm": 14.563919067382812,
      "learning_rate": 1.2936133733390486e-05,
      "loss": 0.4498,
      "step": 24720
    },
    {
      "epoch": 5.300042863266181,
      "grad_norm": 15.646917343139648,
      "learning_rate": 1.293327618231176e-05,
      "loss": 0.4565,
      "step": 24730
    },
    {
      "epoch": 5.302186026575225,
      "grad_norm": 0.4693315029144287,
      "learning_rate": 1.2930418631233035e-05,
      "loss": 0.6153,
      "step": 24740
    },
    {
      "epoch": 5.304329189884269,
      "grad_norm": 14.896440505981445,
      "learning_rate": 1.2927561080154309e-05,
      "loss": 0.9194,
      "step": 24750
    },
    {
      "epoch": 5.306472353193313,
      "grad_norm": 0.6099097728729248,
      "learning_rate": 1.2924703529075585e-05,
      "loss": 1.3207,
      "step": 24760
    },
    {
      "epoch": 5.308615516502358,
      "grad_norm": 14.431581497192383,
      "learning_rate": 1.2921845977996859e-05,
      "loss": 0.5784,
      "step": 24770
    },
    {
      "epoch": 5.310758679811402,
      "grad_norm": 0.6525064706802368,
      "learning_rate": 1.2918988426918133e-05,
      "loss": 0.4366,
      "step": 24780
    },
    {
      "epoch": 5.3129018431204456,
      "grad_norm": 14.402901649475098,
      "learning_rate": 1.2916130875839408e-05,
      "loss": 1.5608,
      "step": 24790
    },
    {
      "epoch": 5.31504500642949,
      "grad_norm": 0.8176445960998535,
      "learning_rate": 1.291327332476068e-05,
      "loss": 0.826,
      "step": 24800
    },
    {
      "epoch": 5.317188169738534,
      "grad_norm": 0.910306453704834,
      "learning_rate": 1.2910415773681954e-05,
      "loss": 0.8106,
      "step": 24810
    },
    {
      "epoch": 5.319331333047578,
      "grad_norm": 0.8865885734558105,
      "learning_rate": 1.290755822260323e-05,
      "loss": 0.6724,
      "step": 24820
    },
    {
      "epoch": 5.321474496356623,
      "grad_norm": 0.7282984256744385,
      "learning_rate": 1.2904700671524504e-05,
      "loss": 0.4183,
      "step": 24830
    },
    {
      "epoch": 5.323617659665667,
      "grad_norm": 14.533864974975586,
      "learning_rate": 1.2901843120445778e-05,
      "loss": 0.708,
      "step": 24840
    },
    {
      "epoch": 5.32576082297471,
      "grad_norm": 0.7763237953186035,
      "learning_rate": 1.2898985569367053e-05,
      "loss": 0.6994,
      "step": 24850
    },
    {
      "epoch": 5.327903986283755,
      "grad_norm": 14.373483657836914,
      "learning_rate": 1.2896128018288327e-05,
      "loss": 0.6884,
      "step": 24860
    },
    {
      "epoch": 5.330047149592799,
      "grad_norm": 0.624876081943512,
      "learning_rate": 1.2893270467209601e-05,
      "loss": 0.1565,
      "step": 24870
    },
    {
      "epoch": 5.332190312901843,
      "grad_norm": 14.507258415222168,
      "learning_rate": 1.2890412916130877e-05,
      "loss": 0.8759,
      "step": 24880
    },
    {
      "epoch": 5.334333476210888,
      "grad_norm": 0.6126009821891785,
      "learning_rate": 1.288755536505215e-05,
      "loss": 0.7316,
      "step": 24890
    },
    {
      "epoch": 5.3364766395199315,
      "grad_norm": 0.6104049682617188,
      "learning_rate": 1.2884697813973426e-05,
      "loss": 0.4413,
      "step": 24900
    },
    {
      "epoch": 5.338619802828975,
      "grad_norm": 0.5530045032501221,
      "learning_rate": 1.28818402628947e-05,
      "loss": 0.3023,
      "step": 24910
    },
    {
      "epoch": 5.34076296613802,
      "grad_norm": 0.5484166145324707,
      "learning_rate": 1.2878982711815974e-05,
      "loss": 1.0518,
      "step": 24920
    },
    {
      "epoch": 5.342906129447064,
      "grad_norm": 0.5947751402854919,
      "learning_rate": 1.287612516073725e-05,
      "loss": 0.739,
      "step": 24930
    },
    {
      "epoch": 5.345049292756108,
      "grad_norm": 0.5713143348693848,
      "learning_rate": 1.2873267609658524e-05,
      "loss": 0.3001,
      "step": 24940
    },
    {
      "epoch": 5.3471924560651525,
      "grad_norm": 0.5547630786895752,
      "learning_rate": 1.28704100585798e-05,
      "loss": 0.8955,
      "step": 24950
    },
    {
      "epoch": 5.349335619374196,
      "grad_norm": 0.5591611862182617,
      "learning_rate": 1.2867552507501073e-05,
      "loss": 0.8921,
      "step": 24960
    },
    {
      "epoch": 5.35147878268324,
      "grad_norm": 14.503252029418945,
      "learning_rate": 1.2864694956422347e-05,
      "loss": 0.596,
      "step": 24970
    },
    {
      "epoch": 5.353621945992285,
      "grad_norm": 0.6045854091644287,
      "learning_rate": 1.2861837405343623e-05,
      "loss": 0.5885,
      "step": 24980
    },
    {
      "epoch": 5.355765109301329,
      "grad_norm": 0.6233004927635193,
      "learning_rate": 1.2858979854264897e-05,
      "loss": 0.443,
      "step": 24990
    },
    {
      "epoch": 5.357908272610373,
      "grad_norm": 14.483648300170898,
      "learning_rate": 1.285612230318617e-05,
      "loss": 0.7346,
      "step": 25000
    },
    {
      "epoch": 5.360051435919417,
      "grad_norm": 0.6251224279403687,
      "learning_rate": 1.2853264752107446e-05,
      "loss": 0.5862,
      "step": 25010
    },
    {
      "epoch": 5.362194599228461,
      "grad_norm": 14.531201362609863,
      "learning_rate": 1.2850407201028719e-05,
      "loss": 0.3066,
      "step": 25020
    },
    {
      "epoch": 5.364337762537505,
      "grad_norm": 14.538641929626465,
      "learning_rate": 1.2847549649949993e-05,
      "loss": 0.8928,
      "step": 25030
    },
    {
      "epoch": 5.36648092584655,
      "grad_norm": 0.5510365962982178,
      "learning_rate": 1.2844692098871268e-05,
      "loss": 0.3061,
      "step": 25040
    },
    {
      "epoch": 5.368624089155594,
      "grad_norm": 0.5385499000549316,
      "learning_rate": 1.2841834547792542e-05,
      "loss": 1.0419,
      "step": 25050
    },
    {
      "epoch": 5.3707672524646375,
      "grad_norm": 0.5411620736122131,
      "learning_rate": 1.2838976996713816e-05,
      "loss": 0.456,
      "step": 25060
    },
    {
      "epoch": 5.372910415773682,
      "grad_norm": 0.48684459924697876,
      "learning_rate": 1.2836119445635092e-05,
      "loss": 0.3122,
      "step": 25070
    },
    {
      "epoch": 5.375053579082726,
      "grad_norm": 0.4238012433052063,
      "learning_rate": 1.2833261894556366e-05,
      "loss": 0.4751,
      "step": 25080
    },
    {
      "epoch": 5.37719674239177,
      "grad_norm": 0.3887302875518799,
      "learning_rate": 1.2830404343477641e-05,
      "loss": 0.1658,
      "step": 25090
    },
    {
      "epoch": 5.379339905700815,
      "grad_norm": 0.3652227520942688,
      "learning_rate": 1.2827546792398915e-05,
      "loss": 0.3282,
      "step": 25100
    },
    {
      "epoch": 5.3814830690098585,
      "grad_norm": 14.729887008666992,
      "learning_rate": 1.2824689241320189e-05,
      "loss": 0.65,
      "step": 25110
    },
    {
      "epoch": 5.383626232318902,
      "grad_norm": 0.3626200258731842,
      "learning_rate": 1.2821831690241465e-05,
      "loss": 0.4916,
      "step": 25120
    },
    {
      "epoch": 5.385769395627947,
      "grad_norm": 29.80772590637207,
      "learning_rate": 1.2818974139162739e-05,
      "loss": 0.9648,
      "step": 25130
    },
    {
      "epoch": 5.387912558936991,
      "grad_norm": 15.032388687133789,
      "learning_rate": 1.2816116588084013e-05,
      "loss": 0.6331,
      "step": 25140
    },
    {
      "epoch": 5.390055722246035,
      "grad_norm": 0.4168713390827179,
      "learning_rate": 1.2813259037005288e-05,
      "loss": 0.1684,
      "step": 25150
    },
    {
      "epoch": 5.3921988855550795,
      "grad_norm": 0.3233942687511444,
      "learning_rate": 1.2810401485926562e-05,
      "loss": 0.326,
      "step": 25160
    },
    {
      "epoch": 5.394342048864123,
      "grad_norm": 0.3140923082828522,
      "learning_rate": 1.2807543934847838e-05,
      "loss": 0.3368,
      "step": 25170
    },
    {
      "epoch": 5.396485212173167,
      "grad_norm": 0.28410589694976807,
      "learning_rate": 1.2804686383769112e-05,
      "loss": 0.8425,
      "step": 25180
    },
    {
      "epoch": 5.398628375482212,
      "grad_norm": 0.3057590126991272,
      "learning_rate": 1.2801828832690386e-05,
      "loss": 0.3471,
      "step": 25190
    },
    {
      "epoch": 5.400771538791256,
      "grad_norm": 0.36325085163116455,
      "learning_rate": 1.2798971281611661e-05,
      "loss": 0.8395,
      "step": 25200
    },
    {
      "epoch": 5.4029147021003,
      "grad_norm": 0.42432335019111633,
      "learning_rate": 1.2796113730532935e-05,
      "loss": 1.13,
      "step": 25210
    },
    {
      "epoch": 5.405057865409344,
      "grad_norm": 0.446229487657547,
      "learning_rate": 1.2793256179454209e-05,
      "loss": 0.3181,
      "step": 25220
    },
    {
      "epoch": 5.407201028718388,
      "grad_norm": 14.6618070602417,
      "learning_rate": 1.2790398628375483e-05,
      "loss": 0.6234,
      "step": 25230
    },
    {
      "epoch": 5.409344192027432,
      "grad_norm": 0.45925992727279663,
      "learning_rate": 1.2787541077296757e-05,
      "loss": 0.4707,
      "step": 25240
    },
    {
      "epoch": 5.411487355336477,
      "grad_norm": 30.13406753540039,
      "learning_rate": 1.2784683526218031e-05,
      "loss": 0.4806,
      "step": 25250
    },
    {
      "epoch": 5.413630518645521,
      "grad_norm": 0.41648924350738525,
      "learning_rate": 1.2781825975139306e-05,
      "loss": 0.963,
      "step": 25260
    },
    {
      "epoch": 5.4157736819545645,
      "grad_norm": 0.428344190120697,
      "learning_rate": 1.277896842406058e-05,
      "loss": 0.7931,
      "step": 25270
    },
    {
      "epoch": 5.417916845263609,
      "grad_norm": 0.466109961271286,
      "learning_rate": 1.2776110872981854e-05,
      "loss": 0.4735,
      "step": 25280
    },
    {
      "epoch": 5.420060008572653,
      "grad_norm": 14.62512493133545,
      "learning_rate": 1.277325332190313e-05,
      "loss": 0.7747,
      "step": 25290
    },
    {
      "epoch": 5.422203171881697,
      "grad_norm": 29.69548988342285,
      "learning_rate": 1.2770395770824404e-05,
      "loss": 1.219,
      "step": 25300
    },
    {
      "epoch": 5.424346335190742,
      "grad_norm": 0.5445728302001953,
      "learning_rate": 1.276753821974568e-05,
      "loss": 0.3095,
      "step": 25310
    },
    {
      "epoch": 5.426489498499786,
      "grad_norm": 0.5564387440681458,
      "learning_rate": 1.2764680668666953e-05,
      "loss": 0.8924,
      "step": 25320
    },
    {
      "epoch": 5.428632661808829,
      "grad_norm": 0.5175887942314148,
      "learning_rate": 1.2761823117588227e-05,
      "loss": 0.3082,
      "step": 25330
    },
    {
      "epoch": 5.430775825117874,
      "grad_norm": 0.5091133117675781,
      "learning_rate": 1.2758965566509503e-05,
      "loss": 0.3093,
      "step": 25340
    },
    {
      "epoch": 5.432918988426918,
      "grad_norm": 0.47980040311813354,
      "learning_rate": 1.2756108015430777e-05,
      "loss": 0.6123,
      "step": 25350
    },
    {
      "epoch": 5.435062151735963,
      "grad_norm": 0.4688653349876404,
      "learning_rate": 1.2753250464352051e-05,
      "loss": 0.6175,
      "step": 25360
    },
    {
      "epoch": 5.437205315045007,
      "grad_norm": 14.893838882446289,
      "learning_rate": 1.2750392913273326e-05,
      "loss": 0.6169,
      "step": 25370
    },
    {
      "epoch": 5.43934847835405,
      "grad_norm": 0.48635783791542053,
      "learning_rate": 1.27475353621946e-05,
      "loss": 0.3114,
      "step": 25380
    },
    {
      "epoch": 5.441491641663095,
      "grad_norm": 0.4840824007987976,
      "learning_rate": 1.2744677811115876e-05,
      "loss": 0.7675,
      "step": 25390
    },
    {
      "epoch": 5.443634804972139,
      "grad_norm": 0.5129240155220032,
      "learning_rate": 1.274182026003715e-05,
      "loss": 0.7671,
      "step": 25400
    },
    {
      "epoch": 5.445777968281183,
      "grad_norm": 29.647457122802734,
      "learning_rate": 1.2738962708958424e-05,
      "loss": 0.9135,
      "step": 25410
    },
    {
      "epoch": 5.447921131590228,
      "grad_norm": 0.5224351286888123,
      "learning_rate": 1.27361051578797e-05,
      "loss": 0.4596,
      "step": 25420
    },
    {
      "epoch": 5.4500642948992715,
      "grad_norm": 29.676132202148438,
      "learning_rate": 1.2733247606800973e-05,
      "loss": 0.9089,
      "step": 25430
    },
    {
      "epoch": 5.452207458208315,
      "grad_norm": 0.5077565312385559,
      "learning_rate": 1.2730390055722247e-05,
      "loss": 0.1609,
      "step": 25440
    },
    {
      "epoch": 5.45435062151736,
      "grad_norm": 15.58800220489502,
      "learning_rate": 1.2727532504643521e-05,
      "loss": 0.7664,
      "step": 25450
    },
    {
      "epoch": 5.456493784826404,
      "grad_norm": 0.48139676451683044,
      "learning_rate": 1.2724674953564795e-05,
      "loss": 0.4654,
      "step": 25460
    },
    {
      "epoch": 5.458636948135448,
      "grad_norm": 0.4688063859939575,
      "learning_rate": 1.2721817402486069e-05,
      "loss": 0.463,
      "step": 25470
    },
    {
      "epoch": 5.4607801114444925,
      "grad_norm": 14.725102424621582,
      "learning_rate": 1.2718959851407345e-05,
      "loss": 0.4799,
      "step": 25480
    },
    {
      "epoch": 5.462923274753536,
      "grad_norm": 0.4335658550262451,
      "learning_rate": 1.2716102300328619e-05,
      "loss": 1.1015,
      "step": 25490
    },
    {
      "epoch": 5.46506643806258,
      "grad_norm": 0.49413976073265076,
      "learning_rate": 1.2713244749249893e-05,
      "loss": 0.928,
      "step": 25500
    },
    {
      "epoch": 5.467209601371625,
      "grad_norm": 0.5432735085487366,
      "learning_rate": 1.2710387198171168e-05,
      "loss": 0.7616,
      "step": 25510
    },
    {
      "epoch": 5.469352764680669,
      "grad_norm": 0.586981475353241,
      "learning_rate": 1.2707529647092442e-05,
      "loss": 0.8942,
      "step": 25520
    },
    {
      "epoch": 5.471495927989713,
      "grad_norm": 14.841479301452637,
      "learning_rate": 1.2704672096013718e-05,
      "loss": 1.3134,
      "step": 25530
    },
    {
      "epoch": 5.473639091298757,
      "grad_norm": 14.616177558898926,
      "learning_rate": 1.2701814544934992e-05,
      "loss": 0.7159,
      "step": 25540
    },
    {
      "epoch": 5.475782254607801,
      "grad_norm": 0.7309958934783936,
      "learning_rate": 1.2698956993856266e-05,
      "loss": 0.7035,
      "step": 25550
    },
    {
      "epoch": 5.477925417916845,
      "grad_norm": 0.6912949681282043,
      "learning_rate": 1.2696099442777541e-05,
      "loss": 0.2927,
      "step": 25560
    },
    {
      "epoch": 5.48006858122589,
      "grad_norm": 14.467912673950195,
      "learning_rate": 1.2693241891698815e-05,
      "loss": 0.7274,
      "step": 25570
    },
    {
      "epoch": 5.482211744534934,
      "grad_norm": 14.582919120788574,
      "learning_rate": 1.2690384340620089e-05,
      "loss": 0.4488,
      "step": 25580
    },
    {
      "epoch": 5.4843549078439775,
      "grad_norm": 0.605461835861206,
      "learning_rate": 1.2687526789541365e-05,
      "loss": 1.0279,
      "step": 25590
    },
    {
      "epoch": 5.486498071153022,
      "grad_norm": 14.522058486938477,
      "learning_rate": 1.2684669238462639e-05,
      "loss": 0.4519,
      "step": 25600
    },
    {
      "epoch": 5.488641234462066,
      "grad_norm": 14.742986679077148,
      "learning_rate": 1.2681811687383914e-05,
      "loss": 0.8931,
      "step": 25610
    },
    {
      "epoch": 5.49078439777111,
      "grad_norm": 0.598335325717926,
      "learning_rate": 1.2678954136305188e-05,
      "loss": 0.5932,
      "step": 25620
    },
    {
      "epoch": 5.492927561080155,
      "grad_norm": 0.5831088423728943,
      "learning_rate": 1.2676096585226462e-05,
      "loss": 0.5913,
      "step": 25630
    },
    {
      "epoch": 5.4950707243891985,
      "grad_norm": 14.553836822509766,
      "learning_rate": 1.2673239034147738e-05,
      "loss": 0.8684,
      "step": 25640
    },
    {
      "epoch": 5.497213887698242,
      "grad_norm": 0.6652311086654663,
      "learning_rate": 1.2670381483069012e-05,
      "loss": 0.4363,
      "step": 25650
    },
    {
      "epoch": 5.499357051007287,
      "grad_norm": 0.6630098819732666,
      "learning_rate": 1.2667523931990284e-05,
      "loss": 0.5751,
      "step": 25660
    },
    {
      "epoch": 5.501500214316331,
      "grad_norm": 0.5760033130645752,
      "learning_rate": 1.266466638091156e-05,
      "loss": 0.1553,
      "step": 25670
    },
    {
      "epoch": 5.503643377625375,
      "grad_norm": 0.607306182384491,
      "learning_rate": 1.2661808829832833e-05,
      "loss": 1.0331,
      "step": 25680
    },
    {
      "epoch": 5.5057865409344195,
      "grad_norm": 15.282629013061523,
      "learning_rate": 1.2658951278754107e-05,
      "loss": 0.3031,
      "step": 25690
    },
    {
      "epoch": 5.507929704243463,
      "grad_norm": 15.230890274047852,
      "learning_rate": 1.2656093727675383e-05,
      "loss": 0.8894,
      "step": 25700
    },
    {
      "epoch": 5.510072867552507,
      "grad_norm": 0.5477327108383179,
      "learning_rate": 1.2653236176596657e-05,
      "loss": 0.2993,
      "step": 25710
    },
    {
      "epoch": 5.512216030861552,
      "grad_norm": 0.4324570298194885,
      "learning_rate": 1.2650378625517931e-05,
      "loss": 0.0108,
      "step": 25720
    },
    {
      "epoch": 5.514359194170596,
      "grad_norm": 14.896580696105957,
      "learning_rate": 1.2647521074439207e-05,
      "loss": 0.6476,
      "step": 25730
    },
    {
      "epoch": 5.51650235747964,
      "grad_norm": 0.3358731269836426,
      "learning_rate": 1.264466352336048e-05,
      "loss": 0.3342,
      "step": 25740
    },
    {
      "epoch": 5.518645520788684,
      "grad_norm": 0.3208179175853729,
      "learning_rate": 1.2641805972281756e-05,
      "loss": 0.6588,
      "step": 25750
    },
    {
      "epoch": 5.520788684097728,
      "grad_norm": 14.940448760986328,
      "learning_rate": 1.263894842120303e-05,
      "loss": 0.8323,
      "step": 25760
    },
    {
      "epoch": 5.522931847406772,
      "grad_norm": 0.34284451603889465,
      "learning_rate": 1.2636090870124304e-05,
      "loss": 0.4937,
      "step": 25770
    },
    {
      "epoch": 5.525075010715817,
      "grad_norm": 15.04532527923584,
      "learning_rate": 1.263323331904558e-05,
      "loss": 0.4951,
      "step": 25780
    },
    {
      "epoch": 5.527218174024861,
      "grad_norm": 14.695923805236816,
      "learning_rate": 1.2630375767966853e-05,
      "loss": 0.8096,
      "step": 25790
    },
    {
      "epoch": 5.529361337333905,
      "grad_norm": 0.4214771091938019,
      "learning_rate": 1.2627518216888129e-05,
      "loss": 0.7936,
      "step": 25800
    },
    {
      "epoch": 5.531504500642949,
      "grad_norm": 0.4462122321128845,
      "learning_rate": 1.2624660665809403e-05,
      "loss": 0.6297,
      "step": 25810
    },
    {
      "epoch": 5.533647663951993,
      "grad_norm": 0.4627048671245575,
      "learning_rate": 1.2621803114730677e-05,
      "loss": 0.926,
      "step": 25820
    },
    {
      "epoch": 5.535790827261037,
      "grad_norm": 0.49486416578292847,
      "learning_rate": 1.2618945563651953e-05,
      "loss": 0.9232,
      "step": 25830
    },
    {
      "epoch": 5.537933990570082,
      "grad_norm": 0.5142198204994202,
      "learning_rate": 1.2616088012573227e-05,
      "loss": 1.0583,
      "step": 25840
    },
    {
      "epoch": 5.540077153879126,
      "grad_norm": 0.5605877041816711,
      "learning_rate": 1.26132304614945e-05,
      "loss": 0.6008,
      "step": 25850
    },
    {
      "epoch": 5.542220317188169,
      "grad_norm": 0.5546348094940186,
      "learning_rate": 1.2610372910415776e-05,
      "loss": 0.5972,
      "step": 25860
    },
    {
      "epoch": 5.544363480497214,
      "grad_norm": 0.5479029417037964,
      "learning_rate": 1.260751535933705e-05,
      "loss": 1.0329,
      "step": 25870
    },
    {
      "epoch": 5.546506643806258,
      "grad_norm": 0.5826938152313232,
      "learning_rate": 1.2604657808258322e-05,
      "loss": 0.5927,
      "step": 25880
    },
    {
      "epoch": 5.548649807115302,
      "grad_norm": 0.5717686414718628,
      "learning_rate": 1.2601800257179598e-05,
      "loss": 0.7405,
      "step": 25890
    },
    {
      "epoch": 5.550792970424347,
      "grad_norm": 14.553301811218262,
      "learning_rate": 1.2598942706100872e-05,
      "loss": 0.3082,
      "step": 25900
    },
    {
      "epoch": 5.5529361337333905,
      "grad_norm": 0.4958496391773224,
      "learning_rate": 1.2596085155022146e-05,
      "loss": 0.6029,
      "step": 25910
    },
    {
      "epoch": 5.555079297042434,
      "grad_norm": 0.49822869896888733,
      "learning_rate": 1.2593227603943421e-05,
      "loss": 0.3084,
      "step": 25920
    },
    {
      "epoch": 5.557222460351479,
      "grad_norm": 29.812183380126953,
      "learning_rate": 1.2590370052864695e-05,
      "loss": 1.0665,
      "step": 25930
    },
    {
      "epoch": 5.559365623660523,
      "grad_norm": 0.5309151411056519,
      "learning_rate": 1.2587512501785971e-05,
      "loss": 0.7668,
      "step": 25940
    },
    {
      "epoch": 5.561508786969567,
      "grad_norm": 14.777234077453613,
      "learning_rate": 1.2584654950707245e-05,
      "loss": 0.4584,
      "step": 25950
    },
    {
      "epoch": 5.5636519502786115,
      "grad_norm": 0.5018961429595947,
      "learning_rate": 1.2581797399628519e-05,
      "loss": 0.6118,
      "step": 25960
    },
    {
      "epoch": 5.565795113587655,
      "grad_norm": 0.4950997233390808,
      "learning_rate": 1.2578939848549794e-05,
      "loss": 0.4579,
      "step": 25970
    },
    {
      "epoch": 5.567938276896699,
      "grad_norm": 0.5303931832313538,
      "learning_rate": 1.2576082297471068e-05,
      "loss": 0.9044,
      "step": 25980
    },
    {
      "epoch": 5.570081440205744,
      "grad_norm": 0.5864093899726868,
      "learning_rate": 1.2573224746392342e-05,
      "loss": 0.602,
      "step": 25990
    },
    {
      "epoch": 5.572224603514788,
      "grad_norm": 14.558364868164062,
      "learning_rate": 1.2570367195313618e-05,
      "loss": 0.5956,
      "step": 26000
    },
    {
      "epoch": 5.574367766823832,
      "grad_norm": 14.527982711791992,
      "learning_rate": 1.2567509644234892e-05,
      "loss": 0.8783,
      "step": 26010
    },
    {
      "epoch": 5.576510930132876,
      "grad_norm": 0.7133597135543823,
      "learning_rate": 1.2564652093156167e-05,
      "loss": 0.7189,
      "step": 26020
    },
    {
      "epoch": 5.57865409344192,
      "grad_norm": 14.398932456970215,
      "learning_rate": 1.2561794542077441e-05,
      "loss": 0.7107,
      "step": 26030
    },
    {
      "epoch": 5.580797256750964,
      "grad_norm": 0.6416457295417786,
      "learning_rate": 1.2558936990998715e-05,
      "loss": 0.2933,
      "step": 26040
    },
    {
      "epoch": 5.582940420060009,
      "grad_norm": 14.569012641906738,
      "learning_rate": 1.2556079439919991e-05,
      "loss": 0.7254,
      "step": 26050
    },
    {
      "epoch": 5.585083583369053,
      "grad_norm": 14.499701499938965,
      "learning_rate": 1.2553221888841265e-05,
      "loss": 1.028,
      "step": 26060
    },
    {
      "epoch": 5.5872267466780965,
      "grad_norm": 0.7084444761276245,
      "learning_rate": 1.2550364337762539e-05,
      "loss": 0.7212,
      "step": 26070
    },
    {
      "epoch": 5.589369909987141,
      "grad_norm": 14.378154754638672,
      "learning_rate": 1.2547506786683814e-05,
      "loss": 1.6429,
      "step": 26080
    },
    {
      "epoch": 5.591513073296185,
      "grad_norm": 0.8812859058380127,
      "learning_rate": 1.2544649235605087e-05,
      "loss": 0.5444,
      "step": 26090
    },
    {
      "epoch": 5.593656236605229,
      "grad_norm": 14.358154296875,
      "learning_rate": 1.254179168452636e-05,
      "loss": 0.6717,
      "step": 26100
    },
    {
      "epoch": 5.595799399914274,
      "grad_norm": 0.8140304684638977,
      "learning_rate": 1.2538934133447636e-05,
      "loss": 0.28,
      "step": 26110
    },
    {
      "epoch": 5.5979425632233175,
      "grad_norm": 0.6863012909889221,
      "learning_rate": 1.253607658236891e-05,
      "loss": 0.1517,
      "step": 26120
    },
    {
      "epoch": 5.600085726532361,
      "grad_norm": 0.5750784873962402,
      "learning_rate": 1.2533219031290184e-05,
      "loss": 0.3025,
      "step": 26130
    },
    {
      "epoch": 5.602228889841406,
      "grad_norm": 44.81629943847656,
      "learning_rate": 1.253036148021146e-05,
      "loss": 1.0566,
      "step": 26140
    },
    {
      "epoch": 5.60437205315045,
      "grad_norm": 0.42768579721450806,
      "learning_rate": 1.2527503929132734e-05,
      "loss": 0.1659,
      "step": 26150
    },
    {
      "epoch": 5.606515216459494,
      "grad_norm": 14.85336971282959,
      "learning_rate": 1.252464637805401e-05,
      "loss": 0.7766,
      "step": 26160
    },
    {
      "epoch": 5.6086583797685385,
      "grad_norm": 0.47611135244369507,
      "learning_rate": 1.2521788826975283e-05,
      "loss": 0.6281,
      "step": 26170
    },
    {
      "epoch": 5.610801543077582,
      "grad_norm": 14.546578407287598,
      "learning_rate": 1.2518931275896557e-05,
      "loss": 0.9112,
      "step": 26180
    },
    {
      "epoch": 5.612944706386626,
      "grad_norm": 0.5703146457672119,
      "learning_rate": 1.2516073724817833e-05,
      "loss": 0.3062,
      "step": 26190
    },
    {
      "epoch": 5.615087869695671,
      "grad_norm": 29.582582473754883,
      "learning_rate": 1.2513216173739107e-05,
      "loss": 0.4569,
      "step": 26200
    },
    {
      "epoch": 5.617231033004715,
      "grad_norm": 0.4809539020061493,
      "learning_rate": 1.251035862266038e-05,
      "loss": 0.7682,
      "step": 26210
    },
    {
      "epoch": 5.619374196313759,
      "grad_norm": 0.4964573383331299,
      "learning_rate": 1.2507501071581656e-05,
      "loss": 0.6131,
      "step": 26220
    },
    {
      "epoch": 5.621517359622803,
      "grad_norm": 14.606736183166504,
      "learning_rate": 1.250464352050293e-05,
      "loss": 0.6135,
      "step": 26230
    },
    {
      "epoch": 5.623660522931847,
      "grad_norm": 0.5288504362106323,
      "learning_rate": 1.2501785969424206e-05,
      "loss": 0.759,
      "step": 26240
    },
    {
      "epoch": 5.625803686240891,
      "grad_norm": 0.5154908299446106,
      "learning_rate": 1.249892841834548e-05,
      "loss": 0.4557,
      "step": 26250
    },
    {
      "epoch": 5.627946849549936,
      "grad_norm": 0.5141295194625854,
      "learning_rate": 1.2496070867266754e-05,
      "loss": 0.7595,
      "step": 26260
    },
    {
      "epoch": 5.63009001285898,
      "grad_norm": 14.534381866455078,
      "learning_rate": 1.2493213316188029e-05,
      "loss": 0.4489,
      "step": 26270
    },
    {
      "epoch": 5.6322331761680235,
      "grad_norm": 0.4982585310935974,
      "learning_rate": 1.2490355765109303e-05,
      "loss": 0.1588,
      "step": 26280
    },
    {
      "epoch": 5.634376339477068,
      "grad_norm": 14.839303016662598,
      "learning_rate": 1.2487498214030577e-05,
      "loss": 0.4769,
      "step": 26290
    },
    {
      "epoch": 5.636519502786112,
      "grad_norm": 0.39131468534469604,
      "learning_rate": 1.2484640662951851e-05,
      "loss": 0.3248,
      "step": 26300
    },
    {
      "epoch": 5.638662666095156,
      "grad_norm": 0.3866264522075653,
      "learning_rate": 1.2481783111873125e-05,
      "loss": 0.8112,
      "step": 26310
    },
    {
      "epoch": 5.640805829404201,
      "grad_norm": 14.88260269165039,
      "learning_rate": 1.2478925560794399e-05,
      "loss": 0.8039,
      "step": 26320
    },
    {
      "epoch": 5.642948992713245,
      "grad_norm": 14.85895824432373,
      "learning_rate": 1.2476068009715674e-05,
      "loss": 0.6304,
      "step": 26330
    },
    {
      "epoch": 5.645092156022288,
      "grad_norm": 0.45010942220687866,
      "learning_rate": 1.2473210458636948e-05,
      "loss": 0.3199,
      "step": 26340
    },
    {
      "epoch": 5.647235319331333,
      "grad_norm": 30.08271598815918,
      "learning_rate": 1.2470352907558222e-05,
      "loss": 0.7714,
      "step": 26350
    },
    {
      "epoch": 5.649378482640377,
      "grad_norm": 0.48876696825027466,
      "learning_rate": 1.2467495356479498e-05,
      "loss": 0.4632,
      "step": 26360
    },
    {
      "epoch": 5.651521645949421,
      "grad_norm": 0.4640749394893646,
      "learning_rate": 1.2464637805400772e-05,
      "loss": 0.3148,
      "step": 26370
    },
    {
      "epoch": 5.653664809258466,
      "grad_norm": 0.4516552686691284,
      "learning_rate": 1.2461780254322047e-05,
      "loss": 0.6236,
      "step": 26380
    },
    {
      "epoch": 5.6558079725675094,
      "grad_norm": 0.4453129470348358,
      "learning_rate": 1.2458922703243321e-05,
      "loss": 0.3177,
      "step": 26390
    },
    {
      "epoch": 5.657951135876553,
      "grad_norm": 0.4396013021469116,
      "learning_rate": 1.2456065152164595e-05,
      "loss": 0.6267,
      "step": 26400
    },
    {
      "epoch": 5.660094299185598,
      "grad_norm": 0.4454546570777893,
      "learning_rate": 1.2453207601085871e-05,
      "loss": 0.3185,
      "step": 26410
    },
    {
      "epoch": 5.662237462494642,
      "grad_norm": 14.661280632019043,
      "learning_rate": 1.2450350050007145e-05,
      "loss": 0.785,
      "step": 26420
    },
    {
      "epoch": 5.664380625803687,
      "grad_norm": 0.43655064702033997,
      "learning_rate": 1.2447492498928419e-05,
      "loss": 0.7882,
      "step": 26430
    },
    {
      "epoch": 5.6665237891127305,
      "grad_norm": 0.43826913833618164,
      "learning_rate": 1.2444634947849694e-05,
      "loss": 0.6254,
      "step": 26440
    },
    {
      "epoch": 5.668666952421774,
      "grad_norm": 0.4410347640514374,
      "learning_rate": 1.2441777396770968e-05,
      "loss": 0.4718,
      "step": 26450
    },
    {
      "epoch": 5.670810115730819,
      "grad_norm": 0.4272797107696533,
      "learning_rate": 1.2438919845692244e-05,
      "loss": 0.7799,
      "step": 26460
    },
    {
      "epoch": 5.672953279039863,
      "grad_norm": 0.4198262095451355,
      "learning_rate": 1.2436062294613518e-05,
      "loss": 0.3199,
      "step": 26470
    },
    {
      "epoch": 5.675096442348907,
      "grad_norm": 14.777410507202148,
      "learning_rate": 1.2433204743534792e-05,
      "loss": 1.1049,
      "step": 26480
    },
    {
      "epoch": 5.6772396056579515,
      "grad_norm": 0.43975210189819336,
      "learning_rate": 1.2430347192456067e-05,
      "loss": 0.6281,
      "step": 26490
    },
    {
      "epoch": 5.679382768966995,
      "grad_norm": 29.600683212280273,
      "learning_rate": 1.2427489641377341e-05,
      "loss": 1.6745,
      "step": 26500
    },
    {
      "epoch": 5.681525932276039,
      "grad_norm": 0.5797317624092102,
      "learning_rate": 1.2424632090298617e-05,
      "loss": 0.3054,
      "step": 26510
    },
    {
      "epoch": 5.683669095585084,
      "grad_norm": 0.606046736240387,
      "learning_rate": 1.242177453921989e-05,
      "loss": 0.591,
      "step": 26520
    },
    {
      "epoch": 5.685812258894128,
      "grad_norm": 0.6171602606773376,
      "learning_rate": 1.2418916988141163e-05,
      "loss": 1.3117,
      "step": 26530
    },
    {
      "epoch": 5.687955422203172,
      "grad_norm": 0.6596049666404724,
      "learning_rate": 1.2416059437062437e-05,
      "loss": 0.2969,
      "step": 26540
    },
    {
      "epoch": 5.690098585512216,
      "grad_norm": 0.6035428047180176,
      "learning_rate": 1.2413201885983713e-05,
      "loss": 0.5831,
      "step": 26550
    },
    {
      "epoch": 5.69224174882126,
      "grad_norm": 0.6036413311958313,
      "learning_rate": 1.2410344334904987e-05,
      "loss": 0.7373,
      "step": 26560
    },
    {
      "epoch": 5.694384912130304,
      "grad_norm": 0.5713716745376587,
      "learning_rate": 1.240748678382626e-05,
      "loss": 0.3017,
      "step": 26570
    },
    {
      "epoch": 5.696528075439349,
      "grad_norm": 14.552326202392578,
      "learning_rate": 1.2404629232747536e-05,
      "loss": 1.0383,
      "step": 26580
    },
    {
      "epoch": 5.698671238748393,
      "grad_norm": 0.5875301361083984,
      "learning_rate": 1.240177168166881e-05,
      "loss": 0.8773,
      "step": 26590
    },
    {
      "epoch": 5.7008144020574365,
      "grad_norm": 0.6264454126358032,
      "learning_rate": 1.2398914130590086e-05,
      "loss": 0.5869,
      "step": 26600
    },
    {
      "epoch": 5.702957565366481,
      "grad_norm": 0.6414563059806824,
      "learning_rate": 1.239605657951136e-05,
      "loss": 0.5804,
      "step": 26610
    },
    {
      "epoch": 5.705100728675525,
      "grad_norm": 0.6302881240844727,
      "learning_rate": 1.2393199028432634e-05,
      "loss": 0.7204,
      "step": 26620
    },
    {
      "epoch": 5.707243891984569,
      "grad_norm": 0.6257584691047668,
      "learning_rate": 1.239034147735391e-05,
      "loss": 0.5797,
      "step": 26630
    },
    {
      "epoch": 5.709387055293614,
      "grad_norm": 0.6807657480239868,
      "learning_rate": 1.2387483926275183e-05,
      "loss": 0.8699,
      "step": 26640
    },
    {
      "epoch": 5.7115302186026575,
      "grad_norm": 0.6548588275909424,
      "learning_rate": 1.2384626375196459e-05,
      "loss": 0.2976,
      "step": 26650
    },
    {
      "epoch": 5.713673381911701,
      "grad_norm": 14.628645896911621,
      "learning_rate": 1.2381768824117733e-05,
      "loss": 0.738,
      "step": 26660
    },
    {
      "epoch": 5.715816545220746,
      "grad_norm": 0.5617290139198303,
      "learning_rate": 1.2378911273039007e-05,
      "loss": 0.4464,
      "step": 26670
    },
    {
      "epoch": 5.71795970852979,
      "grad_norm": 0.5676741600036621,
      "learning_rate": 1.2376053721960282e-05,
      "loss": 0.5936,
      "step": 26680
    },
    {
      "epoch": 5.720102871838834,
      "grad_norm": 0.5860687494277954,
      "learning_rate": 1.2373196170881556e-05,
      "loss": 0.8828,
      "step": 26690
    },
    {
      "epoch": 5.7222460351478786,
      "grad_norm": 0.6085996627807617,
      "learning_rate": 1.237033861980283e-05,
      "loss": 0.4438,
      "step": 26700
    },
    {
      "epoch": 5.724389198456922,
      "grad_norm": 0.5906581282615662,
      "learning_rate": 1.2367481068724106e-05,
      "loss": 0.4505,
      "step": 26710
    },
    {
      "epoch": 5.726532361765966,
      "grad_norm": 0.5766088366508484,
      "learning_rate": 1.236462351764538e-05,
      "loss": 0.4531,
      "step": 26720
    },
    {
      "epoch": 5.728675525075011,
      "grad_norm": 0.5422322154045105,
      "learning_rate": 1.2361765966566652e-05,
      "loss": 0.4511,
      "step": 26730
    },
    {
      "epoch": 5.730818688384055,
      "grad_norm": 14.579263687133789,
      "learning_rate": 1.2358908415487927e-05,
      "loss": 0.3069,
      "step": 26740
    },
    {
      "epoch": 5.732961851693099,
      "grad_norm": 0.4785347878932953,
      "learning_rate": 1.2356050864409201e-05,
      "loss": 0.1629,
      "step": 26750
    },
    {
      "epoch": 5.735105015002143,
      "grad_norm": 0.48815566301345825,
      "learning_rate": 1.2353193313330475e-05,
      "loss": 1.3764,
      "step": 26760
    },
    {
      "epoch": 5.737248178311187,
      "grad_norm": 0.5424847602844238,
      "learning_rate": 1.2350335762251751e-05,
      "loss": 0.9056,
      "step": 26770
    },
    {
      "epoch": 5.739391341620231,
      "grad_norm": 0.5633410215377808,
      "learning_rate": 1.2347478211173025e-05,
      "loss": 1.0354,
      "step": 26780
    },
    {
      "epoch": 5.741534504929276,
      "grad_norm": 0.5922960638999939,
      "learning_rate": 1.23446206600943e-05,
      "loss": 0.299,
      "step": 26790
    },
    {
      "epoch": 5.74367766823832,
      "grad_norm": 0.58766108751297,
      "learning_rate": 1.2341763109015574e-05,
      "loss": 0.3018,
      "step": 26800
    },
    {
      "epoch": 5.745820831547364,
      "grad_norm": 0.49300771951675415,
      "learning_rate": 1.2338905557936848e-05,
      "loss": 0.1613,
      "step": 26810
    },
    {
      "epoch": 5.747963994856408,
      "grad_norm": 14.643014907836914,
      "learning_rate": 1.2336048006858124e-05,
      "loss": 0.4698,
      "step": 26820
    },
    {
      "epoch": 5.750107158165452,
      "grad_norm": 0.4668891727924347,
      "learning_rate": 1.2333190455779398e-05,
      "loss": 0.9341,
      "step": 26830
    },
    {
      "epoch": 5.752250321474496,
      "grad_norm": 0.47464311122894287,
      "learning_rate": 1.2330332904700672e-05,
      "loss": 0.6177,
      "step": 26840
    },
    {
      "epoch": 5.754393484783541,
      "grad_norm": 0.4732772409915924,
      "learning_rate": 1.2327475353621947e-05,
      "loss": 0.464,
      "step": 26850
    },
    {
      "epoch": 5.756536648092585,
      "grad_norm": 14.734856605529785,
      "learning_rate": 1.2324617802543221e-05,
      "loss": 0.7749,
      "step": 26860
    },
    {
      "epoch": 5.758679811401628,
      "grad_norm": 14.622788429260254,
      "learning_rate": 1.2321760251464497e-05,
      "loss": 0.4676,
      "step": 26870
    },
    {
      "epoch": 5.760822974710673,
      "grad_norm": 0.43454641103744507,
      "learning_rate": 1.2318902700385771e-05,
      "loss": 0.6279,
      "step": 26880
    },
    {
      "epoch": 5.762966138019717,
      "grad_norm": 0.4498068690299988,
      "learning_rate": 1.2316045149307045e-05,
      "loss": 0.4699,
      "step": 26890
    },
    {
      "epoch": 5.765109301328761,
      "grad_norm": 0.4365626871585846,
      "learning_rate": 1.231318759822832e-05,
      "loss": 0.4768,
      "step": 26900
    },
    {
      "epoch": 5.767252464637806,
      "grad_norm": 0.44926533102989197,
      "learning_rate": 1.2310330047149594e-05,
      "loss": 0.7817,
      "step": 26910
    },
    {
      "epoch": 5.7693956279468495,
      "grad_norm": 0.4936596155166626,
      "learning_rate": 1.2307472496070868e-05,
      "loss": 0.9299,
      "step": 26920
    },
    {
      "epoch": 5.771538791255894,
      "grad_norm": 14.60188102722168,
      "learning_rate": 1.2304614944992144e-05,
      "loss": 0.6131,
      "step": 26930
    },
    {
      "epoch": 5.773681954564938,
      "grad_norm": 14.557962417602539,
      "learning_rate": 1.2301757393913418e-05,
      "loss": 0.9047,
      "step": 26940
    },
    {
      "epoch": 5.775825117873982,
      "grad_norm": 14.488445281982422,
      "learning_rate": 1.229889984283469e-05,
      "loss": 0.5942,
      "step": 26950
    },
    {
      "epoch": 5.777968281183027,
      "grad_norm": 0.5757414698600769,
      "learning_rate": 1.2296042291755966e-05,
      "loss": 0.4481,
      "step": 26960
    },
    {
      "epoch": 5.7801114444920705,
      "grad_norm": 0.6056480407714844,
      "learning_rate": 1.229318474067724e-05,
      "loss": 0.739,
      "step": 26970
    },
    {
      "epoch": 5.782254607801114,
      "grad_norm": 14.53582763671875,
      "learning_rate": 1.2290327189598514e-05,
      "loss": 0.8913,
      "step": 26980
    },
    {
      "epoch": 5.784397771110159,
      "grad_norm": 0.5583388209342957,
      "learning_rate": 1.228746963851979e-05,
      "loss": 0.4532,
      "step": 26990
    },
    {
      "epoch": 5.786540934419203,
      "grad_norm": 0.518444299697876,
      "learning_rate": 1.2284612087441063e-05,
      "loss": 0.3051,
      "step": 27000
    },
    {
      "epoch": 5.788684097728247,
      "grad_norm": 0.4804505705833435,
      "learning_rate": 1.2281754536362339e-05,
      "loss": 0.4635,
      "step": 27010
    },
    {
      "epoch": 5.7908272610372915,
      "grad_norm": 0.4707428216934204,
      "learning_rate": 1.2278896985283613e-05,
      "loss": 0.3121,
      "step": 27020
    },
    {
      "epoch": 5.792970424346335,
      "grad_norm": 0.40135326981544495,
      "learning_rate": 1.2276039434204887e-05,
      "loss": 0.1623,
      "step": 27030
    },
    {
      "epoch": 5.795113587655379,
      "grad_norm": 0.3578055202960968,
      "learning_rate": 1.2273181883126162e-05,
      "loss": 0.486,
      "step": 27040
    },
    {
      "epoch": 5.797256750964424,
      "grad_norm": 0.3565080463886261,
      "learning_rate": 1.2270324332047436e-05,
      "loss": 1.4716,
      "step": 27050
    },
    {
      "epoch": 5.799399914273468,
      "grad_norm": 0.36421453952789307,
      "learning_rate": 1.226746678096871e-05,
      "loss": 0.3283,
      "step": 27060
    },
    {
      "epoch": 5.801543077582512,
      "grad_norm": 30.287933349609375,
      "learning_rate": 1.2264609229889986e-05,
      "loss": 1.2832,
      "step": 27070
    },
    {
      "epoch": 5.803686240891556,
      "grad_norm": 29.671459197998047,
      "learning_rate": 1.226175167881126e-05,
      "loss": 0.7871,
      "step": 27080
    },
    {
      "epoch": 5.8058294042006,
      "grad_norm": 0.468677282333374,
      "learning_rate": 1.2258894127732535e-05,
      "loss": 0.472,
      "step": 27090
    },
    {
      "epoch": 5.807972567509644,
      "grad_norm": 14.63037109375,
      "learning_rate": 1.225603657665381e-05,
      "loss": 0.3165,
      "step": 27100
    },
    {
      "epoch": 5.810115730818689,
      "grad_norm": 0.3969424366950989,
      "learning_rate": 1.2253179025575083e-05,
      "loss": 0.1658,
      "step": 27110
    },
    {
      "epoch": 5.812258894127733,
      "grad_norm": 14.68836498260498,
      "learning_rate": 1.2250321474496359e-05,
      "loss": 0.9641,
      "step": 27120
    },
    {
      "epoch": 5.8144020574367765,
      "grad_norm": 0.4707143008708954,
      "learning_rate": 1.2247463923417633e-05,
      "loss": 1.1014,
      "step": 27130
    },
    {
      "epoch": 5.816545220745821,
      "grad_norm": 29.54034423828125,
      "learning_rate": 1.2244606372338907e-05,
      "loss": 1.0531,
      "step": 27140
    },
    {
      "epoch": 5.818688384054865,
      "grad_norm": 14.502754211425781,
      "learning_rate": 1.2241748821260182e-05,
      "loss": 0.4474,
      "step": 27150
    },
    {
      "epoch": 5.820831547363909,
      "grad_norm": 14.491482734680176,
      "learning_rate": 1.2238891270181454e-05,
      "loss": 0.5918,
      "step": 27160
    },
    {
      "epoch": 5.822974710672954,
      "grad_norm": 29.5291748046875,
      "learning_rate": 1.2236033719102728e-05,
      "loss": 1.0218,
      "step": 27170
    },
    {
      "epoch": 5.8251178739819975,
      "grad_norm": 0.5957745909690857,
      "learning_rate": 1.2233176168024004e-05,
      "loss": 0.4456,
      "step": 27180
    },
    {
      "epoch": 5.827261037291041,
      "grad_norm": 0.5940566062927246,
      "learning_rate": 1.2230318616945278e-05,
      "loss": 0.8791,
      "step": 27190
    },
    {
      "epoch": 5.829404200600086,
      "grad_norm": 0.5912384390830994,
      "learning_rate": 1.2227461065866552e-05,
      "loss": 0.1564,
      "step": 27200
    },
    {
      "epoch": 5.83154736390913,
      "grad_norm": 0.5453607439994812,
      "learning_rate": 1.2224603514787828e-05,
      "loss": 0.4492,
      "step": 27210
    },
    {
      "epoch": 5.833690527218174,
      "grad_norm": 0.5001440048217773,
      "learning_rate": 1.2221745963709101e-05,
      "loss": 0.3075,
      "step": 27220
    },
    {
      "epoch": 5.835833690527219,
      "grad_norm": 29.585905075073242,
      "learning_rate": 1.2218888412630377e-05,
      "loss": 0.7622,
      "step": 27230
    },
    {
      "epoch": 5.837976853836262,
      "grad_norm": 0.4760344922542572,
      "learning_rate": 1.2216030861551651e-05,
      "loss": 0.4648,
      "step": 27240
    },
    {
      "epoch": 5.840120017145306,
      "grad_norm": 0.4446386396884918,
      "learning_rate": 1.2213173310472925e-05,
      "loss": 0.4711,
      "step": 27250
    },
    {
      "epoch": 5.842263180454351,
      "grad_norm": 14.628013610839844,
      "learning_rate": 1.22103157593942e-05,
      "loss": 0.6238,
      "step": 27260
    },
    {
      "epoch": 5.844406343763395,
      "grad_norm": 14.69684886932373,
      "learning_rate": 1.2207458208315474e-05,
      "loss": 0.6227,
      "step": 27270
    },
    {
      "epoch": 5.846549507072439,
      "grad_norm": 14.643019676208496,
      "learning_rate": 1.2204600657236748e-05,
      "loss": 0.6195,
      "step": 27280
    },
    {
      "epoch": 5.848692670381483,
      "grad_norm": 0.5006216168403625,
      "learning_rate": 1.2201743106158024e-05,
      "loss": 0.4656,
      "step": 27290
    },
    {
      "epoch": 5.850835833690527,
      "grad_norm": 29.628623962402344,
      "learning_rate": 1.2198885555079298e-05,
      "loss": 0.9064,
      "step": 27300
    },
    {
      "epoch": 5.852978996999571,
      "grad_norm": 0.493667334318161,
      "learning_rate": 1.2196028004000574e-05,
      "loss": 0.0114,
      "step": 27310
    },
    {
      "epoch": 5.855122160308616,
      "grad_norm": 14.661398887634277,
      "learning_rate": 1.2193170452921848e-05,
      "loss": 0.7651,
      "step": 27320
    },
    {
      "epoch": 5.85726532361766,
      "grad_norm": 0.4471038281917572,
      "learning_rate": 1.2190312901843121e-05,
      "loss": 0.1621,
      "step": 27330
    },
    {
      "epoch": 5.859408486926704,
      "grad_norm": 0.4114036560058594,
      "learning_rate": 1.2187455350764397e-05,
      "loss": 0.3202,
      "step": 27340
    },
    {
      "epoch": 5.861551650235748,
      "grad_norm": 14.746650695800781,
      "learning_rate": 1.2184597799685671e-05,
      "loss": 0.3215,
      "step": 27350
    },
    {
      "epoch": 5.863694813544792,
      "grad_norm": 0.3871256709098816,
      "learning_rate": 1.2181740248606947e-05,
      "loss": 0.4845,
      "step": 27360
    },
    {
      "epoch": 5.865837976853836,
      "grad_norm": 0.37118902802467346,
      "learning_rate": 1.217888269752822e-05,
      "loss": 0.4888,
      "step": 27370
    },
    {
      "epoch": 5.867981140162881,
      "grad_norm": 0.40725216269493103,
      "learning_rate": 1.2176025146449493e-05,
      "loss": 1.4466,
      "step": 27380
    },
    {
      "epoch": 5.870124303471925,
      "grad_norm": 0.45439982414245605,
      "learning_rate": 1.2173167595370767e-05,
      "loss": 1.255,
      "step": 27390
    },
    {
      "epoch": 5.8722674667809684,
      "grad_norm": 0.43939897418022156,
      "learning_rate": 1.2170310044292042e-05,
      "loss": 0.1659,
      "step": 27400
    },
    {
      "epoch": 5.874410630090013,
      "grad_norm": 14.728194236755371,
      "learning_rate": 1.2167452493213316e-05,
      "loss": 0.4868,
      "step": 27410
    },
    {
      "epoch": 5.876553793399057,
      "grad_norm": 14.716903686523438,
      "learning_rate": 1.216459494213459e-05,
      "loss": 0.4895,
      "step": 27420
    },
    {
      "epoch": 5.878696956708101,
      "grad_norm": 0.4081879258155823,
      "learning_rate": 1.2161737391055866e-05,
      "loss": 1.1249,
      "step": 27430
    },
    {
      "epoch": 5.880840120017146,
      "grad_norm": 0.4360925257205963,
      "learning_rate": 1.215887983997714e-05,
      "loss": 0.6294,
      "step": 27440
    },
    {
      "epoch": 5.8829832833261895,
      "grad_norm": 0.4510882794857025,
      "learning_rate": 1.2156022288898415e-05,
      "loss": 0.3191,
      "step": 27450
    },
    {
      "epoch": 5.885126446635233,
      "grad_norm": 14.755457878112793,
      "learning_rate": 1.215316473781969e-05,
      "loss": 0.4768,
      "step": 27460
    },
    {
      "epoch": 5.887269609944278,
      "grad_norm": 0.4576787054538727,
      "learning_rate": 1.2150307186740963e-05,
      "loss": 0.9442,
      "step": 27470
    },
    {
      "epoch": 5.889412773253322,
      "grad_norm": 0.4683713912963867,
      "learning_rate": 1.2147449635662239e-05,
      "loss": 0.4672,
      "step": 27480
    },
    {
      "epoch": 5.891555936562366,
      "grad_norm": 0.4875432252883911,
      "learning_rate": 1.2144592084583513e-05,
      "loss": 0.617,
      "step": 27490
    },
    {
      "epoch": 5.8936990998714105,
      "grad_norm": 14.688761711120605,
      "learning_rate": 1.2141734533504788e-05,
      "loss": 0.9074,
      "step": 27500
    },
    {
      "epoch": 5.895842263180454,
      "grad_norm": 14.548036575317383,
      "learning_rate": 1.2138876982426062e-05,
      "loss": 0.4508,
      "step": 27510
    },
    {
      "epoch": 5.897985426489498,
      "grad_norm": 16.702417373657227,
      "learning_rate": 1.2136019431347336e-05,
      "loss": 0.5953,
      "step": 27520
    },
    {
      "epoch": 5.900128589798543,
      "grad_norm": 14.464136123657227,
      "learning_rate": 1.2133161880268612e-05,
      "loss": 0.5892,
      "step": 27530
    },
    {
      "epoch": 5.902271753107587,
      "grad_norm": 14.439109802246094,
      "learning_rate": 1.2130304329189886e-05,
      "loss": 1.1552,
      "step": 27540
    },
    {
      "epoch": 5.904414916416631,
      "grad_norm": 0.6674479246139526,
      "learning_rate": 1.212744677811116e-05,
      "loss": 0.2963,
      "step": 27550
    },
    {
      "epoch": 5.906558079725675,
      "grad_norm": 0.6272037029266357,
      "learning_rate": 1.2124589227032435e-05,
      "loss": 0.7198,
      "step": 27560
    },
    {
      "epoch": 5.908701243034719,
      "grad_norm": 29.858083724975586,
      "learning_rate": 1.212173167595371e-05,
      "loss": 0.5918,
      "step": 27570
    },
    {
      "epoch": 5.910844406343763,
      "grad_norm": 0.5613211989402771,
      "learning_rate": 1.2118874124874985e-05,
      "loss": 0.5945,
      "step": 27580
    },
    {
      "epoch": 5.912987569652808,
      "grad_norm": 14.515390396118164,
      "learning_rate": 1.2116016573796257e-05,
      "loss": 1.0346,
      "step": 27590
    },
    {
      "epoch": 5.915130732961852,
      "grad_norm": 0.5502974987030029,
      "learning_rate": 1.2113159022717531e-05,
      "loss": 0.3051,
      "step": 27600
    },
    {
      "epoch": 5.9172738962708955,
      "grad_norm": 0.5093651413917542,
      "learning_rate": 1.2110301471638805e-05,
      "loss": 0.1582,
      "step": 27610
    },
    {
      "epoch": 5.91941705957994,
      "grad_norm": 14.787816047668457,
      "learning_rate": 1.210744392056008e-05,
      "loss": 0.9095,
      "step": 27620
    },
    {
      "epoch": 5.921560222888984,
      "grad_norm": 14.591805458068848,
      "learning_rate": 1.2104586369481355e-05,
      "loss": 0.7649,
      "step": 27630
    },
    {
      "epoch": 5.923703386198028,
      "grad_norm": 14.561321258544922,
      "learning_rate": 1.210172881840263e-05,
      "loss": 1.056,
      "step": 27640
    },
    {
      "epoch": 5.925846549507073,
      "grad_norm": 0.5705229043960571,
      "learning_rate": 1.2098871267323904e-05,
      "loss": 0.7471,
      "step": 27650
    },
    {
      "epoch": 5.9279897128161165,
      "grad_norm": 0.6009975671768188,
      "learning_rate": 1.2096013716245178e-05,
      "loss": 0.5936,
      "step": 27660
    },
    {
      "epoch": 5.93013287612516,
      "grad_norm": 14.443527221679688,
      "learning_rate": 1.2093156165166454e-05,
      "loss": 0.8662,
      "step": 27670
    },
    {
      "epoch": 5.932276039434205,
      "grad_norm": 0.7107172608375549,
      "learning_rate": 1.2090298614087728e-05,
      "loss": 0.5687,
      "step": 27680
    },
    {
      "epoch": 5.934419202743249,
      "grad_norm": 0.7071649432182312,
      "learning_rate": 1.2087441063009001e-05,
      "loss": 0.4313,
      "step": 27690
    },
    {
      "epoch": 5.936562366052293,
      "grad_norm": 14.56076717376709,
      "learning_rate": 1.2084583511930277e-05,
      "loss": 0.1638,
      "step": 27700
    },
    {
      "epoch": 5.9387055293613376,
      "grad_norm": 0.46870678663253784,
      "learning_rate": 1.2081725960851551e-05,
      "loss": 0.4632,
      "step": 27710
    },
    {
      "epoch": 5.940848692670381,
      "grad_norm": 0.40706348419189453,
      "learning_rate": 1.2078868409772827e-05,
      "loss": 0.1648,
      "step": 27720
    },
    {
      "epoch": 5.942991855979425,
      "grad_norm": 14.698853492736816,
      "learning_rate": 1.20760108586941e-05,
      "loss": 0.482,
      "step": 27730
    },
    {
      "epoch": 5.94513501928847,
      "grad_norm": 0.3741777539253235,
      "learning_rate": 1.2073153307615375e-05,
      "loss": 0.1662,
      "step": 27740
    },
    {
      "epoch": 5.947278182597514,
      "grad_norm": 0.3549785912036896,
      "learning_rate": 1.207029575653665e-05,
      "loss": 0.4892,
      "step": 27750
    },
    {
      "epoch": 5.949421345906558,
      "grad_norm": 14.716378211975098,
      "learning_rate": 1.2067438205457924e-05,
      "loss": 0.9735,
      "step": 27760
    },
    {
      "epoch": 5.951564509215602,
      "grad_norm": 15.301836967468262,
      "learning_rate": 1.2064580654379198e-05,
      "loss": 0.9475,
      "step": 27770
    },
    {
      "epoch": 5.953707672524646,
      "grad_norm": 0.4938230812549591,
      "learning_rate": 1.2061723103300474e-05,
      "loss": 0.9254,
      "step": 27780
    },
    {
      "epoch": 5.95585083583369,
      "grad_norm": 0.507685661315918,
      "learning_rate": 1.2058865552221748e-05,
      "loss": 0.3109,
      "step": 27790
    },
    {
      "epoch": 5.957993999142735,
      "grad_norm": 0.49889951944351196,
      "learning_rate": 1.2056008001143023e-05,
      "loss": 0.4612,
      "step": 27800
    },
    {
      "epoch": 5.960137162451779,
      "grad_norm": 0.5000470876693726,
      "learning_rate": 1.2053150450064295e-05,
      "loss": 0.4632,
      "step": 27810
    },
    {
      "epoch": 5.962280325760823,
      "grad_norm": 0.4906487464904785,
      "learning_rate": 1.205029289898557e-05,
      "loss": 0.9104,
      "step": 27820
    },
    {
      "epoch": 5.964423489069867,
      "grad_norm": 14.566473007202148,
      "learning_rate": 1.2047435347906843e-05,
      "loss": 0.7634,
      "step": 27830
    },
    {
      "epoch": 5.966566652378911,
      "grad_norm": 14.508709907531738,
      "learning_rate": 1.2044577796828119e-05,
      "loss": 1.0447,
      "step": 27840
    },
    {
      "epoch": 5.968709815687955,
      "grad_norm": 0.593413233757019,
      "learning_rate": 1.2041720245749393e-05,
      "loss": 0.5931,
      "step": 27850
    },
    {
      "epoch": 5.970852978997,
      "grad_norm": 14.547304153442383,
      "learning_rate": 1.2038862694670668e-05,
      "loss": 0.8779,
      "step": 27860
    },
    {
      "epoch": 5.972996142306044,
      "grad_norm": 0.6471556425094604,
      "learning_rate": 1.2036005143591942e-05,
      "loss": 0.7285,
      "step": 27870
    },
    {
      "epoch": 5.975139305615087,
      "grad_norm": 0.6401844024658203,
      "learning_rate": 1.2033147592513216e-05,
      "loss": 0.5784,
      "step": 27880
    },
    {
      "epoch": 5.977282468924132,
      "grad_norm": 0.6312958002090454,
      "learning_rate": 1.2030290041434492e-05,
      "loss": 0.5802,
      "step": 27890
    },
    {
      "epoch": 5.979425632233176,
      "grad_norm": 29.408926010131836,
      "learning_rate": 1.2027432490355766e-05,
      "loss": 1.0045,
      "step": 27900
    },
    {
      "epoch": 5.98156879554222,
      "grad_norm": 14.400428771972656,
      "learning_rate": 1.202457493927704e-05,
      "loss": 0.4388,
      "step": 27910
    },
    {
      "epoch": 5.983711958851265,
      "grad_norm": 0.6414444446563721,
      "learning_rate": 1.2021717388198315e-05,
      "loss": 0.2961,
      "step": 27920
    },
    {
      "epoch": 5.9858551221603085,
      "grad_norm": 0.5890516638755798,
      "learning_rate": 1.201885983711959e-05,
      "loss": 0.4435,
      "step": 27930
    },
    {
      "epoch": 5.987998285469352,
      "grad_norm": 0.5804857015609741,
      "learning_rate": 1.2016002286040865e-05,
      "loss": 0.4472,
      "step": 27940
    },
    {
      "epoch": 5.990141448778397,
      "grad_norm": 14.559932708740234,
      "learning_rate": 1.2013144734962139e-05,
      "loss": 0.5987,
      "step": 27950
    },
    {
      "epoch": 5.992284612087441,
      "grad_norm": 14.518494606018066,
      "learning_rate": 1.2010287183883413e-05,
      "loss": 0.7451,
      "step": 27960
    },
    {
      "epoch": 5.994427775396485,
      "grad_norm": 0.5700663924217224,
      "learning_rate": 1.2007429632804688e-05,
      "loss": 0.7416,
      "step": 27970
    },
    {
      "epoch": 5.9965709387055295,
      "grad_norm": 0.551138699054718,
      "learning_rate": 1.2004572081725962e-05,
      "loss": 0.304,
      "step": 27980
    },
    {
      "epoch": 5.998714102014573,
      "grad_norm": 0.52901291847229,
      "learning_rate": 1.2001714530647236e-05,
      "loss": 0.7506,
      "step": 27990
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.8676666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.5986974835395813,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 396.4245,
      "eval_samples_per_second": 7.568,
      "eval_steps_per_second": 2.523,
      "step": 27996
    },
    {
      "epoch": 6.000857265323618,
      "grad_norm": 0.534010112285614,
      "learning_rate": 1.1998856979568512e-05,
      "loss": 0.46,
      "step": 28000
    },
    {
      "epoch": 6.003000428632662,
      "grad_norm": 14.713583946228027,
      "learning_rate": 1.1995999428489786e-05,
      "loss": 0.7549,
      "step": 28010
    },
    {
      "epoch": 6.005143591941706,
      "grad_norm": 0.5528984665870667,
      "learning_rate": 1.1993141877411058e-05,
      "loss": 0.5984,
      "step": 28020
    },
    {
      "epoch": 6.0072867552507505,
      "grad_norm": 0.5192219614982605,
      "learning_rate": 1.1990284326332334e-05,
      "loss": 0.3106,
      "step": 28030
    },
    {
      "epoch": 6.009429918559794,
      "grad_norm": 0.49031829833984375,
      "learning_rate": 1.1987426775253608e-05,
      "loss": 0.6126,
      "step": 28040
    },
    {
      "epoch": 6.011573081868838,
      "grad_norm": 0.4996908903121948,
      "learning_rate": 1.1984569224174882e-05,
      "loss": 0.6115,
      "step": 28050
    },
    {
      "epoch": 6.013716245177883,
      "grad_norm": 0.5057535767555237,
      "learning_rate": 1.1981711673096157e-05,
      "loss": 1.0596,
      "step": 28060
    },
    {
      "epoch": 6.015859408486927,
      "grad_norm": 0.5442475080490112,
      "learning_rate": 1.1978854122017431e-05,
      "loss": 0.7549,
      "step": 28070
    },
    {
      "epoch": 6.018002571795971,
      "grad_norm": 14.460372924804688,
      "learning_rate": 1.1975996570938707e-05,
      "loss": 1.0307,
      "step": 28080
    },
    {
      "epoch": 6.020145735105015,
      "grad_norm": 0.6343898177146912,
      "learning_rate": 1.197313901985998e-05,
      "loss": 0.7266,
      "step": 28090
    },
    {
      "epoch": 6.022288898414059,
      "grad_norm": 14.502180099487305,
      "learning_rate": 1.1970281468781255e-05,
      "loss": 0.7182,
      "step": 28100
    },
    {
      "epoch": 6.024432061723103,
      "grad_norm": 0.6871252655982971,
      "learning_rate": 1.196742391770253e-05,
      "loss": 0.7095,
      "step": 28110
    },
    {
      "epoch": 6.026575225032148,
      "grad_norm": 15.065969467163086,
      "learning_rate": 1.1964566366623804e-05,
      "loss": 0.7086,
      "step": 28120
    },
    {
      "epoch": 6.028718388341192,
      "grad_norm": 0.7086673974990845,
      "learning_rate": 1.1961708815545078e-05,
      "loss": 0.4271,
      "step": 28130
    },
    {
      "epoch": 6.0308615516502355,
      "grad_norm": 29.48760414123535,
      "learning_rate": 1.1958851264466354e-05,
      "loss": 0.5738,
      "step": 28140
    },
    {
      "epoch": 6.03300471495928,
      "grad_norm": 14.5266695022583,
      "learning_rate": 1.1955993713387628e-05,
      "loss": 1.1516,
      "step": 28150
    },
    {
      "epoch": 6.035147878268324,
      "grad_norm": 14.395316123962402,
      "learning_rate": 1.1953136162308903e-05,
      "loss": 1.2824,
      "step": 28160
    },
    {
      "epoch": 6.037291041577368,
      "grad_norm": 0.7771247625350952,
      "learning_rate": 1.1950278611230177e-05,
      "loss": 0.5624,
      "step": 28170
    },
    {
      "epoch": 6.039434204886413,
      "grad_norm": 0.7115954756736755,
      "learning_rate": 1.1947421060151451e-05,
      "loss": 0.5584,
      "step": 28180
    },
    {
      "epoch": 6.0415773681954565,
      "grad_norm": 0.7312622666358948,
      "learning_rate": 1.1944563509072727e-05,
      "loss": 0.8351,
      "step": 28190
    },
    {
      "epoch": 6.0437205315045,
      "grad_norm": 0.7311998605728149,
      "learning_rate": 1.1941705957994e-05,
      "loss": 0.695,
      "step": 28200
    },
    {
      "epoch": 6.045863694813545,
      "grad_norm": 0.7608798146247864,
      "learning_rate": 1.1938848406915276e-05,
      "loss": 1.1058,
      "step": 28210
    },
    {
      "epoch": 6.048006858122589,
      "grad_norm": 14.343578338623047,
      "learning_rate": 1.193599085583655e-05,
      "loss": 0.5627,
      "step": 28220
    },
    {
      "epoch": 6.050150021431633,
      "grad_norm": 29.360660552978516,
      "learning_rate": 1.1933133304757824e-05,
      "loss": 1.2476,
      "step": 28230
    },
    {
      "epoch": 6.052293184740678,
      "grad_norm": 0.8260287046432495,
      "learning_rate": 1.1930275753679096e-05,
      "loss": 0.6915,
      "step": 28240
    },
    {
      "epoch": 6.054436348049721,
      "grad_norm": 0.7656453847885132,
      "learning_rate": 1.1927418202600372e-05,
      "loss": 0.4192,
      "step": 28250
    },
    {
      "epoch": 6.056579511358765,
      "grad_norm": 0.6843874454498291,
      "learning_rate": 1.1924560651521646e-05,
      "loss": 0.2887,
      "step": 28260
    },
    {
      "epoch": 6.05872267466781,
      "grad_norm": 0.6160652041435242,
      "learning_rate": 1.192170310044292e-05,
      "loss": 0.58,
      "step": 28270
    },
    {
      "epoch": 6.060865837976854,
      "grad_norm": 0.537468671798706,
      "learning_rate": 1.1918845549364195e-05,
      "loss": 0.3049,
      "step": 28280
    },
    {
      "epoch": 6.063009001285898,
      "grad_norm": 0.5069028735160828,
      "learning_rate": 1.191598799828547e-05,
      "loss": 0.4562,
      "step": 28290
    },
    {
      "epoch": 6.065152164594942,
      "grad_norm": 14.556309700012207,
      "learning_rate": 1.1913130447206745e-05,
      "loss": 1.1988,
      "step": 28300
    },
    {
      "epoch": 6.067295327903986,
      "grad_norm": 0.546557605266571,
      "learning_rate": 1.1910272896128019e-05,
      "loss": 0.5985,
      "step": 28310
    },
    {
      "epoch": 6.06943849121303,
      "grad_norm": 0.5802056193351746,
      "learning_rate": 1.1907415345049293e-05,
      "loss": 0.8894,
      "step": 28320
    },
    {
      "epoch": 6.071581654522075,
      "grad_norm": 0.5689716339111328,
      "learning_rate": 1.1904557793970568e-05,
      "loss": 0.0132,
      "step": 28330
    },
    {
      "epoch": 6.073724817831119,
      "grad_norm": 14.561065673828125,
      "learning_rate": 1.1901700242891842e-05,
      "loss": 0.7515,
      "step": 28340
    },
    {
      "epoch": 6.075867981140163,
      "grad_norm": 0.5142576098442078,
      "learning_rate": 1.1898842691813118e-05,
      "loss": 0.4556,
      "step": 28350
    },
    {
      "epoch": 6.078011144449207,
      "grad_norm": 14.559354782104492,
      "learning_rate": 1.1895985140734392e-05,
      "loss": 0.4582,
      "step": 28360
    },
    {
      "epoch": 6.080154307758251,
      "grad_norm": 0.505041241645813,
      "learning_rate": 1.1893127589655666e-05,
      "loss": 0.7569,
      "step": 28370
    },
    {
      "epoch": 6.082297471067295,
      "grad_norm": 14.7522611618042,
      "learning_rate": 1.1890270038576942e-05,
      "loss": 0.6312,
      "step": 28380
    },
    {
      "epoch": 6.08444063437634,
      "grad_norm": 15.28828239440918,
      "learning_rate": 1.1887412487498215e-05,
      "loss": 0.7781,
      "step": 28390
    },
    {
      "epoch": 6.086583797685384,
      "grad_norm": 0.538673460483551,
      "learning_rate": 1.188455493641949e-05,
      "loss": 1.0604,
      "step": 28400
    },
    {
      "epoch": 6.0887269609944275,
      "grad_norm": 0.5466799139976501,
      "learning_rate": 1.1881697385340765e-05,
      "loss": 0.1593,
      "step": 28410
    },
    {
      "epoch": 6.090870124303472,
      "grad_norm": 0.5128980278968811,
      "learning_rate": 1.1878839834262039e-05,
      "loss": 0.6048,
      "step": 28420
    },
    {
      "epoch": 6.093013287612516,
      "grad_norm": 14.61377239227295,
      "learning_rate": 1.1875982283183315e-05,
      "loss": 0.3144,
      "step": 28430
    },
    {
      "epoch": 6.09515645092156,
      "grad_norm": 0.45811665058135986,
      "learning_rate": 1.1873124732104588e-05,
      "loss": 0.3154,
      "step": 28440
    },
    {
      "epoch": 6.097299614230605,
      "grad_norm": 0.4466414749622345,
      "learning_rate": 1.187026718102586e-05,
      "loss": 0.6168,
      "step": 28450
    },
    {
      "epoch": 6.0994427775396485,
      "grad_norm": 14.704395294189453,
      "learning_rate": 1.1867409629947135e-05,
      "loss": 0.6273,
      "step": 28460
    },
    {
      "epoch": 6.101585940848692,
      "grad_norm": 0.46128049492836,
      "learning_rate": 1.186455207886841e-05,
      "loss": 0.3163,
      "step": 28470
    },
    {
      "epoch": 6.103729104157737,
      "grad_norm": 0.40475618839263916,
      "learning_rate": 1.1861694527789684e-05,
      "loss": 0.1647,
      "step": 28480
    },
    {
      "epoch": 6.105872267466781,
      "grad_norm": 14.753779411315918,
      "learning_rate": 1.185883697671096e-05,
      "loss": 0.7994,
      "step": 28490
    },
    {
      "epoch": 6.108015430775825,
      "grad_norm": 14.737954139709473,
      "learning_rate": 1.1855979425632234e-05,
      "loss": 0.4836,
      "step": 28500
    },
    {
      "epoch": 6.1101585940848695,
      "grad_norm": 0.4070933163166046,
      "learning_rate": 1.1853121874553508e-05,
      "loss": 0.9604,
      "step": 28510
    },
    {
      "epoch": 6.112301757393913,
      "grad_norm": 0.48131346702575684,
      "learning_rate": 1.1850264323474783e-05,
      "loss": 0.9393,
      "step": 28520
    },
    {
      "epoch": 6.114444920702957,
      "grad_norm": 0.5141874551773071,
      "learning_rate": 1.1847406772396057e-05,
      "loss": 1.3677,
      "step": 28530
    },
    {
      "epoch": 6.116588084012002,
      "grad_norm": 14.569519996643066,
      "learning_rate": 1.1844549221317331e-05,
      "loss": 0.7518,
      "step": 28540
    },
    {
      "epoch": 6.118731247321046,
      "grad_norm": 0.5418420433998108,
      "learning_rate": 1.1841691670238607e-05,
      "loss": 0.3051,
      "step": 28550
    },
    {
      "epoch": 6.12087441063009,
      "grad_norm": 14.596382141113281,
      "learning_rate": 1.183883411915988e-05,
      "loss": 0.6046,
      "step": 28560
    },
    {
      "epoch": 6.123017573939134,
      "grad_norm": 0.5129976868629456,
      "learning_rate": 1.1835976568081156e-05,
      "loss": 0.7544,
      "step": 28570
    },
    {
      "epoch": 6.125160737248178,
      "grad_norm": 0.4943673610687256,
      "learning_rate": 1.183311901700243e-05,
      "loss": 0.1611,
      "step": 28580
    },
    {
      "epoch": 6.127303900557222,
      "grad_norm": 0.4727741777896881,
      "learning_rate": 1.1830261465923704e-05,
      "loss": 0.7654,
      "step": 28590
    },
    {
      "epoch": 6.129447063866267,
      "grad_norm": 14.604207038879395,
      "learning_rate": 1.182740391484498e-05,
      "loss": 0.769,
      "step": 28600
    },
    {
      "epoch": 6.131590227175311,
      "grad_norm": 0.47618505358695984,
      "learning_rate": 1.1824546363766254e-05,
      "loss": 0.1616,
      "step": 28610
    },
    {
      "epoch": 6.1337333904843545,
      "grad_norm": 0.4806566834449768,
      "learning_rate": 1.1821688812687528e-05,
      "loss": 0.7667,
      "step": 28620
    },
    {
      "epoch": 6.135876553793399,
      "grad_norm": 0.47521811723709106,
      "learning_rate": 1.1818831261608803e-05,
      "loss": 0.1637,
      "step": 28630
    },
    {
      "epoch": 6.138019717102443,
      "grad_norm": 14.689502716064453,
      "learning_rate": 1.1815973710530077e-05,
      "loss": 0.6327,
      "step": 28640
    },
    {
      "epoch": 6.140162880411487,
      "grad_norm": 14.823505401611328,
      "learning_rate": 1.1813116159451353e-05,
      "loss": 0.4791,
      "step": 28650
    },
    {
      "epoch": 6.142306043720532,
      "grad_norm": 0.4203886389732361,
      "learning_rate": 1.1810258608372627e-05,
      "loss": 0.79,
      "step": 28660
    },
    {
      "epoch": 6.1444492070295755,
      "grad_norm": 0.43209365010261536,
      "learning_rate": 1.1807401057293899e-05,
      "loss": 0.6251,
      "step": 28670
    },
    {
      "epoch": 6.146592370338619,
      "grad_norm": 0.4728253185749054,
      "learning_rate": 1.1804543506215173e-05,
      "loss": 0.7709,
      "step": 28680
    },
    {
      "epoch": 6.148735533647664,
      "grad_norm": 14.827293395996094,
      "learning_rate": 1.1801685955136449e-05,
      "loss": 0.6162,
      "step": 28690
    },
    {
      "epoch": 6.150878696956708,
      "grad_norm": 0.5049146413803101,
      "learning_rate": 1.1798828404057722e-05,
      "loss": 0.3127,
      "step": 28700
    },
    {
      "epoch": 6.153021860265753,
      "grad_norm": 0.47906941175460815,
      "learning_rate": 1.1795970852978998e-05,
      "loss": 0.4651,
      "step": 28710
    },
    {
      "epoch": 6.155165023574797,
      "grad_norm": 14.790640830993652,
      "learning_rate": 1.1793113301900272e-05,
      "loss": 1.0693,
      "step": 28720
    },
    {
      "epoch": 6.15730818688384,
      "grad_norm": 14.57646656036377,
      "learning_rate": 1.1790255750821546e-05,
      "loss": 0.6094,
      "step": 28730
    },
    {
      "epoch": 6.159451350192885,
      "grad_norm": 0.526579737663269,
      "learning_rate": 1.1787398199742822e-05,
      "loss": 0.6024,
      "step": 28740
    },
    {
      "epoch": 6.161594513501929,
      "grad_norm": 0.5225700736045837,
      "learning_rate": 1.1784540648664095e-05,
      "loss": 0.608,
      "step": 28750
    },
    {
      "epoch": 6.163737676810973,
      "grad_norm": 0.47638869285583496,
      "learning_rate": 1.178168309758537e-05,
      "loss": 0.159,
      "step": 28760
    },
    {
      "epoch": 6.165880840120018,
      "grad_norm": 0.4434524178504944,
      "learning_rate": 1.1778825546506645e-05,
      "loss": 0.4669,
      "step": 28770
    },
    {
      "epoch": 6.168024003429061,
      "grad_norm": 14.605487823486328,
      "learning_rate": 1.1775967995427919e-05,
      "loss": 0.9257,
      "step": 28780
    },
    {
      "epoch": 6.170167166738105,
      "grad_norm": 0.4935985505580902,
      "learning_rate": 1.1773110444349195e-05,
      "loss": 1.0674,
      "step": 28790
    },
    {
      "epoch": 6.17231033004715,
      "grad_norm": 0.5002198219299316,
      "learning_rate": 1.1770252893270469e-05,
      "loss": 0.1607,
      "step": 28800
    },
    {
      "epoch": 6.174453493356194,
      "grad_norm": 14.79194164276123,
      "learning_rate": 1.1767395342191742e-05,
      "loss": 0.6166,
      "step": 28810
    },
    {
      "epoch": 6.176596656665238,
      "grad_norm": 0.47590693831443787,
      "learning_rate": 1.1764537791113018e-05,
      "loss": 0.467,
      "step": 28820
    },
    {
      "epoch": 6.1787398199742825,
      "grad_norm": 14.96242618560791,
      "learning_rate": 1.1761680240034292e-05,
      "loss": 0.6229,
      "step": 28830
    },
    {
      "epoch": 6.180882983283326,
      "grad_norm": 0.45355647802352905,
      "learning_rate": 1.1758822688955568e-05,
      "loss": 0.4724,
      "step": 28840
    },
    {
      "epoch": 6.18302614659237,
      "grad_norm": 0.5063644647598267,
      "learning_rate": 1.1755965137876842e-05,
      "loss": 0.7782,
      "step": 28850
    },
    {
      "epoch": 6.185169309901415,
      "grad_norm": 0.5297235250473022,
      "learning_rate": 1.1753107586798115e-05,
      "loss": 0.4619,
      "step": 28860
    },
    {
      "epoch": 6.187312473210459,
      "grad_norm": 0.5301794409751892,
      "learning_rate": 1.1750250035719391e-05,
      "loss": 0.756,
      "step": 28870
    },
    {
      "epoch": 6.189455636519503,
      "grad_norm": 0.4821462333202362,
      "learning_rate": 1.1747392484640663e-05,
      "loss": 0.6024,
      "step": 28880
    },
    {
      "epoch": 6.191598799828547,
      "grad_norm": 0.39370623230934143,
      "learning_rate": 1.1744534933561937e-05,
      "loss": 0.4743,
      "step": 28890
    },
    {
      "epoch": 6.193741963137591,
      "grad_norm": 14.703067779541016,
      "learning_rate": 1.1741677382483211e-05,
      "loss": 0.7938,
      "step": 28900
    },
    {
      "epoch": 6.195885126446635,
      "grad_norm": 29.67595100402832,
      "learning_rate": 1.1738819831404487e-05,
      "loss": 0.9354,
      "step": 28910
    },
    {
      "epoch": 6.19802828975568,
      "grad_norm": 0.4483594298362732,
      "learning_rate": 1.173596228032576e-05,
      "loss": 0.619,
      "step": 28920
    },
    {
      "epoch": 6.200171453064724,
      "grad_norm": 0.46548107266426086,
      "learning_rate": 1.1733104729247036e-05,
      "loss": 0.3151,
      "step": 28930
    },
    {
      "epoch": 6.2023146163737675,
      "grad_norm": 0.44940632581710815,
      "learning_rate": 1.173024717816831e-05,
      "loss": 0.6275,
      "step": 28940
    },
    {
      "epoch": 6.204457779682812,
      "grad_norm": 0.4173773229122162,
      "learning_rate": 1.1727389627089584e-05,
      "loss": 0.3151,
      "step": 28950
    },
    {
      "epoch": 6.206600942991856,
      "grad_norm": 0.3687632977962494,
      "learning_rate": 1.172453207601086e-05,
      "loss": 0.336,
      "step": 28960
    },
    {
      "epoch": 6.2087441063009,
      "grad_norm": 14.802279472351074,
      "learning_rate": 1.1721674524932134e-05,
      "loss": 0.6621,
      "step": 28970
    },
    {
      "epoch": 6.210887269609945,
      "grad_norm": 0.3266546130180359,
      "learning_rate": 1.171881697385341e-05,
      "loss": 0.512,
      "step": 28980
    },
    {
      "epoch": 6.2130304329189885,
      "grad_norm": 0.30583104491233826,
      "learning_rate": 1.1715959422774683e-05,
      "loss": 0.6709,
      "step": 28990
    },
    {
      "epoch": 6.215173596228032,
      "grad_norm": 0.33228057622909546,
      "learning_rate": 1.1713101871695957e-05,
      "loss": 0.8289,
      "step": 29000
    },
    {
      "epoch": 6.217316759537077,
      "grad_norm": 14.732019424438477,
      "learning_rate": 1.1710244320617233e-05,
      "loss": 1.4586,
      "step": 29010
    },
    {
      "epoch": 6.219459922846121,
      "grad_norm": 14.647656440734863,
      "learning_rate": 1.1707386769538507e-05,
      "loss": 0.322,
      "step": 29020
    },
    {
      "epoch": 6.221603086155165,
      "grad_norm": 14.675690650939941,
      "learning_rate": 1.170452921845978e-05,
      "loss": 0.7817,
      "step": 29030
    },
    {
      "epoch": 6.2237462494642095,
      "grad_norm": 0.45591971278190613,
      "learning_rate": 1.1701671667381056e-05,
      "loss": 0.7816,
      "step": 29040
    },
    {
      "epoch": 6.225889412773253,
      "grad_norm": 0.46544864773750305,
      "learning_rate": 1.169881411630233e-05,
      "loss": 0.4709,
      "step": 29050
    },
    {
      "epoch": 6.228032576082297,
      "grad_norm": 14.597010612487793,
      "learning_rate": 1.1695956565223606e-05,
      "loss": 1.0766,
      "step": 29060
    },
    {
      "epoch": 6.230175739391342,
      "grad_norm": 0.5431877374649048,
      "learning_rate": 1.169309901414488e-05,
      "loss": 0.7628,
      "step": 29070
    },
    {
      "epoch": 6.232318902700386,
      "grad_norm": 14.582364082336426,
      "learning_rate": 1.1690241463066154e-05,
      "loss": 0.6064,
      "step": 29080
    },
    {
      "epoch": 6.23446206600943,
      "grad_norm": 0.5283371806144714,
      "learning_rate": 1.168738391198743e-05,
      "loss": 0.7512,
      "step": 29090
    },
    {
      "epoch": 6.236605229318474,
      "grad_norm": 0.5714506506919861,
      "learning_rate": 1.1684526360908702e-05,
      "loss": 0.604,
      "step": 29100
    },
    {
      "epoch": 6.238748392627518,
      "grad_norm": 0.551042914390564,
      "learning_rate": 1.1681668809829976e-05,
      "loss": 0.5968,
      "step": 29110
    },
    {
      "epoch": 6.240891555936562,
      "grad_norm": 0.5326673984527588,
      "learning_rate": 1.1678811258751251e-05,
      "loss": 0.3059,
      "step": 29120
    },
    {
      "epoch": 6.243034719245607,
      "grad_norm": 14.600075721740723,
      "learning_rate": 1.1675953707672525e-05,
      "loss": 0.4618,
      "step": 29130
    },
    {
      "epoch": 6.245177882554651,
      "grad_norm": 0.5021591186523438,
      "learning_rate": 1.1673096156593799e-05,
      "loss": 0.6132,
      "step": 29140
    },
    {
      "epoch": 6.2473210458636945,
      "grad_norm": 0.4820035994052887,
      "learning_rate": 1.1670238605515075e-05,
      "loss": 0.6239,
      "step": 29150
    },
    {
      "epoch": 6.249464209172739,
      "grad_norm": 14.590047836303711,
      "learning_rate": 1.1667381054436349e-05,
      "loss": 1.2223,
      "step": 29160
    },
    {
      "epoch": 6.251607372481783,
      "grad_norm": 0.48892438411712646,
      "learning_rate": 1.1664523503357623e-05,
      "loss": 0.3114,
      "step": 29170
    },
    {
      "epoch": 6.253750535790827,
      "grad_norm": 14.66536808013916,
      "learning_rate": 1.1661665952278898e-05,
      "loss": 0.6183,
      "step": 29180
    },
    {
      "epoch": 6.255893699099872,
      "grad_norm": 14.587594032287598,
      "learning_rate": 1.1658808401200172e-05,
      "loss": 0.6114,
      "step": 29190
    },
    {
      "epoch": 6.2580368624089155,
      "grad_norm": 0.4755007326602936,
      "learning_rate": 1.1655950850121448e-05,
      "loss": 0.6187,
      "step": 29200
    },
    {
      "epoch": 6.260180025717959,
      "grad_norm": 0.45226219296455383,
      "learning_rate": 1.1653093299042722e-05,
      "loss": 0.1619,
      "step": 29210
    },
    {
      "epoch": 6.262323189027004,
      "grad_norm": 14.743962287902832,
      "learning_rate": 1.1650235747963996e-05,
      "loss": 0.3219,
      "step": 29220
    },
    {
      "epoch": 6.264466352336048,
      "grad_norm": 0.4077654778957367,
      "learning_rate": 1.1647378196885271e-05,
      "loss": 0.6363,
      "step": 29230
    },
    {
      "epoch": 6.266609515645092,
      "grad_norm": 14.650805473327637,
      "learning_rate": 1.1644520645806545e-05,
      "loss": 0.7897,
      "step": 29240
    },
    {
      "epoch": 6.268752678954137,
      "grad_norm": 0.40948277711868286,
      "learning_rate": 1.1641663094727819e-05,
      "loss": 0.3222,
      "step": 29250
    },
    {
      "epoch": 6.27089584226318,
      "grad_norm": 0.4286303222179413,
      "learning_rate": 1.1638805543649095e-05,
      "loss": 0.7879,
      "step": 29260
    },
    {
      "epoch": 6.273039005572224,
      "grad_norm": 14.600425720214844,
      "learning_rate": 1.1635947992570369e-05,
      "loss": 1.4037,
      "step": 29270
    },
    {
      "epoch": 6.275182168881269,
      "grad_norm": 0.5101667046546936,
      "learning_rate": 1.1633090441491644e-05,
      "loss": 0.6149,
      "step": 29280
    },
    {
      "epoch": 6.277325332190313,
      "grad_norm": 0.5157489776611328,
      "learning_rate": 1.1630232890412918e-05,
      "loss": 0.4575,
      "step": 29290
    },
    {
      "epoch": 6.279468495499357,
      "grad_norm": 0.5006014704704285,
      "learning_rate": 1.1627375339334192e-05,
      "loss": 0.3112,
      "step": 29300
    },
    {
      "epoch": 6.281611658808401,
      "grad_norm": 0.4724418520927429,
      "learning_rate": 1.1624517788255464e-05,
      "loss": 0.3145,
      "step": 29310
    },
    {
      "epoch": 6.283754822117445,
      "grad_norm": 0.4362795948982239,
      "learning_rate": 1.162166023717674e-05,
      "loss": 0.9379,
      "step": 29320
    },
    {
      "epoch": 6.285897985426489,
      "grad_norm": 14.640392303466797,
      "learning_rate": 1.1618802686098014e-05,
      "loss": 0.9472,
      "step": 29330
    },
    {
      "epoch": 6.288041148735534,
      "grad_norm": 0.4274718165397644,
      "learning_rate": 1.161594513501929e-05,
      "loss": 0.4797,
      "step": 29340
    },
    {
      "epoch": 6.290184312044578,
      "grad_norm": 0.37623709440231323,
      "learning_rate": 1.1613087583940563e-05,
      "loss": 0.009,
      "step": 29350
    },
    {
      "epoch": 6.292327475353622,
      "grad_norm": 0.32068005204200745,
      "learning_rate": 1.1610230032861837e-05,
      "loss": 0.1708,
      "step": 29360
    },
    {
      "epoch": 6.294470638662666,
      "grad_norm": 14.772351264953613,
      "learning_rate": 1.1607372481783113e-05,
      "loss": 0.9988,
      "step": 29370
    },
    {
      "epoch": 6.29661380197171,
      "grad_norm": 0.3205089569091797,
      "learning_rate": 1.1604514930704387e-05,
      "loss": 0.4995,
      "step": 29380
    },
    {
      "epoch": 6.298756965280754,
      "grad_norm": 0.3515585660934448,
      "learning_rate": 1.160165737962566e-05,
      "loss": 0.8212,
      "step": 29390
    },
    {
      "epoch": 6.300900128589799,
      "grad_norm": 0.363725483417511,
      "learning_rate": 1.1598799828546936e-05,
      "loss": 0.8164,
      "step": 29400
    },
    {
      "epoch": 6.303043291898843,
      "grad_norm": 0.421609491109848,
      "learning_rate": 1.159594227746821e-05,
      "loss": 0.7997,
      "step": 29410
    },
    {
      "epoch": 6.3051864552078865,
      "grad_norm": 0.508382260799408,
      "learning_rate": 1.1593084726389486e-05,
      "loss": 0.7693,
      "step": 29420
    },
    {
      "epoch": 6.307329618516931,
      "grad_norm": 0.5263054370880127,
      "learning_rate": 1.159022717531076e-05,
      "loss": 0.7558,
      "step": 29430
    },
    {
      "epoch": 6.309472781825975,
      "grad_norm": 0.46551749110221863,
      "learning_rate": 1.1587369624232034e-05,
      "loss": 0.1599,
      "step": 29440
    },
    {
      "epoch": 6.311615945135019,
      "grad_norm": 0.47371798753738403,
      "learning_rate": 1.158451207315331e-05,
      "loss": 0.4691,
      "step": 29450
    },
    {
      "epoch": 6.313759108444064,
      "grad_norm": 14.638456344604492,
      "learning_rate": 1.1581654522074583e-05,
      "loss": 0.4707,
      "step": 29460
    },
    {
      "epoch": 6.3159022717531075,
      "grad_norm": 0.41069385409355164,
      "learning_rate": 1.1578796970995857e-05,
      "loss": 0.7796,
      "step": 29470
    },
    {
      "epoch": 6.318045435062151,
      "grad_norm": 14.627223014831543,
      "learning_rate": 1.1575939419917133e-05,
      "loss": 0.7785,
      "step": 29480
    },
    {
      "epoch": 6.320188598371196,
      "grad_norm": 0.4320107698440552,
      "learning_rate": 1.1573081868838407e-05,
      "loss": 0.1601,
      "step": 29490
    },
    {
      "epoch": 6.32233176168024,
      "grad_norm": 0.40592023730278015,
      "learning_rate": 1.1570224317759682e-05,
      "loss": 0.4772,
      "step": 29500
    },
    {
      "epoch": 6.324474924989284,
      "grad_norm": 0.4124830663204193,
      "learning_rate": 1.1567366766680956e-05,
      "loss": 0.4791,
      "step": 29510
    },
    {
      "epoch": 6.3266180882983285,
      "grad_norm": 0.4051666557788849,
      "learning_rate": 1.156450921560223e-05,
      "loss": 0.4821,
      "step": 29520
    },
    {
      "epoch": 6.328761251607372,
      "grad_norm": 0.4151754677295685,
      "learning_rate": 1.1561651664523503e-05,
      "loss": 1.112,
      "step": 29530
    },
    {
      "epoch": 6.330904414916416,
      "grad_norm": 14.70280647277832,
      "learning_rate": 1.1558794113444778e-05,
      "loss": 0.3259,
      "step": 29540
    },
    {
      "epoch": 6.333047578225461,
      "grad_norm": 14.735237121582031,
      "learning_rate": 1.1555936562366052e-05,
      "loss": 0.6546,
      "step": 29550
    },
    {
      "epoch": 6.335190741534505,
      "grad_norm": 0.3947523534297943,
      "learning_rate": 1.1553079011287328e-05,
      "loss": 0.9622,
      "step": 29560
    },
    {
      "epoch": 6.337333904843549,
      "grad_norm": 14.585668563842773,
      "learning_rate": 1.1550221460208602e-05,
      "loss": 1.2378,
      "step": 29570
    },
    {
      "epoch": 6.339477068152593,
      "grad_norm": 14.552031517028809,
      "learning_rate": 1.1547363909129876e-05,
      "loss": 0.4572,
      "step": 29580
    },
    {
      "epoch": 6.341620231461637,
      "grad_norm": 0.5439466834068298,
      "learning_rate": 1.1544506358051151e-05,
      "loss": 0.7489,
      "step": 29590
    },
    {
      "epoch": 6.343763394770681,
      "grad_norm": 14.514446258544922,
      "learning_rate": 1.1541648806972425e-05,
      "loss": 1.0398,
      "step": 29600
    },
    {
      "epoch": 6.345906558079726,
      "grad_norm": 0.6100989580154419,
      "learning_rate": 1.1538791255893699e-05,
      "loss": 0.8816,
      "step": 29610
    },
    {
      "epoch": 6.34804972138877,
      "grad_norm": 14.529180526733398,
      "learning_rate": 1.1535933704814975e-05,
      "loss": 0.5833,
      "step": 29620
    },
    {
      "epoch": 6.3501928846978135,
      "grad_norm": 0.6427491903305054,
      "learning_rate": 1.1533076153736249e-05,
      "loss": 0.5827,
      "step": 29630
    },
    {
      "epoch": 6.352336048006858,
      "grad_norm": 14.611538887023926,
      "learning_rate": 1.1530218602657524e-05,
      "loss": 0.4431,
      "step": 29640
    },
    {
      "epoch": 6.354479211315902,
      "grad_norm": 0.6186391711235046,
      "learning_rate": 1.1527361051578798e-05,
      "loss": 0.4425,
      "step": 29650
    },
    {
      "epoch": 6.356622374624947,
      "grad_norm": 0.6872645020484924,
      "learning_rate": 1.1524503500500072e-05,
      "loss": 1.0133,
      "step": 29660
    },
    {
      "epoch": 6.358765537933991,
      "grad_norm": 0.733982264995575,
      "learning_rate": 1.1521645949421348e-05,
      "loss": 0.4335,
      "step": 29670
    },
    {
      "epoch": 6.3609087012430345,
      "grad_norm": 0.5792665481567383,
      "learning_rate": 1.1518788398342622e-05,
      "loss": 0.2963,
      "step": 29680
    },
    {
      "epoch": 6.363051864552079,
      "grad_norm": 14.572498321533203,
      "learning_rate": 1.1515930847263897e-05,
      "loss": 0.8939,
      "step": 29690
    },
    {
      "epoch": 6.365195027861123,
      "grad_norm": 29.67577362060547,
      "learning_rate": 1.1513073296185171e-05,
      "loss": 0.6103,
      "step": 29700
    },
    {
      "epoch": 6.367338191170167,
      "grad_norm": 0.514967143535614,
      "learning_rate": 1.1510215745106445e-05,
      "loss": 0.4651,
      "step": 29710
    },
    {
      "epoch": 6.369481354479212,
      "grad_norm": 0.5342663526535034,
      "learning_rate": 1.150735819402772e-05,
      "loss": 0.7602,
      "step": 29720
    },
    {
      "epoch": 6.371624517788256,
      "grad_norm": 0.48902639746665955,
      "learning_rate": 1.1504500642948995e-05,
      "loss": 0.3075,
      "step": 29730
    },
    {
      "epoch": 6.373767681097299,
      "grad_norm": 0.4954302906990051,
      "learning_rate": 1.1501643091870267e-05,
      "loss": 0.4621,
      "step": 29740
    },
    {
      "epoch": 6.375910844406344,
      "grad_norm": 0.4468032419681549,
      "learning_rate": 1.149878554079154e-05,
      "loss": 0.4616,
      "step": 29750
    },
    {
      "epoch": 6.378054007715388,
      "grad_norm": 14.686264038085938,
      "learning_rate": 1.1495927989712816e-05,
      "loss": 0.4772,
      "step": 29760
    },
    {
      "epoch": 6.380197171024432,
      "grad_norm": 0.4174703359603882,
      "learning_rate": 1.149307043863409e-05,
      "loss": 0.6283,
      "step": 29770
    },
    {
      "epoch": 6.382340334333477,
      "grad_norm": 0.44831976294517517,
      "learning_rate": 1.1490212887555366e-05,
      "loss": 0.6244,
      "step": 29780
    },
    {
      "epoch": 6.38448349764252,
      "grad_norm": 14.90483283996582,
      "learning_rate": 1.148735533647664e-05,
      "loss": 0.9353,
      "step": 29790
    },
    {
      "epoch": 6.386626660951564,
      "grad_norm": 0.4912567734718323,
      "learning_rate": 1.1484497785397914e-05,
      "loss": 0.466,
      "step": 29800
    },
    {
      "epoch": 6.388769824260609,
      "grad_norm": 0.45984265208244324,
      "learning_rate": 1.148164023431919e-05,
      "loss": 0.4651,
      "step": 29810
    },
    {
      "epoch": 6.390912987569653,
      "grad_norm": 0.4106333255767822,
      "learning_rate": 1.1478782683240463e-05,
      "loss": 0.4823,
      "step": 29820
    },
    {
      "epoch": 6.393056150878697,
      "grad_norm": 14.636300086975098,
      "learning_rate": 1.1475925132161739e-05,
      "loss": 1.4134,
      "step": 29830
    },
    {
      "epoch": 6.3951993141877415,
      "grad_norm": 0.5127109885215759,
      "learning_rate": 1.1473067581083013e-05,
      "loss": 0.4635,
      "step": 29840
    },
    {
      "epoch": 6.397342477496785,
      "grad_norm": 29.678712844848633,
      "learning_rate": 1.1470210030004287e-05,
      "loss": 0.7559,
      "step": 29850
    },
    {
      "epoch": 6.399485640805829,
      "grad_norm": 14.543149948120117,
      "learning_rate": 1.1467352478925563e-05,
      "loss": 0.8981,
      "step": 29860
    },
    {
      "epoch": 6.401628804114874,
      "grad_norm": 14.51465892791748,
      "learning_rate": 1.1464494927846836e-05,
      "loss": 0.8813,
      "step": 29870
    },
    {
      "epoch": 6.403771967423918,
      "grad_norm": 0.6480203866958618,
      "learning_rate": 1.146163737676811e-05,
      "loss": 0.5847,
      "step": 29880
    },
    {
      "epoch": 6.405915130732962,
      "grad_norm": 0.6401901245117188,
      "learning_rate": 1.1458779825689386e-05,
      "loss": 0.5804,
      "step": 29890
    },
    {
      "epoch": 6.408058294042006,
      "grad_norm": 14.604759216308594,
      "learning_rate": 1.145592227461066e-05,
      "loss": 0.7363,
      "step": 29900
    },
    {
      "epoch": 6.41020145735105,
      "grad_norm": 0.6158414483070374,
      "learning_rate": 1.1453064723531936e-05,
      "loss": 0.5854,
      "step": 29910
    },
    {
      "epoch": 6.412344620660094,
      "grad_norm": 0.6040617823600769,
      "learning_rate": 1.145020717245321e-05,
      "loss": 0.4436,
      "step": 29920
    },
    {
      "epoch": 6.414487783969139,
      "grad_norm": 0.5935966968536377,
      "learning_rate": 1.1447349621374483e-05,
      "loss": 0.4486,
      "step": 29930
    },
    {
      "epoch": 6.416630947278183,
      "grad_norm": 14.64941120147705,
      "learning_rate": 1.1444492070295759e-05,
      "loss": 0.8729,
      "step": 29940
    },
    {
      "epoch": 6.4187741105872265,
      "grad_norm": 0.6688446998596191,
      "learning_rate": 1.1441634519217031e-05,
      "loss": 0.5839,
      "step": 29950
    },
    {
      "epoch": 6.420917273896271,
      "grad_norm": 0.6504800319671631,
      "learning_rate": 1.1438776968138305e-05,
      "loss": 0.297,
      "step": 29960
    },
    {
      "epoch": 6.423060437205315,
      "grad_norm": 0.6071874499320984,
      "learning_rate": 1.143591941705958e-05,
      "loss": 0.4419,
      "step": 29970
    },
    {
      "epoch": 6.425203600514359,
      "grad_norm": 0.5584445595741272,
      "learning_rate": 1.1433061865980855e-05,
      "loss": 0.4464,
      "step": 29980
    },
    {
      "epoch": 6.427346763823404,
      "grad_norm": 0.5260872840881348,
      "learning_rate": 1.1430204314902129e-05,
      "loss": 0.4542,
      "step": 29990
    },
    {
      "epoch": 6.4294899271324475,
      "grad_norm": 14.590121269226074,
      "learning_rate": 1.1427346763823404e-05,
      "loss": 0.9044,
      "step": 30000
    },
    {
      "epoch": 6.431633090441491,
      "grad_norm": 0.5419750809669495,
      "learning_rate": 1.1424489212744678e-05,
      "loss": 0.3059,
      "step": 30010
    },
    {
      "epoch": 6.433776253750536,
      "grad_norm": 0.5263129472732544,
      "learning_rate": 1.1421631661665952e-05,
      "loss": 0.6109,
      "step": 30020
    },
    {
      "epoch": 6.43591941705958,
      "grad_norm": 0.4816700220108032,
      "learning_rate": 1.1418774110587228e-05,
      "loss": 0.6045,
      "step": 30030
    },
    {
      "epoch": 6.438062580368624,
      "grad_norm": 0.5096861720085144,
      "learning_rate": 1.1415916559508502e-05,
      "loss": 0.7595,
      "step": 30040
    },
    {
      "epoch": 6.4402057436776685,
      "grad_norm": 0.5158764719963074,
      "learning_rate": 1.1413059008429777e-05,
      "loss": 0.4583,
      "step": 30050
    },
    {
      "epoch": 6.442348906986712,
      "grad_norm": 0.5138422846794128,
      "learning_rate": 1.1410201457351051e-05,
      "loss": 0.3159,
      "step": 30060
    },
    {
      "epoch": 6.444492070295756,
      "grad_norm": 0.47890278697013855,
      "learning_rate": 1.1407343906272325e-05,
      "loss": 0.6118,
      "step": 30070
    },
    {
      "epoch": 6.446635233604801,
      "grad_norm": 14.685308456420898,
      "learning_rate": 1.14044863551936e-05,
      "loss": 1.2092,
      "step": 30080
    },
    {
      "epoch": 6.448778396913845,
      "grad_norm": 0.6701502799987793,
      "learning_rate": 1.1401628804114875e-05,
      "loss": 0.8717,
      "step": 30090
    },
    {
      "epoch": 6.450921560222889,
      "grad_norm": 0.6921935677528381,
      "learning_rate": 1.1398771253036149e-05,
      "loss": 0.4334,
      "step": 30100
    },
    {
      "epoch": 6.453064723531933,
      "grad_norm": 0.7166340351104736,
      "learning_rate": 1.1395913701957424e-05,
      "loss": 0.4318,
      "step": 30110
    },
    {
      "epoch": 6.455207886840977,
      "grad_norm": 0.6796494126319885,
      "learning_rate": 1.1393056150878698e-05,
      "loss": 0.4308,
      "step": 30120
    },
    {
      "epoch": 6.457351050150021,
      "grad_norm": 0.5957448482513428,
      "learning_rate": 1.1390198599799974e-05,
      "loss": 0.4422,
      "step": 30130
    },
    {
      "epoch": 6.459494213459066,
      "grad_norm": 0.5381984710693359,
      "learning_rate": 1.1387341048721248e-05,
      "loss": 0.3064,
      "step": 30140
    },
    {
      "epoch": 6.46163737676811,
      "grad_norm": 0.3877013921737671,
      "learning_rate": 1.1384483497642522e-05,
      "loss": 0.3076,
      "step": 30150
    },
    {
      "epoch": 6.4637805400771535,
      "grad_norm": 0.4467252194881439,
      "learning_rate": 1.1381625946563797e-05,
      "loss": 0.9345,
      "step": 30160
    },
    {
      "epoch": 6.465923703386198,
      "grad_norm": 0.5034039616584778,
      "learning_rate": 1.137876839548507e-05,
      "loss": 1.2232,
      "step": 30170
    },
    {
      "epoch": 6.468066866695242,
      "grad_norm": 14.63317584991455,
      "learning_rate": 1.1375910844406343e-05,
      "loss": 0.6065,
      "step": 30180
    },
    {
      "epoch": 6.470210030004286,
      "grad_norm": 0.5446682572364807,
      "learning_rate": 1.1373053293327619e-05,
      "loss": 0.7568,
      "step": 30190
    },
    {
      "epoch": 6.472353193313331,
      "grad_norm": 0.5264649987220764,
      "learning_rate": 1.1370195742248893e-05,
      "loss": 0.7501,
      "step": 30200
    },
    {
      "epoch": 6.4744963566223745,
      "grad_norm": 15.10981559753418,
      "learning_rate": 1.1367338191170167e-05,
      "loss": 0.466,
      "step": 30210
    },
    {
      "epoch": 6.476639519931418,
      "grad_norm": 0.537286639213562,
      "learning_rate": 1.1364480640091443e-05,
      "loss": 0.4654,
      "step": 30220
    },
    {
      "epoch": 6.478782683240463,
      "grad_norm": 0.46932265162467957,
      "learning_rate": 1.1361623089012716e-05,
      "loss": 0.3106,
      "step": 30230
    },
    {
      "epoch": 6.480925846549507,
      "grad_norm": 0.5415728688240051,
      "learning_rate": 1.135876553793399e-05,
      "loss": 0.9209,
      "step": 30240
    },
    {
      "epoch": 6.483069009858551,
      "grad_norm": 14.66187858581543,
      "learning_rate": 1.1355907986855266e-05,
      "loss": 0.4666,
      "step": 30250
    },
    {
      "epoch": 6.485212173167596,
      "grad_norm": 0.4651963412761688,
      "learning_rate": 1.135305043577654e-05,
      "loss": 0.3135,
      "step": 30260
    },
    {
      "epoch": 6.487355336476639,
      "grad_norm": 0.4488213360309601,
      "learning_rate": 1.1350192884697816e-05,
      "loss": 0.6216,
      "step": 30270
    },
    {
      "epoch": 6.489498499785683,
      "grad_norm": 29.851299285888672,
      "learning_rate": 1.134733533361909e-05,
      "loss": 0.7682,
      "step": 30280
    },
    {
      "epoch": 6.491641663094728,
      "grad_norm": 0.5070047378540039,
      "learning_rate": 1.1344477782540363e-05,
      "loss": 0.6094,
      "step": 30290
    },
    {
      "epoch": 6.493784826403772,
      "grad_norm": 14.590089797973633,
      "learning_rate": 1.1341620231461639e-05,
      "loss": 0.4569,
      "step": 30300
    },
    {
      "epoch": 6.495927989712816,
      "grad_norm": 0.46852952241897583,
      "learning_rate": 1.1338762680382913e-05,
      "loss": 0.3109,
      "step": 30310
    },
    {
      "epoch": 6.4980711530218604,
      "grad_norm": 14.724662780761719,
      "learning_rate": 1.1335905129304187e-05,
      "loss": 0.6244,
      "step": 30320
    },
    {
      "epoch": 6.500214316330904,
      "grad_norm": 0.45321789383888245,
      "learning_rate": 1.1333047578225463e-05,
      "loss": 0.6227,
      "step": 30330
    },
    {
      "epoch": 6.502357479639949,
      "grad_norm": 0.4686741530895233,
      "learning_rate": 1.1330190027146736e-05,
      "loss": 0.6214,
      "step": 30340
    },
    {
      "epoch": 6.504500642948993,
      "grad_norm": 14.561318397521973,
      "learning_rate": 1.1327332476068012e-05,
      "loss": 1.0524,
      "step": 30350
    },
    {
      "epoch": 6.506643806258037,
      "grad_norm": 0.5981013774871826,
      "learning_rate": 1.1324474924989286e-05,
      "loss": 0.7421,
      "step": 30360
    },
    {
      "epoch": 6.5087869695670815,
      "grad_norm": 0.6660946011543274,
      "learning_rate": 1.132161737391056e-05,
      "loss": 0.5798,
      "step": 30370
    },
    {
      "epoch": 6.510930132876125,
      "grad_norm": 14.593823432922363,
      "learning_rate": 1.1318759822831832e-05,
      "loss": 0.459,
      "step": 30380
    },
    {
      "epoch": 6.513073296185169,
      "grad_norm": 15.0081205368042,
      "learning_rate": 1.1315902271753108e-05,
      "loss": 1.0605,
      "step": 30390
    },
    {
      "epoch": 6.515216459494214,
      "grad_norm": 0.5310038924217224,
      "learning_rate": 1.1313044720674382e-05,
      "loss": 0.4504,
      "step": 30400
    },
    {
      "epoch": 6.517359622803258,
      "grad_norm": 0.5313768982887268,
      "learning_rate": 1.1310187169595657e-05,
      "loss": 0.9028,
      "step": 30410
    },
    {
      "epoch": 6.519502786112302,
      "grad_norm": 0.5563727021217346,
      "learning_rate": 1.1307329618516931e-05,
      "loss": 0.4558,
      "step": 30420
    },
    {
      "epoch": 6.521645949421346,
      "grad_norm": 0.5371631383895874,
      "learning_rate": 1.1304472067438205e-05,
      "loss": 0.7493,
      "step": 30430
    },
    {
      "epoch": 6.52378911273039,
      "grad_norm": 29.855140686035156,
      "learning_rate": 1.1301614516359481e-05,
      "loss": 0.757,
      "step": 30440
    },
    {
      "epoch": 6.525932276039434,
      "grad_norm": 0.5031607747077942,
      "learning_rate": 1.1298756965280755e-05,
      "loss": 0.7621,
      "step": 30450
    },
    {
      "epoch": 6.528075439348479,
      "grad_norm": 0.5082431435585022,
      "learning_rate": 1.1295899414202029e-05,
      "loss": 0.46,
      "step": 30460
    },
    {
      "epoch": 6.530218602657523,
      "grad_norm": 0.5056375861167908,
      "learning_rate": 1.1293041863123304e-05,
      "loss": 0.4575,
      "step": 30470
    },
    {
      "epoch": 6.5323617659665665,
      "grad_norm": 0.5057298541069031,
      "learning_rate": 1.1290184312044578e-05,
      "loss": 0.4613,
      "step": 30480
    },
    {
      "epoch": 6.534504929275611,
      "grad_norm": 14.590213775634766,
      "learning_rate": 1.1287326760965854e-05,
      "loss": 0.6103,
      "step": 30490
    },
    {
      "epoch": 6.536648092584655,
      "grad_norm": 0.4690801203250885,
      "learning_rate": 1.1284469209887128e-05,
      "loss": 0.1597,
      "step": 30500
    },
    {
      "epoch": 6.538791255893699,
      "grad_norm": 0.4524977505207062,
      "learning_rate": 1.1281611658808402e-05,
      "loss": 0.7814,
      "step": 30510
    },
    {
      "epoch": 6.540934419202744,
      "grad_norm": 14.585673332214355,
      "learning_rate": 1.1278754107729677e-05,
      "loss": 0.6211,
      "step": 30520
    },
    {
      "epoch": 6.5430775825117875,
      "grad_norm": 0.4718693494796753,
      "learning_rate": 1.1275896556650951e-05,
      "loss": 0.4619,
      "step": 30530
    },
    {
      "epoch": 6.545220745820831,
      "grad_norm": 14.876346588134766,
      "learning_rate": 1.1273039005572227e-05,
      "loss": 1.0734,
      "step": 30540
    },
    {
      "epoch": 6.547363909129876,
      "grad_norm": 0.5013850927352905,
      "learning_rate": 1.12701814544935e-05,
      "loss": 0.4641,
      "step": 30550
    },
    {
      "epoch": 6.54950707243892,
      "grad_norm": 0.5339260697364807,
      "learning_rate": 1.1267323903414775e-05,
      "loss": 0.9104,
      "step": 30560
    },
    {
      "epoch": 6.551650235747964,
      "grad_norm": 14.53945255279541,
      "learning_rate": 1.126446635233605e-05,
      "loss": 0.8914,
      "step": 30570
    },
    {
      "epoch": 6.5537933990570085,
      "grad_norm": 14.518770217895508,
      "learning_rate": 1.1261608801257324e-05,
      "loss": 0.3039,
      "step": 30580
    },
    {
      "epoch": 6.555936562366052,
      "grad_norm": 0.578613817691803,
      "learning_rate": 1.1258751250178598e-05,
      "loss": 0.4479,
      "step": 30590
    },
    {
      "epoch": 6.558079725675096,
      "grad_norm": 0.56986004114151,
      "learning_rate": 1.125589369909987e-05,
      "loss": 0.738,
      "step": 30600
    },
    {
      "epoch": 6.560222888984141,
      "grad_norm": 14.569021224975586,
      "learning_rate": 1.1253036148021146e-05,
      "loss": 0.877,
      "step": 30610
    },
    {
      "epoch": 6.562366052293185,
      "grad_norm": 14.483404159545898,
      "learning_rate": 1.125017859694242e-05,
      "loss": 0.5807,
      "step": 30620
    },
    {
      "epoch": 6.564509215602229,
      "grad_norm": 14.67052173614502,
      "learning_rate": 1.1247321045863696e-05,
      "loss": 0.8672,
      "step": 30630
    },
    {
      "epoch": 6.566652378911273,
      "grad_norm": 14.60285472869873,
      "learning_rate": 1.124446349478497e-05,
      "loss": 0.5846,
      "step": 30640
    },
    {
      "epoch": 6.568795542220317,
      "grad_norm": 29.581634521484375,
      "learning_rate": 1.1241605943706244e-05,
      "loss": 0.4379,
      "step": 30650
    },
    {
      "epoch": 6.570938705529361,
      "grad_norm": 0.5912979245185852,
      "learning_rate": 1.1238748392627519e-05,
      "loss": 0.2987,
      "step": 30660
    },
    {
      "epoch": 6.573081868838406,
      "grad_norm": 0.528998613357544,
      "learning_rate": 1.1235890841548793e-05,
      "loss": 0.157,
      "step": 30670
    },
    {
      "epoch": 6.57522503214745,
      "grad_norm": 0.48431387543678284,
      "learning_rate": 1.1233033290470069e-05,
      "loss": 0.31,
      "step": 30680
    },
    {
      "epoch": 6.5773681954564935,
      "grad_norm": 0.43932396173477173,
      "learning_rate": 1.1230175739391343e-05,
      "loss": 0.6223,
      "step": 30690
    },
    {
      "epoch": 6.579511358765538,
      "grad_norm": 0.3660496771335602,
      "learning_rate": 1.1227318188312617e-05,
      "loss": 0.3219,
      "step": 30700
    },
    {
      "epoch": 6.581654522074582,
      "grad_norm": 0.35405203700065613,
      "learning_rate": 1.1224460637233892e-05,
      "loss": 0.646,
      "step": 30710
    },
    {
      "epoch": 6.583797685383626,
      "grad_norm": 15.352147102355957,
      "learning_rate": 1.1221603086155166e-05,
      "loss": 0.4939,
      "step": 30720
    },
    {
      "epoch": 6.585940848692671,
      "grad_norm": 14.852365493774414,
      "learning_rate": 1.121874553507644e-05,
      "loss": 0.488,
      "step": 30730
    },
    {
      "epoch": 6.588084012001715,
      "grad_norm": 0.39392217993736267,
      "learning_rate": 1.1215887983997716e-05,
      "loss": 0.4902,
      "step": 30740
    },
    {
      "epoch": 6.590227175310758,
      "grad_norm": 0.4100486934185028,
      "learning_rate": 1.121303043291899e-05,
      "loss": 1.1116,
      "step": 30750
    },
    {
      "epoch": 6.592370338619803,
      "grad_norm": 0.4353853762149811,
      "learning_rate": 1.1210172881840265e-05,
      "loss": 0.6253,
      "step": 30760
    },
    {
      "epoch": 6.594513501928847,
      "grad_norm": 0.46415987610816956,
      "learning_rate": 1.1207315330761539e-05,
      "loss": 0.7758,
      "step": 30770
    },
    {
      "epoch": 6.596656665237891,
      "grad_norm": 0.4997509717941284,
      "learning_rate": 1.1204457779682813e-05,
      "loss": 0.614,
      "step": 30780
    },
    {
      "epoch": 6.598799828546936,
      "grad_norm": 0.5362324714660645,
      "learning_rate": 1.1201600228604089e-05,
      "loss": 0.7547,
      "step": 30790
    },
    {
      "epoch": 6.600942991855979,
      "grad_norm": 0.5491932034492493,
      "learning_rate": 1.1198742677525363e-05,
      "loss": 0.3071,
      "step": 30800
    },
    {
      "epoch": 6.603086155165023,
      "grad_norm": 0.5450516939163208,
      "learning_rate": 1.1195885126446635e-05,
      "loss": 0.6046,
      "step": 30810
    },
    {
      "epoch": 6.605229318474068,
      "grad_norm": 0.5602673888206482,
      "learning_rate": 1.119302757536791e-05,
      "loss": 0.5982,
      "step": 30820
    },
    {
      "epoch": 6.607372481783112,
      "grad_norm": 0.5793486833572388,
      "learning_rate": 1.1190170024289184e-05,
      "loss": 0.7418,
      "step": 30830
    },
    {
      "epoch": 6.609515645092156,
      "grad_norm": 0.552087128162384,
      "learning_rate": 1.1187312473210458e-05,
      "loss": 0.7461,
      "step": 30840
    },
    {
      "epoch": 6.6116588084012005,
      "grad_norm": 0.48472335934638977,
      "learning_rate": 1.1184454922131734e-05,
      "loss": 0.0116,
      "step": 30850
    },
    {
      "epoch": 6.613801971710244,
      "grad_norm": 0.403516560792923,
      "learning_rate": 1.1181597371053008e-05,
      "loss": 0.6396,
      "step": 30860
    },
    {
      "epoch": 6.615945135019288,
      "grad_norm": 0.38947543501853943,
      "learning_rate": 1.1178739819974282e-05,
      "loss": 0.4846,
      "step": 30870
    },
    {
      "epoch": 6.618088298328333,
      "grad_norm": 0.3830757141113281,
      "learning_rate": 1.1175882268895557e-05,
      "loss": 0.3249,
      "step": 30880
    },
    {
      "epoch": 6.620231461637377,
      "grad_norm": 0.38840988278388977,
      "learning_rate": 1.1173024717816831e-05,
      "loss": 0.4858,
      "step": 30890
    },
    {
      "epoch": 6.622374624946421,
      "grad_norm": 29.727354049682617,
      "learning_rate": 1.1170167166738107e-05,
      "loss": 1.2824,
      "step": 30900
    },
    {
      "epoch": 6.624517788255465,
      "grad_norm": 29.713088989257812,
      "learning_rate": 1.1167309615659381e-05,
      "loss": 0.9454,
      "step": 30910
    },
    {
      "epoch": 6.626660951564509,
      "grad_norm": 14.664041519165039,
      "learning_rate": 1.1164452064580655e-05,
      "loss": 0.4732,
      "step": 30920
    },
    {
      "epoch": 6.628804114873553,
      "grad_norm": 0.45126011967658997,
      "learning_rate": 1.116159451350193e-05,
      "loss": 0.6243,
      "step": 30930
    },
    {
      "epoch": 6.630947278182598,
      "grad_norm": 0.3705647587776184,
      "learning_rate": 1.1158736962423204e-05,
      "loss": 0.331,
      "step": 30940
    },
    {
      "epoch": 6.633090441491642,
      "grad_norm": 0.4139741361141205,
      "learning_rate": 1.1155879411344478e-05,
      "loss": 0.6373,
      "step": 30950
    },
    {
      "epoch": 6.6352336048006855,
      "grad_norm": 0.40439939498901367,
      "learning_rate": 1.1153021860265754e-05,
      "loss": 0.4806,
      "step": 30960
    },
    {
      "epoch": 6.63737676810973,
      "grad_norm": 0.40960410237312317,
      "learning_rate": 1.1150164309187028e-05,
      "loss": 0.3215,
      "step": 30970
    },
    {
      "epoch": 6.639519931418774,
      "grad_norm": 0.40520310401916504,
      "learning_rate": 1.1147306758108303e-05,
      "loss": 0.3219,
      "step": 30980
    },
    {
      "epoch": 6.641663094727818,
      "grad_norm": 14.699874877929688,
      "learning_rate": 1.1144449207029577e-05,
      "loss": 1.1081,
      "step": 30990
    },
    {
      "epoch": 6.643806258036863,
      "grad_norm": 14.632697105407715,
      "learning_rate": 1.1141591655950851e-05,
      "loss": 0.9368,
      "step": 31000
    },
    {
      "epoch": 6.6459494213459065,
      "grad_norm": 0.5029847621917725,
      "learning_rate": 1.1138734104872127e-05,
      "loss": 0.616,
      "step": 31010
    },
    {
      "epoch": 6.64809258465495,
      "grad_norm": 0.5147555470466614,
      "learning_rate": 1.1135876553793401e-05,
      "loss": 0.9104,
      "step": 31020
    },
    {
      "epoch": 6.650235747963995,
      "grad_norm": 0.5378507375717163,
      "learning_rate": 1.1133019002714673e-05,
      "loss": 0.6053,
      "step": 31030
    },
    {
      "epoch": 6.652378911273039,
      "grad_norm": 14.741808891296387,
      "learning_rate": 1.1130161451635949e-05,
      "loss": 0.7456,
      "step": 31040
    },
    {
      "epoch": 6.654522074582083,
      "grad_norm": 0.5440813899040222,
      "learning_rate": 1.1127303900557223e-05,
      "loss": 0.4545,
      "step": 31050
    },
    {
      "epoch": 6.6566652378911275,
      "grad_norm": 14.586455345153809,
      "learning_rate": 1.1124446349478497e-05,
      "loss": 0.4561,
      "step": 31060
    },
    {
      "epoch": 6.658808401200171,
      "grad_norm": 0.49706166982650757,
      "learning_rate": 1.1121588798399772e-05,
      "loss": 0.7599,
      "step": 31070
    },
    {
      "epoch": 6.660951564509215,
      "grad_norm": 0.4983178973197937,
      "learning_rate": 1.1118731247321046e-05,
      "loss": 0.4624,
      "step": 31080
    },
    {
      "epoch": 6.66309472781826,
      "grad_norm": 14.599129676818848,
      "learning_rate": 1.111587369624232e-05,
      "loss": 0.6138,
      "step": 31090
    },
    {
      "epoch": 6.665237891127304,
      "grad_norm": 0.5217297673225403,
      "learning_rate": 1.1113016145163596e-05,
      "loss": 1.3656,
      "step": 31100
    },
    {
      "epoch": 6.667381054436348,
      "grad_norm": 14.513480186462402,
      "learning_rate": 1.111015859408487e-05,
      "loss": 0.7463,
      "step": 31110
    },
    {
      "epoch": 6.669524217745392,
      "grad_norm": 0.6073854565620422,
      "learning_rate": 1.1107301043006145e-05,
      "loss": 0.5889,
      "step": 31120
    },
    {
      "epoch": 6.671667381054436,
      "grad_norm": 14.450125694274902,
      "learning_rate": 1.110444349192742e-05,
      "loss": 0.7225,
      "step": 31130
    },
    {
      "epoch": 6.67381054436348,
      "grad_norm": 14.482451438903809,
      "learning_rate": 1.1101585940848693e-05,
      "loss": 0.7203,
      "step": 31140
    },
    {
      "epoch": 6.675953707672525,
      "grad_norm": 0.7149154543876648,
      "learning_rate": 1.1098728389769969e-05,
      "loss": 0.7112,
      "step": 31150
    },
    {
      "epoch": 6.678096870981569,
      "grad_norm": 0.7395686507225037,
      "learning_rate": 1.1095870838691243e-05,
      "loss": 0.4267,
      "step": 31160
    },
    {
      "epoch": 6.6802400342906125,
      "grad_norm": 0.671561062335968,
      "learning_rate": 1.1093013287612517e-05,
      "loss": 0.705,
      "step": 31170
    },
    {
      "epoch": 6.682383197599657,
      "grad_norm": 0.7397298812866211,
      "learning_rate": 1.1090155736533792e-05,
      "loss": 0.7126,
      "step": 31180
    },
    {
      "epoch": 6.684526360908701,
      "grad_norm": 14.581217765808105,
      "learning_rate": 1.1087298185455066e-05,
      "loss": 0.7087,
      "step": 31190
    },
    {
      "epoch": 6.686669524217745,
      "grad_norm": 14.723133087158203,
      "learning_rate": 1.1084440634376342e-05,
      "loss": 0.5735,
      "step": 31200
    },
    {
      "epoch": 6.68881268752679,
      "grad_norm": 14.417036056518555,
      "learning_rate": 1.1081583083297616e-05,
      "loss": 0.8489,
      "step": 31210
    },
    {
      "epoch": 6.6909558508358336,
      "grad_norm": 14.450925827026367,
      "learning_rate": 1.107872553221889e-05,
      "loss": 0.1568,
      "step": 31220
    },
    {
      "epoch": 6.693099014144877,
      "grad_norm": 0.615466296672821,
      "learning_rate": 1.1075867981140165e-05,
      "loss": 0.299,
      "step": 31230
    },
    {
      "epoch": 6.695242177453922,
      "grad_norm": 14.52128791809082,
      "learning_rate": 1.1073010430061437e-05,
      "loss": 1.3147,
      "step": 31240
    },
    {
      "epoch": 6.697385340762966,
      "grad_norm": 0.6729178428649902,
      "learning_rate": 1.1070152878982711e-05,
      "loss": 1.1547,
      "step": 31250
    },
    {
      "epoch": 6.69952850407201,
      "grad_norm": 0.6968092322349548,
      "learning_rate": 1.1067295327903987e-05,
      "loss": 0.7056,
      "step": 31260
    },
    {
      "epoch": 6.701671667381055,
      "grad_norm": 14.426797866821289,
      "learning_rate": 1.1064437776825261e-05,
      "loss": 0.7028,
      "step": 31270
    },
    {
      "epoch": 6.703814830690098,
      "grad_norm": 14.303731918334961,
      "learning_rate": 1.1061580225746535e-05,
      "loss": 0.5605,
      "step": 31280
    },
    {
      "epoch": 6.705957993999142,
      "grad_norm": 0.746498167514801,
      "learning_rate": 1.105872267466781e-05,
      "loss": 0.5611,
      "step": 31290
    },
    {
      "epoch": 6.708101157308187,
      "grad_norm": 0.7825501561164856,
      "learning_rate": 1.1055865123589084e-05,
      "loss": 0.9763,
      "step": 31300
    },
    {
      "epoch": 6.710244320617231,
      "grad_norm": 0.7526268362998962,
      "learning_rate": 1.1053007572510358e-05,
      "loss": 0.2864,
      "step": 31310
    },
    {
      "epoch": 6.712387483926275,
      "grad_norm": 0.6987539529800415,
      "learning_rate": 1.1050150021431634e-05,
      "loss": 0.4254,
      "step": 31320
    },
    {
      "epoch": 6.7145306472353194,
      "grad_norm": 14.460518836975098,
      "learning_rate": 1.1047292470352908e-05,
      "loss": 0.8591,
      "step": 31330
    },
    {
      "epoch": 6.716673810544363,
      "grad_norm": 0.6372271776199341,
      "learning_rate": 1.1044434919274184e-05,
      "loss": 0.1552,
      "step": 31340
    },
    {
      "epoch": 6.718816973853407,
      "grad_norm": 14.49963665008545,
      "learning_rate": 1.1041577368195457e-05,
      "loss": 0.5857,
      "step": 31350
    },
    {
      "epoch": 6.720960137162452,
      "grad_norm": 14.541179656982422,
      "learning_rate": 1.1038719817116731e-05,
      "loss": 0.4521,
      "step": 31360
    },
    {
      "epoch": 6.723103300471496,
      "grad_norm": 0.49874982237815857,
      "learning_rate": 1.1035862266038007e-05,
      "loss": 0.3087,
      "step": 31370
    },
    {
      "epoch": 6.72524646378054,
      "grad_norm": 14.68449592590332,
      "learning_rate": 1.1033004714959281e-05,
      "loss": 0.3162,
      "step": 31380
    },
    {
      "epoch": 6.727389627089584,
      "grad_norm": 29.962478637695312,
      "learning_rate": 1.1030147163880557e-05,
      "loss": 0.6408,
      "step": 31390
    },
    {
      "epoch": 6.729532790398628,
      "grad_norm": 0.4206237196922302,
      "learning_rate": 1.102728961280183e-05,
      "loss": 0.7975,
      "step": 31400
    },
    {
      "epoch": 6.731675953707673,
      "grad_norm": 14.664615631103516,
      "learning_rate": 1.1024432061723104e-05,
      "loss": 0.3206,
      "step": 31410
    },
    {
      "epoch": 6.733819117016717,
      "grad_norm": 15.032720565795898,
      "learning_rate": 1.102157451064438e-05,
      "loss": 0.6313,
      "step": 31420
    },
    {
      "epoch": 6.735962280325761,
      "grad_norm": 14.662027359008789,
      "learning_rate": 1.1018716959565654e-05,
      "loss": 0.4788,
      "step": 31430
    },
    {
      "epoch": 6.738105443634805,
      "grad_norm": 30.017051696777344,
      "learning_rate": 1.1015859408486928e-05,
      "loss": 0.9382,
      "step": 31440
    },
    {
      "epoch": 6.740248606943849,
      "grad_norm": 0.48739904165267944,
      "learning_rate": 1.1013001857408204e-05,
      "loss": 0.7841,
      "step": 31450
    },
    {
      "epoch": 6.742391770252893,
      "grad_norm": 0.5115975141525269,
      "learning_rate": 1.1010144306329476e-05,
      "loss": 1.0614,
      "step": 31460
    },
    {
      "epoch": 6.744534933561938,
      "grad_norm": 0.5257656574249268,
      "learning_rate": 1.100728675525075e-05,
      "loss": 0.4559,
      "step": 31470
    },
    {
      "epoch": 6.746678096870982,
      "grad_norm": 14.551802635192871,
      "learning_rate": 1.1004429204172025e-05,
      "loss": 0.7547,
      "step": 31480
    },
    {
      "epoch": 6.7488212601800255,
      "grad_norm": 0.500531017780304,
      "learning_rate": 1.10015716530933e-05,
      "loss": 0.4612,
      "step": 31490
    },
    {
      "epoch": 6.75096442348907,
      "grad_norm": 0.5096874833106995,
      "learning_rate": 1.0998714102014573e-05,
      "loss": 0.7576,
      "step": 31500
    },
    {
      "epoch": 6.753107586798114,
      "grad_norm": 0.5230835676193237,
      "learning_rate": 1.0995856550935849e-05,
      "loss": 0.4603,
      "step": 31510
    },
    {
      "epoch": 6.755250750107158,
      "grad_norm": 0.5001581311225891,
      "learning_rate": 1.0992998999857123e-05,
      "loss": 0.9069,
      "step": 31520
    },
    {
      "epoch": 6.757393913416203,
      "grad_norm": 0.49575158953666687,
      "learning_rate": 1.0990141448778398e-05,
      "loss": 0.7601,
      "step": 31530
    },
    {
      "epoch": 6.7595370767252465,
      "grad_norm": 0.46975278854370117,
      "learning_rate": 1.0987283897699672e-05,
      "loss": 0.1608,
      "step": 31540
    },
    {
      "epoch": 6.76168024003429,
      "grad_norm": 14.620292663574219,
      "learning_rate": 1.0984426346620946e-05,
      "loss": 0.6177,
      "step": 31550
    },
    {
      "epoch": 6.763823403343335,
      "grad_norm": 0.4550960659980774,
      "learning_rate": 1.0981568795542222e-05,
      "loss": 0.6234,
      "step": 31560
    },
    {
      "epoch": 6.765966566652379,
      "grad_norm": 14.652262687683105,
      "learning_rate": 1.0978711244463496e-05,
      "loss": 0.7712,
      "step": 31570
    },
    {
      "epoch": 6.768109729961423,
      "grad_norm": 14.67339038848877,
      "learning_rate": 1.097585369338477e-05,
      "loss": 0.4696,
      "step": 31580
    },
    {
      "epoch": 6.7702528932704675,
      "grad_norm": 0.4131240248680115,
      "learning_rate": 1.0972996142306045e-05,
      "loss": 0.3193,
      "step": 31590
    },
    {
      "epoch": 6.772396056579511,
      "grad_norm": 14.807334899902344,
      "learning_rate": 1.097013859122732e-05,
      "loss": 0.9364,
      "step": 31600
    },
    {
      "epoch": 6.774539219888555,
      "grad_norm": 14.569637298583984,
      "learning_rate": 1.0967281040148595e-05,
      "loss": 1.0757,
      "step": 31610
    },
    {
      "epoch": 6.7766823831976,
      "grad_norm": 0.5842368602752686,
      "learning_rate": 1.0964423489069869e-05,
      "loss": 1.3345,
      "step": 31620
    },
    {
      "epoch": 6.778825546506644,
      "grad_norm": 0.6477381587028503,
      "learning_rate": 1.0961565937991143e-05,
      "loss": 0.4457,
      "step": 31630
    },
    {
      "epoch": 6.780968709815688,
      "grad_norm": 0.6489484906196594,
      "learning_rate": 1.0958708386912418e-05,
      "loss": 1.0066,
      "step": 31640
    },
    {
      "epoch": 6.783111873124732,
      "grad_norm": 14.392048835754395,
      "learning_rate": 1.0955850835833692e-05,
      "loss": 0.8522,
      "step": 31650
    },
    {
      "epoch": 6.785255036433776,
      "grad_norm": 14.415271759033203,
      "learning_rate": 1.0952993284754966e-05,
      "loss": 0.2939,
      "step": 31660
    },
    {
      "epoch": 6.78739819974282,
      "grad_norm": 0.6570786237716675,
      "learning_rate": 1.095013573367624e-05,
      "loss": 0.575,
      "step": 31670
    },
    {
      "epoch": 6.789541363051865,
      "grad_norm": 14.458857536315918,
      "learning_rate": 1.0947278182597514e-05,
      "loss": 1.1422,
      "step": 31680
    },
    {
      "epoch": 6.791684526360909,
      "grad_norm": 0.6968400478363037,
      "learning_rate": 1.0944420631518788e-05,
      "loss": 0.7152,
      "step": 31690
    },
    {
      "epoch": 6.7938276896699525,
      "grad_norm": 0.711898684501648,
      "learning_rate": 1.0941563080440064e-05,
      "loss": 0.5685,
      "step": 31700
    },
    {
      "epoch": 6.795970852978997,
      "grad_norm": 0.7317676544189453,
      "learning_rate": 1.0938705529361338e-05,
      "loss": 1.1212,
      "step": 31710
    },
    {
      "epoch": 6.798114016288041,
      "grad_norm": 14.39787483215332,
      "learning_rate": 1.0935847978282611e-05,
      "loss": 0.1549,
      "step": 31720
    },
    {
      "epoch": 6.800257179597085,
      "grad_norm": 14.420385360717773,
      "learning_rate": 1.0932990427203887e-05,
      "loss": 0.7149,
      "step": 31730
    },
    {
      "epoch": 6.80240034290613,
      "grad_norm": 14.458334922790527,
      "learning_rate": 1.0930132876125161e-05,
      "loss": 0.4374,
      "step": 31740
    },
    {
      "epoch": 6.804543506215174,
      "grad_norm": 29.50093650817871,
      "learning_rate": 1.0927275325046437e-05,
      "loss": 0.721,
      "step": 31750
    },
    {
      "epoch": 6.806686669524217,
      "grad_norm": 14.431879043579102,
      "learning_rate": 1.092441777396771e-05,
      "loss": 1.0042,
      "step": 31760
    },
    {
      "epoch": 6.808829832833262,
      "grad_norm": 0.6963117122650146,
      "learning_rate": 1.0921560222888984e-05,
      "loss": 0.2922,
      "step": 31770
    },
    {
      "epoch": 6.810972996142306,
      "grad_norm": 0.7732232809066772,
      "learning_rate": 1.091870267181026e-05,
      "loss": 0.5597,
      "step": 31780
    },
    {
      "epoch": 6.81311615945135,
      "grad_norm": 14.321420669555664,
      "learning_rate": 1.0915845120731534e-05,
      "loss": 0.5586,
      "step": 31790
    },
    {
      "epoch": 6.815259322760395,
      "grad_norm": 14.565818786621094,
      "learning_rate": 1.0912987569652808e-05,
      "loss": 0.6948,
      "step": 31800
    },
    {
      "epoch": 6.817402486069438,
      "grad_norm": 0.7595215439796448,
      "learning_rate": 1.0910130018574084e-05,
      "loss": 0.4233,
      "step": 31810
    },
    {
      "epoch": 6.819545649378482,
      "grad_norm": 0.6381904482841492,
      "learning_rate": 1.0907272467495357e-05,
      "loss": 0.1567,
      "step": 31820
    },
    {
      "epoch": 6.821688812687527,
      "grad_norm": 0.5629293322563171,
      "learning_rate": 1.0904414916416633e-05,
      "loss": 0.4486,
      "step": 31830
    },
    {
      "epoch": 6.823831975996571,
      "grad_norm": 0.4915068447589874,
      "learning_rate": 1.0901557365337907e-05,
      "loss": 0.0115,
      "step": 31840
    },
    {
      "epoch": 6.825975139305615,
      "grad_norm": 0.44551461935043335,
      "learning_rate": 1.0898699814259181e-05,
      "loss": 0.4738,
      "step": 31850
    },
    {
      "epoch": 6.8281183026146595,
      "grad_norm": 0.42288938164711,
      "learning_rate": 1.0895842263180457e-05,
      "loss": 0.6316,
      "step": 31860
    },
    {
      "epoch": 6.830261465923703,
      "grad_norm": 0.4293762147426605,
      "learning_rate": 1.089298471210173e-05,
      "loss": 0.3189,
      "step": 31870
    },
    {
      "epoch": 6.832404629232747,
      "grad_norm": 0.38965436816215515,
      "learning_rate": 1.0890127161023004e-05,
      "loss": 0.3309,
      "step": 31880
    },
    {
      "epoch": 6.834547792541792,
      "grad_norm": 0.36575403809547424,
      "learning_rate": 1.0887269609944278e-05,
      "loss": 0.3243,
      "step": 31890
    },
    {
      "epoch": 6.836690955850836,
      "grad_norm": 0.3440539240837097,
      "learning_rate": 1.0884412058865552e-05,
      "loss": 0.4958,
      "step": 31900
    },
    {
      "epoch": 6.83883411915988,
      "grad_norm": 14.773527145385742,
      "learning_rate": 1.0881554507786826e-05,
      "loss": 0.822,
      "step": 31910
    },
    {
      "epoch": 6.840977282468924,
      "grad_norm": 14.715008735656738,
      "learning_rate": 1.0878696956708102e-05,
      "loss": 0.4919,
      "step": 31920
    },
    {
      "epoch": 6.843120445777968,
      "grad_norm": 0.3854677677154541,
      "learning_rate": 1.0875839405629376e-05,
      "loss": 0.487,
      "step": 31930
    },
    {
      "epoch": 6.845263609087013,
      "grad_norm": 14.737866401672363,
      "learning_rate": 1.087298185455065e-05,
      "loss": 1.119,
      "step": 31940
    },
    {
      "epoch": 6.847406772396057,
      "grad_norm": 0.4433998465538025,
      "learning_rate": 1.0870124303471925e-05,
      "loss": 0.6342,
      "step": 31950
    },
    {
      "epoch": 6.849549935705101,
      "grad_norm": 14.645251274108887,
      "learning_rate": 1.08672667523932e-05,
      "loss": 0.7781,
      "step": 31960
    },
    {
      "epoch": 6.851693099014145,
      "grad_norm": 14.516071319580078,
      "learning_rate": 1.0864409201314475e-05,
      "loss": 1.2143,
      "step": 31970
    },
    {
      "epoch": 6.853836262323189,
      "grad_norm": 14.50899600982666,
      "learning_rate": 1.0861551650235749e-05,
      "loss": 1.0198,
      "step": 31980
    },
    {
      "epoch": 6.855979425632233,
      "grad_norm": 0.6952289938926697,
      "learning_rate": 1.0858694099157023e-05,
      "loss": 0.4352,
      "step": 31990
    },
    {
      "epoch": 6.858122588941278,
      "grad_norm": 29.498136520385742,
      "learning_rate": 1.0855836548078298e-05,
      "loss": 0.8456,
      "step": 32000
    },
    {
      "epoch": 6.860265752250322,
      "grad_norm": 14.42760181427002,
      "learning_rate": 1.0852978996999572e-05,
      "loss": 0.2965,
      "step": 32010
    },
    {
      "epoch": 6.8624089155593655,
      "grad_norm": 14.46749210357666,
      "learning_rate": 1.0850121445920846e-05,
      "loss": 0.8631,
      "step": 32020
    },
    {
      "epoch": 6.86455207886841,
      "grad_norm": 0.6918096542358398,
      "learning_rate": 1.0847263894842122e-05,
      "loss": 0.8541,
      "step": 32030
    },
    {
      "epoch": 6.866695242177454,
      "grad_norm": 14.33435344696045,
      "learning_rate": 1.0844406343763396e-05,
      "loss": 0.9779,
      "step": 32040
    },
    {
      "epoch": 6.868838405486498,
      "grad_norm": 14.254653930664062,
      "learning_rate": 1.0841548792684671e-05,
      "loss": 0.5538,
      "step": 32050
    },
    {
      "epoch": 6.870981568795543,
      "grad_norm": 0.7874337434768677,
      "learning_rate": 1.0838691241605945e-05,
      "loss": 0.8244,
      "step": 32060
    },
    {
      "epoch": 6.8731247321045865,
      "grad_norm": 14.413983345031738,
      "learning_rate": 1.083583369052722e-05,
      "loss": 0.555,
      "step": 32070
    },
    {
      "epoch": 6.87526789541363,
      "grad_norm": 0.7371510863304138,
      "learning_rate": 1.0832976139448495e-05,
      "loss": 0.5625,
      "step": 32080
    },
    {
      "epoch": 6.877411058722675,
      "grad_norm": 0.7328317165374756,
      "learning_rate": 1.0830118588369769e-05,
      "loss": 0.8415,
      "step": 32090
    },
    {
      "epoch": 6.879554222031719,
      "grad_norm": 0.6936460733413696,
      "learning_rate": 1.0827261037291041e-05,
      "loss": 0.2924,
      "step": 32100
    },
    {
      "epoch": 6.881697385340763,
      "grad_norm": 0.6932631134986877,
      "learning_rate": 1.0824403486212317e-05,
      "loss": 0.5733,
      "step": 32110
    },
    {
      "epoch": 6.8838405486498075,
      "grad_norm": 0.6117159128189087,
      "learning_rate": 1.082154593513359e-05,
      "loss": 0.723,
      "step": 32120
    },
    {
      "epoch": 6.885983711958851,
      "grad_norm": 0.6018141508102417,
      "learning_rate": 1.0818688384054865e-05,
      "loss": 0.5841,
      "step": 32130
    },
    {
      "epoch": 6.888126875267895,
      "grad_norm": 14.715956687927246,
      "learning_rate": 1.081583083297614e-05,
      "loss": 0.5908,
      "step": 32140
    },
    {
      "epoch": 6.89027003857694,
      "grad_norm": 0.5942884683609009,
      "learning_rate": 1.0812973281897414e-05,
      "loss": 0.5902,
      "step": 32150
    },
    {
      "epoch": 6.892413201885984,
      "grad_norm": 15.078630447387695,
      "learning_rate": 1.0810115730818688e-05,
      "loss": 0.4499,
      "step": 32160
    },
    {
      "epoch": 6.894556365195028,
      "grad_norm": 0.5779976844787598,
      "learning_rate": 1.0807258179739964e-05,
      "loss": 0.4446,
      "step": 32170
    },
    {
      "epoch": 6.896699528504072,
      "grad_norm": 0.4586614966392517,
      "learning_rate": 1.0804400628661238e-05,
      "loss": 0.619,
      "step": 32180
    },
    {
      "epoch": 6.898842691813116,
      "grad_norm": 0.46838071942329407,
      "learning_rate": 1.0801543077582513e-05,
      "loss": 0.6236,
      "step": 32190
    },
    {
      "epoch": 6.90098585512216,
      "grad_norm": 14.601085662841797,
      "learning_rate": 1.0798685526503787e-05,
      "loss": 0.9222,
      "step": 32200
    },
    {
      "epoch": 6.903129018431205,
      "grad_norm": 14.631070137023926,
      "learning_rate": 1.0795827975425061e-05,
      "loss": 0.597,
      "step": 32210
    },
    {
      "epoch": 6.905272181740249,
      "grad_norm": 14.443686485290527,
      "learning_rate": 1.0792970424346337e-05,
      "loss": 1.1534,
      "step": 32220
    },
    {
      "epoch": 6.9074153450492926,
      "grad_norm": 0.6553699374198914,
      "learning_rate": 1.079011287326761e-05,
      "loss": 0.2967,
      "step": 32230
    },
    {
      "epoch": 6.909558508358337,
      "grad_norm": 0.6912088394165039,
      "learning_rate": 1.0787255322188886e-05,
      "loss": 1.416,
      "step": 32240
    },
    {
      "epoch": 6.911701671667381,
      "grad_norm": 0.7301645278930664,
      "learning_rate": 1.078439777111016e-05,
      "loss": 0.7033,
      "step": 32250
    },
    {
      "epoch": 6.913844834976425,
      "grad_norm": 14.330607414245605,
      "learning_rate": 1.0781540220031434e-05,
      "loss": 0.8259,
      "step": 32260
    },
    {
      "epoch": 6.91598799828547,
      "grad_norm": 0.7293410897254944,
      "learning_rate": 1.077868266895271e-05,
      "loss": 0.1531,
      "step": 32270
    },
    {
      "epoch": 6.918131161594514,
      "grad_norm": 0.6792552471160889,
      "learning_rate": 1.0775825117873984e-05,
      "loss": 0.431,
      "step": 32280
    },
    {
      "epoch": 6.920274324903557,
      "grad_norm": 14.433536529541016,
      "learning_rate": 1.0772967566795258e-05,
      "loss": 0.7191,
      "step": 32290
    },
    {
      "epoch": 6.922417488212602,
      "grad_norm": 29.593490600585938,
      "learning_rate": 1.0770110015716533e-05,
      "loss": 0.7233,
      "step": 32300
    },
    {
      "epoch": 6.924560651521646,
      "grad_norm": 0.6295722126960754,
      "learning_rate": 1.0767252464637807e-05,
      "loss": 0.4395,
      "step": 32310
    },
    {
      "epoch": 6.92670381483069,
      "grad_norm": 0.5856940150260925,
      "learning_rate": 1.076439491355908e-05,
      "loss": 0.3017,
      "step": 32320
    },
    {
      "epoch": 6.928846978139735,
      "grad_norm": 0.5483117699623108,
      "learning_rate": 1.0761537362480355e-05,
      "loss": 0.4502,
      "step": 32330
    },
    {
      "epoch": 6.9309901414487785,
      "grad_norm": 0.5336937308311462,
      "learning_rate": 1.0758679811401629e-05,
      "loss": 0.8963,
      "step": 32340
    },
    {
      "epoch": 6.933133304757822,
      "grad_norm": 0.5298064351081848,
      "learning_rate": 1.0755822260322903e-05,
      "loss": 0.3047,
      "step": 32350
    },
    {
      "epoch": 6.935276468066867,
      "grad_norm": 0.5059402585029602,
      "learning_rate": 1.0752964709244178e-05,
      "loss": 0.6028,
      "step": 32360
    },
    {
      "epoch": 6.937419631375911,
      "grad_norm": 14.5745267868042,
      "learning_rate": 1.0750107158165452e-05,
      "loss": 0.6037,
      "step": 32370
    },
    {
      "epoch": 6.939562794684955,
      "grad_norm": 14.553999900817871,
      "learning_rate": 1.0747249607086728e-05,
      "loss": 0.4614,
      "step": 32380
    },
    {
      "epoch": 6.9417059579939995,
      "grad_norm": 29.62122344970703,
      "learning_rate": 1.0744392056008002e-05,
      "loss": 0.7608,
      "step": 32390
    },
    {
      "epoch": 6.943849121303043,
      "grad_norm": 0.5118979811668396,
      "learning_rate": 1.0741534504929276e-05,
      "loss": 0.4593,
      "step": 32400
    },
    {
      "epoch": 6.945992284612087,
      "grad_norm": 0.5457995533943176,
      "learning_rate": 1.0738676953850551e-05,
      "loss": 0.8993,
      "step": 32410
    },
    {
      "epoch": 6.948135447921132,
      "grad_norm": 0.5316317677497864,
      "learning_rate": 1.0735819402771825e-05,
      "loss": 0.1598,
      "step": 32420
    },
    {
      "epoch": 6.950278611230176,
      "grad_norm": 14.579716682434082,
      "learning_rate": 1.07329618516931e-05,
      "loss": 0.6071,
      "step": 32430
    },
    {
      "epoch": 6.95242177453922,
      "grad_norm": 14.738969802856445,
      "learning_rate": 1.0730104300614375e-05,
      "loss": 0.6121,
      "step": 32440
    },
    {
      "epoch": 6.954564937848264,
      "grad_norm": 14.880030632019043,
      "learning_rate": 1.0727246749535649e-05,
      "loss": 1.2176,
      "step": 32450
    },
    {
      "epoch": 6.956708101157308,
      "grad_norm": 0.5248299241065979,
      "learning_rate": 1.0724389198456924e-05,
      "loss": 0.1597,
      "step": 32460
    },
    {
      "epoch": 6.958851264466352,
      "grad_norm": 14.843894958496094,
      "learning_rate": 1.0721531647378198e-05,
      "loss": 0.6103,
      "step": 32470
    },
    {
      "epoch": 6.960994427775397,
      "grad_norm": 0.48398396372795105,
      "learning_rate": 1.0718674096299472e-05,
      "loss": 0.3146,
      "step": 32480
    },
    {
      "epoch": 6.963137591084441,
      "grad_norm": 0.4444429576396942,
      "learning_rate": 1.0715816545220748e-05,
      "loss": 0.4675,
      "step": 32490
    },
    {
      "epoch": 6.9652807543934845,
      "grad_norm": 14.699539184570312,
      "learning_rate": 1.0712958994142022e-05,
      "loss": 0.4757,
      "step": 32500
    },
    {
      "epoch": 6.967423917702529,
      "grad_norm": 29.695283889770508,
      "learning_rate": 1.0710101443063296e-05,
      "loss": 0.9461,
      "step": 32510
    },
    {
      "epoch": 6.969567081011573,
      "grad_norm": 14.680415153503418,
      "learning_rate": 1.0707243891984571e-05,
      "loss": 0.4733,
      "step": 32520
    },
    {
      "epoch": 6.971710244320617,
      "grad_norm": 0.43660759925842285,
      "learning_rate": 1.0704386340905844e-05,
      "loss": 0.6284,
      "step": 32530
    },
    {
      "epoch": 6.973853407629662,
      "grad_norm": 0.44465896487236023,
      "learning_rate": 1.0701528789827118e-05,
      "loss": 0.6311,
      "step": 32540
    },
    {
      "epoch": 6.9759965709387055,
      "grad_norm": 14.6254301071167,
      "learning_rate": 1.0698671238748393e-05,
      "loss": 0.7731,
      "step": 32550
    },
    {
      "epoch": 6.978139734247749,
      "grad_norm": 14.831594467163086,
      "learning_rate": 1.0695813687669667e-05,
      "loss": 1.0746,
      "step": 32560
    },
    {
      "epoch": 6.980282897556794,
      "grad_norm": 0.5385935306549072,
      "learning_rate": 1.0692956136590941e-05,
      "loss": 0.4583,
      "step": 32570
    },
    {
      "epoch": 6.982426060865838,
      "grad_norm": 0.5287162065505981,
      "learning_rate": 1.0690098585512217e-05,
      "loss": 0.905,
      "step": 32580
    },
    {
      "epoch": 6.984569224174882,
      "grad_norm": 0.5119341015815735,
      "learning_rate": 1.068724103443349e-05,
      "loss": 0.1595,
      "step": 32590
    },
    {
      "epoch": 6.9867123874839265,
      "grad_norm": 0.44522419571876526,
      "learning_rate": 1.0684383483354766e-05,
      "loss": 0.4701,
      "step": 32600
    },
    {
      "epoch": 6.98885555079297,
      "grad_norm": 14.755678176879883,
      "learning_rate": 1.068152593227604e-05,
      "loss": 0.4767,
      "step": 32610
    },
    {
      "epoch": 6.990998714102014,
      "grad_norm": 0.36715760827064514,
      "learning_rate": 1.0678668381197314e-05,
      "loss": 0.4885,
      "step": 32620
    },
    {
      "epoch": 6.993141877411059,
      "grad_norm": 0.3403428792953491,
      "learning_rate": 1.067581083011859e-05,
      "loss": 0.1671,
      "step": 32630
    },
    {
      "epoch": 6.995285040720103,
      "grad_norm": 0.32372158765792847,
      "learning_rate": 1.0672953279039864e-05,
      "loss": 0.5005,
      "step": 32640
    },
    {
      "epoch": 6.997428204029147,
      "grad_norm": 0.3269159495830536,
      "learning_rate": 1.0670095727961138e-05,
      "loss": 0.5017,
      "step": 32650
    },
    {
      "epoch": 6.999571367338191,
      "grad_norm": 0.33064770698547363,
      "learning_rate": 1.0667238176882413e-05,
      "loss": 0.4981,
      "step": 32660
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.8676666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.655095636844635,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 396.6899,
      "eval_samples_per_second": 7.563,
      "eval_steps_per_second": 2.521,
      "step": 32662
    },
    {
      "epoch": 7.001714530647235,
      "grad_norm": 0.34857869148254395,
      "learning_rate": 1.0664380625803687e-05,
      "loss": 0.8234,
      "step": 32670
    },
    {
      "epoch": 7.003857693956279,
      "grad_norm": 29.885066986083984,
      "learning_rate": 1.0661523074724963e-05,
      "loss": 0.6566,
      "step": 32680
    },
    {
      "epoch": 7.006000857265324,
      "grad_norm": 0.36022254824638367,
      "learning_rate": 1.0658665523646237e-05,
      "loss": 0.4863,
      "step": 32690
    },
    {
      "epoch": 7.008144020574368,
      "grad_norm": 0.43979379534721375,
      "learning_rate": 1.065580797256751e-05,
      "loss": 0.7956,
      "step": 32700
    },
    {
      "epoch": 7.0102871838834115,
      "grad_norm": 0.4568748474121094,
      "learning_rate": 1.0652950421488786e-05,
      "loss": 0.4732,
      "step": 32710
    },
    {
      "epoch": 7.012430347192456,
      "grad_norm": 0.446137011051178,
      "learning_rate": 1.065009287041006e-05,
      "loss": 0.7721,
      "step": 32720
    },
    {
      "epoch": 7.0145735105015,
      "grad_norm": 0.4584719240665436,
      "learning_rate": 1.0647235319331334e-05,
      "loss": 0.6217,
      "step": 32730
    },
    {
      "epoch": 7.016716673810544,
      "grad_norm": 0.4945087432861328,
      "learning_rate": 1.064437776825261e-05,
      "loss": 0.9231,
      "step": 32740
    },
    {
      "epoch": 7.018859837119589,
      "grad_norm": 30.58045196533203,
      "learning_rate": 1.0641520217173882e-05,
      "loss": 0.9058,
      "step": 32750
    },
    {
      "epoch": 7.021003000428633,
      "grad_norm": 14.478509902954102,
      "learning_rate": 1.0638662666095156e-05,
      "loss": 1.1745,
      "step": 32760
    },
    {
      "epoch": 7.023146163737676,
      "grad_norm": 0.6310628056526184,
      "learning_rate": 1.0635805115016431e-05,
      "loss": 0.5833,
      "step": 32770
    },
    {
      "epoch": 7.025289327046721,
      "grad_norm": 0.6653047204017639,
      "learning_rate": 1.0632947563937705e-05,
      "loss": 0.7237,
      "step": 32780
    },
    {
      "epoch": 7.027432490355765,
      "grad_norm": 0.6183890700340271,
      "learning_rate": 1.063009001285898e-05,
      "loss": 0.7243,
      "step": 32790
    },
    {
      "epoch": 7.029575653664809,
      "grad_norm": 0.6149488687515259,
      "learning_rate": 1.0627232461780255e-05,
      "loss": 0.5815,
      "step": 32800
    },
    {
      "epoch": 7.031718816973854,
      "grad_norm": 14.435009956359863,
      "learning_rate": 1.0624374910701529e-05,
      "loss": 1.2932,
      "step": 32810
    },
    {
      "epoch": 7.033861980282897,
      "grad_norm": 14.61939811706543,
      "learning_rate": 1.0621517359622805e-05,
      "loss": 0.5778,
      "step": 32820
    },
    {
      "epoch": 7.036005143591941,
      "grad_norm": 0.6383088827133179,
      "learning_rate": 1.0618659808544078e-05,
      "loss": 0.1555,
      "step": 32830
    },
    {
      "epoch": 7.038148306900986,
      "grad_norm": 0.6162306666374207,
      "learning_rate": 1.0615802257465352e-05,
      "loss": 0.4439,
      "step": 32840
    },
    {
      "epoch": 7.04029147021003,
      "grad_norm": 0.5560113191604614,
      "learning_rate": 1.0612944706386628e-05,
      "loss": 0.1581,
      "step": 32850
    },
    {
      "epoch": 7.042434633519074,
      "grad_norm": 0.5180033445358276,
      "learning_rate": 1.0610087155307902e-05,
      "loss": 0.7502,
      "step": 32860
    },
    {
      "epoch": 7.0445777968281185,
      "grad_norm": 0.5114198327064514,
      "learning_rate": 1.0607229604229176e-05,
      "loss": 0.6057,
      "step": 32870
    },
    {
      "epoch": 7.046720960137162,
      "grad_norm": 0.5197571516036987,
      "learning_rate": 1.0604372053150451e-05,
      "loss": 0.761,
      "step": 32880
    },
    {
      "epoch": 7.048864123446206,
      "grad_norm": 0.47920385003089905,
      "learning_rate": 1.0601514502071725e-05,
      "loss": 0.011,
      "step": 32890
    },
    {
      "epoch": 7.051007286755251,
      "grad_norm": 0.4545826017856598,
      "learning_rate": 1.0598656950993001e-05,
      "loss": 0.7831,
      "step": 32900
    },
    {
      "epoch": 7.053150450064295,
      "grad_norm": 0.43768125772476196,
      "learning_rate": 1.0595799399914275e-05,
      "loss": 0.3168,
      "step": 32910
    },
    {
      "epoch": 7.055293613373339,
      "grad_norm": 0.409318745136261,
      "learning_rate": 1.0592941848835549e-05,
      "loss": 0.3195,
      "step": 32920
    },
    {
      "epoch": 7.057436776682383,
      "grad_norm": 0.4388670027256012,
      "learning_rate": 1.0590084297756825e-05,
      "loss": 0.7886,
      "step": 32930
    },
    {
      "epoch": 7.059579939991427,
      "grad_norm": 0.4260040521621704,
      "learning_rate": 1.0587226746678098e-05,
      "loss": 0.6284,
      "step": 32940
    },
    {
      "epoch": 7.061723103300472,
      "grad_norm": 0.4040800929069519,
      "learning_rate": 1.0584369195599374e-05,
      "loss": 0.32,
      "step": 32950
    },
    {
      "epoch": 7.063866266609516,
      "grad_norm": 0.4207712411880493,
      "learning_rate": 1.0581511644520646e-05,
      "loss": 0.7961,
      "step": 32960
    },
    {
      "epoch": 7.06600942991856,
      "grad_norm": 0.43730440735816956,
      "learning_rate": 1.057865409344192e-05,
      "loss": 0.4818,
      "step": 32970
    },
    {
      "epoch": 7.068152593227604,
      "grad_norm": 0.441611111164093,
      "learning_rate": 1.0575796542363194e-05,
      "loss": 0.6345,
      "step": 32980
    },
    {
      "epoch": 7.070295756536648,
      "grad_norm": 14.838072776794434,
      "learning_rate": 1.057293899128447e-05,
      "loss": 0.625,
      "step": 32990
    },
    {
      "epoch": 7.072438919845692,
      "grad_norm": 14.62348461151123,
      "learning_rate": 1.0570081440205744e-05,
      "loss": 0.4707,
      "step": 33000
    },
    {
      "epoch": 7.074582083154737,
      "grad_norm": 0.4554137885570526,
      "learning_rate": 1.0567223889127018e-05,
      "loss": 0.3121,
      "step": 33010
    },
    {
      "epoch": 7.076725246463781,
      "grad_norm": 0.446525514125824,
      "learning_rate": 1.0564366338048293e-05,
      "loss": 0.3191,
      "step": 33020
    },
    {
      "epoch": 7.0788684097728245,
      "grad_norm": 0.4214971959590912,
      "learning_rate": 1.0561508786969567e-05,
      "loss": 0.3183,
      "step": 33030
    },
    {
      "epoch": 7.081011573081869,
      "grad_norm": 0.4152643382549286,
      "learning_rate": 1.0558651235890843e-05,
      "loss": 0.6354,
      "step": 33040
    },
    {
      "epoch": 7.083154736390913,
      "grad_norm": 0.41903507709503174,
      "learning_rate": 1.0555793684812117e-05,
      "loss": 0.4789,
      "step": 33050
    },
    {
      "epoch": 7.085297899699957,
      "grad_norm": 17.130346298217773,
      "learning_rate": 1.055293613373339e-05,
      "loss": 1.2558,
      "step": 33060
    },
    {
      "epoch": 7.087441063009002,
      "grad_norm": 14.634671211242676,
      "learning_rate": 1.0550078582654666e-05,
      "loss": 0.6247,
      "step": 33070
    },
    {
      "epoch": 7.0895842263180455,
      "grad_norm": 0.47028374671936035,
      "learning_rate": 1.054722103157594e-05,
      "loss": 0.6205,
      "step": 33080
    },
    {
      "epoch": 7.091727389627089,
      "grad_norm": 0.4959961771965027,
      "learning_rate": 1.0544363480497216e-05,
      "loss": 0.6151,
      "step": 33090
    },
    {
      "epoch": 7.093870552936134,
      "grad_norm": 14.570745468139648,
      "learning_rate": 1.054150592941849e-05,
      "loss": 0.6091,
      "step": 33100
    },
    {
      "epoch": 7.096013716245178,
      "grad_norm": 0.5133154988288879,
      "learning_rate": 1.0538648378339764e-05,
      "loss": 0.4602,
      "step": 33110
    },
    {
      "epoch": 7.098156879554222,
      "grad_norm": 0.49989035725593567,
      "learning_rate": 1.053579082726104e-05,
      "loss": 0.4607,
      "step": 33120
    },
    {
      "epoch": 7.1003000428632665,
      "grad_norm": 0.5240222215652466,
      "learning_rate": 1.0532933276182313e-05,
      "loss": 0.757,
      "step": 33130
    },
    {
      "epoch": 7.10244320617231,
      "grad_norm": 14.710262298583984,
      "learning_rate": 1.0530075725103587e-05,
      "loss": 0.453,
      "step": 33140
    },
    {
      "epoch": 7.104586369481354,
      "grad_norm": 0.5431733727455139,
      "learning_rate": 1.0527218174024863e-05,
      "loss": 0.6001,
      "step": 33150
    },
    {
      "epoch": 7.106729532790399,
      "grad_norm": 0.49941352009773254,
      "learning_rate": 1.0524360622946137e-05,
      "loss": 0.1625,
      "step": 33160
    },
    {
      "epoch": 7.108872696099443,
      "grad_norm": 0.488761305809021,
      "learning_rate": 1.0521503071867412e-05,
      "loss": 0.6126,
      "step": 33170
    },
    {
      "epoch": 7.111015859408487,
      "grad_norm": 0.46220988035202026,
      "learning_rate": 1.0518645520788685e-05,
      "loss": 0.1621,
      "step": 33180
    },
    {
      "epoch": 7.113159022717531,
      "grad_norm": 0.4516366720199585,
      "learning_rate": 1.0515787969709959e-05,
      "loss": 0.3155,
      "step": 33190
    },
    {
      "epoch": 7.115302186026575,
      "grad_norm": 0.37702232599258423,
      "learning_rate": 1.0512930418631232e-05,
      "loss": 0.3237,
      "step": 33200
    },
    {
      "epoch": 7.117445349335619,
      "grad_norm": 0.40729236602783203,
      "learning_rate": 1.0510072867552508e-05,
      "loss": 1.1134,
      "step": 33210
    },
    {
      "epoch": 7.119588512644664,
      "grad_norm": 14.889466285705566,
      "learning_rate": 1.0507215316473782e-05,
      "loss": 0.7905,
      "step": 33220
    },
    {
      "epoch": 7.121731675953708,
      "grad_norm": 0.4345203936100006,
      "learning_rate": 1.0504357765395058e-05,
      "loss": 0.9411,
      "step": 33230
    },
    {
      "epoch": 7.123874839262752,
      "grad_norm": 14.621676445007324,
      "learning_rate": 1.0501500214316332e-05,
      "loss": 1.0792,
      "step": 33240
    },
    {
      "epoch": 7.126018002571796,
      "grad_norm": 14.611727714538574,
      "learning_rate": 1.0498642663237605e-05,
      "loss": 0.1621,
      "step": 33250
    },
    {
      "epoch": 7.12816116588084,
      "grad_norm": 0.4681952893733978,
      "learning_rate": 1.0495785112158881e-05,
      "loss": 0.465,
      "step": 33260
    },
    {
      "epoch": 7.130304329189884,
      "grad_norm": 0.4649534523487091,
      "learning_rate": 1.0492927561080155e-05,
      "loss": 0.6179,
      "step": 33270
    },
    {
      "epoch": 7.132447492498929,
      "grad_norm": 0.4871026277542114,
      "learning_rate": 1.0490070010001429e-05,
      "loss": 0.6163,
      "step": 33280
    },
    {
      "epoch": 7.134590655807973,
      "grad_norm": 0.5119419097900391,
      "learning_rate": 1.0487212458922705e-05,
      "loss": 0.9135,
      "step": 33290
    },
    {
      "epoch": 7.136733819117016,
      "grad_norm": 14.799492835998535,
      "learning_rate": 1.0484354907843978e-05,
      "loss": 0.4604,
      "step": 33300
    },
    {
      "epoch": 7.138876982426061,
      "grad_norm": 0.45587918162345886,
      "learning_rate": 1.0481497356765254e-05,
      "loss": 0.3144,
      "step": 33310
    },
    {
      "epoch": 7.141020145735105,
      "grad_norm": 14.71448040008545,
      "learning_rate": 1.0478639805686528e-05,
      "loss": 0.4729,
      "step": 33320
    },
    {
      "epoch": 7.143163309044149,
      "grad_norm": 0.4424292743206024,
      "learning_rate": 1.0475782254607802e-05,
      "loss": 0.476,
      "step": 33330
    },
    {
      "epoch": 7.145306472353194,
      "grad_norm": 14.623592376708984,
      "learning_rate": 1.0472924703529078e-05,
      "loss": 0.7817,
      "step": 33340
    },
    {
      "epoch": 7.1474496356622375,
      "grad_norm": 0.44182828068733215,
      "learning_rate": 1.0470067152450352e-05,
      "loss": 0.4713,
      "step": 33350
    },
    {
      "epoch": 7.149592798971281,
      "grad_norm": 14.650175094604492,
      "learning_rate": 1.0467209601371625e-05,
      "loss": 0.4715,
      "step": 33360
    },
    {
      "epoch": 7.151735962280326,
      "grad_norm": 0.44918569922447205,
      "learning_rate": 1.0464352050292901e-05,
      "loss": 1.5496,
      "step": 33370
    },
    {
      "epoch": 7.15387912558937,
      "grad_norm": 0.45305681228637695,
      "learning_rate": 1.0461494499214175e-05,
      "loss": 0.3149,
      "step": 33380
    },
    {
      "epoch": 7.156022288898414,
      "grad_norm": 0.4691127836704254,
      "learning_rate": 1.0458636948135447e-05,
      "loss": 0.7734,
      "step": 33390
    },
    {
      "epoch": 7.1581654522074585,
      "grad_norm": 0.4602839946746826,
      "learning_rate": 1.0455779397056723e-05,
      "loss": 0.4692,
      "step": 33400
    },
    {
      "epoch": 7.160308615516502,
      "grad_norm": 0.44118332862854004,
      "learning_rate": 1.0452921845977997e-05,
      "loss": 0.3175,
      "step": 33410
    },
    {
      "epoch": 7.162451778825546,
      "grad_norm": 0.43258732557296753,
      "learning_rate": 1.045006429489927e-05,
      "loss": 0.3207,
      "step": 33420
    },
    {
      "epoch": 7.164594942134591,
      "grad_norm": 0.41867077350616455,
      "learning_rate": 1.0447206743820546e-05,
      "loss": 0.6334,
      "step": 33430
    },
    {
      "epoch": 7.166738105443635,
      "grad_norm": 14.725724220275879,
      "learning_rate": 1.044434919274182e-05,
      "loss": 0.4774,
      "step": 33440
    },
    {
      "epoch": 7.168881268752679,
      "grad_norm": 14.656998634338379,
      "learning_rate": 1.0441491641663096e-05,
      "loss": 0.6339,
      "step": 33450
    },
    {
      "epoch": 7.171024432061723,
      "grad_norm": 0.4225827157497406,
      "learning_rate": 1.043863409058437e-05,
      "loss": 0.4782,
      "step": 33460
    },
    {
      "epoch": 7.173167595370767,
      "grad_norm": 0.4369429647922516,
      "learning_rate": 1.0435776539505644e-05,
      "loss": 0.785,
      "step": 33470
    },
    {
      "epoch": 7.175310758679811,
      "grad_norm": 0.47665199637413025,
      "learning_rate": 1.043291898842692e-05,
      "loss": 0.7767,
      "step": 33480
    },
    {
      "epoch": 7.177453921988856,
      "grad_norm": 0.5025683641433716,
      "learning_rate": 1.0430061437348193e-05,
      "loss": 0.9155,
      "step": 33490
    },
    {
      "epoch": 7.1795970852979,
      "grad_norm": 0.4876304566860199,
      "learning_rate": 1.0427203886269467e-05,
      "loss": 0.4613,
      "step": 33500
    },
    {
      "epoch": 7.1817402486069435,
      "grad_norm": 0.49326658248901367,
      "learning_rate": 1.0424346335190743e-05,
      "loss": 1.0654,
      "step": 33510
    },
    {
      "epoch": 7.183883411915988,
      "grad_norm": 0.5208851099014282,
      "learning_rate": 1.0421488784112017e-05,
      "loss": 0.3106,
      "step": 33520
    },
    {
      "epoch": 7.186026575225032,
      "grad_norm": 0.5195277333259583,
      "learning_rate": 1.0418631233033292e-05,
      "loss": 1.0549,
      "step": 33530
    },
    {
      "epoch": 7.188169738534076,
      "grad_norm": 14.581361770629883,
      "learning_rate": 1.0415773681954566e-05,
      "loss": 0.3099,
      "step": 33540
    },
    {
      "epoch": 7.190312901843121,
      "grad_norm": 0.49926018714904785,
      "learning_rate": 1.041291613087584e-05,
      "loss": 0.7627,
      "step": 33550
    },
    {
      "epoch": 7.1924560651521645,
      "grad_norm": 0.4765867590904236,
      "learning_rate": 1.0410058579797116e-05,
      "loss": 0.618,
      "step": 33560
    },
    {
      "epoch": 7.194599228461208,
      "grad_norm": 14.596835136413574,
      "learning_rate": 1.040720102871839e-05,
      "loss": 0.4624,
      "step": 33570
    },
    {
      "epoch": 7.196742391770253,
      "grad_norm": 0.48889991641044617,
      "learning_rate": 1.0404343477639664e-05,
      "loss": 0.6149,
      "step": 33580
    },
    {
      "epoch": 7.198885555079297,
      "grad_norm": 14.572340965270996,
      "learning_rate": 1.040148592656094e-05,
      "loss": 0.763,
      "step": 33590
    },
    {
      "epoch": 7.201028718388341,
      "grad_norm": 14.981218338012695,
      "learning_rate": 1.0398628375482212e-05,
      "loss": 0.6059,
      "step": 33600
    },
    {
      "epoch": 7.2031718816973855,
      "grad_norm": 14.482830047607422,
      "learning_rate": 1.0395770824403486e-05,
      "loss": 0.4478,
      "step": 33610
    },
    {
      "epoch": 7.205315045006429,
      "grad_norm": 14.498614311218262,
      "learning_rate": 1.0392913273324761e-05,
      "loss": 0.5909,
      "step": 33620
    },
    {
      "epoch": 7.207458208315473,
      "grad_norm": 14.736505508422852,
      "learning_rate": 1.0390055722246035e-05,
      "loss": 0.589,
      "step": 33630
    },
    {
      "epoch": 7.209601371624518,
      "grad_norm": 0.6225990653038025,
      "learning_rate": 1.0387198171167309e-05,
      "loss": 0.4423,
      "step": 33640
    },
    {
      "epoch": 7.211744534933562,
      "grad_norm": 0.6254374384880066,
      "learning_rate": 1.0384340620088585e-05,
      "loss": 0.7246,
      "step": 33650
    },
    {
      "epoch": 7.213887698242606,
      "grad_norm": 14.810556411743164,
      "learning_rate": 1.0381483069009859e-05,
      "loss": 0.7327,
      "step": 33660
    },
    {
      "epoch": 7.21603086155165,
      "grad_norm": 0.6404682397842407,
      "learning_rate": 1.0378625517931134e-05,
      "loss": 0.5857,
      "step": 33670
    },
    {
      "epoch": 7.218174024860694,
      "grad_norm": 0.6147737503051758,
      "learning_rate": 1.0375767966852408e-05,
      "loss": 0.436,
      "step": 33680
    },
    {
      "epoch": 7.220317188169738,
      "grad_norm": 14.478693962097168,
      "learning_rate": 1.0372910415773682e-05,
      "loss": 1.0135,
      "step": 33690
    },
    {
      "epoch": 7.222460351478783,
      "grad_norm": 14.466017723083496,
      "learning_rate": 1.0370052864694958e-05,
      "loss": 0.7249,
      "step": 33700
    },
    {
      "epoch": 7.224603514787827,
      "grad_norm": 0.6605166792869568,
      "learning_rate": 1.0367195313616232e-05,
      "loss": 0.4359,
      "step": 33710
    },
    {
      "epoch": 7.226746678096871,
      "grad_norm": 0.6269498467445374,
      "learning_rate": 1.0364337762537506e-05,
      "loss": 0.4363,
      "step": 33720
    },
    {
      "epoch": 7.228889841405915,
      "grad_norm": 0.6083215475082397,
      "learning_rate": 1.0361480211458781e-05,
      "loss": 0.4463,
      "step": 33730
    },
    {
      "epoch": 7.231033004714959,
      "grad_norm": 14.765636444091797,
      "learning_rate": 1.0358622660380055e-05,
      "loss": 0.8838,
      "step": 33740
    },
    {
      "epoch": 7.233176168024004,
      "grad_norm": 0.6086131930351257,
      "learning_rate": 1.035576510930133e-05,
      "loss": 0.4464,
      "step": 33750
    },
    {
      "epoch": 7.235319331333048,
      "grad_norm": 14.610669136047363,
      "learning_rate": 1.0352907558222605e-05,
      "loss": 0.8735,
      "step": 33760
    },
    {
      "epoch": 7.237462494642092,
      "grad_norm": 14.387426376342773,
      "learning_rate": 1.0350050007143879e-05,
      "loss": 0.9963,
      "step": 33770
    },
    {
      "epoch": 7.239605657951136,
      "grad_norm": 0.7254716753959656,
      "learning_rate": 1.0347192456065154e-05,
      "loss": 0.4307,
      "step": 33780
    },
    {
      "epoch": 7.24174882126018,
      "grad_norm": 0.7287861108779907,
      "learning_rate": 1.0344334904986428e-05,
      "loss": 1.1163,
      "step": 33790
    },
    {
      "epoch": 7.243891984569224,
      "grad_norm": 14.38970947265625,
      "learning_rate": 1.0341477353907704e-05,
      "loss": 0.7009,
      "step": 33800
    },
    {
      "epoch": 7.246035147878269,
      "grad_norm": 0.7354059815406799,
      "learning_rate": 1.0338619802828978e-05,
      "loss": 0.9879,
      "step": 33810
    },
    {
      "epoch": 7.248178311187313,
      "grad_norm": 0.7320600748062134,
      "learning_rate": 1.033576225175025e-05,
      "loss": 0.1534,
      "step": 33820
    },
    {
      "epoch": 7.2503214744963564,
      "grad_norm": 0.6477347612380981,
      "learning_rate": 1.0332904700671524e-05,
      "loss": 0.8567,
      "step": 33830
    },
    {
      "epoch": 7.252464637805401,
      "grad_norm": 0.7071204781532288,
      "learning_rate": 1.03300471495928e-05,
      "loss": 0.8504,
      "step": 33840
    },
    {
      "epoch": 7.254607801114445,
      "grad_norm": 0.6724726557731628,
      "learning_rate": 1.0327189598514073e-05,
      "loss": 0.4329,
      "step": 33850
    },
    {
      "epoch": 7.256750964423489,
      "grad_norm": 0.6512498259544373,
      "learning_rate": 1.0324332047435347e-05,
      "loss": 0.5794,
      "step": 33860
    },
    {
      "epoch": 7.258894127732534,
      "grad_norm": 0.6264340281486511,
      "learning_rate": 1.0321474496356623e-05,
      "loss": 0.2961,
      "step": 33870
    },
    {
      "epoch": 7.2610372910415775,
      "grad_norm": 30.130537033081055,
      "learning_rate": 1.0318616945277897e-05,
      "loss": 0.7467,
      "step": 33880
    },
    {
      "epoch": 7.263180454350621,
      "grad_norm": 29.64267921447754,
      "learning_rate": 1.0315759394199172e-05,
      "loss": 0.6077,
      "step": 33890
    },
    {
      "epoch": 7.265323617659666,
      "grad_norm": 0.5211003422737122,
      "learning_rate": 1.0312901843120446e-05,
      "loss": 0.6066,
      "step": 33900
    },
    {
      "epoch": 7.26746678096871,
      "grad_norm": 0.5050193071365356,
      "learning_rate": 1.031004429204172e-05,
      "loss": 0.4575,
      "step": 33910
    },
    {
      "epoch": 7.269609944277754,
      "grad_norm": 14.870869636535645,
      "learning_rate": 1.0307186740962996e-05,
      "loss": 0.6085,
      "step": 33920
    },
    {
      "epoch": 7.2717531075867985,
      "grad_norm": 14.491026878356934,
      "learning_rate": 1.030432918988427e-05,
      "loss": 1.4847,
      "step": 33930
    },
    {
      "epoch": 7.273896270895842,
      "grad_norm": 29.515113830566406,
      "learning_rate": 1.0301471638805545e-05,
      "loss": 0.7194,
      "step": 33940
    },
    {
      "epoch": 7.276039434204886,
      "grad_norm": 0.7608659267425537,
      "learning_rate": 1.029861408772682e-05,
      "loss": 0.9808,
      "step": 33950
    },
    {
      "epoch": 7.278182597513931,
      "grad_norm": 0.6952186226844788,
      "learning_rate": 1.0295756536648093e-05,
      "loss": 0.0167,
      "step": 33960
    },
    {
      "epoch": 7.280325760822975,
      "grad_norm": 0.6578408479690552,
      "learning_rate": 1.0292898985569369e-05,
      "loss": 0.4375,
      "step": 33970
    },
    {
      "epoch": 7.282468924132019,
      "grad_norm": 0.6275475025177002,
      "learning_rate": 1.0290041434490643e-05,
      "loss": 0.5823,
      "step": 33980
    },
    {
      "epoch": 7.284612087441063,
      "grad_norm": 0.5914416313171387,
      "learning_rate": 1.0287183883411917e-05,
      "loss": 0.2996,
      "step": 33990
    },
    {
      "epoch": 7.286755250750107,
      "grad_norm": 0.5483959317207336,
      "learning_rate": 1.0284326332333192e-05,
      "loss": 0.5978,
      "step": 34000
    },
    {
      "epoch": 7.288898414059151,
      "grad_norm": 0.4920808672904968,
      "learning_rate": 1.0281468781254466e-05,
      "loss": 0.1627,
      "step": 34010
    },
    {
      "epoch": 7.291041577368196,
      "grad_norm": 0.4599320590496063,
      "learning_rate": 1.0278611230175742e-05,
      "loss": 0.3115,
      "step": 34020
    },
    {
      "epoch": 7.29318474067724,
      "grad_norm": 14.823151588439941,
      "learning_rate": 1.0275753679097014e-05,
      "loss": 0.4791,
      "step": 34030
    },
    {
      "epoch": 7.2953279039862835,
      "grad_norm": 0.4152867794036865,
      "learning_rate": 1.0272896128018288e-05,
      "loss": 0.641,
      "step": 34040
    },
    {
      "epoch": 7.297471067295328,
      "grad_norm": 14.73818588256836,
      "learning_rate": 1.0270038576939562e-05,
      "loss": 0.4822,
      "step": 34050
    },
    {
      "epoch": 7.299614230604372,
      "grad_norm": 29.98992347717285,
      "learning_rate": 1.0267181025860838e-05,
      "loss": 1.1072,
      "step": 34060
    },
    {
      "epoch": 7.301757393913416,
      "grad_norm": 0.47411200404167175,
      "learning_rate": 1.0264323474782112e-05,
      "loss": 1.0877,
      "step": 34070
    },
    {
      "epoch": 7.303900557222461,
      "grad_norm": 0.5245159864425659,
      "learning_rate": 1.0261465923703387e-05,
      "loss": 0.7612,
      "step": 34080
    },
    {
      "epoch": 7.3060437205315045,
      "grad_norm": 29.53348159790039,
      "learning_rate": 1.0258608372624661e-05,
      "loss": 0.8994,
      "step": 34090
    },
    {
      "epoch": 7.308186883840548,
      "grad_norm": 0.545464277267456,
      "learning_rate": 1.0255750821545935e-05,
      "loss": 0.7489,
      "step": 34100
    },
    {
      "epoch": 7.310330047149593,
      "grad_norm": 14.55506706237793,
      "learning_rate": 1.025289327046721e-05,
      "loss": 0.4503,
      "step": 34110
    },
    {
      "epoch": 7.312473210458637,
      "grad_norm": 0.5707074403762817,
      "learning_rate": 1.0250035719388485e-05,
      "loss": 0.4501,
      "step": 34120
    },
    {
      "epoch": 7.314616373767681,
      "grad_norm": 14.663687705993652,
      "learning_rate": 1.0247178168309759e-05,
      "loss": 0.5975,
      "step": 34130
    },
    {
      "epoch": 7.3167595370767256,
      "grad_norm": 14.538225173950195,
      "learning_rate": 1.0244320617231034e-05,
      "loss": 0.3054,
      "step": 34140
    },
    {
      "epoch": 7.318902700385769,
      "grad_norm": 15.367878913879395,
      "learning_rate": 1.0241463066152308e-05,
      "loss": 0.7458,
      "step": 34150
    },
    {
      "epoch": 7.321045863694813,
      "grad_norm": 0.5209922790527344,
      "learning_rate": 1.0238605515073584e-05,
      "loss": 0.458,
      "step": 34160
    },
    {
      "epoch": 7.323189027003858,
      "grad_norm": 0.5512928366661072,
      "learning_rate": 1.0235747963994858e-05,
      "loss": 1.0464,
      "step": 34170
    },
    {
      "epoch": 7.325332190312902,
      "grad_norm": 14.694222450256348,
      "learning_rate": 1.0232890412916132e-05,
      "loss": 0.4513,
      "step": 34180
    },
    {
      "epoch": 7.327475353621946,
      "grad_norm": 0.5666899681091309,
      "learning_rate": 1.0230032861837407e-05,
      "loss": 0.1586,
      "step": 34190
    },
    {
      "epoch": 7.32961851693099,
      "grad_norm": 0.5136955976486206,
      "learning_rate": 1.0227175310758681e-05,
      "loss": 0.4566,
      "step": 34200
    },
    {
      "epoch": 7.331761680240034,
      "grad_norm": 14.909745216369629,
      "learning_rate": 1.0224317759679955e-05,
      "loss": 0.7567,
      "step": 34210
    },
    {
      "epoch": 7.333904843549078,
      "grad_norm": 14.546107292175293,
      "learning_rate": 1.022146020860123e-05,
      "loss": 1.0494,
      "step": 34220
    },
    {
      "epoch": 7.336048006858123,
      "grad_norm": 0.6219324469566345,
      "learning_rate": 1.0218602657522505e-05,
      "loss": 0.8823,
      "step": 34230
    },
    {
      "epoch": 7.338191170167167,
      "grad_norm": 0.8000010848045349,
      "learning_rate": 1.021574510644378e-05,
      "loss": 0.5824,
      "step": 34240
    },
    {
      "epoch": 7.340334333476211,
      "grad_norm": 14.709785461425781,
      "learning_rate": 1.0212887555365053e-05,
      "loss": 0.715,
      "step": 34250
    },
    {
      "epoch": 7.342477496785255,
      "grad_norm": 0.6794243454933167,
      "learning_rate": 1.0210030004286326e-05,
      "loss": 0.295,
      "step": 34260
    },
    {
      "epoch": 7.344620660094299,
      "grad_norm": 0.6050806641578674,
      "learning_rate": 1.02071724532076e-05,
      "loss": 0.2944,
      "step": 34270
    },
    {
      "epoch": 7.346763823403343,
      "grad_norm": 14.502333641052246,
      "learning_rate": 1.0204314902128876e-05,
      "loss": 1.1784,
      "step": 34280
    },
    {
      "epoch": 7.348906986712388,
      "grad_norm": 29.506563186645508,
      "learning_rate": 1.020145735105015e-05,
      "loss": 1.4368,
      "step": 34290
    },
    {
      "epoch": 7.351050150021432,
      "grad_norm": 0.7056960463523865,
      "learning_rate": 1.0198599799971426e-05,
      "loss": 0.5765,
      "step": 34300
    },
    {
      "epoch": 7.353193313330475,
      "grad_norm": 14.3650484085083,
      "learning_rate": 1.01957422488927e-05,
      "loss": 0.9793,
      "step": 34310
    },
    {
      "epoch": 7.35533647663952,
      "grad_norm": 0.7937036156654358,
      "learning_rate": 1.0192884697813973e-05,
      "loss": 0.6913,
      "step": 34320
    },
    {
      "epoch": 7.357479639948564,
      "grad_norm": 0.7736212611198425,
      "learning_rate": 1.0190027146735249e-05,
      "loss": 0.5508,
      "step": 34330
    },
    {
      "epoch": 7.359622803257608,
      "grad_norm": 0.7259541153907776,
      "learning_rate": 1.0187169595656523e-05,
      "loss": 0.2869,
      "step": 34340
    },
    {
      "epoch": 7.361765966566653,
      "grad_norm": 0.6756089925765991,
      "learning_rate": 1.0184312044577797e-05,
      "loss": 0.4321,
      "step": 34350
    },
    {
      "epoch": 7.3639091298756965,
      "grad_norm": 14.512935638427734,
      "learning_rate": 1.0181454493499072e-05,
      "loss": 0.5848,
      "step": 34360
    },
    {
      "epoch": 7.36605229318474,
      "grad_norm": 0.5649985671043396,
      "learning_rate": 1.0178596942420346e-05,
      "loss": 0.446,
      "step": 34370
    },
    {
      "epoch": 7.368195456493785,
      "grad_norm": 0.5276667475700378,
      "learning_rate": 1.0175739391341622e-05,
      "loss": 0.3045,
      "step": 34380
    },
    {
      "epoch": 7.370338619802829,
      "grad_norm": 0.4890322685241699,
      "learning_rate": 1.0172881840262896e-05,
      "loss": 0.4583,
      "step": 34390
    },
    {
      "epoch": 7.372481783111873,
      "grad_norm": 0.45996055006980896,
      "learning_rate": 1.017002428918417e-05,
      "loss": 0.4657,
      "step": 34400
    },
    {
      "epoch": 7.3746249464209175,
      "grad_norm": 29.583635330200195,
      "learning_rate": 1.0167166738105446e-05,
      "loss": 0.4724,
      "step": 34410
    },
    {
      "epoch": 7.376768109729961,
      "grad_norm": 0.4349718391895294,
      "learning_rate": 1.016430918702672e-05,
      "loss": 0.6286,
      "step": 34420
    },
    {
      "epoch": 7.378911273039005,
      "grad_norm": 0.44317322969436646,
      "learning_rate": 1.0161451635947993e-05,
      "loss": 0.7831,
      "step": 34430
    },
    {
      "epoch": 7.38105443634805,
      "grad_norm": 0.46452903747558594,
      "learning_rate": 1.0158594084869269e-05,
      "loss": 0.3191,
      "step": 34440
    },
    {
      "epoch": 7.383197599657094,
      "grad_norm": 0.4602702558040619,
      "learning_rate": 1.0155736533790543e-05,
      "loss": 0.6195,
      "step": 34450
    },
    {
      "epoch": 7.385340762966138,
      "grad_norm": 0.4590110182762146,
      "learning_rate": 1.0152878982711815e-05,
      "loss": 0.7751,
      "step": 34460
    },
    {
      "epoch": 7.387483926275182,
      "grad_norm": 0.45498356223106384,
      "learning_rate": 1.015002143163309e-05,
      "loss": 0.3161,
      "step": 34470
    },
    {
      "epoch": 7.389627089584226,
      "grad_norm": 0.46339768171310425,
      "learning_rate": 1.0147163880554365e-05,
      "loss": 0.6182,
      "step": 34480
    },
    {
      "epoch": 7.39177025289327,
      "grad_norm": 0.47198107838630676,
      "learning_rate": 1.0144306329475639e-05,
      "loss": 0.4655,
      "step": 34490
    },
    {
      "epoch": 7.393913416202315,
      "grad_norm": 0.45214658975601196,
      "learning_rate": 1.0141448778396914e-05,
      "loss": 0.3127,
      "step": 34500
    },
    {
      "epoch": 7.396056579511359,
      "grad_norm": 0.3890992999076843,
      "learning_rate": 1.0138591227318188e-05,
      "loss": 0.1623,
      "step": 34510
    },
    {
      "epoch": 7.3981997428204025,
      "grad_norm": 0.37135234475135803,
      "learning_rate": 1.0135733676239464e-05,
      "loss": 0.485,
      "step": 34520
    },
    {
      "epoch": 7.400342906129447,
      "grad_norm": 0.359025776386261,
      "learning_rate": 1.0132876125160738e-05,
      "loss": 0.4872,
      "step": 34530
    },
    {
      "epoch": 7.402486069438491,
      "grad_norm": 14.773956298828125,
      "learning_rate": 1.0130018574082012e-05,
      "loss": 0.8162,
      "step": 34540
    },
    {
      "epoch": 7.404629232747535,
      "grad_norm": 0.3628038167953491,
      "learning_rate": 1.0127161023003287e-05,
      "loss": 0.3297,
      "step": 34550
    },
    {
      "epoch": 7.40677239605658,
      "grad_norm": 14.872020721435547,
      "learning_rate": 1.0124303471924561e-05,
      "loss": 0.8098,
      "step": 34560
    },
    {
      "epoch": 7.4089155593656235,
      "grad_norm": 0.43702346086502075,
      "learning_rate": 1.0121445920845835e-05,
      "loss": 0.6359,
      "step": 34570
    },
    {
      "epoch": 7.411058722674667,
      "grad_norm": 14.63927936553955,
      "learning_rate": 1.011858836976711e-05,
      "loss": 0.9342,
      "step": 34580
    },
    {
      "epoch": 7.413201885983712,
      "grad_norm": 14.700218200683594,
      "learning_rate": 1.0115730818688385e-05,
      "loss": 0.9268,
      "step": 34590
    },
    {
      "epoch": 7.415345049292756,
      "grad_norm": 14.644617080688477,
      "learning_rate": 1.011287326760966e-05,
      "loss": 0.6195,
      "step": 34600
    },
    {
      "epoch": 7.4174882126018,
      "grad_norm": 14.660726547241211,
      "learning_rate": 1.0110015716530934e-05,
      "loss": 0.6248,
      "step": 34610
    },
    {
      "epoch": 7.4196313759108445,
      "grad_norm": 0.48497453331947327,
      "learning_rate": 1.0107158165452208e-05,
      "loss": 0.4642,
      "step": 34620
    },
    {
      "epoch": 7.421774539219888,
      "grad_norm": 0.4586455225944519,
      "learning_rate": 1.0104300614373484e-05,
      "loss": 0.3147,
      "step": 34630
    },
    {
      "epoch": 7.423917702528932,
      "grad_norm": 0.48240000009536743,
      "learning_rate": 1.0101443063294758e-05,
      "loss": 1.387,
      "step": 34640
    },
    {
      "epoch": 7.426060865837977,
      "grad_norm": 14.582576751708984,
      "learning_rate": 1.0098585512216033e-05,
      "loss": 0.6142,
      "step": 34650
    },
    {
      "epoch": 7.428204029147021,
      "grad_norm": 0.5014276504516602,
      "learning_rate": 1.0095727961137307e-05,
      "loss": 0.3124,
      "step": 34660
    },
    {
      "epoch": 7.430347192456066,
      "grad_norm": 14.62333869934082,
      "learning_rate": 1.0092870410058581e-05,
      "loss": 0.7619,
      "step": 34670
    },
    {
      "epoch": 7.432490355765109,
      "grad_norm": 0.5050426721572876,
      "learning_rate": 1.0090012858979853e-05,
      "loss": 0.6083,
      "step": 34680
    },
    {
      "epoch": 7.434633519074153,
      "grad_norm": 0.5064910054206848,
      "learning_rate": 1.0087155307901129e-05,
      "loss": 0.6101,
      "step": 34690
    },
    {
      "epoch": 7.436776682383198,
      "grad_norm": 0.49065250158309937,
      "learning_rate": 1.0084297756822403e-05,
      "loss": 0.4591,
      "step": 34700
    },
    {
      "epoch": 7.438919845692242,
      "grad_norm": 0.48109084367752075,
      "learning_rate": 1.0081440205743677e-05,
      "loss": 0.4625,
      "step": 34710
    },
    {
      "epoch": 7.441063009001286,
      "grad_norm": 0.44683635234832764,
      "learning_rate": 1.0078582654664953e-05,
      "loss": 0.3183,
      "step": 34720
    },
    {
      "epoch": 7.44320617231033,
      "grad_norm": 29.780149459838867,
      "learning_rate": 1.0075725103586226e-05,
      "loss": 0.6304,
      "step": 34730
    },
    {
      "epoch": 7.445349335619374,
      "grad_norm": 14.67965316772461,
      "learning_rate": 1.0072867552507502e-05,
      "loss": 0.9358,
      "step": 34740
    },
    {
      "epoch": 7.447492498928418,
      "grad_norm": 14.590797424316406,
      "learning_rate": 1.0070010001428776e-05,
      "loss": 1.2255,
      "step": 34750
    },
    {
      "epoch": 7.449635662237463,
      "grad_norm": 0.528675377368927,
      "learning_rate": 1.006715245035005e-05,
      "loss": 0.6103,
      "step": 34760
    },
    {
      "epoch": 7.451778825546507,
      "grad_norm": 14.580438613891602,
      "learning_rate": 1.0064294899271326e-05,
      "loss": 0.3088,
      "step": 34770
    },
    {
      "epoch": 7.453921988855551,
      "grad_norm": 14.558098793029785,
      "learning_rate": 1.00614373481926e-05,
      "loss": 0.7542,
      "step": 34780
    },
    {
      "epoch": 7.456065152164595,
      "grad_norm": 0.5503132939338684,
      "learning_rate": 1.0058579797113875e-05,
      "loss": 0.8948,
      "step": 34790
    },
    {
      "epoch": 7.458208315473639,
      "grad_norm": 0.5573252439498901,
      "learning_rate": 1.0055722246035149e-05,
      "loss": 0.3046,
      "step": 34800
    },
    {
      "epoch": 7.460351478782683,
      "grad_norm": 14.530167579650879,
      "learning_rate": 1.0052864694956423e-05,
      "loss": 0.8951,
      "step": 34810
    },
    {
      "epoch": 7.462494642091728,
      "grad_norm": 0.5524642467498779,
      "learning_rate": 1.0050007143877699e-05,
      "loss": 0.7454,
      "step": 34820
    },
    {
      "epoch": 7.464637805400772,
      "grad_norm": 0.5403593182563782,
      "learning_rate": 1.0047149592798973e-05,
      "loss": 0.305,
      "step": 34830
    },
    {
      "epoch": 7.4667809687098154,
      "grad_norm": 0.5418968796730042,
      "learning_rate": 1.0044292041720246e-05,
      "loss": 0.7479,
      "step": 34840
    },
    {
      "epoch": 7.46892413201886,
      "grad_norm": 14.566801071166992,
      "learning_rate": 1.0041434490641522e-05,
      "loss": 0.7476,
      "step": 34850
    },
    {
      "epoch": 7.471067295327904,
      "grad_norm": 0.5647067427635193,
      "learning_rate": 1.0038576939562796e-05,
      "loss": 0.4496,
      "step": 34860
    },
    {
      "epoch": 7.473210458636948,
      "grad_norm": 0.58383709192276,
      "learning_rate": 1.0035719388484072e-05,
      "loss": 1.0284,
      "step": 34870
    },
    {
      "epoch": 7.475353621945993,
      "grad_norm": 14.438031196594238,
      "learning_rate": 1.0032861837405346e-05,
      "loss": 1.1582,
      "step": 34880
    },
    {
      "epoch": 7.4774967852550365,
      "grad_norm": 14.444765090942383,
      "learning_rate": 1.0030004286326618e-05,
      "loss": 0.86,
      "step": 34890
    },
    {
      "epoch": 7.47963994856408,
      "grad_norm": 0.6644539833068848,
      "learning_rate": 1.0027146735247892e-05,
      "loss": 0.2931,
      "step": 34900
    },
    {
      "epoch": 7.481783111873125,
      "grad_norm": 0.6609445214271545,
      "learning_rate": 1.0024289184169167e-05,
      "loss": 0.4363,
      "step": 34910
    },
    {
      "epoch": 7.483926275182169,
      "grad_norm": 0.5600287914276123,
      "learning_rate": 1.0021431633090441e-05,
      "loss": 0.5877,
      "step": 34920
    },
    {
      "epoch": 7.486069438491213,
      "grad_norm": 14.64426040649414,
      "learning_rate": 1.0018574082011717e-05,
      "loss": 0.4486,
      "step": 34930
    },
    {
      "epoch": 7.4882126018002575,
      "grad_norm": 0.5077173709869385,
      "learning_rate": 1.001571653093299e-05,
      "loss": 0.5964,
      "step": 34940
    },
    {
      "epoch": 7.490355765109301,
      "grad_norm": 0.5417636036872864,
      "learning_rate": 1.0012858979854265e-05,
      "loss": 0.605,
      "step": 34950
    },
    {
      "epoch": 7.492498928418345,
      "grad_norm": 0.5299837589263916,
      "learning_rate": 1.001000142877554e-05,
      "loss": 0.7484,
      "step": 34960
    },
    {
      "epoch": 7.49464209172739,
      "grad_norm": 0.5423461198806763,
      "learning_rate": 1.0007143877696814e-05,
      "loss": 0.4525,
      "step": 34970
    },
    {
      "epoch": 7.496785255036434,
      "grad_norm": 0.536396324634552,
      "learning_rate": 1.0004286326618088e-05,
      "loss": 0.1583,
      "step": 34980
    },
    {
      "epoch": 7.498928418345478,
      "grad_norm": 0.4165744185447693,
      "learning_rate": 1.0001428775539364e-05,
      "loss": 0.1589,
      "step": 34990
    },
    {
      "epoch": 7.501071581654522,
      "grad_norm": 29.76503562927246,
      "learning_rate": 9.998571224460638e-06,
      "loss": 0.6512,
      "step": 35000
    },
    {
      "epoch": 7.503214744963566,
      "grad_norm": 14.772147178649902,
      "learning_rate": 9.995713673381913e-06,
      "loss": 0.6589,
      "step": 35010
    },
    {
      "epoch": 7.50535790827261,
      "grad_norm": 14.744563102722168,
      "learning_rate": 9.992856122303187e-06,
      "loss": 0.8239,
      "step": 35020
    },
    {
      "epoch": 7.507501071581655,
      "grad_norm": 14.734781265258789,
      "learning_rate": 9.989998571224461e-06,
      "loss": 0.9751,
      "step": 35030
    },
    {
      "epoch": 7.509644234890699,
      "grad_norm": 14.671684265136719,
      "learning_rate": 9.987141020145737e-06,
      "loss": 1.1228,
      "step": 35040
    },
    {
      "epoch": 7.5117873981997425,
      "grad_norm": 0.44413572549819946,
      "learning_rate": 9.984283469067009e-06,
      "loss": 0.9477,
      "step": 35050
    },
    {
      "epoch": 7.513930561508787,
      "grad_norm": 0.451674222946167,
      "learning_rate": 9.981425917988285e-06,
      "loss": 0.1631,
      "step": 35060
    },
    {
      "epoch": 7.516073724817831,
      "grad_norm": 0.3910976052284241,
      "learning_rate": 9.978568366909559e-06,
      "loss": 0.1646,
      "step": 35070
    },
    {
      "epoch": 7.518216888126875,
      "grad_norm": 0.39924493432044983,
      "learning_rate": 9.975710815830834e-06,
      "loss": 0.4811,
      "step": 35080
    },
    {
      "epoch": 7.52036005143592,
      "grad_norm": 14.989703178405762,
      "learning_rate": 9.972853264752108e-06,
      "loss": 0.6449,
      "step": 35090
    },
    {
      "epoch": 7.5225032147449635,
      "grad_norm": 0.375529944896698,
      "learning_rate": 9.969995713673382e-06,
      "loss": 0.6471,
      "step": 35100
    },
    {
      "epoch": 7.524646378054007,
      "grad_norm": 29.71988868713379,
      "learning_rate": 9.967138162594658e-06,
      "loss": 1.123,
      "step": 35110
    },
    {
      "epoch": 7.526789541363052,
      "grad_norm": 0.40951964259147644,
      "learning_rate": 9.964280611515932e-06,
      "loss": 0.7967,
      "step": 35120
    },
    {
      "epoch": 7.528932704672096,
      "grad_norm": 0.4220718741416931,
      "learning_rate": 9.961423060437206e-06,
      "loss": 0.3202,
      "step": 35130
    },
    {
      "epoch": 7.53107586798114,
      "grad_norm": 14.95793342590332,
      "learning_rate": 9.958565509358481e-06,
      "loss": 0.9413,
      "step": 35140
    },
    {
      "epoch": 7.5332190312901846,
      "grad_norm": 14.86404037475586,
      "learning_rate": 9.955707958279755e-06,
      "loss": 0.7841,
      "step": 35150
    },
    {
      "epoch": 7.535362194599228,
      "grad_norm": 0.5031838417053223,
      "learning_rate": 9.952850407201029e-06,
      "loss": 0.7729,
      "step": 35160
    },
    {
      "epoch": 7.537505357908272,
      "grad_norm": 0.48067334294319153,
      "learning_rate": 9.949992856122303e-06,
      "loss": 0.3113,
      "step": 35170
    },
    {
      "epoch": 7.539648521217317,
      "grad_norm": 0.4646587371826172,
      "learning_rate": 9.947135305043579e-06,
      "loss": 0.3135,
      "step": 35180
    },
    {
      "epoch": 7.541791684526361,
      "grad_norm": 0.4268037974834442,
      "learning_rate": 9.944277753964853e-06,
      "loss": 0.3181,
      "step": 35190
    },
    {
      "epoch": 7.543934847835405,
      "grad_norm": 0.4270745813846588,
      "learning_rate": 9.941420202886127e-06,
      "loss": 0.6297,
      "step": 35200
    },
    {
      "epoch": 7.546078011144449,
      "grad_norm": 14.79875659942627,
      "learning_rate": 9.938562651807402e-06,
      "loss": 0.7865,
      "step": 35210
    },
    {
      "epoch": 7.548221174453493,
      "grad_norm": 14.629734992980957,
      "learning_rate": 9.935705100728676e-06,
      "loss": 0.6254,
      "step": 35220
    },
    {
      "epoch": 7.550364337762537,
      "grad_norm": 0.4584628641605377,
      "learning_rate": 9.932847549649952e-06,
      "loss": 0.4732,
      "step": 35230
    },
    {
      "epoch": 7.552507501071582,
      "grad_norm": 14.659418106079102,
      "learning_rate": 9.929989998571226e-06,
      "loss": 0.7724,
      "step": 35240
    },
    {
      "epoch": 7.554650664380626,
      "grad_norm": 14.61487865447998,
      "learning_rate": 9.9271324474925e-06,
      "loss": 0.7729,
      "step": 35250
    },
    {
      "epoch": 7.5567938276896705,
      "grad_norm": 0.5152748823165894,
      "learning_rate": 9.924274896413775e-06,
      "loss": 0.4627,
      "step": 35260
    },
    {
      "epoch": 7.558936990998714,
      "grad_norm": 0.526419460773468,
      "learning_rate": 9.921417345335047e-06,
      "loss": 0.9067,
      "step": 35270
    },
    {
      "epoch": 7.561080154307758,
      "grad_norm": 14.833525657653809,
      "learning_rate": 9.918559794256323e-06,
      "loss": 0.4561,
      "step": 35280
    },
    {
      "epoch": 7.563223317616803,
      "grad_norm": 14.596421241760254,
      "learning_rate": 9.915702243177597e-06,
      "loss": 0.7625,
      "step": 35290
    },
    {
      "epoch": 7.565366480925847,
      "grad_norm": 14.60643196105957,
      "learning_rate": 9.912844692098873e-06,
      "loss": 0.9085,
      "step": 35300
    },
    {
      "epoch": 7.567509644234891,
      "grad_norm": 14.499019622802734,
      "learning_rate": 9.909987141020146e-06,
      "loss": 0.745,
      "step": 35310
    },
    {
      "epoch": 7.569652807543935,
      "grad_norm": 14.448443412780762,
      "learning_rate": 9.90712958994142e-06,
      "loss": 1.0164,
      "step": 35320
    },
    {
      "epoch": 7.571795970852979,
      "grad_norm": 0.6542789936065674,
      "learning_rate": 9.904272038862696e-06,
      "loss": 0.4382,
      "step": 35330
    },
    {
      "epoch": 7.573939134162023,
      "grad_norm": 0.6384837627410889,
      "learning_rate": 9.90141448778397e-06,
      "loss": 0.7216,
      "step": 35340
    },
    {
      "epoch": 7.576082297471068,
      "grad_norm": 0.6457085013389587,
      "learning_rate": 9.898556936705246e-06,
      "loss": 0.5821,
      "step": 35350
    },
    {
      "epoch": 7.578225460780112,
      "grad_norm": 0.6267582178115845,
      "learning_rate": 9.89569938562652e-06,
      "loss": 1.0162,
      "step": 35360
    },
    {
      "epoch": 7.5803686240891555,
      "grad_norm": 0.6515048146247864,
      "learning_rate": 9.892841834547793e-06,
      "loss": 0.4397,
      "step": 35370
    },
    {
      "epoch": 7.5825117873982,
      "grad_norm": 14.452078819274902,
      "learning_rate": 9.889984283469067e-06,
      "loss": 0.7231,
      "step": 35380
    },
    {
      "epoch": 7.584654950707244,
      "grad_norm": 0.6624096632003784,
      "learning_rate": 9.887126732390341e-06,
      "loss": 0.4371,
      "step": 35390
    },
    {
      "epoch": 7.586798114016288,
      "grad_norm": 0.6218364834785461,
      "learning_rate": 9.884269181311617e-06,
      "loss": 0.1548,
      "step": 35400
    },
    {
      "epoch": 7.588941277325333,
      "grad_norm": 15.139154434204102,
      "learning_rate": 9.881411630232891e-06,
      "loss": 0.7297,
      "step": 35410
    },
    {
      "epoch": 7.5910844406343765,
      "grad_norm": 29.49709701538086,
      "learning_rate": 9.878554079154166e-06,
      "loss": 1.0199,
      "step": 35420
    },
    {
      "epoch": 7.59322760394342,
      "grad_norm": 0.6638894081115723,
      "learning_rate": 9.87569652807544e-06,
      "loss": 0.4401,
      "step": 35430
    },
    {
      "epoch": 7.595370767252465,
      "grad_norm": 0.5999974012374878,
      "learning_rate": 9.872838976996714e-06,
      "loss": 0.3018,
      "step": 35440
    },
    {
      "epoch": 7.597513930561509,
      "grad_norm": 14.670591354370117,
      "learning_rate": 9.86998142591799e-06,
      "loss": 0.5971,
      "step": 35450
    },
    {
      "epoch": 7.599657093870553,
      "grad_norm": 0.5423246026039124,
      "learning_rate": 9.867123874839264e-06,
      "loss": 0.6016,
      "step": 35460
    },
    {
      "epoch": 7.6018002571795975,
      "grad_norm": 14.529637336730957,
      "learning_rate": 9.864266323760538e-06,
      "loss": 0.4518,
      "step": 35470
    },
    {
      "epoch": 7.603943420488641,
      "grad_norm": 0.5501330494880676,
      "learning_rate": 9.861408772681812e-06,
      "loss": 0.8963,
      "step": 35480
    },
    {
      "epoch": 7.606086583797685,
      "grad_norm": 15.544334411621094,
      "learning_rate": 9.858551221603087e-06,
      "loss": 0.744,
      "step": 35490
    },
    {
      "epoch": 7.60822974710673,
      "grad_norm": 0.5311721563339233,
      "learning_rate": 9.855693670524361e-06,
      "loss": 0.4529,
      "step": 35500
    },
    {
      "epoch": 7.610372910415774,
      "grad_norm": 0.5180803537368774,
      "learning_rate": 9.852836119445635e-06,
      "loss": 0.4568,
      "step": 35510
    },
    {
      "epoch": 7.612516073724818,
      "grad_norm": 0.5077332258224487,
      "learning_rate": 9.849978568366911e-06,
      "loss": 0.7583,
      "step": 35520
    },
    {
      "epoch": 7.614659237033862,
      "grad_norm": 0.48945048451423645,
      "learning_rate": 9.847121017288185e-06,
      "loss": 0.4598,
      "step": 35530
    },
    {
      "epoch": 7.616802400342906,
      "grad_norm": 0.5018835067749023,
      "learning_rate": 9.844263466209459e-06,
      "loss": 0.7627,
      "step": 35540
    },
    {
      "epoch": 7.61894556365195,
      "grad_norm": 0.48708948493003845,
      "learning_rate": 9.841405915130734e-06,
      "loss": 0.0112,
      "step": 35550
    },
    {
      "epoch": 7.621088726960995,
      "grad_norm": 0.4764138162136078,
      "learning_rate": 9.838548364052008e-06,
      "loss": 0.4683,
      "step": 35560
    },
    {
      "epoch": 7.623231890270039,
      "grad_norm": 14.687396049499512,
      "learning_rate": 9.835690812973284e-06,
      "loss": 0.4746,
      "step": 35570
    },
    {
      "epoch": 7.6253750535790825,
      "grad_norm": 0.43310049176216125,
      "learning_rate": 9.832833261894558e-06,
      "loss": 1.0955,
      "step": 35580
    },
    {
      "epoch": 7.627518216888127,
      "grad_norm": 0.44586580991744995,
      "learning_rate": 9.829975710815832e-06,
      "loss": 0.3182,
      "step": 35590
    },
    {
      "epoch": 7.629661380197171,
      "grad_norm": 0.46690306067466736,
      "learning_rate": 9.827118159737106e-06,
      "loss": 1.0838,
      "step": 35600
    },
    {
      "epoch": 7.631804543506215,
      "grad_norm": 0.489629328250885,
      "learning_rate": 9.82426060865838e-06,
      "loss": 0.4648,
      "step": 35610
    },
    {
      "epoch": 7.63394770681526,
      "grad_norm": 0.4754598140716553,
      "learning_rate": 9.821403057579655e-06,
      "loss": 0.7665,
      "step": 35620
    },
    {
      "epoch": 7.6360908701243035,
      "grad_norm": 14.599447250366211,
      "learning_rate": 9.818545506500929e-06,
      "loss": 0.315,
      "step": 35630
    },
    {
      "epoch": 7.638234033433347,
      "grad_norm": 0.47083380818367004,
      "learning_rate": 9.815687955422205e-06,
      "loss": 0.4668,
      "step": 35640
    },
    {
      "epoch": 7.640377196742392,
      "grad_norm": 0.49082696437835693,
      "learning_rate": 9.812830404343479e-06,
      "loss": 0.7702,
      "step": 35650
    },
    {
      "epoch": 7.642520360051436,
      "grad_norm": 14.565347671508789,
      "learning_rate": 9.809972853264753e-06,
      "loss": 0.7666,
      "step": 35660
    },
    {
      "epoch": 7.64466352336048,
      "grad_norm": 0.5540171265602112,
      "learning_rate": 9.807115302186028e-06,
      "loss": 1.508,
      "step": 35670
    },
    {
      "epoch": 7.646806686669525,
      "grad_norm": 0.5884172916412354,
      "learning_rate": 9.804257751107302e-06,
      "loss": 0.8857,
      "step": 35680
    },
    {
      "epoch": 7.648949849978568,
      "grad_norm": 14.48144245147705,
      "learning_rate": 9.801400200028576e-06,
      "loss": 0.4427,
      "step": 35690
    },
    {
      "epoch": 7.651093013287612,
      "grad_norm": 14.469526290893555,
      "learning_rate": 9.79854264894985e-06,
      "loss": 1.0198,
      "step": 35700
    },
    {
      "epoch": 7.653236176596657,
      "grad_norm": 14.467488288879395,
      "learning_rate": 9.795685097871126e-06,
      "loss": 0.2989,
      "step": 35710
    },
    {
      "epoch": 7.655379339905701,
      "grad_norm": 0.5971848964691162,
      "learning_rate": 9.7928275467924e-06,
      "loss": 0.2994,
      "step": 35720
    },
    {
      "epoch": 7.657522503214745,
      "grad_norm": 14.475066184997559,
      "learning_rate": 9.789969995713674e-06,
      "loss": 1.308,
      "step": 35730
    },
    {
      "epoch": 7.659665666523789,
      "grad_norm": 14.429672241210938,
      "learning_rate": 9.787112444634949e-06,
      "loss": 0.5792,
      "step": 35740
    },
    {
      "epoch": 7.661808829832833,
      "grad_norm": 0.6447346210479736,
      "learning_rate": 9.784254893556223e-06,
      "loss": 0.0145,
      "step": 35750
    },
    {
      "epoch": 7.663951993141877,
      "grad_norm": 14.604656219482422,
      "learning_rate": 9.781397342477497e-06,
      "loss": 0.593,
      "step": 35760
    },
    {
      "epoch": 7.666095156450922,
      "grad_norm": 0.5677391886711121,
      "learning_rate": 9.778539791398773e-06,
      "loss": 0.4492,
      "step": 35770
    },
    {
      "epoch": 7.668238319759966,
      "grad_norm": 14.547507286071777,
      "learning_rate": 9.775682240320047e-06,
      "loss": 0.4501,
      "step": 35780
    },
    {
      "epoch": 7.67038148306901,
      "grad_norm": 0.5432772636413574,
      "learning_rate": 9.772824689241322e-06,
      "loss": 0.4524,
      "step": 35790
    },
    {
      "epoch": 7.672524646378054,
      "grad_norm": 0.500874936580658,
      "learning_rate": 9.769967138162596e-06,
      "loss": 0.0116,
      "step": 35800
    },
    {
      "epoch": 7.674667809687098,
      "grad_norm": 0.44361943006515503,
      "learning_rate": 9.76710958708387e-06,
      "loss": 0.3148,
      "step": 35810
    },
    {
      "epoch": 7.676810972996142,
      "grad_norm": 0.43557459115982056,
      "learning_rate": 9.764252036005144e-06,
      "loss": 0.7807,
      "step": 35820
    },
    {
      "epoch": 7.678954136305187,
      "grad_norm": 29.66135597229004,
      "learning_rate": 9.761394484926418e-06,
      "loss": 0.7796,
      "step": 35830
    },
    {
      "epoch": 7.681097299614231,
      "grad_norm": 0.469946950674057,
      "learning_rate": 9.758536933847693e-06,
      "loss": 0.7695,
      "step": 35840
    },
    {
      "epoch": 7.6832404629232744,
      "grad_norm": 14.851996421813965,
      "learning_rate": 9.755679382768967e-06,
      "loss": 0.4657,
      "step": 35850
    },
    {
      "epoch": 7.685383626232319,
      "grad_norm": 14.892114639282227,
      "learning_rate": 9.752821831690243e-06,
      "loss": 0.466,
      "step": 35860
    },
    {
      "epoch": 7.687526789541363,
      "grad_norm": 0.4030616283416748,
      "learning_rate": 9.749964280611517e-06,
      "loss": 0.1662,
      "step": 35870
    },
    {
      "epoch": 7.689669952850407,
      "grad_norm": 14.719816207885742,
      "learning_rate": 9.747106729532791e-06,
      "loss": 0.6362,
      "step": 35880
    },
    {
      "epoch": 7.691813116159452,
      "grad_norm": 0.3821091055870056,
      "learning_rate": 9.744249178454067e-06,
      "loss": 0.6413,
      "step": 35890
    },
    {
      "epoch": 7.6939562794684955,
      "grad_norm": 0.4384278357028961,
      "learning_rate": 9.74139162737534e-06,
      "loss": 1.257,
      "step": 35900
    },
    {
      "epoch": 7.696099442777539,
      "grad_norm": 0.4954182803630829,
      "learning_rate": 9.738534076296614e-06,
      "loss": 0.7716,
      "step": 35910
    },
    {
      "epoch": 7.698242606086584,
      "grad_norm": 0.48828399181365967,
      "learning_rate": 9.735676525217888e-06,
      "loss": 0.4601,
      "step": 35920
    },
    {
      "epoch": 7.700385769395628,
      "grad_norm": 0.5164876580238342,
      "learning_rate": 9.732818974139164e-06,
      "loss": 0.7584,
      "step": 35930
    },
    {
      "epoch": 7.702528932704672,
      "grad_norm": 14.616522789001465,
      "learning_rate": 9.729961423060438e-06,
      "loss": 0.7574,
      "step": 35940
    },
    {
      "epoch": 7.7046720960137165,
      "grad_norm": 29.63897705078125,
      "learning_rate": 9.727103871981712e-06,
      "loss": 0.4615,
      "step": 35950
    },
    {
      "epoch": 7.70681525932276,
      "grad_norm": 14.6697416305542,
      "learning_rate": 9.724246320902987e-06,
      "loss": 0.9112,
      "step": 35960
    },
    {
      "epoch": 7.708958422631804,
      "grad_norm": 0.5441542267799377,
      "learning_rate": 9.721388769824261e-06,
      "loss": 1.0525,
      "step": 35970
    },
    {
      "epoch": 7.711101585940849,
      "grad_norm": 0.5598481893539429,
      "learning_rate": 9.718531218745535e-06,
      "loss": 0.3042,
      "step": 35980
    },
    {
      "epoch": 7.713244749249893,
      "grad_norm": 0.5796461701393127,
      "learning_rate": 9.715673667666811e-06,
      "loss": 0.744,
      "step": 35990
    },
    {
      "epoch": 7.715387912558937,
      "grad_norm": 0.5680239200592041,
      "learning_rate": 9.712816116588085e-06,
      "loss": 0.447,
      "step": 36000
    },
    {
      "epoch": 7.717531075867981,
      "grad_norm": 0.5753121376037598,
      "learning_rate": 9.70995856550936e-06,
      "loss": 0.8888,
      "step": 36010
    },
    {
      "epoch": 7.719674239177025,
      "grad_norm": 14.529081344604492,
      "learning_rate": 9.707101014430633e-06,
      "loss": 0.592,
      "step": 36020
    },
    {
      "epoch": 7.721817402486069,
      "grad_norm": 0.569836437702179,
      "learning_rate": 9.704243463351908e-06,
      "loss": 0.3047,
      "step": 36030
    },
    {
      "epoch": 7.723960565795114,
      "grad_norm": 14.533366203308105,
      "learning_rate": 9.701385912273182e-06,
      "loss": 1.0346,
      "step": 36040
    },
    {
      "epoch": 7.726103729104158,
      "grad_norm": 0.5573680400848389,
      "learning_rate": 9.698528361194456e-06,
      "loss": 0.5957,
      "step": 36050
    },
    {
      "epoch": 7.7282468924132015,
      "grad_norm": 14.52009391784668,
      "learning_rate": 9.695670810115732e-06,
      "loss": 0.4511,
      "step": 36060
    },
    {
      "epoch": 7.730390055722246,
      "grad_norm": 0.5068251490592957,
      "learning_rate": 9.692813259037006e-06,
      "loss": 0.1595,
      "step": 36070
    },
    {
      "epoch": 7.73253321903129,
      "grad_norm": 0.5105774402618408,
      "learning_rate": 9.689955707958281e-06,
      "loss": 0.9149,
      "step": 36080
    },
    {
      "epoch": 7.734676382340334,
      "grad_norm": 14.87792682647705,
      "learning_rate": 9.687098156879555e-06,
      "loss": 0.6174,
      "step": 36090
    },
    {
      "epoch": 7.736819545649379,
      "grad_norm": 0.48334255814552307,
      "learning_rate": 9.68424060580083e-06,
      "loss": 0.4693,
      "step": 36100
    },
    {
      "epoch": 7.7389627089584225,
      "grad_norm": 14.745501518249512,
      "learning_rate": 9.681383054722105e-06,
      "loss": 0.6122,
      "step": 36110
    },
    {
      "epoch": 7.741105872267466,
      "grad_norm": 0.46540501713752747,
      "learning_rate": 9.678525503643379e-06,
      "loss": 0.3127,
      "step": 36120
    },
    {
      "epoch": 7.743249035576511,
      "grad_norm": 0.4993009865283966,
      "learning_rate": 9.675667952564653e-06,
      "loss": 0.9115,
      "step": 36130
    },
    {
      "epoch": 7.745392198885555,
      "grad_norm": 0.5100855827331543,
      "learning_rate": 9.672810401485927e-06,
      "loss": 0.1605,
      "step": 36140
    },
    {
      "epoch": 7.747535362194599,
      "grad_norm": 0.4686059057712555,
      "learning_rate": 9.669952850407202e-06,
      "loss": 0.7646,
      "step": 36150
    },
    {
      "epoch": 7.7496785255036436,
      "grad_norm": 14.653642654418945,
      "learning_rate": 9.667095299328476e-06,
      "loss": 0.4687,
      "step": 36160
    },
    {
      "epoch": 7.751821688812687,
      "grad_norm": 30.267780303955078,
      "learning_rate": 9.66423774824975e-06,
      "loss": 0.4773,
      "step": 36170
    },
    {
      "epoch": 7.753964852121731,
      "grad_norm": 0.45861122012138367,
      "learning_rate": 9.661380197171026e-06,
      "loss": 0.9465,
      "step": 36180
    },
    {
      "epoch": 7.756108015430776,
      "grad_norm": 0.4689677357673645,
      "learning_rate": 9.6585226460923e-06,
      "loss": 0.1639,
      "step": 36190
    },
    {
      "epoch": 7.75825117873982,
      "grad_norm": 14.653583526611328,
      "learning_rate": 9.655665095013575e-06,
      "loss": 0.6212,
      "step": 36200
    },
    {
      "epoch": 7.760394342048864,
      "grad_norm": 0.4353586435317993,
      "learning_rate": 9.65280754393485e-06,
      "loss": 0.3179,
      "step": 36210
    },
    {
      "epoch": 7.762537505357908,
      "grad_norm": 14.622288703918457,
      "learning_rate": 9.649949992856123e-06,
      "loss": 0.9371,
      "step": 36220
    },
    {
      "epoch": 7.764680668666952,
      "grad_norm": 14.62222957611084,
      "learning_rate": 9.647092441777397e-06,
      "loss": 0.771,
      "step": 36230
    },
    {
      "epoch": 7.766823831975996,
      "grad_norm": 0.4917866587638855,
      "learning_rate": 9.644234890698671e-06,
      "loss": 0.4659,
      "step": 36240
    },
    {
      "epoch": 7.768966995285041,
      "grad_norm": 0.5006263852119446,
      "learning_rate": 9.641377339619947e-06,
      "loss": 0.7618,
      "step": 36250
    },
    {
      "epoch": 7.771110158594085,
      "grad_norm": 0.53555828332901,
      "learning_rate": 9.63851978854122e-06,
      "loss": 0.8995,
      "step": 36260
    },
    {
      "epoch": 7.773253321903129,
      "grad_norm": 29.58826446533203,
      "learning_rate": 9.635662237462496e-06,
      "loss": 0.5956,
      "step": 36270
    },
    {
      "epoch": 7.775396485212173,
      "grad_norm": 0.5967767238616943,
      "learning_rate": 9.63280468638377e-06,
      "loss": 1.0354,
      "step": 36280
    },
    {
      "epoch": 7.777539648521217,
      "grad_norm": 0.6439505219459534,
      "learning_rate": 9.629947135305044e-06,
      "loss": 0.7294,
      "step": 36290
    },
    {
      "epoch": 7.779682811830261,
      "grad_norm": 0.6542791724205017,
      "learning_rate": 9.62708958422632e-06,
      "loss": 0.4396,
      "step": 36300
    },
    {
      "epoch": 7.781825975139306,
      "grad_norm": 0.5936707854270935,
      "learning_rate": 9.624232033147594e-06,
      "loss": 0.1568,
      "step": 36310
    },
    {
      "epoch": 7.78396913844835,
      "grad_norm": 0.554667055606842,
      "learning_rate": 9.621374482068867e-06,
      "loss": 0.5982,
      "step": 36320
    },
    {
      "epoch": 7.786112301757393,
      "grad_norm": 15.178262710571289,
      "learning_rate": 9.618516930990143e-06,
      "loss": 0.7605,
      "step": 36330
    },
    {
      "epoch": 7.788255465066438,
      "grad_norm": 14.500691413879395,
      "learning_rate": 9.615659379911417e-06,
      "loss": 1.0451,
      "step": 36340
    },
    {
      "epoch": 7.790398628375482,
      "grad_norm": 14.464396476745605,
      "learning_rate": 9.612801828832691e-06,
      "loss": 0.5862,
      "step": 36350
    },
    {
      "epoch": 7.792541791684526,
      "grad_norm": 0.6552417278289795,
      "learning_rate": 9.609944277753965e-06,
      "loss": 0.7182,
      "step": 36360
    },
    {
      "epoch": 7.794684954993571,
      "grad_norm": 0.6627888083457947,
      "learning_rate": 9.60708672667524e-06,
      "loss": 0.7143,
      "step": 36370
    },
    {
      "epoch": 7.7968281183026145,
      "grad_norm": 14.414915084838867,
      "learning_rate": 9.604229175596514e-06,
      "loss": 0.7136,
      "step": 36380
    },
    {
      "epoch": 7.798971281611658,
      "grad_norm": 0.6496544480323792,
      "learning_rate": 9.601371624517788e-06,
      "loss": 0.1556,
      "step": 36390
    },
    {
      "epoch": 7.801114444920703,
      "grad_norm": 29.612524032592773,
      "learning_rate": 9.598514073439064e-06,
      "loss": 1.3143,
      "step": 36400
    },
    {
      "epoch": 7.803257608229747,
      "grad_norm": 14.928393363952637,
      "learning_rate": 9.595656522360338e-06,
      "loss": 0.86,
      "step": 36410
    },
    {
      "epoch": 7.805400771538792,
      "grad_norm": 0.7358129024505615,
      "learning_rate": 9.592798971281614e-06,
      "loss": 0.7102,
      "step": 36420
    },
    {
      "epoch": 7.8075439348478355,
      "grad_norm": 0.7586994171142578,
      "learning_rate": 9.589941420202887e-06,
      "loss": 0.5619,
      "step": 36430
    },
    {
      "epoch": 7.809687098156879,
      "grad_norm": 0.7847390174865723,
      "learning_rate": 9.587083869124161e-06,
      "loss": 0.5587,
      "step": 36440
    },
    {
      "epoch": 7.811830261465924,
      "grad_norm": 0.7194120287895203,
      "learning_rate": 9.584226318045435e-06,
      "loss": 0.4239,
      "step": 36450
    },
    {
      "epoch": 7.813973424774968,
      "grad_norm": 0.6768212914466858,
      "learning_rate": 9.58136876696671e-06,
      "loss": 0.5762,
      "step": 36460
    },
    {
      "epoch": 7.816116588084012,
      "grad_norm": 0.6160628795623779,
      "learning_rate": 9.578511215887985e-06,
      "loss": 0.2958,
      "step": 36470
    },
    {
      "epoch": 7.8182597513930565,
      "grad_norm": 14.495234489440918,
      "learning_rate": 9.575653664809259e-06,
      "loss": 0.7325,
      "step": 36480
    },
    {
      "epoch": 7.8204029147021,
      "grad_norm": 0.6045952439308167,
      "learning_rate": 9.572796113730534e-06,
      "loss": 0.5891,
      "step": 36490
    },
    {
      "epoch": 7.822546078011144,
      "grad_norm": 14.500303268432617,
      "learning_rate": 9.569938562651808e-06,
      "loss": 0.7367,
      "step": 36500
    },
    {
      "epoch": 7.824689241320189,
      "grad_norm": 14.474637031555176,
      "learning_rate": 9.567081011573082e-06,
      "loss": 0.7322,
      "step": 36510
    },
    {
      "epoch": 7.826832404629233,
      "grad_norm": 0.6264029741287231,
      "learning_rate": 9.564223460494358e-06,
      "loss": 0.7263,
      "step": 36520
    },
    {
      "epoch": 7.828975567938277,
      "grad_norm": 0.6355665922164917,
      "learning_rate": 9.561365909415632e-06,
      "loss": 0.4406,
      "step": 36530
    },
    {
      "epoch": 7.831118731247321,
      "grad_norm": 0.6664573550224304,
      "learning_rate": 9.558508358336906e-06,
      "loss": 1.2824,
      "step": 36540
    },
    {
      "epoch": 7.833261894556365,
      "grad_norm": 0.6981253623962402,
      "learning_rate": 9.555650807258181e-06,
      "loss": 0.4307,
      "step": 36550
    },
    {
      "epoch": 7.835405057865409,
      "grad_norm": 0.6575720906257629,
      "learning_rate": 9.552793256179455e-06,
      "loss": 0.1551,
      "step": 36560
    },
    {
      "epoch": 7.837548221174454,
      "grad_norm": 0.6300035715103149,
      "learning_rate": 9.54993570510073e-06,
      "loss": 0.5809,
      "step": 36570
    },
    {
      "epoch": 7.839691384483498,
      "grad_norm": 0.5877394080162048,
      "learning_rate": 9.547078154022003e-06,
      "loss": 0.5902,
      "step": 36580
    },
    {
      "epoch": 7.8418345477925415,
      "grad_norm": 0.56610506772995,
      "learning_rate": 9.544220602943279e-06,
      "loss": 0.5907,
      "step": 36590
    },
    {
      "epoch": 7.843977711101586,
      "grad_norm": 14.521232604980469,
      "learning_rate": 9.541363051864553e-06,
      "loss": 0.7484,
      "step": 36600
    },
    {
      "epoch": 7.84612087441063,
      "grad_norm": 0.5726339221000671,
      "learning_rate": 9.538505500785827e-06,
      "loss": 0.7433,
      "step": 36610
    },
    {
      "epoch": 7.848264037719674,
      "grad_norm": 14.532256126403809,
      "learning_rate": 9.535647949707102e-06,
      "loss": 0.3033,
      "step": 36620
    },
    {
      "epoch": 7.850407201028719,
      "grad_norm": 14.585896492004395,
      "learning_rate": 9.532790398628376e-06,
      "loss": 0.9034,
      "step": 36630
    },
    {
      "epoch": 7.8525503643377625,
      "grad_norm": 0.4867050051689148,
      "learning_rate": 9.529932847549652e-06,
      "loss": 0.0109,
      "step": 36640
    },
    {
      "epoch": 7.854693527646806,
      "grad_norm": 14.775586128234863,
      "learning_rate": 9.527075296470926e-06,
      "loss": 1.0639,
      "step": 36650
    },
    {
      "epoch": 7.856836690955851,
      "grad_norm": 16.105077743530273,
      "learning_rate": 9.5242177453922e-06,
      "loss": 0.5986,
      "step": 36660
    },
    {
      "epoch": 7.858979854264895,
      "grad_norm": 0.6041676998138428,
      "learning_rate": 9.521360194313474e-06,
      "loss": 0.8818,
      "step": 36670
    },
    {
      "epoch": 7.861123017573939,
      "grad_norm": 0.6120330691337585,
      "learning_rate": 9.518502643234748e-06,
      "loss": 0.4448,
      "step": 36680
    },
    {
      "epoch": 7.863266180882984,
      "grad_norm": 14.531508445739746,
      "learning_rate": 9.515645092156023e-06,
      "loss": 0.1581,
      "step": 36690
    },
    {
      "epoch": 7.865409344192027,
      "grad_norm": 0.5628301501274109,
      "learning_rate": 9.512787541077297e-06,
      "loss": 1.1785,
      "step": 36700
    },
    {
      "epoch": 7.867552507501071,
      "grad_norm": 0.5758888125419617,
      "learning_rate": 9.509929989998573e-06,
      "loss": 0.3027,
      "step": 36710
    },
    {
      "epoch": 7.869695670810116,
      "grad_norm": 14.548497200012207,
      "learning_rate": 9.507072438919847e-06,
      "loss": 0.4495,
      "step": 36720
    },
    {
      "epoch": 7.87183883411916,
      "grad_norm": 0.5495452880859375,
      "learning_rate": 9.50421488784112e-06,
      "loss": 1.0411,
      "step": 36730
    },
    {
      "epoch": 7.873981997428204,
      "grad_norm": 14.578526496887207,
      "learning_rate": 9.501357336762396e-06,
      "loss": 0.4509,
      "step": 36740
    },
    {
      "epoch": 7.876125160737248,
      "grad_norm": 14.856096267700195,
      "learning_rate": 9.49849978568367e-06,
      "loss": 1.037,
      "step": 36750
    },
    {
      "epoch": 7.878268324046292,
      "grad_norm": 0.6327426433563232,
      "learning_rate": 9.495642234604944e-06,
      "loss": 0.8826,
      "step": 36760
    },
    {
      "epoch": 7.880411487355336,
      "grad_norm": 0.6934381723403931,
      "learning_rate": 9.492784683526218e-06,
      "loss": 0.4397,
      "step": 36770
    },
    {
      "epoch": 7.882554650664381,
      "grad_norm": 14.45146369934082,
      "learning_rate": 9.489927132447494e-06,
      "loss": 0.2984,
      "step": 36780
    },
    {
      "epoch": 7.884697813973425,
      "grad_norm": 0.6179192662239075,
      "learning_rate": 9.487069581368768e-06,
      "loss": 1.0105,
      "step": 36790
    },
    {
      "epoch": 7.886840977282469,
      "grad_norm": 14.503679275512695,
      "learning_rate": 9.484212030290041e-06,
      "loss": 0.4448,
      "step": 36800
    },
    {
      "epoch": 7.888984140591513,
      "grad_norm": 0.5543048977851868,
      "learning_rate": 9.481354479211317e-06,
      "loss": 0.4504,
      "step": 36810
    },
    {
      "epoch": 7.891127303900557,
      "grad_norm": 14.537016868591309,
      "learning_rate": 9.478496928132591e-06,
      "loss": 0.304,
      "step": 36820
    },
    {
      "epoch": 7.893270467209601,
      "grad_norm": 0.5309580564498901,
      "learning_rate": 9.475639377053865e-06,
      "loss": 0.4555,
      "step": 36830
    },
    {
      "epoch": 7.895413630518646,
      "grad_norm": 0.5029999017715454,
      "learning_rate": 9.47278182597514e-06,
      "loss": 0.3097,
      "step": 36840
    },
    {
      "epoch": 7.89755679382769,
      "grad_norm": 0.492977499961853,
      "learning_rate": 9.469924274896414e-06,
      "loss": 0.7616,
      "step": 36850
    },
    {
      "epoch": 7.8996999571367335,
      "grad_norm": 14.567056655883789,
      "learning_rate": 9.46706672381769e-06,
      "loss": 1.0591,
      "step": 36860
    },
    {
      "epoch": 7.901843120445778,
      "grad_norm": 14.742307662963867,
      "learning_rate": 9.464209172738964e-06,
      "loss": 0.6037,
      "step": 36870
    },
    {
      "epoch": 7.903986283754822,
      "grad_norm": 14.578850746154785,
      "learning_rate": 9.461351621660238e-06,
      "loss": 0.6062,
      "step": 36880
    },
    {
      "epoch": 7.906129447063866,
      "grad_norm": 0.5311456918716431,
      "learning_rate": 9.458494070581512e-06,
      "loss": 0.4558,
      "step": 36890
    },
    {
      "epoch": 7.908272610372911,
      "grad_norm": 0.5405717492103577,
      "learning_rate": 9.455636519502786e-06,
      "loss": 1.5004,
      "step": 36900
    },
    {
      "epoch": 7.9104157736819545,
      "grad_norm": 29.558340072631836,
      "learning_rate": 9.452778968424061e-06,
      "loss": 0.7541,
      "step": 36910
    },
    {
      "epoch": 7.912558936990998,
      "grad_norm": 0.5552678108215332,
      "learning_rate": 9.449921417345335e-06,
      "loss": 1.0389,
      "step": 36920
    },
    {
      "epoch": 7.914702100300043,
      "grad_norm": 14.506500244140625,
      "learning_rate": 9.447063866266611e-06,
      "loss": 0.4498,
      "step": 36930
    },
    {
      "epoch": 7.916845263609087,
      "grad_norm": 0.6089593768119812,
      "learning_rate": 9.444206315187885e-06,
      "loss": 0.8807,
      "step": 36940
    },
    {
      "epoch": 7.918988426918132,
      "grad_norm": 0.6082099676132202,
      "learning_rate": 9.441348764109159e-06,
      "loss": 0.2997,
      "step": 36950
    },
    {
      "epoch": 7.9211315902271755,
      "grad_norm": 0.578295111656189,
      "learning_rate": 9.438491213030434e-06,
      "loss": 0.4453,
      "step": 36960
    },
    {
      "epoch": 7.923274753536219,
      "grad_norm": 0.5697396993637085,
      "learning_rate": 9.435633661951708e-06,
      "loss": 0.4451,
      "step": 36970
    },
    {
      "epoch": 7.925417916845264,
      "grad_norm": 0.5320491194725037,
      "learning_rate": 9.432776110872984e-06,
      "loss": 0.1588,
      "step": 36980
    },
    {
      "epoch": 7.927561080154308,
      "grad_norm": 14.578408241271973,
      "learning_rate": 9.429918559794256e-06,
      "loss": 0.6098,
      "step": 36990
    },
    {
      "epoch": 7.929704243463352,
      "grad_norm": 0.5010589361190796,
      "learning_rate": 9.427061008715532e-06,
      "loss": 0.6123,
      "step": 37000
    },
    {
      "epoch": 7.9318474067723965,
      "grad_norm": 0.49029815196990967,
      "learning_rate": 9.424203457636806e-06,
      "loss": 0.311,
      "step": 37010
    },
    {
      "epoch": 7.93399057008144,
      "grad_norm": 0.4828161895275116,
      "learning_rate": 9.42134590655808e-06,
      "loss": 0.7678,
      "step": 37020
    },
    {
      "epoch": 7.936133733390484,
      "grad_norm": 0.4822661280632019,
      "learning_rate": 9.418488355479355e-06,
      "loss": 0.6149,
      "step": 37030
    },
    {
      "epoch": 7.938276896699529,
      "grad_norm": 0.49774348735809326,
      "learning_rate": 9.41563080440063e-06,
      "loss": 0.7637,
      "step": 37040
    },
    {
      "epoch": 7.940420060008573,
      "grad_norm": 0.5064497590065002,
      "learning_rate": 9.412773253321905e-06,
      "loss": 0.4597,
      "step": 37050
    },
    {
      "epoch": 7.942563223317617,
      "grad_norm": 0.4956991970539093,
      "learning_rate": 9.409915702243179e-06,
      "loss": 0.6103,
      "step": 37060
    },
    {
      "epoch": 7.944706386626661,
      "grad_norm": 0.49862873554229736,
      "learning_rate": 9.407058151164453e-06,
      "loss": 0.7606,
      "step": 37070
    },
    {
      "epoch": 7.946849549935705,
      "grad_norm": 0.5116919875144958,
      "learning_rate": 9.404200600085728e-06,
      "loss": 0.6082,
      "step": 37080
    },
    {
      "epoch": 7.948992713244749,
      "grad_norm": 0.5004248023033142,
      "learning_rate": 9.401343049007e-06,
      "loss": 0.3104,
      "step": 37090
    },
    {
      "epoch": 7.951135876553794,
      "grad_norm": 0.48846492171287537,
      "learning_rate": 9.398485497928276e-06,
      "loss": 0.9092,
      "step": 37100
    },
    {
      "epoch": 7.953279039862838,
      "grad_norm": 0.5004556775093079,
      "learning_rate": 9.39562794684955e-06,
      "loss": 0.7611,
      "step": 37110
    },
    {
      "epoch": 7.9554222031718815,
      "grad_norm": 14.709461212158203,
      "learning_rate": 9.392770395770826e-06,
      "loss": 0.9117,
      "step": 37120
    },
    {
      "epoch": 7.957565366480926,
      "grad_norm": 0.5173186659812927,
      "learning_rate": 9.3899128446921e-06,
      "loss": 0.6078,
      "step": 37130
    },
    {
      "epoch": 7.95970852978997,
      "grad_norm": 0.5225741863250732,
      "learning_rate": 9.387055293613374e-06,
      "loss": 0.4564,
      "step": 37140
    },
    {
      "epoch": 7.961851693099014,
      "grad_norm": 0.511336624622345,
      "learning_rate": 9.38419774253465e-06,
      "loss": 0.7574,
      "step": 37150
    },
    {
      "epoch": 7.963994856408059,
      "grad_norm": 0.5200726389884949,
      "learning_rate": 9.381340191455923e-06,
      "loss": 1.3514,
      "step": 37160
    },
    {
      "epoch": 7.966138019717103,
      "grad_norm": 0.5187464952468872,
      "learning_rate": 9.378482640377197e-06,
      "loss": 0.3077,
      "step": 37170
    },
    {
      "epoch": 7.968281183026146,
      "grad_norm": 29.54319953918457,
      "learning_rate": 9.375625089298473e-06,
      "loss": 1.0494,
      "step": 37180
    },
    {
      "epoch": 7.970424346335191,
      "grad_norm": 0.5333645343780518,
      "learning_rate": 9.372767538219747e-06,
      "loss": 0.6052,
      "step": 37190
    },
    {
      "epoch": 7.972567509644235,
      "grad_norm": 0.5180622935295105,
      "learning_rate": 9.36990998714102e-06,
      "loss": 0.6058,
      "step": 37200
    },
    {
      "epoch": 7.974710672953279,
      "grad_norm": 0.49941563606262207,
      "learning_rate": 9.367052436062295e-06,
      "loss": 0.9131,
      "step": 37210
    },
    {
      "epoch": 7.976853836262324,
      "grad_norm": 14.579815864562988,
      "learning_rate": 9.36419488498357e-06,
      "loss": 0.7646,
      "step": 37220
    },
    {
      "epoch": 7.978996999571367,
      "grad_norm": 0.5152206420898438,
      "learning_rate": 9.361337333904844e-06,
      "loss": 0.4592,
      "step": 37230
    },
    {
      "epoch": 7.981140162880411,
      "grad_norm": 29.615131378173828,
      "learning_rate": 9.358479782826118e-06,
      "loss": 0.7569,
      "step": 37240
    },
    {
      "epoch": 7.983283326189456,
      "grad_norm": 0.5502786636352539,
      "learning_rate": 9.355622231747394e-06,
      "loss": 0.6078,
      "step": 37250
    },
    {
      "epoch": 7.9854264894985,
      "grad_norm": 0.5740960836410522,
      "learning_rate": 9.352764680668668e-06,
      "loss": 0.8912,
      "step": 37260
    },
    {
      "epoch": 7.987569652807544,
      "grad_norm": 0.5590432286262512,
      "learning_rate": 9.349907129589943e-06,
      "loss": 0.4504,
      "step": 37270
    },
    {
      "epoch": 7.9897128161165885,
      "grad_norm": 0.5462257266044617,
      "learning_rate": 9.347049578511217e-06,
      "loss": 0.7444,
      "step": 37280
    },
    {
      "epoch": 7.991855979425632,
      "grad_norm": 0.5371405482292175,
      "learning_rate": 9.344192027432491e-06,
      "loss": 0.3083,
      "step": 37290
    },
    {
      "epoch": 7.993999142734676,
      "grad_norm": 14.604005813598633,
      "learning_rate": 9.341334476353767e-06,
      "loss": 0.3123,
      "step": 37300
    },
    {
      "epoch": 7.996142306043721,
      "grad_norm": 0.4908093214035034,
      "learning_rate": 9.338476925275039e-06,
      "loss": 0.4631,
      "step": 37310
    },
    {
      "epoch": 7.998285469352765,
      "grad_norm": 14.599045753479004,
      "learning_rate": 9.335619374196315e-06,
      "loss": 0.4633,
      "step": 37320
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.8676666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.6081411838531494,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 396.108,
      "eval_samples_per_second": 7.574,
      "eval_steps_per_second": 2.525,
      "step": 37328
    },
    {
      "epoch": 8.000428632661809,
      "grad_norm": 0.47905805706977844,
      "learning_rate": 9.332761823117588e-06,
      "loss": 0.7711,
      "step": 37330
    },
    {
      "epoch": 8.002571795970853,
      "grad_norm": 0.4658600986003876,
      "learning_rate": 9.329904272038864e-06,
      "loss": 0.7664,
      "step": 37340
    },
    {
      "epoch": 8.004714959279896,
      "grad_norm": 15.055849075317383,
      "learning_rate": 9.327046720960138e-06,
      "loss": 1.0647,
      "step": 37350
    },
    {
      "epoch": 8.006858122588941,
      "grad_norm": 14.72690486907959,
      "learning_rate": 9.324189169881412e-06,
      "loss": 0.7612,
      "step": 37360
    },
    {
      "epoch": 8.009001285897986,
      "grad_norm": 14.561100006103516,
      "learning_rate": 9.321331618802688e-06,
      "loss": 0.4562,
      "step": 37370
    },
    {
      "epoch": 8.011144449207029,
      "grad_norm": 14.614825248718262,
      "learning_rate": 9.318474067723961e-06,
      "loss": 0.6039,
      "step": 37380
    },
    {
      "epoch": 8.013287612516073,
      "grad_norm": 0.5044841170310974,
      "learning_rate": 9.315616516645235e-06,
      "loss": 0.163,
      "step": 37390
    },
    {
      "epoch": 8.015430775825118,
      "grad_norm": 0.4730859398841858,
      "learning_rate": 9.312758965566511e-06,
      "loss": 0.6118,
      "step": 37400
    },
    {
      "epoch": 8.017573939134161,
      "grad_norm": 0.4646106958389282,
      "learning_rate": 9.309901414487785e-06,
      "loss": 0.6122,
      "step": 37410
    },
    {
      "epoch": 8.019717102443206,
      "grad_norm": 0.4906797707080841,
      "learning_rate": 9.307043863409059e-06,
      "loss": 0.3137,
      "step": 37420
    },
    {
      "epoch": 8.02186026575225,
      "grad_norm": 0.45033279061317444,
      "learning_rate": 9.304186312330333e-06,
      "loss": 0.617,
      "step": 37430
    },
    {
      "epoch": 8.024003429061294,
      "grad_norm": 15.057994842529297,
      "learning_rate": 9.301328761251608e-06,
      "loss": 0.3193,
      "step": 37440
    },
    {
      "epoch": 8.026146592370338,
      "grad_norm": 15.002070426940918,
      "learning_rate": 9.298471210172882e-06,
      "loss": 0.7758,
      "step": 37450
    },
    {
      "epoch": 8.028289755679383,
      "grad_norm": 0.47254183888435364,
      "learning_rate": 9.295613659094156e-06,
      "loss": 0.6175,
      "step": 37460
    },
    {
      "epoch": 8.030432918988426,
      "grad_norm": 0.474285751581192,
      "learning_rate": 9.292756108015432e-06,
      "loss": 0.3088,
      "step": 37470
    },
    {
      "epoch": 8.03257608229747,
      "grad_norm": 15.24004077911377,
      "learning_rate": 9.289898556936706e-06,
      "loss": 0.6334,
      "step": 37480
    },
    {
      "epoch": 8.034719245606516,
      "grad_norm": 0.4320087134838104,
      "learning_rate": 9.287041005857981e-06,
      "loss": 0.3211,
      "step": 37490
    },
    {
      "epoch": 8.03686240891556,
      "grad_norm": 14.679909706115723,
      "learning_rate": 9.284183454779255e-06,
      "loss": 0.3215,
      "step": 37500
    },
    {
      "epoch": 8.039005572224603,
      "grad_norm": 14.884988784790039,
      "learning_rate": 9.28132590370053e-06,
      "loss": 0.4829,
      "step": 37510
    },
    {
      "epoch": 8.041148735533648,
      "grad_norm": 14.654858589172363,
      "learning_rate": 9.278468352621803e-06,
      "loss": 1.4117,
      "step": 37520
    },
    {
      "epoch": 8.043291898842693,
      "grad_norm": 29.643836975097656,
      "learning_rate": 9.275610801543077e-06,
      "loss": 0.6165,
      "step": 37530
    },
    {
      "epoch": 8.045435062151736,
      "grad_norm": 0.48081913590431213,
      "learning_rate": 9.272753250464353e-06,
      "loss": 0.4645,
      "step": 37540
    },
    {
      "epoch": 8.04757822546078,
      "grad_norm": 0.49544769525527954,
      "learning_rate": 9.269895699385627e-06,
      "loss": 0.7646,
      "step": 37550
    },
    {
      "epoch": 8.049721388769825,
      "grad_norm": 14.774818420410156,
      "learning_rate": 9.267038148306902e-06,
      "loss": 0.7585,
      "step": 37560
    },
    {
      "epoch": 8.051864552078868,
      "grad_norm": 14.554494857788086,
      "learning_rate": 9.264180597228176e-06,
      "loss": 0.4573,
      "step": 37570
    },
    {
      "epoch": 8.054007715387913,
      "grad_norm": 0.5521295070648193,
      "learning_rate": 9.26132304614945e-06,
      "loss": 1.0426,
      "step": 37580
    },
    {
      "epoch": 8.056150878696958,
      "grad_norm": 14.519807815551758,
      "learning_rate": 9.258465495070726e-06,
      "loss": 0.5951,
      "step": 37590
    },
    {
      "epoch": 8.058294042006,
      "grad_norm": 14.574767112731934,
      "learning_rate": 9.255607943992e-06,
      "loss": 0.3046,
      "step": 37600
    },
    {
      "epoch": 8.060437205315045,
      "grad_norm": 14.48279094696045,
      "learning_rate": 9.252750392913274e-06,
      "loss": 1.0312,
      "step": 37610
    },
    {
      "epoch": 8.06258036862409,
      "grad_norm": 14.515946388244629,
      "learning_rate": 9.24989284183455e-06,
      "loss": 0.1584,
      "step": 37620
    },
    {
      "epoch": 8.064723531933133,
      "grad_norm": 0.5414459705352783,
      "learning_rate": 9.247035290755823e-06,
      "loss": 0.599,
      "step": 37630
    },
    {
      "epoch": 8.066866695242178,
      "grad_norm": 29.557998657226562,
      "learning_rate": 9.244177739677097e-06,
      "loss": 1.3359,
      "step": 37640
    },
    {
      "epoch": 8.069009858551222,
      "grad_norm": 14.471063613891602,
      "learning_rate": 9.241320188598371e-06,
      "loss": 0.8862,
      "step": 37650
    },
    {
      "epoch": 8.071153021860265,
      "grad_norm": 0.6376363039016724,
      "learning_rate": 9.238462637519647e-06,
      "loss": 0.7279,
      "step": 37660
    },
    {
      "epoch": 8.07329618516931,
      "grad_norm": 14.568885803222656,
      "learning_rate": 9.23560508644092e-06,
      "loss": 0.868,
      "step": 37670
    },
    {
      "epoch": 8.075439348478355,
      "grad_norm": 14.402351379394531,
      "learning_rate": 9.232747535362195e-06,
      "loss": 0.7142,
      "step": 37680
    },
    {
      "epoch": 8.077582511787398,
      "grad_norm": 0.6607720255851746,
      "learning_rate": 9.22988998428347e-06,
      "loss": 0.1544,
      "step": 37690
    },
    {
      "epoch": 8.079725675096443,
      "grad_norm": 0.6222673058509827,
      "learning_rate": 9.227032433204744e-06,
      "loss": 0.2965,
      "step": 37700
    },
    {
      "epoch": 8.081868838405487,
      "grad_norm": 0.5645279288291931,
      "learning_rate": 9.22417488212602e-06,
      "loss": 0.5919,
      "step": 37710
    },
    {
      "epoch": 8.08401200171453,
      "grad_norm": 0.53641277551651,
      "learning_rate": 9.221317331047294e-06,
      "loss": 0.892,
      "step": 37720
    },
    {
      "epoch": 8.086155165023575,
      "grad_norm": 0.5479286909103394,
      "learning_rate": 9.218459779968568e-06,
      "loss": 0.7458,
      "step": 37730
    },
    {
      "epoch": 8.08829832833262,
      "grad_norm": 0.5550150871276855,
      "learning_rate": 9.215602228889842e-06,
      "loss": 0.3047,
      "step": 37740
    },
    {
      "epoch": 8.090441491641663,
      "grad_norm": 0.5442484617233276,
      "learning_rate": 9.212744677811115e-06,
      "loss": 0.8944,
      "step": 37750
    },
    {
      "epoch": 8.092584654950707,
      "grad_norm": 0.5341784954071045,
      "learning_rate": 9.209887126732391e-06,
      "loss": 0.3057,
      "step": 37760
    },
    {
      "epoch": 8.094727818259752,
      "grad_norm": 0.5184030532836914,
      "learning_rate": 9.207029575653665e-06,
      "loss": 0.4529,
      "step": 37770
    },
    {
      "epoch": 8.096870981568795,
      "grad_norm": 0.4983643889427185,
      "learning_rate": 9.20417202457494e-06,
      "loss": 0.6068,
      "step": 37780
    },
    {
      "epoch": 8.09901414487784,
      "grad_norm": 15.052104949951172,
      "learning_rate": 9.201314473496215e-06,
      "loss": 0.6102,
      "step": 37790
    },
    {
      "epoch": 8.101157308186885,
      "grad_norm": 29.769474029541016,
      "learning_rate": 9.198456922417488e-06,
      "loss": 0.9106,
      "step": 37800
    },
    {
      "epoch": 8.103300471495928,
      "grad_norm": 0.5297778248786926,
      "learning_rate": 9.195599371338764e-06,
      "loss": 0.3085,
      "step": 37810
    },
    {
      "epoch": 8.105443634804972,
      "grad_norm": 14.521973609924316,
      "learning_rate": 9.192741820260038e-06,
      "loss": 0.7551,
      "step": 37820
    },
    {
      "epoch": 8.107586798114017,
      "grad_norm": 0.5263358354568481,
      "learning_rate": 9.189884269181314e-06,
      "loss": 0.606,
      "step": 37830
    },
    {
      "epoch": 8.10972996142306,
      "grad_norm": 14.568652153015137,
      "learning_rate": 9.187026718102586e-06,
      "loss": 0.7475,
      "step": 37840
    },
    {
      "epoch": 8.111873124732105,
      "grad_norm": 0.5796902179718018,
      "learning_rate": 9.184169167023862e-06,
      "loss": 0.7452,
      "step": 37850
    },
    {
      "epoch": 8.11401628804115,
      "grad_norm": 0.5840362906455994,
      "learning_rate": 9.181311615945135e-06,
      "loss": 0.7368,
      "step": 37860
    },
    {
      "epoch": 8.116159451350192,
      "grad_norm": 29.430639266967773,
      "learning_rate": 9.17845406486641e-06,
      "loss": 1.0172,
      "step": 37870
    },
    {
      "epoch": 8.118302614659237,
      "grad_norm": 0.6650676131248474,
      "learning_rate": 9.175596513787685e-06,
      "loss": 1.1518,
      "step": 37880
    },
    {
      "epoch": 8.120445777968282,
      "grad_norm": 0.7228432297706604,
      "learning_rate": 9.172738962708959e-06,
      "loss": 1.1318,
      "step": 37890
    },
    {
      "epoch": 8.122588941277325,
      "grad_norm": 0.6894814372062683,
      "learning_rate": 9.169881411630235e-06,
      "loss": 0.4338,
      "step": 37900
    },
    {
      "epoch": 8.12473210458637,
      "grad_norm": 0.6880671977996826,
      "learning_rate": 9.167023860551508e-06,
      "loss": 0.7116,
      "step": 37910
    },
    {
      "epoch": 8.126875267895414,
      "grad_norm": 14.588751792907715,
      "learning_rate": 9.164166309472782e-06,
      "loss": 0.4341,
      "step": 37920
    },
    {
      "epoch": 8.129018431204457,
      "grad_norm": 0.6803085207939148,
      "learning_rate": 9.161308758394058e-06,
      "loss": 0.8581,
      "step": 37930
    },
    {
      "epoch": 8.131161594513502,
      "grad_norm": 0.6251643896102905,
      "learning_rate": 9.158451207315332e-06,
      "loss": 0.0149,
      "step": 37940
    },
    {
      "epoch": 8.133304757822547,
      "grad_norm": 14.551342010498047,
      "learning_rate": 9.155593656236606e-06,
      "loss": 0.7281,
      "step": 37950
    },
    {
      "epoch": 8.13544792113159,
      "grad_norm": 29.57732391357422,
      "learning_rate": 9.15273610515788e-06,
      "loss": 0.7381,
      "step": 37960
    },
    {
      "epoch": 8.137591084440635,
      "grad_norm": 0.48731765151023865,
      "learning_rate": 9.149878554079155e-06,
      "loss": 0.0117,
      "step": 37970
    },
    {
      "epoch": 8.13973424774968,
      "grad_norm": 29.71847152709961,
      "learning_rate": 9.14702100300043e-06,
      "loss": 0.6247,
      "step": 37980
    },
    {
      "epoch": 8.141877411058722,
      "grad_norm": 0.44100695848464966,
      "learning_rate": 9.144163451921703e-06,
      "loss": 0.4723,
      "step": 37990
    },
    {
      "epoch": 8.144020574367767,
      "grad_norm": 0.4381246566772461,
      "learning_rate": 9.141305900842979e-06,
      "loss": 0.9366,
      "step": 38000
    },
    {
      "epoch": 8.146163737676812,
      "grad_norm": 14.635128021240234,
      "learning_rate": 9.138448349764253e-06,
      "loss": 0.933,
      "step": 38010
    },
    {
      "epoch": 8.148306900985855,
      "grad_norm": 14.602890968322754,
      "learning_rate": 9.135590798685527e-06,
      "loss": 0.6162,
      "step": 38020
    },
    {
      "epoch": 8.1504500642949,
      "grad_norm": 0.5067601799964905,
      "learning_rate": 9.132733247606802e-06,
      "loss": 0.6088,
      "step": 38030
    },
    {
      "epoch": 8.152593227603944,
      "grad_norm": 29.75931167602539,
      "learning_rate": 9.129875696528076e-06,
      "loss": 1.0604,
      "step": 38040
    },
    {
      "epoch": 8.154736390912987,
      "grad_norm": 0.5347748398780823,
      "learning_rate": 9.127018145449352e-06,
      "loss": 0.311,
      "step": 38050
    },
    {
      "epoch": 8.156879554222032,
      "grad_norm": 14.619058609008789,
      "learning_rate": 9.124160594370624e-06,
      "loss": 0.4629,
      "step": 38060
    },
    {
      "epoch": 8.159022717531077,
      "grad_norm": 14.917622566223145,
      "learning_rate": 9.1213030432919e-06,
      "loss": 0.6237,
      "step": 38070
    },
    {
      "epoch": 8.16116588084012,
      "grad_norm": 0.5008355379104614,
      "learning_rate": 9.118445492213174e-06,
      "loss": 0.7635,
      "step": 38080
    },
    {
      "epoch": 8.163309044149164,
      "grad_norm": 14.773229598999023,
      "learning_rate": 9.115587941134448e-06,
      "loss": 0.4587,
      "step": 38090
    },
    {
      "epoch": 8.165452207458209,
      "grad_norm": 0.5089226365089417,
      "learning_rate": 9.112730390055723e-06,
      "loss": 0.7584,
      "step": 38100
    },
    {
      "epoch": 8.167595370767252,
      "grad_norm": 0.5255666971206665,
      "learning_rate": 9.109872838976997e-06,
      "loss": 0.7567,
      "step": 38110
    },
    {
      "epoch": 8.169738534076297,
      "grad_norm": 0.5073253512382507,
      "learning_rate": 9.107015287898273e-06,
      "loss": 0.4589,
      "step": 38120
    },
    {
      "epoch": 8.171881697385341,
      "grad_norm": 14.566468238830566,
      "learning_rate": 9.104157736819547e-06,
      "loss": 0.9079,
      "step": 38130
    },
    {
      "epoch": 8.174024860694384,
      "grad_norm": 0.48845598101615906,
      "learning_rate": 9.10130018574082e-06,
      "loss": 0.0113,
      "step": 38140
    },
    {
      "epoch": 8.17616802400343,
      "grad_norm": 0.45917776226997375,
      "learning_rate": 9.098442634662096e-06,
      "loss": 0.7698,
      "step": 38150
    },
    {
      "epoch": 8.178311187312474,
      "grad_norm": 0.47761937975883484,
      "learning_rate": 9.09558508358337e-06,
      "loss": 0.6212,
      "step": 38160
    },
    {
      "epoch": 8.180454350621517,
      "grad_norm": 0.4707781970500946,
      "learning_rate": 9.092727532504644e-06,
      "loss": 0.6196,
      "step": 38170
    },
    {
      "epoch": 8.182597513930562,
      "grad_norm": 0.47356754541397095,
      "learning_rate": 9.089869981425918e-06,
      "loss": 0.4643,
      "step": 38180
    },
    {
      "epoch": 8.184740677239606,
      "grad_norm": 14.843405723571777,
      "learning_rate": 9.087012430347194e-06,
      "loss": 0.7714,
      "step": 38190
    },
    {
      "epoch": 8.18688384054865,
      "grad_norm": 14.604520797729492,
      "learning_rate": 9.084154879268468e-06,
      "loss": 0.9169,
      "step": 38200
    },
    {
      "epoch": 8.189027003857694,
      "grad_norm": 0.5106631517410278,
      "learning_rate": 9.081297328189742e-06,
      "loss": 0.3095,
      "step": 38210
    },
    {
      "epoch": 8.191170167166739,
      "grad_norm": 0.5822957158088684,
      "learning_rate": 9.078439777111017e-06,
      "loss": 1.1959,
      "step": 38220
    },
    {
      "epoch": 8.193313330475782,
      "grad_norm": 29.6002140045166,
      "learning_rate": 9.075582226032291e-06,
      "loss": 0.7468,
      "step": 38230
    },
    {
      "epoch": 8.195456493784826,
      "grad_norm": 0.5438111424446106,
      "learning_rate": 9.072724674953565e-06,
      "loss": 0.1605,
      "step": 38240
    },
    {
      "epoch": 8.197599657093871,
      "grad_norm": 0.47447317838668823,
      "learning_rate": 9.06986712387484e-06,
      "loss": 0.4599,
      "step": 38250
    },
    {
      "epoch": 8.199742820402914,
      "grad_norm": 0.4386378824710846,
      "learning_rate": 9.067009572796115e-06,
      "loss": 0.3177,
      "step": 38260
    },
    {
      "epoch": 8.201885983711959,
      "grad_norm": 0.4194338023662567,
      "learning_rate": 9.064152021717389e-06,
      "loss": 0.0095,
      "step": 38270
    },
    {
      "epoch": 8.204029147021004,
      "grad_norm": 0.3240107595920563,
      "learning_rate": 9.061294470638662e-06,
      "loss": 0.3339,
      "step": 38280
    },
    {
      "epoch": 8.206172310330047,
      "grad_norm": 29.927385330200195,
      "learning_rate": 9.058436919559938e-06,
      "loss": 0.6677,
      "step": 38290
    },
    {
      "epoch": 8.208315473639091,
      "grad_norm": 0.3427610397338867,
      "learning_rate": 9.055579368481212e-06,
      "loss": 0.4979,
      "step": 38300
    },
    {
      "epoch": 8.210458636948136,
      "grad_norm": 29.83814239501953,
      "learning_rate": 9.052721817402486e-06,
      "loss": 1.3013,
      "step": 38310
    },
    {
      "epoch": 8.212601800257179,
      "grad_norm": 0.42016682028770447,
      "learning_rate": 9.049864266323762e-06,
      "loss": 0.6466,
      "step": 38320
    },
    {
      "epoch": 8.214744963566224,
      "grad_norm": 0.40099188685417175,
      "learning_rate": 9.047006715245035e-06,
      "loss": 0.1665,
      "step": 38330
    },
    {
      "epoch": 8.216888126875268,
      "grad_norm": 0.3869067132472992,
      "learning_rate": 9.044149164166311e-06,
      "loss": 0.4797,
      "step": 38340
    },
    {
      "epoch": 8.219031290184311,
      "grad_norm": 0.3839808404445648,
      "learning_rate": 9.041291613087585e-06,
      "loss": 0.6402,
      "step": 38350
    },
    {
      "epoch": 8.221174453493356,
      "grad_norm": 0.38325440883636475,
      "learning_rate": 9.038434062008859e-06,
      "loss": 0.6499,
      "step": 38360
    },
    {
      "epoch": 8.223317616802401,
      "grad_norm": 0.4118891954421997,
      "learning_rate": 9.035576510930135e-06,
      "loss": 0.3266,
      "step": 38370
    },
    {
      "epoch": 8.225460780111444,
      "grad_norm": 0.37737926840782166,
      "learning_rate": 9.032718959851407e-06,
      "loss": 0.3206,
      "step": 38380
    },
    {
      "epoch": 8.227603943420489,
      "grad_norm": 14.65495491027832,
      "learning_rate": 9.029861408772682e-06,
      "loss": 1.1163,
      "step": 38390
    },
    {
      "epoch": 8.229747106729533,
      "grad_norm": 29.583404541015625,
      "learning_rate": 9.027003857693956e-06,
      "loss": 0.7786,
      "step": 38400
    },
    {
      "epoch": 8.231890270038576,
      "grad_norm": 0.455118328332901,
      "learning_rate": 9.024146306615232e-06,
      "loss": 0.1634,
      "step": 38410
    },
    {
      "epoch": 8.234033433347621,
      "grad_norm": 14.639313697814941,
      "learning_rate": 9.021288755536506e-06,
      "loss": 0.3209,
      "step": 38420
    },
    {
      "epoch": 8.236176596656666,
      "grad_norm": 0.4185853898525238,
      "learning_rate": 9.01843120445778e-06,
      "loss": 0.3196,
      "step": 38430
    },
    {
      "epoch": 8.238319759965709,
      "grad_norm": 0.4020497798919678,
      "learning_rate": 9.015573653379055e-06,
      "loss": 0.4794,
      "step": 38440
    },
    {
      "epoch": 8.240462923274753,
      "grad_norm": 14.717483520507812,
      "learning_rate": 9.01271610230033e-06,
      "loss": 0.3247,
      "step": 38450
    },
    {
      "epoch": 8.242606086583798,
      "grad_norm": 14.688095092773438,
      "learning_rate": 9.009858551221603e-06,
      "loss": 0.9591,
      "step": 38460
    },
    {
      "epoch": 8.244749249892841,
      "grad_norm": 14.688514709472656,
      "learning_rate": 9.007001000142879e-06,
      "loss": 0.6308,
      "step": 38470
    },
    {
      "epoch": 8.246892413201886,
      "grad_norm": 0.4648609459400177,
      "learning_rate": 9.004143449064153e-06,
      "loss": 0.6272,
      "step": 38480
    },
    {
      "epoch": 8.24903557651093,
      "grad_norm": 14.635838508605957,
      "learning_rate": 9.001285897985427e-06,
      "loss": 0.7686,
      "step": 38490
    },
    {
      "epoch": 8.251178739819974,
      "grad_norm": 0.4985370635986328,
      "learning_rate": 8.9984283469067e-06,
      "loss": 0.315,
      "step": 38500
    },
    {
      "epoch": 8.253321903129018,
      "grad_norm": 14.613884925842285,
      "learning_rate": 8.995570795827976e-06,
      "loss": 1.0551,
      "step": 38510
    },
    {
      "epoch": 8.255465066438063,
      "grad_norm": 14.76183032989502,
      "learning_rate": 8.99271324474925e-06,
      "loss": 0.8897,
      "step": 38520
    },
    {
      "epoch": 8.257608229747106,
      "grad_norm": 14.457980155944824,
      "learning_rate": 8.989855693670524e-06,
      "loss": 0.4437,
      "step": 38530
    },
    {
      "epoch": 8.25975139305615,
      "grad_norm": 15.22101879119873,
      "learning_rate": 8.9869981425918e-06,
      "loss": 0.5893,
      "step": 38540
    },
    {
      "epoch": 8.261894556365196,
      "grad_norm": 0.6061800718307495,
      "learning_rate": 8.984140591513074e-06,
      "loss": 0.7291,
      "step": 38550
    },
    {
      "epoch": 8.264037719674239,
      "grad_norm": 0.6012723445892334,
      "learning_rate": 8.98128304043435e-06,
      "loss": 0.5852,
      "step": 38560
    },
    {
      "epoch": 8.266180882983283,
      "grad_norm": 0.578199565410614,
      "learning_rate": 8.978425489355623e-06,
      "loss": 0.4437,
      "step": 38570
    },
    {
      "epoch": 8.268324046292328,
      "grad_norm": 0.6562801003456116,
      "learning_rate": 8.975567938276897e-06,
      "loss": 1.3025,
      "step": 38580
    },
    {
      "epoch": 8.270467209601371,
      "grad_norm": 0.6576526761054993,
      "learning_rate": 8.972710387198173e-06,
      "loss": 0.5795,
      "step": 38590
    },
    {
      "epoch": 8.272610372910416,
      "grad_norm": 0.6475791931152344,
      "learning_rate": 8.969852836119445e-06,
      "loss": 0.8628,
      "step": 38600
    },
    {
      "epoch": 8.27475353621946,
      "grad_norm": 0.6855385899543762,
      "learning_rate": 8.96699528504072e-06,
      "loss": 0.5751,
      "step": 38610
    },
    {
      "epoch": 8.276896699528503,
      "grad_norm": 14.43095588684082,
      "learning_rate": 8.964137733961995e-06,
      "loss": 0.7158,
      "step": 38620
    },
    {
      "epoch": 8.279039862837548,
      "grad_norm": 29.410879135131836,
      "learning_rate": 8.96128018288327e-06,
      "loss": 0.99,
      "step": 38630
    },
    {
      "epoch": 8.281183026146593,
      "grad_norm": 14.361342430114746,
      "learning_rate": 8.958422631804544e-06,
      "loss": 0.7068,
      "step": 38640
    },
    {
      "epoch": 8.283326189455636,
      "grad_norm": 0.7276344895362854,
      "learning_rate": 8.955565080725818e-06,
      "loss": 0.5648,
      "step": 38650
    },
    {
      "epoch": 8.28546935276468,
      "grad_norm": 14.376639366149902,
      "learning_rate": 8.952707529647094e-06,
      "loss": 0.1544,
      "step": 38660
    },
    {
      "epoch": 8.287612516073725,
      "grad_norm": 0.6491069197654724,
      "learning_rate": 8.949849978568368e-06,
      "loss": 0.4355,
      "step": 38670
    },
    {
      "epoch": 8.289755679382768,
      "grad_norm": 0.5907716751098633,
      "learning_rate": 8.946992427489643e-06,
      "loss": 0.2987,
      "step": 38680
    },
    {
      "epoch": 8.291898842691813,
      "grad_norm": 0.5546783804893494,
      "learning_rate": 8.944134876410917e-06,
      "loss": 0.596,
      "step": 38690
    },
    {
      "epoch": 8.294042006000858,
      "grad_norm": 0.5988662242889404,
      "learning_rate": 8.941277325332191e-06,
      "loss": 1.0307,
      "step": 38700
    },
    {
      "epoch": 8.2961851693099,
      "grad_norm": 0.615149736404419,
      "learning_rate": 8.938419774253465e-06,
      "loss": 0.3006,
      "step": 38710
    },
    {
      "epoch": 8.298328332618945,
      "grad_norm": 14.533111572265625,
      "learning_rate": 8.935562223174739e-06,
      "loss": 0.5899,
      "step": 38720
    },
    {
      "epoch": 8.30047149592799,
      "grad_norm": 14.533309936523438,
      "learning_rate": 8.932704672096015e-06,
      "loss": 0.4515,
      "step": 38730
    },
    {
      "epoch": 8.302614659237033,
      "grad_norm": 0.5570111274719238,
      "learning_rate": 8.929847121017289e-06,
      "loss": 0.8918,
      "step": 38740
    },
    {
      "epoch": 8.304757822546078,
      "grad_norm": 14.499398231506348,
      "learning_rate": 8.926989569938564e-06,
      "loss": 1.042,
      "step": 38750
    },
    {
      "epoch": 8.306900985855123,
      "grad_norm": 14.474124908447266,
      "learning_rate": 8.924132018859838e-06,
      "loss": 1.0243,
      "step": 38760
    },
    {
      "epoch": 8.309044149164166,
      "grad_norm": 0.617483377456665,
      "learning_rate": 8.921274467781112e-06,
      "loss": 0.4412,
      "step": 38770
    },
    {
      "epoch": 8.31118731247321,
      "grad_norm": 14.425741195678711,
      "learning_rate": 8.918416916702388e-06,
      "loss": 1.1449,
      "step": 38780
    },
    {
      "epoch": 8.313330475782255,
      "grad_norm": 0.68979811668396,
      "learning_rate": 8.915559365623662e-06,
      "loss": 0.2954,
      "step": 38790
    },
    {
      "epoch": 8.315473639091298,
      "grad_norm": 14.396418571472168,
      "learning_rate": 8.912701814544936e-06,
      "loss": 0.4338,
      "step": 38800
    },
    {
      "epoch": 8.317616802400343,
      "grad_norm": 0.6664519906044006,
      "learning_rate": 8.90984426346621e-06,
      "loss": 1.1509,
      "step": 38810
    },
    {
      "epoch": 8.319759965709387,
      "grad_norm": 0.6478496193885803,
      "learning_rate": 8.906986712387485e-06,
      "loss": 0.2959,
      "step": 38820
    },
    {
      "epoch": 8.32190312901843,
      "grad_norm": 14.427815437316895,
      "learning_rate": 8.904129161308759e-06,
      "loss": 0.8588,
      "step": 38830
    },
    {
      "epoch": 8.324046292327475,
      "grad_norm": 0.6576170325279236,
      "learning_rate": 8.901271610230033e-06,
      "loss": 0.5821,
      "step": 38840
    },
    {
      "epoch": 8.32618945563652,
      "grad_norm": 14.551643371582031,
      "learning_rate": 8.898414059151309e-06,
      "loss": 0.4399,
      "step": 38850
    },
    {
      "epoch": 8.328332618945563,
      "grad_norm": 14.45505428314209,
      "learning_rate": 8.895556508072582e-06,
      "loss": 0.7215,
      "step": 38860
    },
    {
      "epoch": 8.330475782254608,
      "grad_norm": 0.617780864238739,
      "learning_rate": 8.892698956993856e-06,
      "loss": 0.5808,
      "step": 38870
    },
    {
      "epoch": 8.332618945563652,
      "grad_norm": 14.50045394897461,
      "learning_rate": 8.889841405915132e-06,
      "loss": 0.5873,
      "step": 38880
    },
    {
      "epoch": 8.334762108872695,
      "grad_norm": 14.504071235656738,
      "learning_rate": 8.886983854836406e-06,
      "loss": 0.5899,
      "step": 38890
    },
    {
      "epoch": 8.33690527218174,
      "grad_norm": 0.6085779070854187,
      "learning_rate": 8.884126303757682e-06,
      "loss": 0.8756,
      "step": 38900
    },
    {
      "epoch": 8.339048435490785,
      "grad_norm": 0.6556026339530945,
      "learning_rate": 8.881268752678955e-06,
      "loss": 0.8693,
      "step": 38910
    },
    {
      "epoch": 8.341191598799828,
      "grad_norm": 0.637496292591095,
      "learning_rate": 8.87841120160023e-06,
      "loss": 0.5752,
      "step": 38920
    },
    {
      "epoch": 8.343334762108872,
      "grad_norm": 0.6145226955413818,
      "learning_rate": 8.875553650521503e-06,
      "loss": 0.5824,
      "step": 38930
    },
    {
      "epoch": 8.345477925417917,
      "grad_norm": 0.6319125890731812,
      "learning_rate": 8.872696099442777e-06,
      "loss": 0.5875,
      "step": 38940
    },
    {
      "epoch": 8.34762108872696,
      "grad_norm": 0.5815800428390503,
      "learning_rate": 8.869838548364053e-06,
      "loss": 0.4467,
      "step": 38950
    },
    {
      "epoch": 8.349764252036005,
      "grad_norm": 0.6094557642936707,
      "learning_rate": 8.866980997285327e-06,
      "loss": 1.1676,
      "step": 38960
    },
    {
      "epoch": 8.35190741534505,
      "grad_norm": 29.44964599609375,
      "learning_rate": 8.864123446206602e-06,
      "loss": 0.7248,
      "step": 38970
    },
    {
      "epoch": 8.354050578654093,
      "grad_norm": 0.614105224609375,
      "learning_rate": 8.861265895127876e-06,
      "loss": 0.3012,
      "step": 38980
    },
    {
      "epoch": 8.356193741963137,
      "grad_norm": 0.6067207455635071,
      "learning_rate": 8.85840834404915e-06,
      "loss": 0.4465,
      "step": 38990
    },
    {
      "epoch": 8.358336905272182,
      "grad_norm": 0.5425211191177368,
      "learning_rate": 8.855550792970426e-06,
      "loss": 0.1594,
      "step": 39000
    },
    {
      "epoch": 8.360480068581225,
      "grad_norm": 0.5316336154937744,
      "learning_rate": 8.8526932418917e-06,
      "loss": 0.7507,
      "step": 39010
    },
    {
      "epoch": 8.36262323189027,
      "grad_norm": 29.595478057861328,
      "learning_rate": 8.849835690812974e-06,
      "loss": 0.8975,
      "step": 39020
    },
    {
      "epoch": 8.364766395199315,
      "grad_norm": 0.5807597637176514,
      "learning_rate": 8.846978139734248e-06,
      "loss": 0.7409,
      "step": 39030
    },
    {
      "epoch": 8.366909558508357,
      "grad_norm": 0.5545477867126465,
      "learning_rate": 8.844120588655523e-06,
      "loss": 0.4439,
      "step": 39040
    },
    {
      "epoch": 8.369052721817402,
      "grad_norm": 0.5773362517356873,
      "learning_rate": 8.841263037576797e-06,
      "loss": 0.5938,
      "step": 39050
    },
    {
      "epoch": 8.371195885126447,
      "grad_norm": 0.5739467144012451,
      "learning_rate": 8.838405486498071e-06,
      "loss": 0.8854,
      "step": 39060
    },
    {
      "epoch": 8.37333904843549,
      "grad_norm": 14.50365161895752,
      "learning_rate": 8.835547935419347e-06,
      "loss": 1.0302,
      "step": 39070
    },
    {
      "epoch": 8.375482211744535,
      "grad_norm": 14.484899520874023,
      "learning_rate": 8.83269038434062e-06,
      "loss": 0.4438,
      "step": 39080
    },
    {
      "epoch": 8.37762537505358,
      "grad_norm": 14.466941833496094,
      "learning_rate": 8.829832833261895e-06,
      "loss": 0.8768,
      "step": 39090
    },
    {
      "epoch": 8.379768538362622,
      "grad_norm": 29.509550094604492,
      "learning_rate": 8.82697528218317e-06,
      "loss": 0.8729,
      "step": 39100
    },
    {
      "epoch": 8.381911701671667,
      "grad_norm": 0.678776741027832,
      "learning_rate": 8.824117731104444e-06,
      "loss": 1.0058,
      "step": 39110
    },
    {
      "epoch": 8.384054864980712,
      "grad_norm": 0.7177436947822571,
      "learning_rate": 8.82126018002572e-06,
      "loss": 0.8496,
      "step": 39120
    },
    {
      "epoch": 8.386198028289755,
      "grad_norm": 0.6471953988075256,
      "learning_rate": 8.818402628946992e-06,
      "loss": 0.1572,
      "step": 39130
    },
    {
      "epoch": 8.3883411915988,
      "grad_norm": 0.6161099076271057,
      "learning_rate": 8.815545077868268e-06,
      "loss": 0.7211,
      "step": 39140
    },
    {
      "epoch": 8.390484354907844,
      "grad_norm": 0.592052161693573,
      "learning_rate": 8.812687526789542e-06,
      "loss": 0.444,
      "step": 39150
    },
    {
      "epoch": 8.392627518216887,
      "grad_norm": 0.547588586807251,
      "learning_rate": 8.809829975710816e-06,
      "loss": 0.3058,
      "step": 39160
    },
    {
      "epoch": 8.394770681525932,
      "grad_norm": 29.556598663330078,
      "learning_rate": 8.806972424632091e-06,
      "loss": 0.9007,
      "step": 39170
    },
    {
      "epoch": 8.396913844834977,
      "grad_norm": 14.738595008850098,
      "learning_rate": 8.804114873553365e-06,
      "loss": 0.9047,
      "step": 39180
    },
    {
      "epoch": 8.39905700814402,
      "grad_norm": 14.541553497314453,
      "learning_rate": 8.80125732247464e-06,
      "loss": 0.6041,
      "step": 39190
    },
    {
      "epoch": 8.401200171453064,
      "grad_norm": 0.5573005080223083,
      "learning_rate": 8.798399771395915e-06,
      "loss": 0.5997,
      "step": 39200
    },
    {
      "epoch": 8.40334333476211,
      "grad_norm": 0.5614535808563232,
      "learning_rate": 8.795542220317189e-06,
      "loss": 0.449,
      "step": 39210
    },
    {
      "epoch": 8.405486498071152,
      "grad_norm": 0.5514058470726013,
      "learning_rate": 8.792684669238464e-06,
      "loss": 0.3041,
      "step": 39220
    },
    {
      "epoch": 8.407629661380197,
      "grad_norm": 14.578507423400879,
      "learning_rate": 8.789827118159738e-06,
      "loss": 0.3087,
      "step": 39230
    },
    {
      "epoch": 8.409772824689242,
      "grad_norm": 0.5109285712242126,
      "learning_rate": 8.786969567081012e-06,
      "loss": 0.4615,
      "step": 39240
    },
    {
      "epoch": 8.411915987998286,
      "grad_norm": 0.47971808910369873,
      "learning_rate": 8.784112016002286e-06,
      "loss": 0.4624,
      "step": 39250
    },
    {
      "epoch": 8.41405915130733,
      "grad_norm": 14.627548217773438,
      "learning_rate": 8.781254464923562e-06,
      "loss": 0.7638,
      "step": 39260
    },
    {
      "epoch": 8.416202314616374,
      "grad_norm": 0.51166832447052,
      "learning_rate": 8.778396913844836e-06,
      "loss": 0.61,
      "step": 39270
    },
    {
      "epoch": 8.418345477925419,
      "grad_norm": 0.5411173105239868,
      "learning_rate": 8.77553936276611e-06,
      "loss": 0.7524,
      "step": 39280
    },
    {
      "epoch": 8.420488641234462,
      "grad_norm": 0.5252895355224609,
      "learning_rate": 8.772681811687385e-06,
      "loss": 0.1588,
      "step": 39290
    },
    {
      "epoch": 8.422631804543506,
      "grad_norm": 0.5318760275840759,
      "learning_rate": 8.769824260608659e-06,
      "loss": 0.8951,
      "step": 39300
    },
    {
      "epoch": 8.424774967852551,
      "grad_norm": 0.511049211025238,
      "learning_rate": 8.766966709529933e-06,
      "loss": 0.3085,
      "step": 39310
    },
    {
      "epoch": 8.426918131161594,
      "grad_norm": 0.44941866397857666,
      "learning_rate": 8.764109158451209e-06,
      "loss": 0.3095,
      "step": 39320
    },
    {
      "epoch": 8.429061294470639,
      "grad_norm": 15.239583969116211,
      "learning_rate": 8.761251607372483e-06,
      "loss": 0.7641,
      "step": 39330
    },
    {
      "epoch": 8.431204457779684,
      "grad_norm": 14.569293975830078,
      "learning_rate": 8.758394056293758e-06,
      "loss": 0.9124,
      "step": 39340
    },
    {
      "epoch": 8.433347621088727,
      "grad_norm": 14.55481243133545,
      "learning_rate": 8.75553650521503e-06,
      "loss": 0.9001,
      "step": 39350
    },
    {
      "epoch": 8.435490784397771,
      "grad_norm": 0.5928137898445129,
      "learning_rate": 8.752678954136306e-06,
      "loss": 0.3033,
      "step": 39360
    },
    {
      "epoch": 8.437633947706816,
      "grad_norm": 0.5937514305114746,
      "learning_rate": 8.74982140305758e-06,
      "loss": 0.5877,
      "step": 39370
    },
    {
      "epoch": 8.439777111015859,
      "grad_norm": 0.5810840725898743,
      "learning_rate": 8.746963851978854e-06,
      "loss": 0.444,
      "step": 39380
    },
    {
      "epoch": 8.441920274324904,
      "grad_norm": 14.51732349395752,
      "learning_rate": 8.74410630090013e-06,
      "loss": 0.5924,
      "step": 39390
    },
    {
      "epoch": 8.444063437633949,
      "grad_norm": 29.83442497253418,
      "learning_rate": 8.741248749821403e-06,
      "loss": 0.7462,
      "step": 39400
    },
    {
      "epoch": 8.446206600942991,
      "grad_norm": 14.523000717163086,
      "learning_rate": 8.738391198742679e-06,
      "loss": 1.0292,
      "step": 39410
    },
    {
      "epoch": 8.448349764252036,
      "grad_norm": 14.446860313415527,
      "learning_rate": 8.735533647663953e-06,
      "loss": 1.161,
      "step": 39420
    },
    {
      "epoch": 8.450492927561081,
      "grad_norm": 0.6694261431694031,
      "learning_rate": 8.732676096585227e-06,
      "loss": 0.2915,
      "step": 39430
    },
    {
      "epoch": 8.452636090870124,
      "grad_norm": 14.43618392944336,
      "learning_rate": 8.729818545506502e-06,
      "loss": 0.8637,
      "step": 39440
    },
    {
      "epoch": 8.454779254179169,
      "grad_norm": 0.6626933813095093,
      "learning_rate": 8.726960994427775e-06,
      "loss": 0.7266,
      "step": 39450
    },
    {
      "epoch": 8.456922417488213,
      "grad_norm": 0.6140506267547607,
      "learning_rate": 8.72410344334905e-06,
      "loss": 0.1567,
      "step": 39460
    },
    {
      "epoch": 8.459065580797256,
      "grad_norm": 0.5840718150138855,
      "learning_rate": 8.721245892270324e-06,
      "loss": 0.4463,
      "step": 39470
    },
    {
      "epoch": 8.461208744106301,
      "grad_norm": 0.5745309591293335,
      "learning_rate": 8.7183883411916e-06,
      "loss": 0.5925,
      "step": 39480
    },
    {
      "epoch": 8.463351907415346,
      "grad_norm": 0.5279867649078369,
      "learning_rate": 8.715530790112874e-06,
      "loss": 0.3029,
      "step": 39490
    },
    {
      "epoch": 8.465495070724389,
      "grad_norm": 14.566557884216309,
      "learning_rate": 8.712673239034148e-06,
      "loss": 0.603,
      "step": 39500
    },
    {
      "epoch": 8.467638234033434,
      "grad_norm": 0.4592154026031494,
      "learning_rate": 8.709815687955423e-06,
      "loss": 0.1635,
      "step": 39510
    },
    {
      "epoch": 8.469781397342478,
      "grad_norm": 14.789597511291504,
      "learning_rate": 8.706958136876697e-06,
      "loss": 0.9296,
      "step": 39520
    },
    {
      "epoch": 8.471924560651521,
      "grad_norm": 14.59130859375,
      "learning_rate": 8.704100585797973e-06,
      "loss": 0.9201,
      "step": 39530
    },
    {
      "epoch": 8.474067723960566,
      "grad_norm": 14.570462226867676,
      "learning_rate": 8.701243034719247e-06,
      "loss": 0.6109,
      "step": 39540
    },
    {
      "epoch": 8.47621088726961,
      "grad_norm": 0.519833505153656,
      "learning_rate": 8.69838548364052e-06,
      "loss": 0.3096,
      "step": 39550
    },
    {
      "epoch": 8.478354050578654,
      "grad_norm": 0.49652475118637085,
      "learning_rate": 8.695527932561795e-06,
      "loss": 0.3063,
      "step": 39560
    },
    {
      "epoch": 8.480497213887698,
      "grad_norm": 0.4718266725540161,
      "learning_rate": 8.692670381483069e-06,
      "loss": 0.7627,
      "step": 39570
    },
    {
      "epoch": 8.482640377196743,
      "grad_norm": 14.676695823669434,
      "learning_rate": 8.689812830404344e-06,
      "loss": 0.461,
      "step": 39580
    },
    {
      "epoch": 8.484783540505786,
      "grad_norm": 0.5768975019454956,
      "learning_rate": 8.686955279325618e-06,
      "loss": 1.4995,
      "step": 39590
    },
    {
      "epoch": 8.48692670381483,
      "grad_norm": 0.6308298707008362,
      "learning_rate": 8.684097728246894e-06,
      "loss": 1.0153,
      "step": 39600
    },
    {
      "epoch": 8.489069867123876,
      "grad_norm": 0.6849349141120911,
      "learning_rate": 8.681240177168168e-06,
      "loss": 1.4227,
      "step": 39610
    },
    {
      "epoch": 8.491213030432919,
      "grad_norm": 29.4364013671875,
      "learning_rate": 8.678382626089442e-06,
      "loss": 0.994,
      "step": 39620
    },
    {
      "epoch": 8.493356193741963,
      "grad_norm": 14.328169822692871,
      "learning_rate": 8.675525075010717e-06,
      "loss": 0.6932,
      "step": 39630
    },
    {
      "epoch": 8.495499357051008,
      "grad_norm": 0.7571495771408081,
      "learning_rate": 8.672667523931991e-06,
      "loss": 0.9708,
      "step": 39640
    },
    {
      "epoch": 8.497642520360051,
      "grad_norm": 0.8796029686927795,
      "learning_rate": 8.669809972853265e-06,
      "loss": 0.814,
      "step": 39650
    },
    {
      "epoch": 8.499785683669096,
      "grad_norm": 0.9120644330978394,
      "learning_rate": 8.66695242177454e-06,
      "loss": 0.6664,
      "step": 39660
    },
    {
      "epoch": 8.50192884697814,
      "grad_norm": 14.23178768157959,
      "learning_rate": 8.664094870695815e-06,
      "loss": 0.4131,
      "step": 39670
    },
    {
      "epoch": 8.504072010287183,
      "grad_norm": 0.8086444735527039,
      "learning_rate": 8.661237319617089e-06,
      "loss": 0.5514,
      "step": 39680
    },
    {
      "epoch": 8.506215173596228,
      "grad_norm": 0.7421539425849915,
      "learning_rate": 8.658379768538363e-06,
      "loss": 0.4254,
      "step": 39690
    },
    {
      "epoch": 8.508358336905273,
      "grad_norm": 0.6787124872207642,
      "learning_rate": 8.655522217459638e-06,
      "loss": 0.2918,
      "step": 39700
    },
    {
      "epoch": 8.510501500214316,
      "grad_norm": 30.114551544189453,
      "learning_rate": 8.652664666380912e-06,
      "loss": 1.0095,
      "step": 39710
    },
    {
      "epoch": 8.51264466352336,
      "grad_norm": 0.5993309020996094,
      "learning_rate": 8.649807115302186e-06,
      "loss": 0.159,
      "step": 39720
    },
    {
      "epoch": 8.514787826832405,
      "grad_norm": 14.499994277954102,
      "learning_rate": 8.646949564223462e-06,
      "loss": 0.5873,
      "step": 39730
    },
    {
      "epoch": 8.516930990141448,
      "grad_norm": 0.5621193051338196,
      "learning_rate": 8.644092013144736e-06,
      "loss": 0.5974,
      "step": 39740
    },
    {
      "epoch": 8.519074153450493,
      "grad_norm": 14.542193412780762,
      "learning_rate": 8.641234462066011e-06,
      "loss": 0.7415,
      "step": 39750
    },
    {
      "epoch": 8.521217316759538,
      "grad_norm": 0.5476930141448975,
      "learning_rate": 8.638376910987285e-06,
      "loss": 0.302,
      "step": 39760
    },
    {
      "epoch": 8.52336048006858,
      "grad_norm": 0.5311095714569092,
      "learning_rate": 8.635519359908559e-06,
      "loss": 0.4522,
      "step": 39770
    },
    {
      "epoch": 8.525503643377625,
      "grad_norm": 0.5159612894058228,
      "learning_rate": 8.632661808829833e-06,
      "loss": 0.455,
      "step": 39780
    },
    {
      "epoch": 8.52764680668667,
      "grad_norm": 14.637826919555664,
      "learning_rate": 8.629804257751107e-06,
      "loss": 0.6105,
      "step": 39790
    },
    {
      "epoch": 8.529789969995713,
      "grad_norm": 0.4724555015563965,
      "learning_rate": 8.626946706672383e-06,
      "loss": 0.6202,
      "step": 39800
    },
    {
      "epoch": 8.531933133304758,
      "grad_norm": 14.624696731567383,
      "learning_rate": 8.624089155593656e-06,
      "loss": 0.6115,
      "step": 39810
    },
    {
      "epoch": 8.534076296613803,
      "grad_norm": 14.589044570922852,
      "learning_rate": 8.621231604514932e-06,
      "loss": 1.0579,
      "step": 39820
    },
    {
      "epoch": 8.536219459922846,
      "grad_norm": 0.566707968711853,
      "learning_rate": 8.618374053436206e-06,
      "loss": 0.7523,
      "step": 39830
    },
    {
      "epoch": 8.53836262323189,
      "grad_norm": 15.268716812133789,
      "learning_rate": 8.61551650235748e-06,
      "loss": 0.8771,
      "step": 39840
    },
    {
      "epoch": 8.540505786540935,
      "grad_norm": 0.6496421694755554,
      "learning_rate": 8.612658951278756e-06,
      "loss": 0.726,
      "step": 39850
    },
    {
      "epoch": 8.542648949849978,
      "grad_norm": 0.6587260365486145,
      "learning_rate": 8.60980140020003e-06,
      "loss": 0.8589,
      "step": 39860
    },
    {
      "epoch": 8.544792113159023,
      "grad_norm": 0.6518498659133911,
      "learning_rate": 8.606943849121303e-06,
      "loss": 0.1544,
      "step": 39870
    },
    {
      "epoch": 8.546935276468067,
      "grad_norm": 0.5971497893333435,
      "learning_rate": 8.604086298042577e-06,
      "loss": 0.2974,
      "step": 39880
    },
    {
      "epoch": 8.54907843977711,
      "grad_norm": 14.52190113067627,
      "learning_rate": 8.601228746963853e-06,
      "loss": 0.7372,
      "step": 39890
    },
    {
      "epoch": 8.551221603086155,
      "grad_norm": 0.5733822584152222,
      "learning_rate": 8.598371195885127e-06,
      "loss": 0.5949,
      "step": 39900
    },
    {
      "epoch": 8.5533647663952,
      "grad_norm": 14.539381980895996,
      "learning_rate": 8.5955136448064e-06,
      "loss": 0.7387,
      "step": 39910
    },
    {
      "epoch": 8.555507929704243,
      "grad_norm": 0.5973708033561707,
      "learning_rate": 8.592656093727676e-06,
      "loss": 0.5928,
      "step": 39920
    },
    {
      "epoch": 8.557651093013288,
      "grad_norm": 0.5814921259880066,
      "learning_rate": 8.58979854264895e-06,
      "loss": 0.4461,
      "step": 39930
    },
    {
      "epoch": 8.559794256322332,
      "grad_norm": 0.5864261984825134,
      "learning_rate": 8.586940991570224e-06,
      "loss": 1.022,
      "step": 39940
    },
    {
      "epoch": 8.561937419631375,
      "grad_norm": 0.5921859741210938,
      "learning_rate": 8.5840834404915e-06,
      "loss": 0.8762,
      "step": 39950
    },
    {
      "epoch": 8.56408058294042,
      "grad_norm": 0.601958155632019,
      "learning_rate": 8.581225889412774e-06,
      "loss": 0.4421,
      "step": 39960
    },
    {
      "epoch": 8.566223746249465,
      "grad_norm": 14.628602981567383,
      "learning_rate": 8.57836833833405e-06,
      "loss": 0.5897,
      "step": 39970
    },
    {
      "epoch": 8.568366909558508,
      "grad_norm": 0.5672439932823181,
      "learning_rate": 8.575510787255323e-06,
      "loss": 0.7441,
      "step": 39980
    },
    {
      "epoch": 8.570510072867553,
      "grad_norm": 15.457371711730957,
      "learning_rate": 8.572653236176597e-06,
      "loss": 0.5986,
      "step": 39990
    },
    {
      "epoch": 8.572653236176597,
      "grad_norm": 15.170204162597656,
      "learning_rate": 8.569795685097871e-06,
      "loss": 0.5986,
      "step": 40000
    },
    {
      "epoch": 8.57479639948564,
      "grad_norm": 0.5556941628456116,
      "learning_rate": 8.566938134019145e-06,
      "loss": 0.5979,
      "step": 40010
    },
    {
      "epoch": 8.576939562794685,
      "grad_norm": 0.5584655404090881,
      "learning_rate": 8.56408058294042e-06,
      "loss": 0.4505,
      "step": 40020
    },
    {
      "epoch": 8.57908272610373,
      "grad_norm": 14.741863250732422,
      "learning_rate": 8.561223031861695e-06,
      "loss": 0.7539,
      "step": 40030
    },
    {
      "epoch": 8.581225889412773,
      "grad_norm": 0.5662725567817688,
      "learning_rate": 8.55836548078297e-06,
      "loss": 0.4472,
      "step": 40040
    },
    {
      "epoch": 8.583369052721817,
      "grad_norm": 0.49859413504600525,
      "learning_rate": 8.555507929704244e-06,
      "loss": 0.0119,
      "step": 40050
    },
    {
      "epoch": 8.585512216030862,
      "grad_norm": 15.150300025939941,
      "learning_rate": 8.552650378625518e-06,
      "loss": 0.9274,
      "step": 40060
    },
    {
      "epoch": 8.587655379339905,
      "grad_norm": 0.45076313614845276,
      "learning_rate": 8.549792827546794e-06,
      "loss": 0.3197,
      "step": 40070
    },
    {
      "epoch": 8.58979854264895,
      "grad_norm": 0.45212671160697937,
      "learning_rate": 8.546935276468068e-06,
      "loss": 0.315,
      "step": 40080
    },
    {
      "epoch": 8.591941705957995,
      "grad_norm": 0.4144309163093567,
      "learning_rate": 8.544077725389342e-06,
      "loss": 0.3165,
      "step": 40090
    },
    {
      "epoch": 8.594084869267038,
      "grad_norm": 0.4331294298171997,
      "learning_rate": 8.541220174310616e-06,
      "loss": 0.6339,
      "step": 40100
    },
    {
      "epoch": 8.596228032576082,
      "grad_norm": 0.41573959589004517,
      "learning_rate": 8.538362623231891e-06,
      "loss": 0.6332,
      "step": 40110
    },
    {
      "epoch": 8.598371195885127,
      "grad_norm": 0.42310550808906555,
      "learning_rate": 8.535505072153165e-06,
      "loss": 0.3249,
      "step": 40120
    },
    {
      "epoch": 8.60051435919417,
      "grad_norm": 29.78066062927246,
      "learning_rate": 8.532647521074439e-06,
      "loss": 0.7841,
      "step": 40130
    },
    {
      "epoch": 8.602657522503215,
      "grad_norm": 14.649951934814453,
      "learning_rate": 8.529789969995715e-06,
      "loss": 0.4738,
      "step": 40140
    },
    {
      "epoch": 8.60480068581226,
      "grad_norm": 0.43642181158065796,
      "learning_rate": 8.526932418916989e-06,
      "loss": 0.6271,
      "step": 40150
    },
    {
      "epoch": 8.606943849121302,
      "grad_norm": 0.4470597207546234,
      "learning_rate": 8.524074867838263e-06,
      "loss": 1.0874,
      "step": 40160
    },
    {
      "epoch": 8.609087012430347,
      "grad_norm": 0.4520634114742279,
      "learning_rate": 8.521217316759538e-06,
      "loss": 0.619,
      "step": 40170
    },
    {
      "epoch": 8.611230175739392,
      "grad_norm": 14.665545463562012,
      "learning_rate": 8.518359765680812e-06,
      "loss": 0.7732,
      "step": 40180
    },
    {
      "epoch": 8.613373339048435,
      "grad_norm": 0.49020737409591675,
      "learning_rate": 8.515502214602088e-06,
      "loss": 0.7647,
      "step": 40190
    },
    {
      "epoch": 8.61551650235748,
      "grad_norm": 14.752544403076172,
      "learning_rate": 8.512644663523362e-06,
      "loss": 0.9056,
      "step": 40200
    },
    {
      "epoch": 8.617659665666524,
      "grad_norm": 0.5364652872085571,
      "learning_rate": 8.509787112444636e-06,
      "loss": 0.4543,
      "step": 40210
    },
    {
      "epoch": 8.619802828975567,
      "grad_norm": 0.5453929305076599,
      "learning_rate": 8.50692956136591e-06,
      "loss": 0.6034,
      "step": 40220
    },
    {
      "epoch": 8.621945992284612,
      "grad_norm": 0.5453084111213684,
      "learning_rate": 8.504072010287183e-06,
      "loss": 0.4529,
      "step": 40230
    },
    {
      "epoch": 8.624089155593657,
      "grad_norm": 0.5419787764549255,
      "learning_rate": 8.501214459208459e-06,
      "loss": 0.7429,
      "step": 40240
    },
    {
      "epoch": 8.6262323189027,
      "grad_norm": 14.55547046661377,
      "learning_rate": 8.498356908129733e-06,
      "loss": 0.5969,
      "step": 40250
    },
    {
      "epoch": 8.628375482211744,
      "grad_norm": 0.5314714312553406,
      "learning_rate": 8.495499357051009e-06,
      "loss": 0.752,
      "step": 40260
    },
    {
      "epoch": 8.63051864552079,
      "grad_norm": 0.5334963798522949,
      "learning_rate": 8.492641805972283e-06,
      "loss": 0.4546,
      "step": 40270
    },
    {
      "epoch": 8.632661808829832,
      "grad_norm": 29.819063186645508,
      "learning_rate": 8.489784254893557e-06,
      "loss": 0.9042,
      "step": 40280
    },
    {
      "epoch": 8.634804972138877,
      "grad_norm": 29.738800048828125,
      "learning_rate": 8.486926703814832e-06,
      "loss": 1.4803,
      "step": 40290
    },
    {
      "epoch": 8.636948135447922,
      "grad_norm": 14.465306282043457,
      "learning_rate": 8.484069152736106e-06,
      "loss": 0.4442,
      "step": 40300
    },
    {
      "epoch": 8.639091298756965,
      "grad_norm": 14.524725914001465,
      "learning_rate": 8.48121160165738e-06,
      "loss": 0.3013,
      "step": 40310
    },
    {
      "epoch": 8.64123446206601,
      "grad_norm": 0.5849031805992126,
      "learning_rate": 8.478354050578654e-06,
      "loss": 0.7357,
      "step": 40320
    },
    {
      "epoch": 8.643377625375054,
      "grad_norm": 14.45977783203125,
      "learning_rate": 8.47549649949993e-06,
      "loss": 1.0245,
      "step": 40330
    },
    {
      "epoch": 8.645520788684097,
      "grad_norm": 0.659529983997345,
      "learning_rate": 8.472638948421203e-06,
      "loss": 0.7276,
      "step": 40340
    },
    {
      "epoch": 8.647663951993142,
      "grad_norm": 0.673825204372406,
      "learning_rate": 8.469781397342477e-06,
      "loss": 0.4357,
      "step": 40350
    },
    {
      "epoch": 8.649807115302186,
      "grad_norm": 14.421731948852539,
      "learning_rate": 8.466923846263753e-06,
      "loss": 0.5719,
      "step": 40360
    },
    {
      "epoch": 8.65195027861123,
      "grad_norm": 14.562063217163086,
      "learning_rate": 8.464066295185027e-06,
      "loss": 0.574,
      "step": 40370
    },
    {
      "epoch": 8.654093441920274,
      "grad_norm": 0.6503174901008606,
      "learning_rate": 8.461208744106303e-06,
      "loss": 0.2964,
      "step": 40380
    },
    {
      "epoch": 8.656236605229319,
      "grad_norm": 14.426431655883789,
      "learning_rate": 8.458351193027577e-06,
      "loss": 1.0057,
      "step": 40390
    },
    {
      "epoch": 8.658379768538362,
      "grad_norm": 0.6383640766143799,
      "learning_rate": 8.45549364194885e-06,
      "loss": 0.4348,
      "step": 40400
    },
    {
      "epoch": 8.660522931847407,
      "grad_norm": 14.458518981933594,
      "learning_rate": 8.452636090870126e-06,
      "loss": 0.4338,
      "step": 40410
    },
    {
      "epoch": 8.662666095156451,
      "grad_norm": 14.708901405334473,
      "learning_rate": 8.449778539791398e-06,
      "loss": 1.5589,
      "step": 40420
    },
    {
      "epoch": 8.664809258465494,
      "grad_norm": 0.7132609486579895,
      "learning_rate": 8.446920988712674e-06,
      "loss": 0.4271,
      "step": 40430
    },
    {
      "epoch": 8.666952421774539,
      "grad_norm": 0.7465100884437561,
      "learning_rate": 8.444063437633948e-06,
      "loss": 0.5657,
      "step": 40440
    },
    {
      "epoch": 8.669095585083584,
      "grad_norm": 0.6843662261962891,
      "learning_rate": 8.441205886555223e-06,
      "loss": 0.2914,
      "step": 40450
    },
    {
      "epoch": 8.671238748392627,
      "grad_norm": 0.6642516851425171,
      "learning_rate": 8.438348335476497e-06,
      "loss": 0.7176,
      "step": 40460
    },
    {
      "epoch": 8.673381911701671,
      "grad_norm": 0.5944998264312744,
      "learning_rate": 8.435490784397771e-06,
      "loss": 0.1559,
      "step": 40470
    },
    {
      "epoch": 8.675525075010716,
      "grad_norm": 0.5297015309333801,
      "learning_rate": 8.432633233319047e-06,
      "loss": 0.3025,
      "step": 40480
    },
    {
      "epoch": 8.67766823831976,
      "grad_norm": 0.4955537021160126,
      "learning_rate": 8.429775682240321e-06,
      "loss": 0.6067,
      "step": 40490
    },
    {
      "epoch": 8.679811401628804,
      "grad_norm": 29.620803833007812,
      "learning_rate": 8.426918131161595e-06,
      "loss": 1.0554,
      "step": 40500
    },
    {
      "epoch": 8.681954564937849,
      "grad_norm": 0.5054922103881836,
      "learning_rate": 8.42406058008287e-06,
      "loss": 1.3538,
      "step": 40510
    },
    {
      "epoch": 8.684097728246892,
      "grad_norm": 0.5341243147850037,
      "learning_rate": 8.421203029004144e-06,
      "loss": 0.1603,
      "step": 40520
    },
    {
      "epoch": 8.686240891555936,
      "grad_norm": 0.48718157410621643,
      "learning_rate": 8.418345477925418e-06,
      "loss": 0.161,
      "step": 40530
    },
    {
      "epoch": 8.688384054864981,
      "grad_norm": 0.4141242802143097,
      "learning_rate": 8.415487926846692e-06,
      "loss": 0.319,
      "step": 40540
    },
    {
      "epoch": 8.690527218174024,
      "grad_norm": 0.40004995465278625,
      "learning_rate": 8.412630375767968e-06,
      "loss": 0.3205,
      "step": 40550
    },
    {
      "epoch": 8.692670381483069,
      "grad_norm": 14.783773422241211,
      "learning_rate": 8.409772824689242e-06,
      "loss": 0.6421,
      "step": 40560
    },
    {
      "epoch": 8.694813544792114,
      "grad_norm": 15.661186218261719,
      "learning_rate": 8.406915273610516e-06,
      "loss": 0.6512,
      "step": 40570
    },
    {
      "epoch": 8.696956708101157,
      "grad_norm": 14.784341812133789,
      "learning_rate": 8.404057722531791e-06,
      "loss": 0.8017,
      "step": 40580
    },
    {
      "epoch": 8.699099871410201,
      "grad_norm": 15.155177116394043,
      "learning_rate": 8.401200171453065e-06,
      "loss": 0.7972,
      "step": 40590
    },
    {
      "epoch": 8.701243034719246,
      "grad_norm": 0.41539284586906433,
      "learning_rate": 8.398342620374341e-06,
      "loss": 0.6321,
      "step": 40600
    },
    {
      "epoch": 8.703386198028289,
      "grad_norm": 0.47667717933654785,
      "learning_rate": 8.395485069295615e-06,
      "loss": 0.7834,
      "step": 40610
    },
    {
      "epoch": 8.705529361337334,
      "grad_norm": 0.4593808948993683,
      "learning_rate": 8.392627518216889e-06,
      "loss": 0.6154,
      "step": 40620
    },
    {
      "epoch": 8.707672524646378,
      "grad_norm": 14.682511329650879,
      "learning_rate": 8.389769967138164e-06,
      "loss": 0.9126,
      "step": 40630
    },
    {
      "epoch": 8.709815687955421,
      "grad_norm": 14.536320686340332,
      "learning_rate": 8.386912416059437e-06,
      "loss": 0.4555,
      "step": 40640
    },
    {
      "epoch": 8.711958851264466,
      "grad_norm": 0.5168392658233643,
      "learning_rate": 8.384054864980712e-06,
      "loss": 0.3071,
      "step": 40650
    },
    {
      "epoch": 8.71410201457351,
      "grad_norm": 15.088547706604004,
      "learning_rate": 8.381197313901986e-06,
      "loss": 0.9056,
      "step": 40660
    },
    {
      "epoch": 8.716245177882554,
      "grad_norm": 0.5890830755233765,
      "learning_rate": 8.378339762823262e-06,
      "loss": 0.7448,
      "step": 40670
    },
    {
      "epoch": 8.718388341191599,
      "grad_norm": 0.584683895111084,
      "learning_rate": 8.375482211744536e-06,
      "loss": 0.7357,
      "step": 40680
    },
    {
      "epoch": 8.720531504500643,
      "grad_norm": 0.6389214992523193,
      "learning_rate": 8.37262466066581e-06,
      "loss": 0.734,
      "step": 40690
    },
    {
      "epoch": 8.722674667809688,
      "grad_norm": 14.431392669677734,
      "learning_rate": 8.369767109587085e-06,
      "loss": 0.8706,
      "step": 40700
    },
    {
      "epoch": 8.724817831118731,
      "grad_norm": 14.571846008300781,
      "learning_rate": 8.366909558508359e-06,
      "loss": 1.0082,
      "step": 40710
    },
    {
      "epoch": 8.726960994427776,
      "grad_norm": 14.425955772399902,
      "learning_rate": 8.364052007429633e-06,
      "loss": 0.7191,
      "step": 40720
    },
    {
      "epoch": 8.72910415773682,
      "grad_norm": 0.629546582698822,
      "learning_rate": 8.361194456350909e-06,
      "loss": 0.5836,
      "step": 40730
    },
    {
      "epoch": 8.731247321045863,
      "grad_norm": 14.47381591796875,
      "learning_rate": 8.358336905272183e-06,
      "loss": 0.1565,
      "step": 40740
    },
    {
      "epoch": 8.733390484354908,
      "grad_norm": 0.5252736806869507,
      "learning_rate": 8.355479354193457e-06,
      "loss": 0.3038,
      "step": 40750
    },
    {
      "epoch": 8.735533647663953,
      "grad_norm": 0.4774598479270935,
      "learning_rate": 8.35262180311473e-06,
      "loss": 0.7631,
      "step": 40760
    },
    {
      "epoch": 8.737676810972996,
      "grad_norm": 0.44872748851776123,
      "learning_rate": 8.349764252036006e-06,
      "loss": 0.0104,
      "step": 40770
    },
    {
      "epoch": 8.73981997428204,
      "grad_norm": 14.681551933288574,
      "learning_rate": 8.34690670095728e-06,
      "loss": 0.7848,
      "step": 40780
    },
    {
      "epoch": 8.741963137591085,
      "grad_norm": 0.4467525780200958,
      "learning_rate": 8.344049149878554e-06,
      "loss": 0.6335,
      "step": 40790
    },
    {
      "epoch": 8.744106300900128,
      "grad_norm": 29.662609100341797,
      "learning_rate": 8.34119159879983e-06,
      "loss": 1.0777,
      "step": 40800
    },
    {
      "epoch": 8.746249464209173,
      "grad_norm": 14.615374565124512,
      "learning_rate": 8.338334047721104e-06,
      "loss": 0.6124,
      "step": 40810
    },
    {
      "epoch": 8.748392627518218,
      "grad_norm": 0.5024534463882446,
      "learning_rate": 8.335476496642379e-06,
      "loss": 0.3085,
      "step": 40820
    },
    {
      "epoch": 8.75053579082726,
      "grad_norm": 0.5186823606491089,
      "learning_rate": 8.332618945563653e-06,
      "loss": 0.9106,
      "step": 40830
    },
    {
      "epoch": 8.752678954136305,
      "grad_norm": 0.5391051173210144,
      "learning_rate": 8.329761394484927e-06,
      "loss": 0.7601,
      "step": 40840
    },
    {
      "epoch": 8.75482211744535,
      "grad_norm": 14.51574993133545,
      "learning_rate": 8.326903843406201e-06,
      "loss": 0.5958,
      "step": 40850
    },
    {
      "epoch": 8.756965280754393,
      "grad_norm": 0.5553577542304993,
      "learning_rate": 8.324046292327475e-06,
      "loss": 0.7443,
      "step": 40860
    },
    {
      "epoch": 8.759108444063438,
      "grad_norm": 0.5443171858787537,
      "learning_rate": 8.32118874124875e-06,
      "loss": 0.3035,
      "step": 40870
    },
    {
      "epoch": 8.761251607372483,
      "grad_norm": 0.49594855308532715,
      "learning_rate": 8.318331190170024e-06,
      "loss": 0.1589,
      "step": 40880
    },
    {
      "epoch": 8.763394770681526,
      "grad_norm": 14.713624000549316,
      "learning_rate": 8.3154736390913e-06,
      "loss": 0.4565,
      "step": 40890
    },
    {
      "epoch": 8.76553793399057,
      "grad_norm": 15.453202247619629,
      "learning_rate": 8.312616088012574e-06,
      "loss": 0.6191,
      "step": 40900
    },
    {
      "epoch": 8.767681097299615,
      "grad_norm": 0.5059385895729065,
      "learning_rate": 8.309758536933848e-06,
      "loss": 0.9195,
      "step": 40910
    },
    {
      "epoch": 8.769824260608658,
      "grad_norm": 0.5081188082695007,
      "learning_rate": 8.306900985855123e-06,
      "loss": 0.1603,
      "step": 40920
    },
    {
      "epoch": 8.771967423917703,
      "grad_norm": 44.76707077026367,
      "learning_rate": 8.304043434776397e-06,
      "loss": 0.4661,
      "step": 40930
    },
    {
      "epoch": 8.774110587226748,
      "grad_norm": 0.44156911969184875,
      "learning_rate": 8.301185883697671e-06,
      "loss": 0.3163,
      "step": 40940
    },
    {
      "epoch": 8.77625375053579,
      "grad_norm": 0.4304450750350952,
      "learning_rate": 8.298328332618947e-06,
      "loss": 0.6278,
      "step": 40950
    },
    {
      "epoch": 8.778396913844835,
      "grad_norm": 0.42000865936279297,
      "learning_rate": 8.295470781540221e-06,
      "loss": 0.1627,
      "step": 40960
    },
    {
      "epoch": 8.78054007715388,
      "grad_norm": 15.107049942016602,
      "learning_rate": 8.292613230461495e-06,
      "loss": 1.0994,
      "step": 40970
    },
    {
      "epoch": 8.782683240462923,
      "grad_norm": 0.43417850136756897,
      "learning_rate": 8.289755679382769e-06,
      "loss": 0.1643,
      "step": 40980
    },
    {
      "epoch": 8.784826403771968,
      "grad_norm": 14.655573844909668,
      "learning_rate": 8.286898128304044e-06,
      "loss": 0.9408,
      "step": 40990
    },
    {
      "epoch": 8.786969567081012,
      "grad_norm": 0.46536916494369507,
      "learning_rate": 8.284040577225318e-06,
      "loss": 0.6245,
      "step": 41000
    },
    {
      "epoch": 8.789112730390055,
      "grad_norm": 14.77933406829834,
      "learning_rate": 8.281183026146592e-06,
      "loss": 0.3143,
      "step": 41010
    },
    {
      "epoch": 8.7912558936991,
      "grad_norm": 0.4847539961338043,
      "learning_rate": 8.278325475067868e-06,
      "loss": 0.9209,
      "step": 41020
    },
    {
      "epoch": 8.793399057008145,
      "grad_norm": 0.5216535925865173,
      "learning_rate": 8.275467923989142e-06,
      "loss": 1.0655,
      "step": 41030
    },
    {
      "epoch": 8.795542220317188,
      "grad_norm": 14.618387222290039,
      "learning_rate": 8.272610372910417e-06,
      "loss": 0.6015,
      "step": 41040
    },
    {
      "epoch": 8.797685383626233,
      "grad_norm": 0.579293966293335,
      "learning_rate": 8.269752821831691e-06,
      "loss": 1.036,
      "step": 41050
    },
    {
      "epoch": 8.799828546935277,
      "grad_norm": 0.5700809955596924,
      "learning_rate": 8.266895270752965e-06,
      "loss": 0.3047,
      "step": 41060
    },
    {
      "epoch": 8.80197171024432,
      "grad_norm": 14.511897087097168,
      "learning_rate": 8.26403771967424e-06,
      "loss": 0.8876,
      "step": 41070
    },
    {
      "epoch": 8.804114873553365,
      "grad_norm": 0.6035382151603699,
      "learning_rate": 8.261180168595513e-06,
      "loss": 0.7243,
      "step": 41080
    },
    {
      "epoch": 8.80625803686241,
      "grad_norm": 0.627642035484314,
      "learning_rate": 8.258322617516789e-06,
      "loss": 0.8678,
      "step": 41090
    },
    {
      "epoch": 8.808401200171453,
      "grad_norm": 0.6485989689826965,
      "learning_rate": 8.255465066438063e-06,
      "loss": 0.5792,
      "step": 41100
    },
    {
      "epoch": 8.810544363480497,
      "grad_norm": 14.462654113769531,
      "learning_rate": 8.252607515359338e-06,
      "loss": 1.2829,
      "step": 41110
    },
    {
      "epoch": 8.812687526789542,
      "grad_norm": 0.6861852407455444,
      "learning_rate": 8.249749964280612e-06,
      "loss": 0.1556,
      "step": 41120
    },
    {
      "epoch": 8.814830690098585,
      "grad_norm": 0.6428253054618835,
      "learning_rate": 8.246892413201886e-06,
      "loss": 0.7201,
      "step": 41130
    },
    {
      "epoch": 8.81697385340763,
      "grad_norm": 0.614372730255127,
      "learning_rate": 8.244034862123162e-06,
      "loss": 0.4389,
      "step": 41140
    },
    {
      "epoch": 8.819117016716675,
      "grad_norm": 14.59736156463623,
      "learning_rate": 8.241177311044436e-06,
      "loss": 1.3083,
      "step": 41150
    },
    {
      "epoch": 8.821260180025718,
      "grad_norm": 0.6709574460983276,
      "learning_rate": 8.238319759965711e-06,
      "loss": 0.4376,
      "step": 41160
    },
    {
      "epoch": 8.823403343334762,
      "grad_norm": 0.6634312868118286,
      "learning_rate": 8.235462208886984e-06,
      "loss": 0.8555,
      "step": 41170
    },
    {
      "epoch": 8.825546506643807,
      "grad_norm": 29.760299682617188,
      "learning_rate": 8.23260465780826e-06,
      "loss": 0.8615,
      "step": 41180
    },
    {
      "epoch": 8.82768966995285,
      "grad_norm": 14.619097709655762,
      "learning_rate": 8.229747106729533e-06,
      "loss": 0.1596,
      "step": 41190
    },
    {
      "epoch": 8.829832833261895,
      "grad_norm": 0.5412964224815369,
      "learning_rate": 8.226889555650807e-06,
      "loss": 0.3065,
      "step": 41200
    },
    {
      "epoch": 8.83197599657094,
      "grad_norm": 0.5180597305297852,
      "learning_rate": 8.224032004572083e-06,
      "loss": 0.3089,
      "step": 41210
    },
    {
      "epoch": 8.834119159879982,
      "grad_norm": 14.613760948181152,
      "learning_rate": 8.221174453493357e-06,
      "loss": 0.4632,
      "step": 41220
    },
    {
      "epoch": 8.836262323189027,
      "grad_norm": 0.46805405616760254,
      "learning_rate": 8.218316902414632e-06,
      "loss": 0.3124,
      "step": 41230
    },
    {
      "epoch": 8.838405486498072,
      "grad_norm": 14.6704740524292,
      "learning_rate": 8.215459351335906e-06,
      "loss": 0.6216,
      "step": 41240
    },
    {
      "epoch": 8.840548649807115,
      "grad_norm": 0.4307599365711212,
      "learning_rate": 8.21260180025718e-06,
      "loss": 0.4755,
      "step": 41250
    },
    {
      "epoch": 8.84269181311616,
      "grad_norm": 0.45111510157585144,
      "learning_rate": 8.209744249178456e-06,
      "loss": 0.63,
      "step": 41260
    },
    {
      "epoch": 8.844834976425204,
      "grad_norm": 0.4155208170413971,
      "learning_rate": 8.20688669809973e-06,
      "loss": 0.4763,
      "step": 41270
    },
    {
      "epoch": 8.846978139734247,
      "grad_norm": 0.4293341636657715,
      "learning_rate": 8.204029147021004e-06,
      "loss": 0.6321,
      "step": 41280
    },
    {
      "epoch": 8.849121303043292,
      "grad_norm": 0.45336684584617615,
      "learning_rate": 8.201171595942277e-06,
      "loss": 0.6275,
      "step": 41290
    },
    {
      "epoch": 8.851264466352337,
      "grad_norm": 0.45759138464927673,
      "learning_rate": 8.198314044863553e-06,
      "loss": 0.4695,
      "step": 41300
    },
    {
      "epoch": 8.85340762966138,
      "grad_norm": 0.45386046171188354,
      "learning_rate": 8.195456493784827e-06,
      "loss": 0.9279,
      "step": 41310
    },
    {
      "epoch": 8.855550792970424,
      "grad_norm": 0.4834429919719696,
      "learning_rate": 8.192598942706101e-06,
      "loss": 0.3144,
      "step": 41320
    },
    {
      "epoch": 8.85769395627947,
      "grad_norm": 0.4597614109516144,
      "learning_rate": 8.189741391627377e-06,
      "loss": 0.6165,
      "step": 41330
    },
    {
      "epoch": 8.859837119588512,
      "grad_norm": 0.4449600875377655,
      "learning_rate": 8.18688384054865e-06,
      "loss": 0.3162,
      "step": 41340
    },
    {
      "epoch": 8.861980282897557,
      "grad_norm": 0.39962470531463623,
      "learning_rate": 8.184026289469924e-06,
      "loss": 0.1628,
      "step": 41350
    },
    {
      "epoch": 8.864123446206602,
      "grad_norm": 0.35801687836647034,
      "learning_rate": 8.1811687383912e-06,
      "loss": 0.3326,
      "step": 41360
    },
    {
      "epoch": 8.866266609515645,
      "grad_norm": 14.95975112915039,
      "learning_rate": 8.178311187312474e-06,
      "loss": 0.9819,
      "step": 41370
    },
    {
      "epoch": 8.86840977282469,
      "grad_norm": 14.77038288116455,
      "learning_rate": 8.17545363623375e-06,
      "loss": 0.8175,
      "step": 41380
    },
    {
      "epoch": 8.870552936133734,
      "grad_norm": 0.3725578188896179,
      "learning_rate": 8.172596085155022e-06,
      "loss": 1.1334,
      "step": 41390
    },
    {
      "epoch": 8.872696099442777,
      "grad_norm": 0.39028000831604004,
      "learning_rate": 8.169738534076297e-06,
      "loss": 0.3259,
      "step": 41400
    },
    {
      "epoch": 8.874839262751822,
      "grad_norm": 14.711690902709961,
      "learning_rate": 8.166880982997571e-06,
      "loss": 0.6445,
      "step": 41410
    },
    {
      "epoch": 8.876982426060867,
      "grad_norm": 0.39107292890548706,
      "learning_rate": 8.164023431918845e-06,
      "loss": 0.1675,
      "step": 41420
    },
    {
      "epoch": 8.87912558936991,
      "grad_norm": 15.043931007385254,
      "learning_rate": 8.161165880840121e-06,
      "loss": 0.6434,
      "step": 41430
    },
    {
      "epoch": 8.881268752678954,
      "grad_norm": 14.716597557067871,
      "learning_rate": 8.158308329761395e-06,
      "loss": 0.3277,
      "step": 41440
    },
    {
      "epoch": 8.883411915987999,
      "grad_norm": 29.673648834228516,
      "learning_rate": 8.15545077868267e-06,
      "loss": 0.8037,
      "step": 41450
    },
    {
      "epoch": 8.885555079297042,
      "grad_norm": 29.69738006591797,
      "learning_rate": 8.152593227603944e-06,
      "loss": 0.4845,
      "step": 41460
    },
    {
      "epoch": 8.887698242606087,
      "grad_norm": 0.40453189611434937,
      "learning_rate": 8.149735676525218e-06,
      "loss": 0.7986,
      "step": 41470
    },
    {
      "epoch": 8.889841405915131,
      "grad_norm": 29.671537399291992,
      "learning_rate": 8.146878125446494e-06,
      "loss": 0.4782,
      "step": 41480
    },
    {
      "epoch": 8.891984569224174,
      "grad_norm": 0.414335697889328,
      "learning_rate": 8.144020574367766e-06,
      "loss": 0.4797,
      "step": 41490
    },
    {
      "epoch": 8.894127732533219,
      "grad_norm": 0.39534875750541687,
      "learning_rate": 8.141163023289042e-06,
      "loss": 0.4803,
      "step": 41500
    },
    {
      "epoch": 8.896270895842264,
      "grad_norm": 0.4000735580921173,
      "learning_rate": 8.138305472210316e-06,
      "loss": 0.7977,
      "step": 41510
    },
    {
      "epoch": 8.898414059151307,
      "grad_norm": 14.779818534851074,
      "learning_rate": 8.135447921131591e-06,
      "loss": 0.1679,
      "step": 41520
    },
    {
      "epoch": 8.900557222460352,
      "grad_norm": 14.706808090209961,
      "learning_rate": 8.132590370052865e-06,
      "loss": 0.4857,
      "step": 41530
    },
    {
      "epoch": 8.902700385769396,
      "grad_norm": 0.35961535573005676,
      "learning_rate": 8.12973281897414e-06,
      "loss": 0.1682,
      "step": 41540
    },
    {
      "epoch": 8.90484354907844,
      "grad_norm": 0.3546628952026367,
      "learning_rate": 8.126875267895415e-06,
      "loss": 0.1699,
      "step": 41550
    },
    {
      "epoch": 8.906986712387484,
      "grad_norm": 0.3166404366493225,
      "learning_rate": 8.124017716816689e-06,
      "loss": 0.4969,
      "step": 41560
    },
    {
      "epoch": 8.909129875696529,
      "grad_norm": 0.35336270928382874,
      "learning_rate": 8.121160165737963e-06,
      "loss": 1.3158,
      "step": 41570
    },
    {
      "epoch": 8.911273039005572,
      "grad_norm": 29.84760093688965,
      "learning_rate": 8.118302614659238e-06,
      "loss": 0.4942,
      "step": 41580
    },
    {
      "epoch": 8.913416202314616,
      "grad_norm": 0.35991498827934265,
      "learning_rate": 8.115445063580512e-06,
      "loss": 0.6534,
      "step": 41590
    },
    {
      "epoch": 8.915559365623661,
      "grad_norm": 0.35625725984573364,
      "learning_rate": 8.112587512501786e-06,
      "loss": 0.332,
      "step": 41600
    },
    {
      "epoch": 8.917702528932704,
      "grad_norm": 0.3275846540927887,
      "learning_rate": 8.10972996142306e-06,
      "loss": 0.4945,
      "step": 41610
    },
    {
      "epoch": 8.919845692241749,
      "grad_norm": 0.32074978947639465,
      "learning_rate": 8.106872410344336e-06,
      "loss": 0.4972,
      "step": 41620
    },
    {
      "epoch": 8.921988855550794,
      "grad_norm": 0.34839627146720886,
      "learning_rate": 8.10401485926561e-06,
      "loss": 0.671,
      "step": 41630
    },
    {
      "epoch": 8.924132018859837,
      "grad_norm": 0.34181398153305054,
      "learning_rate": 8.101157308186884e-06,
      "loss": 0.4945,
      "step": 41640
    },
    {
      "epoch": 8.926275182168881,
      "grad_norm": 15.30052661895752,
      "learning_rate": 8.09829975710816e-06,
      "loss": 0.5003,
      "step": 41650
    },
    {
      "epoch": 8.928418345477926,
      "grad_norm": 14.739450454711914,
      "learning_rate": 8.095442206029433e-06,
      "loss": 0.5036,
      "step": 41660
    },
    {
      "epoch": 8.930561508786969,
      "grad_norm": 29.72890281677246,
      "learning_rate": 8.092584654950709e-06,
      "loss": 0.9954,
      "step": 41670
    },
    {
      "epoch": 8.932704672096014,
      "grad_norm": 0.3249434232711792,
      "learning_rate": 8.089727103871983e-06,
      "loss": 0.3286,
      "step": 41680
    },
    {
      "epoch": 8.934847835405058,
      "grad_norm": 0.3433579206466675,
      "learning_rate": 8.086869552793257e-06,
      "loss": 0.4973,
      "step": 41690
    },
    {
      "epoch": 8.936990998714101,
      "grad_norm": 30.308717727661133,
      "learning_rate": 8.084012001714532e-06,
      "loss": 0.9717,
      "step": 41700
    },
    {
      "epoch": 8.939134162023146,
      "grad_norm": 0.406721293926239,
      "learning_rate": 8.081154450635804e-06,
      "loss": 0.6563,
      "step": 41710
    },
    {
      "epoch": 8.94127732533219,
      "grad_norm": 0.4455731213092804,
      "learning_rate": 8.07829689955708e-06,
      "loss": 0.9431,
      "step": 41720
    },
    {
      "epoch": 8.943420488641234,
      "grad_norm": 15.152853965759277,
      "learning_rate": 8.075439348478354e-06,
      "loss": 0.6173,
      "step": 41730
    },
    {
      "epoch": 8.945563651950279,
      "grad_norm": 0.5058017373085022,
      "learning_rate": 8.07258179739963e-06,
      "loss": 0.9384,
      "step": 41740
    },
    {
      "epoch": 8.947706815259323,
      "grad_norm": 0.4571914076805115,
      "learning_rate": 8.069724246320904e-06,
      "loss": 0.7662,
      "step": 41750
    },
    {
      "epoch": 8.949849978568366,
      "grad_norm": 0.5346428751945496,
      "learning_rate": 8.066866695242178e-06,
      "loss": 1.0588,
      "step": 41760
    },
    {
      "epoch": 8.951993141877411,
      "grad_norm": 30.64133071899414,
      "learning_rate": 8.064009144163453e-06,
      "loss": 0.5967,
      "step": 41770
    },
    {
      "epoch": 8.954136305186456,
      "grad_norm": 0.5304502248764038,
      "learning_rate": 8.061151593084727e-06,
      "loss": 0.4536,
      "step": 41780
    },
    {
      "epoch": 8.956279468495499,
      "grad_norm": 0.5828642845153809,
      "learning_rate": 8.058294042006003e-06,
      "loss": 1.1914,
      "step": 41790
    },
    {
      "epoch": 8.958422631804543,
      "grad_norm": 15.322622299194336,
      "learning_rate": 8.055436490927277e-06,
      "loss": 0.5957,
      "step": 41800
    },
    {
      "epoch": 8.960565795113588,
      "grad_norm": 0.5877628922462463,
      "learning_rate": 8.05257893984855e-06,
      "loss": 0.4479,
      "step": 41810
    },
    {
      "epoch": 8.962708958422631,
      "grad_norm": 14.559794425964355,
      "learning_rate": 8.049721388769824e-06,
      "loss": 0.306,
      "step": 41820
    },
    {
      "epoch": 8.964852121731676,
      "grad_norm": 0.5100767612457275,
      "learning_rate": 8.046863837691098e-06,
      "loss": 0.4586,
      "step": 41830
    },
    {
      "epoch": 8.96699528504072,
      "grad_norm": 14.621883392333984,
      "learning_rate": 8.044006286612374e-06,
      "loss": 0.9095,
      "step": 41840
    },
    {
      "epoch": 8.969138448349764,
      "grad_norm": 0.5496936440467834,
      "learning_rate": 8.041148735533648e-06,
      "loss": 0.7565,
      "step": 41850
    },
    {
      "epoch": 8.971281611658808,
      "grad_norm": 14.537516593933105,
      "learning_rate": 8.038291184454924e-06,
      "loss": 0.3051,
      "step": 41860
    },
    {
      "epoch": 8.973424774967853,
      "grad_norm": 0.552350640296936,
      "learning_rate": 8.035433633376198e-06,
      "loss": 0.7505,
      "step": 41870
    },
    {
      "epoch": 8.975567938276896,
      "grad_norm": 15.542613983154297,
      "learning_rate": 8.032576082297471e-06,
      "loss": 1.04,
      "step": 41880
    },
    {
      "epoch": 8.97771110158594,
      "grad_norm": 14.566407203674316,
      "learning_rate": 8.029718531218747e-06,
      "loss": 1.1729,
      "step": 41890
    },
    {
      "epoch": 8.979854264894986,
      "grad_norm": 0.6559792160987854,
      "learning_rate": 8.026860980140021e-06,
      "loss": 0.7211,
      "step": 41900
    },
    {
      "epoch": 8.981997428204028,
      "grad_norm": 0.6560595631599426,
      "learning_rate": 8.024003429061295e-06,
      "loss": 0.2986,
      "step": 41910
    },
    {
      "epoch": 8.984140591513073,
      "grad_norm": 0.6223679184913635,
      "learning_rate": 8.021145877982569e-06,
      "loss": 0.2979,
      "step": 41920
    },
    {
      "epoch": 8.986283754822118,
      "grad_norm": 0.5303014516830444,
      "learning_rate": 8.018288326903844e-06,
      "loss": 0.4438,
      "step": 41930
    },
    {
      "epoch": 8.988426918131161,
      "grad_norm": 0.4583478569984436,
      "learning_rate": 8.015430775825118e-06,
      "loss": 0.4557,
      "step": 41940
    },
    {
      "epoch": 8.990570081440206,
      "grad_norm": 0.40159571170806885,
      "learning_rate": 8.012573224746392e-06,
      "loss": 0.3189,
      "step": 41950
    },
    {
      "epoch": 8.99271324474925,
      "grad_norm": 0.4198373854160309,
      "learning_rate": 8.009715673667668e-06,
      "loss": 0.4837,
      "step": 41960
    },
    {
      "epoch": 8.994856408058293,
      "grad_norm": 0.39124834537506104,
      "learning_rate": 8.006858122588942e-06,
      "loss": 0.4868,
      "step": 41970
    },
    {
      "epoch": 8.996999571367338,
      "grad_norm": 0.3781414330005646,
      "learning_rate": 8.004000571510216e-06,
      "loss": 0.4924,
      "step": 41980
    },
    {
      "epoch": 8.999142734676383,
      "grad_norm": 0.3844466507434845,
      "learning_rate": 8.001143020431491e-06,
      "loss": 0.9548,
      "step": 41990
    },
    {
      "epoch": 9.0,
      "eval_accuracy": 0.8676666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.6231423616409302,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 395.664,
      "eval_samples_per_second": 7.582,
      "eval_steps_per_second": 2.527,
      "step": 41994
    },
    {
      "epoch": 9.001285897985426,
      "grad_norm": 0.43677476048469543,
      "learning_rate": 7.998285469352765e-06,
      "loss": 0.4752,
      "step": 42000
    },
    {
      "epoch": 9.00342906129447,
      "grad_norm": 14.66409969329834,
      "learning_rate": 7.995427918274041e-06,
      "loss": 0.4764,
      "step": 42010
    },
    {
      "epoch": 9.005572224603515,
      "grad_norm": 0.40835440158843994,
      "learning_rate": 7.992570367195315e-06,
      "loss": 0.7974,
      "step": 42020
    },
    {
      "epoch": 9.007715387912558,
      "grad_norm": 0.4097338616847992,
      "learning_rate": 7.989712816116589e-06,
      "loss": 0.9571,
      "step": 42030
    },
    {
      "epoch": 9.009858551221603,
      "grad_norm": 0.44679978489875793,
      "learning_rate": 7.986855265037863e-06,
      "loss": 0.3182,
      "step": 42040
    },
    {
      "epoch": 9.012001714530648,
      "grad_norm": 0.43576177954673767,
      "learning_rate": 7.983997713959137e-06,
      "loss": 0.6292,
      "step": 42050
    },
    {
      "epoch": 9.01414487783969,
      "grad_norm": 30.196434020996094,
      "learning_rate": 7.981140162880412e-06,
      "loss": 1.2415,
      "step": 42060
    },
    {
      "epoch": 9.016288041148735,
      "grad_norm": 0.4660707712173462,
      "learning_rate": 7.978282611801686e-06,
      "loss": 0.3114,
      "step": 42070
    },
    {
      "epoch": 9.01843120445778,
      "grad_norm": 14.626656532287598,
      "learning_rate": 7.975425060722962e-06,
      "loss": 0.6196,
      "step": 42080
    },
    {
      "epoch": 9.020574367766823,
      "grad_norm": 0.4587562680244446,
      "learning_rate": 7.972567509644236e-06,
      "loss": 0.4672,
      "step": 42090
    },
    {
      "epoch": 9.022717531075868,
      "grad_norm": 0.4720102846622467,
      "learning_rate": 7.96970995856551e-06,
      "loss": 1.0715,
      "step": 42100
    },
    {
      "epoch": 9.024860694384913,
      "grad_norm": 0.4982687830924988,
      "learning_rate": 7.966852407486785e-06,
      "loss": 0.9163,
      "step": 42110
    },
    {
      "epoch": 9.027003857693956,
      "grad_norm": 14.578585624694824,
      "learning_rate": 7.96399485640806e-06,
      "loss": 0.4589,
      "step": 42120
    },
    {
      "epoch": 9.029147021003,
      "grad_norm": 0.5036783218383789,
      "learning_rate": 7.961137305329333e-06,
      "loss": 0.7589,
      "step": 42130
    },
    {
      "epoch": 9.031290184312045,
      "grad_norm": 0.48121336102485657,
      "learning_rate": 7.958279754250607e-06,
      "loss": 0.31,
      "step": 42140
    },
    {
      "epoch": 9.033433347621088,
      "grad_norm": 0.46986180543899536,
      "learning_rate": 7.955422203171883e-06,
      "loss": 0.6169,
      "step": 42150
    },
    {
      "epoch": 9.035576510930133,
      "grad_norm": 0.426484078168869,
      "learning_rate": 7.952564652093157e-06,
      "loss": 0.4682,
      "step": 42160
    },
    {
      "epoch": 9.037719674239177,
      "grad_norm": 14.596757888793945,
      "learning_rate": 7.94970710101443e-06,
      "loss": 0.9247,
      "step": 42170
    },
    {
      "epoch": 9.03986283754822,
      "grad_norm": 0.5066775679588318,
      "learning_rate": 7.946849549935706e-06,
      "loss": 0.7765,
      "step": 42180
    },
    {
      "epoch": 9.042006000857265,
      "grad_norm": 0.5081511735916138,
      "learning_rate": 7.94399199885698e-06,
      "loss": 0.463,
      "step": 42190
    },
    {
      "epoch": 9.04414916416631,
      "grad_norm": 14.692560195922852,
      "learning_rate": 7.941134447778254e-06,
      "loss": 0.7587,
      "step": 42200
    },
    {
      "epoch": 9.046292327475353,
      "grad_norm": 14.558629989624023,
      "learning_rate": 7.93827689669953e-06,
      "loss": 0.6013,
      "step": 42210
    },
    {
      "epoch": 9.048435490784398,
      "grad_norm": 0.5577500462532043,
      "learning_rate": 7.935419345620804e-06,
      "loss": 0.5968,
      "step": 42220
    },
    {
      "epoch": 9.050578654093442,
      "grad_norm": 16.478679656982422,
      "learning_rate": 7.93256179454208e-06,
      "loss": 1.1717,
      "step": 42230
    },
    {
      "epoch": 9.052721817402485,
      "grad_norm": 0.6767125129699707,
      "learning_rate": 7.929704243463353e-06,
      "loss": 0.3009,
      "step": 42240
    },
    {
      "epoch": 9.05486498071153,
      "grad_norm": 0.6686802506446838,
      "learning_rate": 7.926846692384627e-06,
      "loss": 0.5794,
      "step": 42250
    },
    {
      "epoch": 9.057008144020575,
      "grad_norm": 0.6745179891586304,
      "learning_rate": 7.923989141305901e-06,
      "loss": 0.7206,
      "step": 42260
    },
    {
      "epoch": 9.059151307329618,
      "grad_norm": 0.6665425300598145,
      "learning_rate": 7.921131590227175e-06,
      "loss": 0.5864,
      "step": 42270
    },
    {
      "epoch": 9.061294470638662,
      "grad_norm": 0.6119531989097595,
      "learning_rate": 7.91827403914845e-06,
      "loss": 0.7298,
      "step": 42280
    },
    {
      "epoch": 9.063437633947707,
      "grad_norm": 0.5437954068183899,
      "learning_rate": 7.915416488069725e-06,
      "loss": 0.5969,
      "step": 42290
    },
    {
      "epoch": 9.06558079725675,
      "grad_norm": 14.68488883972168,
      "learning_rate": 7.912558936991e-06,
      "loss": 0.6038,
      "step": 42300
    },
    {
      "epoch": 9.067723960565795,
      "grad_norm": 0.5363250374794006,
      "learning_rate": 7.909701385912274e-06,
      "loss": 0.7623,
      "step": 42310
    },
    {
      "epoch": 9.06986712387484,
      "grad_norm": 0.5673972964286804,
      "learning_rate": 7.906843834833548e-06,
      "loss": 0.4512,
      "step": 42320
    },
    {
      "epoch": 9.072010287183883,
      "grad_norm": 30.304920196533203,
      "learning_rate": 7.903986283754824e-06,
      "loss": 0.7648,
      "step": 42330
    },
    {
      "epoch": 9.074153450492927,
      "grad_norm": 29.6633243560791,
      "learning_rate": 7.901128732676098e-06,
      "loss": 0.7509,
      "step": 42340
    },
    {
      "epoch": 9.076296613801972,
      "grad_norm": 0.5592362880706787,
      "learning_rate": 7.898271181597371e-06,
      "loss": 0.4548,
      "step": 42350
    },
    {
      "epoch": 9.078439777111015,
      "grad_norm": 0.5631505846977234,
      "learning_rate": 7.895413630518645e-06,
      "loss": 0.7487,
      "step": 42360
    },
    {
      "epoch": 9.08058294042006,
      "grad_norm": 14.50558090209961,
      "learning_rate": 7.892556079439921e-06,
      "loss": 1.0376,
      "step": 42370
    },
    {
      "epoch": 9.082726103729104,
      "grad_norm": 0.5799493789672852,
      "learning_rate": 7.889698528361195e-06,
      "loss": 0.5956,
      "step": 42380
    },
    {
      "epoch": 9.084869267038147,
      "grad_norm": 0.5811642408370972,
      "learning_rate": 7.886840977282469e-06,
      "loss": 0.4395,
      "step": 42390
    },
    {
      "epoch": 9.087012430347192,
      "grad_norm": 14.526507377624512,
      "learning_rate": 7.883983426203745e-06,
      "loss": 0.3045,
      "step": 42400
    },
    {
      "epoch": 9.089155593656237,
      "grad_norm": 29.61850357055664,
      "learning_rate": 7.881125875125018e-06,
      "loss": 0.7446,
      "step": 42410
    },
    {
      "epoch": 9.09129875696528,
      "grad_norm": 14.55030345916748,
      "learning_rate": 7.878268324046292e-06,
      "loss": 1.337,
      "step": 42420
    },
    {
      "epoch": 9.093441920274325,
      "grad_norm": 14.48919677734375,
      "learning_rate": 7.875410772967568e-06,
      "loss": 0.5943,
      "step": 42430
    },
    {
      "epoch": 9.09558508358337,
      "grad_norm": 0.5794715881347656,
      "learning_rate": 7.872553221888842e-06,
      "loss": 0.4501,
      "step": 42440
    },
    {
      "epoch": 9.097728246892412,
      "grad_norm": 14.551466941833496,
      "learning_rate": 7.869695670810118e-06,
      "loss": 0.1596,
      "step": 42450
    },
    {
      "epoch": 9.099871410201457,
      "grad_norm": 0.5266921520233154,
      "learning_rate": 7.86683811973139e-06,
      "loss": 0.7531,
      "step": 42460
    },
    {
      "epoch": 9.102014573510502,
      "grad_norm": 14.56082534790039,
      "learning_rate": 7.863980568652665e-06,
      "loss": 0.7496,
      "step": 42470
    },
    {
      "epoch": 9.104157736819545,
      "grad_norm": 0.5350815653800964,
      "learning_rate": 7.86112301757394e-06,
      "loss": 0.4541,
      "step": 42480
    },
    {
      "epoch": 9.10630090012859,
      "grad_norm": 0.5387583374977112,
      "learning_rate": 7.858265466495213e-06,
      "loss": 0.9001,
      "step": 42490
    },
    {
      "epoch": 9.108444063437634,
      "grad_norm": 14.579002380371094,
      "learning_rate": 7.855407915416489e-06,
      "loss": 0.6013,
      "step": 42500
    },
    {
      "epoch": 9.110587226746677,
      "grad_norm": 14.560351371765137,
      "learning_rate": 7.852550364337763e-06,
      "loss": 0.7448,
      "step": 42510
    },
    {
      "epoch": 9.112730390055722,
      "grad_norm": 14.564144134521484,
      "learning_rate": 7.849692813259038e-06,
      "loss": 0.3055,
      "step": 42520
    },
    {
      "epoch": 9.114873553364767,
      "grad_norm": 15.495261192321777,
      "learning_rate": 7.846835262180312e-06,
      "loss": 1.0593,
      "step": 42530
    },
    {
      "epoch": 9.117016716673811,
      "grad_norm": 0.5987873673439026,
      "learning_rate": 7.843977711101586e-06,
      "loss": 0.4505,
      "step": 42540
    },
    {
      "epoch": 9.119159879982854,
      "grad_norm": 0.5349926948547363,
      "learning_rate": 7.841120160022862e-06,
      "loss": 0.3058,
      "step": 42550
    },
    {
      "epoch": 9.1213030432919,
      "grad_norm": 14.540969848632812,
      "learning_rate": 7.838262608944136e-06,
      "loss": 0.8966,
      "step": 42560
    },
    {
      "epoch": 9.123446206600944,
      "grad_norm": 14.586857795715332,
      "learning_rate": 7.83540505786541e-06,
      "loss": 0.8963,
      "step": 42570
    },
    {
      "epoch": 9.125589369909987,
      "grad_norm": 29.544750213623047,
      "learning_rate": 7.832547506786684e-06,
      "loss": 0.8905,
      "step": 42580
    },
    {
      "epoch": 9.127732533219032,
      "grad_norm": 14.491951942443848,
      "learning_rate": 7.82968995570796e-06,
      "loss": 1.0212,
      "step": 42590
    },
    {
      "epoch": 9.129875696528076,
      "grad_norm": 0.6202067136764526,
      "learning_rate": 7.826832404629233e-06,
      "loss": 0.0137,
      "step": 42600
    },
    {
      "epoch": 9.13201885983712,
      "grad_norm": 0.6152387857437134,
      "learning_rate": 7.823974853550507e-06,
      "loss": 0.874,
      "step": 42610
    },
    {
      "epoch": 9.134162023146164,
      "grad_norm": 0.6005731821060181,
      "learning_rate": 7.821117302471783e-06,
      "loss": 0.4405,
      "step": 42620
    },
    {
      "epoch": 9.136305186455209,
      "grad_norm": 0.5370758175849915,
      "learning_rate": 7.818259751393057e-06,
      "loss": 0.3098,
      "step": 42630
    },
    {
      "epoch": 9.138448349764252,
      "grad_norm": 0.516925036907196,
      "learning_rate": 7.815402200314332e-06,
      "loss": 0.3095,
      "step": 42640
    },
    {
      "epoch": 9.140591513073296,
      "grad_norm": 15.843290328979492,
      "learning_rate": 7.812544649235606e-06,
      "loss": 0.3118,
      "step": 42650
    },
    {
      "epoch": 9.142734676382341,
      "grad_norm": 0.45846667885780334,
      "learning_rate": 7.80968709815688e-06,
      "loss": 0.4666,
      "step": 42660
    },
    {
      "epoch": 9.144877839691384,
      "grad_norm": 14.637898445129395,
      "learning_rate": 7.806829547078156e-06,
      "loss": 0.6285,
      "step": 42670
    },
    {
      "epoch": 9.147021003000429,
      "grad_norm": 14.623382568359375,
      "learning_rate": 7.803971995999428e-06,
      "loss": 0.9293,
      "step": 42680
    },
    {
      "epoch": 9.149164166309474,
      "grad_norm": 0.4629839360713959,
      "learning_rate": 7.801114444920704e-06,
      "loss": 1.2378,
      "step": 42690
    },
    {
      "epoch": 9.151307329618517,
      "grad_norm": 0.47543999552726746,
      "learning_rate": 7.798256893841978e-06,
      "loss": 0.1592,
      "step": 42700
    },
    {
      "epoch": 9.153450492927561,
      "grad_norm": 14.707661628723145,
      "learning_rate": 7.795399342763253e-06,
      "loss": 0.6263,
      "step": 42710
    },
    {
      "epoch": 9.155593656236606,
      "grad_norm": 0.41946497559547424,
      "learning_rate": 7.792541791684527e-06,
      "loss": 0.0096,
      "step": 42720
    },
    {
      "epoch": 9.157736819545649,
      "grad_norm": 14.6494140625,
      "learning_rate": 7.789684240605801e-06,
      "loss": 1.2617,
      "step": 42730
    },
    {
      "epoch": 9.159879982854694,
      "grad_norm": 0.44930049777030945,
      "learning_rate": 7.786826689527077e-06,
      "loss": 0.3175,
      "step": 42740
    },
    {
      "epoch": 9.162023146163738,
      "grad_norm": 0.4527510106563568,
      "learning_rate": 7.78396913844835e-06,
      "loss": 0.7743,
      "step": 42750
    },
    {
      "epoch": 9.164166309472781,
      "grad_norm": 14.609267234802246,
      "learning_rate": 7.781111587369625e-06,
      "loss": 0.929,
      "step": 42760
    },
    {
      "epoch": 9.166309472781826,
      "grad_norm": 0.4844715893268585,
      "learning_rate": 7.7782540362909e-06,
      "loss": 0.6183,
      "step": 42770
    },
    {
      "epoch": 9.168452636090871,
      "grad_norm": 0.4824356734752655,
      "learning_rate": 7.775396485212174e-06,
      "loss": 0.3129,
      "step": 42780
    },
    {
      "epoch": 9.170595799399914,
      "grad_norm": 0.47937682271003723,
      "learning_rate": 7.772538934133448e-06,
      "loss": 0.4631,
      "step": 42790
    },
    {
      "epoch": 9.172738962708959,
      "grad_norm": 14.672234535217285,
      "learning_rate": 7.769681383054722e-06,
      "loss": 0.6242,
      "step": 42800
    },
    {
      "epoch": 9.174882126018003,
      "grad_norm": 14.59567928314209,
      "learning_rate": 7.766823831975998e-06,
      "loss": 0.928,
      "step": 42810
    },
    {
      "epoch": 9.177025289327046,
      "grad_norm": 0.4745795428752899,
      "learning_rate": 7.763966280897272e-06,
      "loss": 0.4652,
      "step": 42820
    },
    {
      "epoch": 9.179168452636091,
      "grad_norm": 0.473651647567749,
      "learning_rate": 7.761108729818545e-06,
      "loss": 0.3144,
      "step": 42830
    },
    {
      "epoch": 9.181311615945136,
      "grad_norm": 14.734334945678711,
      "learning_rate": 7.758251178739821e-06,
      "loss": 0.4705,
      "step": 42840
    },
    {
      "epoch": 9.183454779254179,
      "grad_norm": 14.637655258178711,
      "learning_rate": 7.755393627661095e-06,
      "loss": 0.3162,
      "step": 42850
    },
    {
      "epoch": 9.185597942563223,
      "grad_norm": 0.42837992310523987,
      "learning_rate": 7.75253607658237e-06,
      "loss": 0.1657,
      "step": 42860
    },
    {
      "epoch": 9.187741105872268,
      "grad_norm": 0.4289543926715851,
      "learning_rate": 7.749678525503645e-06,
      "loss": 0.786,
      "step": 42870
    },
    {
      "epoch": 9.189884269181311,
      "grad_norm": 0.42663779854774475,
      "learning_rate": 7.746820974424918e-06,
      "loss": 0.4764,
      "step": 42880
    },
    {
      "epoch": 9.192027432490356,
      "grad_norm": 0.42803338170051575,
      "learning_rate": 7.743963423346192e-06,
      "loss": 0.3199,
      "step": 42890
    },
    {
      "epoch": 9.1941705957994,
      "grad_norm": 0.40659525990486145,
      "learning_rate": 7.741105872267466e-06,
      "loss": 0.3208,
      "step": 42900
    },
    {
      "epoch": 9.196313759108444,
      "grad_norm": 0.39215517044067383,
      "learning_rate": 7.738248321188742e-06,
      "loss": 0.4811,
      "step": 42910
    },
    {
      "epoch": 9.198456922417488,
      "grad_norm": 14.666473388671875,
      "learning_rate": 7.735390770110016e-06,
      "loss": 0.638,
      "step": 42920
    },
    {
      "epoch": 9.200600085726533,
      "grad_norm": 0.37691909074783325,
      "learning_rate": 7.732533219031292e-06,
      "loss": 0.3246,
      "step": 42930
    },
    {
      "epoch": 9.202743249035576,
      "grad_norm": 14.891683578491211,
      "learning_rate": 7.729675667952565e-06,
      "loss": 0.8165,
      "step": 42940
    },
    {
      "epoch": 9.20488641234462,
      "grad_norm": 0.40234583616256714,
      "learning_rate": 7.72681811687384e-06,
      "loss": 0.8112,
      "step": 42950
    },
    {
      "epoch": 9.207029575653666,
      "grad_norm": 0.42816075682640076,
      "learning_rate": 7.723960565795115e-06,
      "loss": 0.9472,
      "step": 42960
    },
    {
      "epoch": 9.209172738962708,
      "grad_norm": 0.4484803080558777,
      "learning_rate": 7.721103014716389e-06,
      "loss": 0.4725,
      "step": 42970
    },
    {
      "epoch": 9.211315902271753,
      "grad_norm": 14.642548561096191,
      "learning_rate": 7.718245463637663e-06,
      "loss": 1.0807,
      "step": 42980
    },
    {
      "epoch": 9.213459065580798,
      "grad_norm": 0.49904200434684753,
      "learning_rate": 7.715387912558938e-06,
      "loss": 0.9206,
      "step": 42990
    },
    {
      "epoch": 9.215602228889841,
      "grad_norm": 0.550734281539917,
      "learning_rate": 7.712530361480212e-06,
      "loss": 0.4581,
      "step": 43000
    },
    {
      "epoch": 9.217745392198886,
      "grad_norm": 14.617002487182617,
      "learning_rate": 7.709672810401486e-06,
      "loss": 0.7511,
      "step": 43010
    },
    {
      "epoch": 9.21988855550793,
      "grad_norm": 0.5615535974502563,
      "learning_rate": 7.70681525932276e-06,
      "loss": 0.6019,
      "step": 43020
    },
    {
      "epoch": 9.222031718816973,
      "grad_norm": 29.61370849609375,
      "learning_rate": 7.703957708244036e-06,
      "loss": 0.4539,
      "step": 43030
    },
    {
      "epoch": 9.224174882126018,
      "grad_norm": 0.5390946865081787,
      "learning_rate": 7.70110015716531e-06,
      "loss": 0.5989,
      "step": 43040
    },
    {
      "epoch": 9.226318045435063,
      "grad_norm": 0.546570897102356,
      "learning_rate": 7.698242606086584e-06,
      "loss": 0.8951,
      "step": 43050
    },
    {
      "epoch": 9.228461208744106,
      "grad_norm": 14.620776176452637,
      "learning_rate": 7.69538505500786e-06,
      "loss": 1.0378,
      "step": 43060
    },
    {
      "epoch": 9.23060437205315,
      "grad_norm": 14.520927429199219,
      "learning_rate": 7.692527503929133e-06,
      "loss": 0.5976,
      "step": 43070
    },
    {
      "epoch": 9.232747535362195,
      "grad_norm": 14.579319953918457,
      "learning_rate": 7.689669952850409e-06,
      "loss": 0.16,
      "step": 43080
    },
    {
      "epoch": 9.234890698671238,
      "grad_norm": 0.5522915720939636,
      "learning_rate": 7.686812401771683e-06,
      "loss": 1.1918,
      "step": 43090
    },
    {
      "epoch": 9.237033861980283,
      "grad_norm": 0.5437493920326233,
      "learning_rate": 7.683954850692957e-06,
      "loss": 0.5972,
      "step": 43100
    },
    {
      "epoch": 9.239177025289328,
      "grad_norm": 0.545063316822052,
      "learning_rate": 7.68109729961423e-06,
      "loss": 0.4515,
      "step": 43110
    },
    {
      "epoch": 9.24132018859837,
      "grad_norm": 14.579835891723633,
      "learning_rate": 7.678239748535505e-06,
      "loss": 0.8964,
      "step": 43120
    },
    {
      "epoch": 9.243463351907415,
      "grad_norm": 0.5571298599243164,
      "learning_rate": 7.67538219745678e-06,
      "loss": 0.749,
      "step": 43130
    },
    {
      "epoch": 9.24560651521646,
      "grad_norm": 0.5928307771682739,
      "learning_rate": 7.672524646378054e-06,
      "loss": 0.8839,
      "step": 43140
    },
    {
      "epoch": 9.247749678525503,
      "grad_norm": 0.5914289355278015,
      "learning_rate": 7.66966709529933e-06,
      "loss": 0.1575,
      "step": 43150
    },
    {
      "epoch": 9.249892841834548,
      "grad_norm": 0.5835986733436584,
      "learning_rate": 7.666809544220604e-06,
      "loss": 0.7367,
      "step": 43160
    },
    {
      "epoch": 9.252036005143593,
      "grad_norm": 14.65534496307373,
      "learning_rate": 7.663951993141878e-06,
      "loss": 0.4523,
      "step": 43170
    },
    {
      "epoch": 9.254179168452636,
      "grad_norm": 0.5496488213539124,
      "learning_rate": 7.661094442063153e-06,
      "loss": 1.0415,
      "step": 43180
    },
    {
      "epoch": 9.25632233176168,
      "grad_norm": 0.5743377804756165,
      "learning_rate": 7.658236890984427e-06,
      "loss": 0.595,
      "step": 43190
    },
    {
      "epoch": 9.258465495070725,
      "grad_norm": 0.5827751159667969,
      "learning_rate": 7.655379339905701e-06,
      "loss": 0.4462,
      "step": 43200
    },
    {
      "epoch": 9.260608658379768,
      "grad_norm": 14.49826431274414,
      "learning_rate": 7.652521788826975e-06,
      "loss": 0.5917,
      "step": 43210
    },
    {
      "epoch": 9.262751821688813,
      "grad_norm": 14.538859367370605,
      "learning_rate": 7.64966423774825e-06,
      "loss": 0.3031,
      "step": 43220
    },
    {
      "epoch": 9.264894984997857,
      "grad_norm": 0.5455228090286255,
      "learning_rate": 7.646806686669525e-06,
      "loss": 0.3057,
      "step": 43230
    },
    {
      "epoch": 9.2670381483069,
      "grad_norm": 0.5096728205680847,
      "learning_rate": 7.643949135590799e-06,
      "loss": 0.4541,
      "step": 43240
    },
    {
      "epoch": 9.269181311615945,
      "grad_norm": 0.5327193737030029,
      "learning_rate": 7.641091584512074e-06,
      "loss": 0.7528,
      "step": 43250
    },
    {
      "epoch": 9.27132447492499,
      "grad_norm": 0.5216721892356873,
      "learning_rate": 7.638234033433348e-06,
      "loss": 1.0422,
      "step": 43260
    },
    {
      "epoch": 9.273467638234033,
      "grad_norm": 0.5595502853393555,
      "learning_rate": 7.635376482354622e-06,
      "loss": 0.3039,
      "step": 43270
    },
    {
      "epoch": 9.275610801543078,
      "grad_norm": 15.043413162231445,
      "learning_rate": 7.632518931275898e-06,
      "loss": 0.5953,
      "step": 43280
    },
    {
      "epoch": 9.277753964852122,
      "grad_norm": 0.5352424383163452,
      "learning_rate": 7.629661380197172e-06,
      "loss": 0.6099,
      "step": 43290
    },
    {
      "epoch": 9.279897128161165,
      "grad_norm": 0.5148159861564636,
      "learning_rate": 7.626803829118446e-06,
      "loss": 0.4571,
      "step": 43300
    },
    {
      "epoch": 9.28204029147021,
      "grad_norm": 0.4919513463973999,
      "learning_rate": 7.623946278039721e-06,
      "loss": 0.1617,
      "step": 43310
    },
    {
      "epoch": 9.284183454779255,
      "grad_norm": 14.629673957824707,
      "learning_rate": 7.621088726960994e-06,
      "loss": 0.7687,
      "step": 43320
    },
    {
      "epoch": 9.286326618088298,
      "grad_norm": 0.4526313245296478,
      "learning_rate": 7.618231175882269e-06,
      "loss": 0.1631,
      "step": 43330
    },
    {
      "epoch": 9.288469781397342,
      "grad_norm": 0.4126453697681427,
      "learning_rate": 7.615373624803544e-06,
      "loss": 0.0097,
      "step": 43340
    },
    {
      "epoch": 9.290612944706387,
      "grad_norm": 0.3763865828514099,
      "learning_rate": 7.6125160737248185e-06,
      "loss": 0.321,
      "step": 43350
    },
    {
      "epoch": 9.29275610801543,
      "grad_norm": 14.673402786254883,
      "learning_rate": 7.6096585226460924e-06,
      "loss": 0.4857,
      "step": 43360
    },
    {
      "epoch": 9.294899271324475,
      "grad_norm": 0.3614013195037842,
      "learning_rate": 7.606800971567367e-06,
      "loss": 0.6501,
      "step": 43370
    },
    {
      "epoch": 9.29704243463352,
      "grad_norm": 29.744291305541992,
      "learning_rate": 7.603943420488642e-06,
      "loss": 0.6443,
      "step": 43380
    },
    {
      "epoch": 9.299185597942563,
      "grad_norm": 0.40221107006073,
      "learning_rate": 7.601085869409917e-06,
      "loss": 0.9598,
      "step": 43390
    },
    {
      "epoch": 9.301328761251607,
      "grad_norm": 0.416358083486557,
      "learning_rate": 7.5982283183311915e-06,
      "loss": 0.4777,
      "step": 43400
    },
    {
      "epoch": 9.303471924560652,
      "grad_norm": 14.649360656738281,
      "learning_rate": 7.5953707672524655e-06,
      "loss": 0.9464,
      "step": 43410
    },
    {
      "epoch": 9.305615087869695,
      "grad_norm": 14.684281349182129,
      "learning_rate": 7.59251321617374e-06,
      "loss": 1.0918,
      "step": 43420
    },
    {
      "epoch": 9.30775825117874,
      "grad_norm": 0.48802950978279114,
      "learning_rate": 7.589655665095013e-06,
      "loss": 0.9303,
      "step": 43430
    },
    {
      "epoch": 9.309901414487785,
      "grad_norm": 0.48079878091812134,
      "learning_rate": 7.586798114016288e-06,
      "loss": 0.3124,
      "step": 43440
    },
    {
      "epoch": 9.312044577796827,
      "grad_norm": 29.662370681762695,
      "learning_rate": 7.583940562937563e-06,
      "loss": 0.7684,
      "step": 43450
    },
    {
      "epoch": 9.314187741105872,
      "grad_norm": 0.47175365686416626,
      "learning_rate": 7.581083011858838e-06,
      "loss": 0.6198,
      "step": 43460
    },
    {
      "epoch": 9.316330904414917,
      "grad_norm": 0.48548921942710876,
      "learning_rate": 7.5782254607801124e-06,
      "loss": 0.7662,
      "step": 43470
    },
    {
      "epoch": 9.31847406772396,
      "grad_norm": 0.46987485885620117,
      "learning_rate": 7.575367909701386e-06,
      "loss": 0.011,
      "step": 43480
    },
    {
      "epoch": 9.320617231033005,
      "grad_norm": 0.47434669733047485,
      "learning_rate": 7.572510358622661e-06,
      "loss": 0.9223,
      "step": 43490
    },
    {
      "epoch": 9.32276039434205,
      "grad_norm": 0.48094043135643005,
      "learning_rate": 7.569652807543936e-06,
      "loss": 0.6151,
      "step": 43500
    },
    {
      "epoch": 9.324903557651092,
      "grad_norm": 0.49148881435394287,
      "learning_rate": 7.566795256465211e-06,
      "loss": 0.9198,
      "step": 43510
    },
    {
      "epoch": 9.327046720960137,
      "grad_norm": 0.50077885389328,
      "learning_rate": 7.563937705386485e-06,
      "loss": 0.763,
      "step": 43520
    },
    {
      "epoch": 9.329189884269182,
      "grad_norm": 14.577558517456055,
      "learning_rate": 7.5610801543077585e-06,
      "loss": 0.3099,
      "step": 43530
    },
    {
      "epoch": 9.331333047578225,
      "grad_norm": 14.635653495788574,
      "learning_rate": 7.558222603229033e-06,
      "loss": 0.1612,
      "step": 43540
    },
    {
      "epoch": 9.33347621088727,
      "grad_norm": 0.47503185272216797,
      "learning_rate": 7.555365052150307e-06,
      "loss": 0.6136,
      "step": 43550
    },
    {
      "epoch": 9.335619374196314,
      "grad_norm": 14.581500053405762,
      "learning_rate": 7.552507501071582e-06,
      "loss": 0.9162,
      "step": 43560
    },
    {
      "epoch": 9.337762537505357,
      "grad_norm": 0.5178213715553284,
      "learning_rate": 7.549649949992857e-06,
      "loss": 0.4599,
      "step": 43570
    },
    {
      "epoch": 9.339905700814402,
      "grad_norm": 14.654349327087402,
      "learning_rate": 7.5467923989141316e-06,
      "loss": 1.063,
      "step": 43580
    },
    {
      "epoch": 9.342048864123447,
      "grad_norm": 0.4901837408542633,
      "learning_rate": 7.5439348478354055e-06,
      "loss": 0.1611,
      "step": 43590
    },
    {
      "epoch": 9.34419202743249,
      "grad_norm": 0.5041879415512085,
      "learning_rate": 7.54107729675668e-06,
      "loss": 0.6106,
      "step": 43600
    },
    {
      "epoch": 9.346335190741534,
      "grad_norm": 14.601670265197754,
      "learning_rate": 7.538219745677955e-06,
      "loss": 0.3121,
      "step": 43610
    },
    {
      "epoch": 9.34847835405058,
      "grad_norm": 0.4971494972705841,
      "learning_rate": 7.53536219459923e-06,
      "loss": 0.7647,
      "step": 43620
    },
    {
      "epoch": 9.350621517359622,
      "grad_norm": 0.49698030948638916,
      "learning_rate": 7.532504643520504e-06,
      "loss": 0.312,
      "step": 43630
    },
    {
      "epoch": 9.352764680668667,
      "grad_norm": 0.4852277636528015,
      "learning_rate": 7.529647092441778e-06,
      "loss": 0.4642,
      "step": 43640
    },
    {
      "epoch": 9.354907843977712,
      "grad_norm": 14.632126808166504,
      "learning_rate": 7.5267895413630525e-06,
      "loss": 0.4663,
      "step": 43650
    },
    {
      "epoch": 9.357051007286755,
      "grad_norm": 0.4716455340385437,
      "learning_rate": 7.523931990284326e-06,
      "loss": 0.6186,
      "step": 43660
    },
    {
      "epoch": 9.3591941705958,
      "grad_norm": 14.611437797546387,
      "learning_rate": 7.521074439205601e-06,
      "loss": 0.6169,
      "step": 43670
    },
    {
      "epoch": 9.361337333904844,
      "grad_norm": 0.4774502217769623,
      "learning_rate": 7.518216888126876e-06,
      "loss": 0.6198,
      "step": 43680
    },
    {
      "epoch": 9.363480497213887,
      "grad_norm": 14.582084655761719,
      "learning_rate": 7.515359337048151e-06,
      "loss": 0.7641,
      "step": 43690
    },
    {
      "epoch": 9.365623660522932,
      "grad_norm": 14.547019004821777,
      "learning_rate": 7.512501785969425e-06,
      "loss": 1.1987,
      "step": 43700
    },
    {
      "epoch": 9.367766823831976,
      "grad_norm": 0.5459821820259094,
      "learning_rate": 7.509644234890699e-06,
      "loss": 0.4538,
      "step": 43710
    },
    {
      "epoch": 9.36990998714102,
      "grad_norm": 14.571368217468262,
      "learning_rate": 7.506786683811974e-06,
      "loss": 0.7453,
      "step": 43720
    },
    {
      "epoch": 9.372053150450064,
      "grad_norm": 0.5450122952461243,
      "learning_rate": 7.503929132733249e-06,
      "loss": 0.3043,
      "step": 43730
    },
    {
      "epoch": 9.374196313759109,
      "grad_norm": 30.323196411132812,
      "learning_rate": 7.501071581654523e-06,
      "loss": 0.59,
      "step": 43740
    },
    {
      "epoch": 9.376339477068152,
      "grad_norm": 0.5399035215377808,
      "learning_rate": 7.498214030575797e-06,
      "loss": 1.0347,
      "step": 43750
    },
    {
      "epoch": 9.378482640377197,
      "grad_norm": 0.6148263216018677,
      "learning_rate": 7.495356479497072e-06,
      "loss": 0.8844,
      "step": 43760
    },
    {
      "epoch": 9.380625803686241,
      "grad_norm": 14.473590850830078,
      "learning_rate": 7.4924989284183455e-06,
      "loss": 0.4424,
      "step": 43770
    },
    {
      "epoch": 9.382768966995284,
      "grad_norm": 0.5940383672714233,
      "learning_rate": 7.48964137733962e-06,
      "loss": 0.1573,
      "step": 43780
    },
    {
      "epoch": 9.384912130304329,
      "grad_norm": 0.6373133063316345,
      "learning_rate": 7.486783826260895e-06,
      "loss": 1.3106,
      "step": 43790
    },
    {
      "epoch": 9.387055293613374,
      "grad_norm": 0.6138550043106079,
      "learning_rate": 7.48392627518217e-06,
      "loss": 0.5874,
      "step": 43800
    },
    {
      "epoch": 9.389198456922417,
      "grad_norm": 0.5964046716690063,
      "learning_rate": 7.481068724103444e-06,
      "loss": 0.157,
      "step": 43810
    },
    {
      "epoch": 9.391341620231461,
      "grad_norm": 0.5421110987663269,
      "learning_rate": 7.4782111730247186e-06,
      "loss": 0.1593,
      "step": 43820
    },
    {
      "epoch": 9.393484783540506,
      "grad_norm": 0.5251776576042175,
      "learning_rate": 7.475353621945993e-06,
      "loss": 0.8957,
      "step": 43830
    },
    {
      "epoch": 9.39562794684955,
      "grad_norm": 29.610811233520508,
      "learning_rate": 7.472496070867268e-06,
      "loss": 1.3464,
      "step": 43840
    },
    {
      "epoch": 9.397771110158594,
      "grad_norm": 0.5787836909294128,
      "learning_rate": 7.469638519788542e-06,
      "loss": 0.5965,
      "step": 43850
    },
    {
      "epoch": 9.399914273467639,
      "grad_norm": 0.5675579309463501,
      "learning_rate": 7.466780968709816e-06,
      "loss": 0.3024,
      "step": 43860
    },
    {
      "epoch": 9.402057436776682,
      "grad_norm": 14.547368049621582,
      "learning_rate": 7.463923417631091e-06,
      "loss": 0.3061,
      "step": 43870
    },
    {
      "epoch": 9.404200600085726,
      "grad_norm": 0.5470449924468994,
      "learning_rate": 7.461065866552365e-06,
      "loss": 0.4529,
      "step": 43880
    },
    {
      "epoch": 9.406343763394771,
      "grad_norm": 0.5191828608512878,
      "learning_rate": 7.4582083154736394e-06,
      "loss": 0.6049,
      "step": 43890
    },
    {
      "epoch": 9.408486926703814,
      "grad_norm": 29.55800437927246,
      "learning_rate": 7.455350764394914e-06,
      "loss": 1.6291,
      "step": 43900
    },
    {
      "epoch": 9.410630090012859,
      "grad_norm": 0.5978567004203796,
      "learning_rate": 7.452493213316189e-06,
      "loss": 0.4504,
      "step": 43910
    },
    {
      "epoch": 9.412773253321904,
      "grad_norm": 0.6361671090126038,
      "learning_rate": 7.449635662237463e-06,
      "loss": 0.8734,
      "step": 43920
    },
    {
      "epoch": 9.414916416630946,
      "grad_norm": 0.6228542327880859,
      "learning_rate": 7.446778111158738e-06,
      "loss": 0.7095,
      "step": 43930
    },
    {
      "epoch": 9.417059579939991,
      "grad_norm": 29.584449768066406,
      "learning_rate": 7.4439205600800125e-06,
      "loss": 1.0158,
      "step": 43940
    },
    {
      "epoch": 9.419202743249036,
      "grad_norm": 14.403308868408203,
      "learning_rate": 7.441063009001287e-06,
      "loss": 0.5789,
      "step": 43950
    },
    {
      "epoch": 9.421345906558079,
      "grad_norm": 0.6951785683631897,
      "learning_rate": 7.43820545792256e-06,
      "loss": 0.5718,
      "step": 43960
    },
    {
      "epoch": 9.423489069867124,
      "grad_norm": 14.402297973632812,
      "learning_rate": 7.435347906843835e-06,
      "loss": 0.7136,
      "step": 43970
    },
    {
      "epoch": 9.425632233176168,
      "grad_norm": 0.6542777419090271,
      "learning_rate": 7.43249035576511e-06,
      "loss": 0.2919,
      "step": 43980
    },
    {
      "epoch": 9.427775396485211,
      "grad_norm": 0.6112907528877258,
      "learning_rate": 7.429632804686384e-06,
      "loss": 0.439,
      "step": 43990
    },
    {
      "epoch": 9.429918559794256,
      "grad_norm": 14.474381446838379,
      "learning_rate": 7.426775253607659e-06,
      "loss": 1.0156,
      "step": 44000
    },
    {
      "epoch": 9.4320617231033,
      "grad_norm": 14.50512409210205,
      "learning_rate": 7.423917702528933e-06,
      "loss": 0.4459,
      "step": 44010
    },
    {
      "epoch": 9.434204886412344,
      "grad_norm": 0.6045941114425659,
      "learning_rate": 7.421060151450208e-06,
      "loss": 0.5888,
      "step": 44020
    },
    {
      "epoch": 9.436348049721389,
      "grad_norm": 0.580941915512085,
      "learning_rate": 7.418202600371482e-06,
      "loss": 0.1582,
      "step": 44030
    },
    {
      "epoch": 9.438491213030433,
      "grad_norm": 0.5481755137443542,
      "learning_rate": 7.415345049292757e-06,
      "loss": 0.1615,
      "step": 44040
    },
    {
      "epoch": 9.440634376339476,
      "grad_norm": 14.564949989318848,
      "learning_rate": 7.412487498214032e-06,
      "loss": 0.6057,
      "step": 44050
    },
    {
      "epoch": 9.442777539648521,
      "grad_norm": 0.5085574984550476,
      "learning_rate": 7.409629947135306e-06,
      "loss": 0.4604,
      "step": 44060
    },
    {
      "epoch": 9.444920702957566,
      "grad_norm": 0.5197357535362244,
      "learning_rate": 7.4067723960565795e-06,
      "loss": 0.9071,
      "step": 44070
    },
    {
      "epoch": 9.447063866266609,
      "grad_norm": 0.511675238609314,
      "learning_rate": 7.403914844977854e-06,
      "loss": 0.7594,
      "step": 44080
    },
    {
      "epoch": 9.449207029575653,
      "grad_norm": 0.5228734016418457,
      "learning_rate": 7.401057293899129e-06,
      "loss": 0.9052,
      "step": 44090
    },
    {
      "epoch": 9.451350192884698,
      "grad_norm": 0.559379518032074,
      "learning_rate": 7.398199742820403e-06,
      "loss": 0.8998,
      "step": 44100
    },
    {
      "epoch": 9.453493356193743,
      "grad_norm": 0.5809088945388794,
      "learning_rate": 7.395342191741678e-06,
      "loss": 0.5948,
      "step": 44110
    },
    {
      "epoch": 9.455636519502786,
      "grad_norm": 0.5415268540382385,
      "learning_rate": 7.3924846406629525e-06,
      "loss": 0.157,
      "step": 44120
    },
    {
      "epoch": 9.45777968281183,
      "grad_norm": 14.574827194213867,
      "learning_rate": 7.389627089584227e-06,
      "loss": 0.7478,
      "step": 44130
    },
    {
      "epoch": 9.459922846120875,
      "grad_norm": 14.570878982543945,
      "learning_rate": 7.386769538505501e-06,
      "loss": 0.8963,
      "step": 44140
    },
    {
      "epoch": 9.462066009429918,
      "grad_norm": 0.586001992225647,
      "learning_rate": 7.383911987426776e-06,
      "loss": 0.5959,
      "step": 44150
    },
    {
      "epoch": 9.464209172738963,
      "grad_norm": 0.5767199993133545,
      "learning_rate": 7.381054436348051e-06,
      "loss": 0.3034,
      "step": 44160
    },
    {
      "epoch": 9.466352336048008,
      "grad_norm": 0.56048184633255,
      "learning_rate": 7.3781968852693255e-06,
      "loss": 0.748,
      "step": 44170
    },
    {
      "epoch": 9.46849549935705,
      "grad_norm": 0.5369605422019958,
      "learning_rate": 7.375339334190599e-06,
      "loss": 0.5947,
      "step": 44180
    },
    {
      "epoch": 9.470638662666095,
      "grad_norm": 0.48924413323402405,
      "learning_rate": 7.372481783111873e-06,
      "loss": 0.0118,
      "step": 44190
    },
    {
      "epoch": 9.47278182597514,
      "grad_norm": 15.162861824035645,
      "learning_rate": 7.369624232033148e-06,
      "loss": 0.7743,
      "step": 44200
    },
    {
      "epoch": 9.474924989284183,
      "grad_norm": 0.41343259811401367,
      "learning_rate": 7.366766680954422e-06,
      "loss": 0.4686,
      "step": 44210
    },
    {
      "epoch": 9.477068152593228,
      "grad_norm": 14.6458101272583,
      "learning_rate": 7.363909129875697e-06,
      "loss": 0.9371,
      "step": 44220
    },
    {
      "epoch": 9.479211315902273,
      "grad_norm": 15.086395263671875,
      "learning_rate": 7.361051578796972e-06,
      "loss": 1.0831,
      "step": 44230
    },
    {
      "epoch": 9.481354479211316,
      "grad_norm": 0.4615892469882965,
      "learning_rate": 7.358194027718246e-06,
      "loss": 0.0107,
      "step": 44240
    },
    {
      "epoch": 9.48349764252036,
      "grad_norm": 14.843722343444824,
      "learning_rate": 7.355336476639521e-06,
      "loss": 1.0737,
      "step": 44250
    },
    {
      "epoch": 9.485640805829405,
      "grad_norm": 0.5211212635040283,
      "learning_rate": 7.352478925560795e-06,
      "loss": 0.4618,
      "step": 44260
    },
    {
      "epoch": 9.487783969138448,
      "grad_norm": 0.5113855004310608,
      "learning_rate": 7.34962137448207e-06,
      "loss": 0.4564,
      "step": 44270
    },
    {
      "epoch": 9.489927132447493,
      "grad_norm": 14.592843055725098,
      "learning_rate": 7.346763823403345e-06,
      "loss": 0.3068,
      "step": 44280
    },
    {
      "epoch": 9.492070295756537,
      "grad_norm": 0.5027497410774231,
      "learning_rate": 7.343906272324618e-06,
      "loss": 0.6162,
      "step": 44290
    },
    {
      "epoch": 9.49421345906558,
      "grad_norm": 0.5038002729415894,
      "learning_rate": 7.3410487212458925e-06,
      "loss": 0.6131,
      "step": 44300
    },
    {
      "epoch": 9.496356622374625,
      "grad_norm": 0.5212859511375427,
      "learning_rate": 7.338191170167167e-06,
      "loss": 1.2052,
      "step": 44310
    },
    {
      "epoch": 9.49849978568367,
      "grad_norm": 0.5612612962722778,
      "learning_rate": 7.335333619088442e-06,
      "loss": 0.455,
      "step": 44320
    },
    {
      "epoch": 9.500642948992713,
      "grad_norm": 0.5878528356552124,
      "learning_rate": 7.332476068009716e-06,
      "loss": 0.7416,
      "step": 44330
    },
    {
      "epoch": 9.502786112301758,
      "grad_norm": 0.581230640411377,
      "learning_rate": 7.329618516930991e-06,
      "loss": 0.4488,
      "step": 44340
    },
    {
      "epoch": 9.504929275610802,
      "grad_norm": 14.468887329101562,
      "learning_rate": 7.3267609658522655e-06,
      "loss": 0.7334,
      "step": 44350
    },
    {
      "epoch": 9.507072438919845,
      "grad_norm": 0.5985162854194641,
      "learning_rate": 7.32390341477354e-06,
      "loss": 0.7305,
      "step": 44360
    },
    {
      "epoch": 9.50921560222889,
      "grad_norm": 14.51234245300293,
      "learning_rate": 7.321045863694814e-06,
      "loss": 0.8682,
      "step": 44370
    },
    {
      "epoch": 9.511358765537935,
      "grad_norm": 14.413175582885742,
      "learning_rate": 7.318188312616089e-06,
      "loss": 0.7204,
      "step": 44380
    },
    {
      "epoch": 9.513501928846978,
      "grad_norm": 0.6603837609291077,
      "learning_rate": 7.315330761537363e-06,
      "loss": 0.7163,
      "step": 44390
    },
    {
      "epoch": 9.515645092156022,
      "grad_norm": 0.7018402218818665,
      "learning_rate": 7.312473210458637e-06,
      "loss": 0.5713,
      "step": 44400
    },
    {
      "epoch": 9.517788255465067,
      "grad_norm": 0.6391816139221191,
      "learning_rate": 7.309615659379912e-06,
      "loss": 0.4343,
      "step": 44410
    },
    {
      "epoch": 9.51993141877411,
      "grad_norm": 0.6373151540756226,
      "learning_rate": 7.3067581083011864e-06,
      "loss": 0.9937,
      "step": 44420
    },
    {
      "epoch": 9.522074582083155,
      "grad_norm": 0.6254451870918274,
      "learning_rate": 7.303900557222461e-06,
      "loss": 0.1542,
      "step": 44430
    },
    {
      "epoch": 9.5242177453922,
      "grad_norm": 0.5852994918823242,
      "learning_rate": 7.301043006143735e-06,
      "loss": 0.4442,
      "step": 44440
    },
    {
      "epoch": 9.526360908701243,
      "grad_norm": 14.645822525024414,
      "learning_rate": 7.29818545506501e-06,
      "loss": 0.5949,
      "step": 44450
    },
    {
      "epoch": 9.528504072010287,
      "grad_norm": 0.5801504850387573,
      "learning_rate": 7.295327903986285e-06,
      "loss": 0.598,
      "step": 44460
    },
    {
      "epoch": 9.530647235319332,
      "grad_norm": 0.5450315475463867,
      "learning_rate": 7.2924703529075595e-06,
      "loss": 0.7451,
      "step": 44470
    },
    {
      "epoch": 9.532790398628375,
      "grad_norm": 0.5238882899284363,
      "learning_rate": 7.289612801828833e-06,
      "loss": 0.736,
      "step": 44480
    },
    {
      "epoch": 9.53493356193742,
      "grad_norm": 14.69057846069336,
      "learning_rate": 7.286755250750108e-06,
      "loss": 0.3099,
      "step": 44490
    },
    {
      "epoch": 9.537076725246465,
      "grad_norm": 0.49950429797172546,
      "learning_rate": 7.283897699671382e-06,
      "loss": 0.4628,
      "step": 44500
    },
    {
      "epoch": 9.539219888555508,
      "grad_norm": 0.4735518991947174,
      "learning_rate": 7.281040148592656e-06,
      "loss": 0.7694,
      "step": 44510
    },
    {
      "epoch": 9.541363051864552,
      "grad_norm": 0.503705620765686,
      "learning_rate": 7.278182597513931e-06,
      "loss": 0.6063,
      "step": 44520
    },
    {
      "epoch": 9.543506215173597,
      "grad_norm": 0.49808281660079956,
      "learning_rate": 7.2753250464352056e-06,
      "loss": 0.4632,
      "step": 44530
    },
    {
      "epoch": 9.54564937848264,
      "grad_norm": 14.776335716247559,
      "learning_rate": 7.27246749535648e-06,
      "loss": 0.7633,
      "step": 44540
    },
    {
      "epoch": 9.547792541791685,
      "grad_norm": 14.586134910583496,
      "learning_rate": 7.269609944277754e-06,
      "loss": 0.4599,
      "step": 44550
    },
    {
      "epoch": 9.54993570510073,
      "grad_norm": 0.47040385007858276,
      "learning_rate": 7.266752393199029e-06,
      "loss": 0.1626,
      "step": 44560
    },
    {
      "epoch": 9.552078868409772,
      "grad_norm": 0.459883451461792,
      "learning_rate": 7.263894842120304e-06,
      "loss": 1.0768,
      "step": 44570
    },
    {
      "epoch": 9.554222031718817,
      "grad_norm": 0.48847585916519165,
      "learning_rate": 7.261037291041579e-06,
      "loss": 0.6159,
      "step": 44580
    },
    {
      "epoch": 9.556365195027862,
      "grad_norm": 0.5112106204032898,
      "learning_rate": 7.2581797399628525e-06,
      "loss": 1.2123,
      "step": 44590
    },
    {
      "epoch": 9.558508358336905,
      "grad_norm": 0.5161224007606506,
      "learning_rate": 7.255322188884127e-06,
      "loss": 0.3085,
      "step": 44600
    },
    {
      "epoch": 9.56065152164595,
      "grad_norm": 14.560811042785645,
      "learning_rate": 7.252464637805401e-06,
      "loss": 1.1879,
      "step": 44610
    },
    {
      "epoch": 9.562794684954994,
      "grad_norm": 0.5653931498527527,
      "learning_rate": 7.249607086726675e-06,
      "loss": 0.5936,
      "step": 44620
    },
    {
      "epoch": 9.564937848264037,
      "grad_norm": 0.5509604811668396,
      "learning_rate": 7.24674953564795e-06,
      "loss": 0.3023,
      "step": 44630
    },
    {
      "epoch": 9.567081011573082,
      "grad_norm": 0.5414016246795654,
      "learning_rate": 7.243891984569225e-06,
      "loss": 0.3046,
      "step": 44640
    },
    {
      "epoch": 9.569224174882127,
      "grad_norm": 0.5370208024978638,
      "learning_rate": 7.2410344334904995e-06,
      "loss": 1.0548,
      "step": 44650
    },
    {
      "epoch": 9.57136733819117,
      "grad_norm": 0.5881810188293457,
      "learning_rate": 7.238176882411773e-06,
      "loss": 0.8953,
      "step": 44660
    },
    {
      "epoch": 9.573510501500214,
      "grad_norm": 0.565839409828186,
      "learning_rate": 7.235319331333048e-06,
      "loss": 0.4499,
      "step": 44670
    },
    {
      "epoch": 9.57565366480926,
      "grad_norm": 14.703167915344238,
      "learning_rate": 7.232461780254323e-06,
      "loss": 0.7397,
      "step": 44680
    },
    {
      "epoch": 9.577796828118302,
      "grad_norm": 14.512725830078125,
      "learning_rate": 7.229604229175598e-06,
      "loss": 0.5906,
      "step": 44690
    },
    {
      "epoch": 9.579939991427347,
      "grad_norm": 0.593435525894165,
      "learning_rate": 7.226746678096872e-06,
      "loss": 0.8755,
      "step": 44700
    },
    {
      "epoch": 9.582083154736392,
      "grad_norm": 0.6232811808586121,
      "learning_rate": 7.223889127018146e-06,
      "loss": 0.8717,
      "step": 44710
    },
    {
      "epoch": 9.584226318045435,
      "grad_norm": 0.6110153198242188,
      "learning_rate": 7.22103157593942e-06,
      "loss": 0.4447,
      "step": 44720
    },
    {
      "epoch": 9.58636948135448,
      "grad_norm": 29.65842056274414,
      "learning_rate": 7.218174024860694e-06,
      "loss": 0.7397,
      "step": 44730
    },
    {
      "epoch": 9.588512644663524,
      "grad_norm": 14.637411117553711,
      "learning_rate": 7.215316473781969e-06,
      "loss": 0.7428,
      "step": 44740
    },
    {
      "epoch": 9.590655807972567,
      "grad_norm": 14.638370513916016,
      "learning_rate": 7.212458922703244e-06,
      "loss": 0.6122,
      "step": 44750
    },
    {
      "epoch": 9.592798971281612,
      "grad_norm": 0.5163925886154175,
      "learning_rate": 7.209601371624519e-06,
      "loss": 0.3097,
      "step": 44760
    },
    {
      "epoch": 9.594942134590656,
      "grad_norm": 14.60793399810791,
      "learning_rate": 7.2067438205457926e-06,
      "loss": 0.7585,
      "step": 44770
    },
    {
      "epoch": 9.5970852978997,
      "grad_norm": 15.435684204101562,
      "learning_rate": 7.203886269467067e-06,
      "loss": 0.9151,
      "step": 44780
    },
    {
      "epoch": 9.599228461208744,
      "grad_norm": 0.5563578009605408,
      "learning_rate": 7.201028718388342e-06,
      "loss": 0.7554,
      "step": 44790
    },
    {
      "epoch": 9.601371624517789,
      "grad_norm": 14.566461563110352,
      "learning_rate": 7.198171167309617e-06,
      "loss": 0.5982,
      "step": 44800
    },
    {
      "epoch": 9.603514787826832,
      "grad_norm": 14.550256729125977,
      "learning_rate": 7.195313616230891e-06,
      "loss": 1.018,
      "step": 44810
    },
    {
      "epoch": 9.605657951135877,
      "grad_norm": 14.899471282958984,
      "learning_rate": 7.192456065152165e-06,
      "loss": 0.582,
      "step": 44820
    },
    {
      "epoch": 9.607801114444921,
      "grad_norm": 0.6710556745529175,
      "learning_rate": 7.1895985140734395e-06,
      "loss": 0.4389,
      "step": 44830
    },
    {
      "epoch": 9.609944277753964,
      "grad_norm": 0.6390466690063477,
      "learning_rate": 7.1867409629947134e-06,
      "loss": 0.4359,
      "step": 44840
    },
    {
      "epoch": 9.612087441063009,
      "grad_norm": 14.61478328704834,
      "learning_rate": 7.183883411915988e-06,
      "loss": 0.3042,
      "step": 44850
    },
    {
      "epoch": 9.614230604372054,
      "grad_norm": 14.530654907226562,
      "learning_rate": 7.181025860837263e-06,
      "loss": 0.5916,
      "step": 44860
    },
    {
      "epoch": 9.616373767681097,
      "grad_norm": 0.5777150392532349,
      "learning_rate": 7.178168309758538e-06,
      "loss": 0.8869,
      "step": 44870
    },
    {
      "epoch": 9.618516930990141,
      "grad_norm": 0.5755766034126282,
      "learning_rate": 7.175310758679812e-06,
      "loss": 0.1567,
      "step": 44880
    },
    {
      "epoch": 9.620660094299186,
      "grad_norm": 14.672730445861816,
      "learning_rate": 7.1724532076010865e-06,
      "loss": 0.8929,
      "step": 44890
    },
    {
      "epoch": 9.62280325760823,
      "grad_norm": 14.615986824035645,
      "learning_rate": 7.169595656522361e-06,
      "loss": 0.3112,
      "step": 44900
    },
    {
      "epoch": 9.624946420917274,
      "grad_norm": 0.5512662529945374,
      "learning_rate": 7.166738105443636e-06,
      "loss": 0.4556,
      "step": 44910
    },
    {
      "epoch": 9.627089584226319,
      "grad_norm": 0.51541668176651,
      "learning_rate": 7.16388055436491e-06,
      "loss": 0.7529,
      "step": 44920
    },
    {
      "epoch": 9.629232747535362,
      "grad_norm": 30.050373077392578,
      "learning_rate": 7.161023003286184e-06,
      "loss": 0.765,
      "step": 44930
    },
    {
      "epoch": 9.631375910844406,
      "grad_norm": 29.81825828552246,
      "learning_rate": 7.158165452207459e-06,
      "loss": 0.7609,
      "step": 44940
    },
    {
      "epoch": 9.633519074153451,
      "grad_norm": 0.4687528908252716,
      "learning_rate": 7.155307901128733e-06,
      "loss": 0.4571,
      "step": 44950
    },
    {
      "epoch": 9.635662237462494,
      "grad_norm": 14.66812801361084,
      "learning_rate": 7.152450350050007e-06,
      "loss": 0.4709,
      "step": 44960
    },
    {
      "epoch": 9.637805400771539,
      "grad_norm": 0.5080761909484863,
      "learning_rate": 7.149592798971282e-06,
      "loss": 0.4654,
      "step": 44970
    },
    {
      "epoch": 9.639948564080584,
      "grad_norm": 0.5286096930503845,
      "learning_rate": 7.146735247892557e-06,
      "loss": 0.4554,
      "step": 44980
    },
    {
      "epoch": 9.642091727389626,
      "grad_norm": 0.5207740068435669,
      "learning_rate": 7.143877696813831e-06,
      "loss": 0.7617,
      "step": 44990
    },
    {
      "epoch": 9.644234890698671,
      "grad_norm": 14.673628807067871,
      "learning_rate": 7.141020145735106e-06,
      "loss": 0.4636,
      "step": 45000
    },
    {
      "epoch": 9.646378054007716,
      "grad_norm": 14.618717193603516,
      "learning_rate": 7.13816259465638e-06,
      "loss": 0.767,
      "step": 45010
    },
    {
      "epoch": 9.648521217316759,
      "grad_norm": 15.124110221862793,
      "learning_rate": 7.135305043577655e-06,
      "loss": 0.3176,
      "step": 45020
    },
    {
      "epoch": 9.650664380625804,
      "grad_norm": 0.4406343698501587,
      "learning_rate": 7.13244749249893e-06,
      "loss": 0.6261,
      "step": 45030
    },
    {
      "epoch": 9.652807543934848,
      "grad_norm": 15.859759330749512,
      "learning_rate": 7.129589941420203e-06,
      "loss": 0.47,
      "step": 45040
    },
    {
      "epoch": 9.654950707243891,
      "grad_norm": 0.436517596244812,
      "learning_rate": 7.126732390341478e-06,
      "loss": 0.6316,
      "step": 45050
    },
    {
      "epoch": 9.657093870552936,
      "grad_norm": 29.674386978149414,
      "learning_rate": 7.123874839262752e-06,
      "loss": 0.6235,
      "step": 45060
    },
    {
      "epoch": 9.65923703386198,
      "grad_norm": 14.718811988830566,
      "learning_rate": 7.1210172881840265e-06,
      "loss": 0.3157,
      "step": 45070
    },
    {
      "epoch": 9.661380197171024,
      "grad_norm": 0.36809226870536804,
      "learning_rate": 7.118159737105301e-06,
      "loss": 0.4803,
      "step": 45080
    },
    {
      "epoch": 9.663523360480069,
      "grad_norm": 15.141286849975586,
      "learning_rate": 7.115302186026576e-06,
      "loss": 0.4799,
      "step": 45090
    },
    {
      "epoch": 9.665666523789113,
      "grad_norm": 14.783615112304688,
      "learning_rate": 7.112444634947851e-06,
      "loss": 0.4879,
      "step": 45100
    },
    {
      "epoch": 9.667809687098156,
      "grad_norm": 0.384789377450943,
      "learning_rate": 7.109587083869125e-06,
      "loss": 0.4875,
      "step": 45110
    },
    {
      "epoch": 9.669952850407201,
      "grad_norm": 0.36001652479171753,
      "learning_rate": 7.1067295327903995e-06,
      "loss": 0.4927,
      "step": 45120
    },
    {
      "epoch": 9.672096013716246,
      "grad_norm": 14.723795890808105,
      "learning_rate": 7.103871981711674e-06,
      "loss": 0.8098,
      "step": 45130
    },
    {
      "epoch": 9.674239177025289,
      "grad_norm": 0.37244725227355957,
      "learning_rate": 7.101014430632947e-06,
      "loss": 0.1714,
      "step": 45140
    },
    {
      "epoch": 9.676382340334333,
      "grad_norm": 0.3637252748012543,
      "learning_rate": 7.098156879554222e-06,
      "loss": 0.3301,
      "step": 45150
    },
    {
      "epoch": 9.678525503643378,
      "grad_norm": 0.32887643575668335,
      "learning_rate": 7.095299328475497e-06,
      "loss": 0.3309,
      "step": 45160
    },
    {
      "epoch": 9.680668666952421,
      "grad_norm": 14.88027572631836,
      "learning_rate": 7.092441777396772e-06,
      "loss": 0.8202,
      "step": 45170
    },
    {
      "epoch": 9.682811830261466,
      "grad_norm": 14.795098304748535,
      "learning_rate": 7.089584226318046e-06,
      "loss": 1.4582,
      "step": 45180
    },
    {
      "epoch": 9.68495499357051,
      "grad_norm": 0.4127039909362793,
      "learning_rate": 7.08672667523932e-06,
      "loss": 0.3243,
      "step": 45190
    },
    {
      "epoch": 9.687098156879554,
      "grad_norm": 29.769384384155273,
      "learning_rate": 7.083869124160595e-06,
      "loss": 1.0956,
      "step": 45200
    },
    {
      "epoch": 9.689241320188598,
      "grad_norm": 0.43467050790786743,
      "learning_rate": 7.08101157308187e-06,
      "loss": 0.0097,
      "step": 45210
    },
    {
      "epoch": 9.691384483497643,
      "grad_norm": 0.42725467681884766,
      "learning_rate": 7.078154022003144e-06,
      "loss": 0.4711,
      "step": 45220
    },
    {
      "epoch": 9.693527646806686,
      "grad_norm": 0.4276462197303772,
      "learning_rate": 7.075296470924419e-06,
      "loss": 0.4708,
      "step": 45230
    },
    {
      "epoch": 9.69567081011573,
      "grad_norm": 30.14188003540039,
      "learning_rate": 7.0724389198456934e-06,
      "loss": 1.4152,
      "step": 45240
    },
    {
      "epoch": 9.697813973424775,
      "grad_norm": 14.65540599822998,
      "learning_rate": 7.0695813687669665e-06,
      "loss": 0.6281,
      "step": 45250
    },
    {
      "epoch": 9.699957136733818,
      "grad_norm": 0.5032666921615601,
      "learning_rate": 7.066723817688241e-06,
      "loss": 0.7685,
      "step": 45260
    },
    {
      "epoch": 9.702100300042863,
      "grad_norm": 0.5060656666755676,
      "learning_rate": 7.063866266609516e-06,
      "loss": 0.3099,
      "step": 45270
    },
    {
      "epoch": 9.704243463351908,
      "grad_norm": 0.49449440836906433,
      "learning_rate": 7.061008715530791e-06,
      "loss": 0.3119,
      "step": 45280
    },
    {
      "epoch": 9.70638662666095,
      "grad_norm": 0.49227339029312134,
      "learning_rate": 7.058151164452065e-06,
      "loss": 0.7738,
      "step": 45290
    },
    {
      "epoch": 9.708529789969996,
      "grad_norm": 0.4835154116153717,
      "learning_rate": 7.0552936133733396e-06,
      "loss": 0.9183,
      "step": 45300
    },
    {
      "epoch": 9.71067295327904,
      "grad_norm": 14.761581420898438,
      "learning_rate": 7.052436062294614e-06,
      "loss": 0.4627,
      "step": 45310
    },
    {
      "epoch": 9.712816116588083,
      "grad_norm": 0.5450887680053711,
      "learning_rate": 7.049578511215889e-06,
      "loss": 0.6068,
      "step": 45320
    },
    {
      "epoch": 9.714959279897128,
      "grad_norm": 0.5324483513832092,
      "learning_rate": 7.046720960137163e-06,
      "loss": 0.3078,
      "step": 45330
    },
    {
      "epoch": 9.717102443206173,
      "grad_norm": 0.4950329065322876,
      "learning_rate": 7.043863409058438e-06,
      "loss": 0.3085,
      "step": 45340
    },
    {
      "epoch": 9.719245606515216,
      "grad_norm": 14.599839210510254,
      "learning_rate": 7.041005857979713e-06,
      "loss": 0.4615,
      "step": 45350
    },
    {
      "epoch": 9.72138876982426,
      "grad_norm": 14.64358139038086,
      "learning_rate": 7.038148306900986e-06,
      "loss": 0.6207,
      "step": 45360
    },
    {
      "epoch": 9.723531933133305,
      "grad_norm": 0.45186010003089905,
      "learning_rate": 7.0352907558222604e-06,
      "loss": 0.7701,
      "step": 45370
    },
    {
      "epoch": 9.725675096442348,
      "grad_norm": 0.4865887761116028,
      "learning_rate": 7.032433204743535e-06,
      "loss": 1.0739,
      "step": 45380
    },
    {
      "epoch": 9.727818259751393,
      "grad_norm": 0.5056678652763367,
      "learning_rate": 7.02957565366481e-06,
      "loss": 0.9135,
      "step": 45390
    },
    {
      "epoch": 9.729961423060438,
      "grad_norm": 0.5277678966522217,
      "learning_rate": 7.026718102586084e-06,
      "loss": 1.0551,
      "step": 45400
    },
    {
      "epoch": 9.73210458636948,
      "grad_norm": 0.5491440892219543,
      "learning_rate": 7.023860551507359e-06,
      "loss": 0.8931,
      "step": 45410
    },
    {
      "epoch": 9.734247749678525,
      "grad_norm": 0.5973383188247681,
      "learning_rate": 7.0210030004286335e-06,
      "loss": 0.7357,
      "step": 45420
    },
    {
      "epoch": 9.73639091298757,
      "grad_norm": 0.6933761239051819,
      "learning_rate": 7.018145449349908e-06,
      "loss": 0.8675,
      "step": 45430
    },
    {
      "epoch": 9.738534076296613,
      "grad_norm": 0.6415657997131348,
      "learning_rate": 7.015287898271182e-06,
      "loss": 0.1547,
      "step": 45440
    },
    {
      "epoch": 9.740677239605658,
      "grad_norm": 15.315314292907715,
      "learning_rate": 7.012430347192457e-06,
      "loss": 0.5745,
      "step": 45450
    },
    {
      "epoch": 9.742820402914703,
      "grad_norm": 0.6041118502616882,
      "learning_rate": 7.009572796113732e-06,
      "loss": 0.2982,
      "step": 45460
    },
    {
      "epoch": 9.744963566223745,
      "grad_norm": 29.58742332458496,
      "learning_rate": 7.006715245035005e-06,
      "loss": 0.8778,
      "step": 45470
    },
    {
      "epoch": 9.74710672953279,
      "grad_norm": 14.695404052734375,
      "learning_rate": 7.00385769395628e-06,
      "loss": 0.4527,
      "step": 45480
    },
    {
      "epoch": 9.749249892841835,
      "grad_norm": 0.5815393328666687,
      "learning_rate": 7.001000142877554e-06,
      "loss": 0.1593,
      "step": 45490
    },
    {
      "epoch": 9.751393056150878,
      "grad_norm": 14.897636413574219,
      "learning_rate": 6.998142591798829e-06,
      "loss": 0.4505,
      "step": 45500
    },
    {
      "epoch": 9.753536219459923,
      "grad_norm": 14.731301307678223,
      "learning_rate": 6.995285040720103e-06,
      "loss": 1.0449,
      "step": 45510
    },
    {
      "epoch": 9.755679382768967,
      "grad_norm": 0.5407983064651489,
      "learning_rate": 6.992427489641378e-06,
      "loss": 0.4568,
      "step": 45520
    },
    {
      "epoch": 9.75782254607801,
      "grad_norm": 0.5029326677322388,
      "learning_rate": 6.989569938562653e-06,
      "loss": 0.162,
      "step": 45530
    },
    {
      "epoch": 9.759965709387055,
      "grad_norm": 0.501559317111969,
      "learning_rate": 6.986712387483927e-06,
      "loss": 0.9082,
      "step": 45540
    },
    {
      "epoch": 9.7621088726961,
      "grad_norm": 0.509304940700531,
      "learning_rate": 6.983854836405201e-06,
      "loss": 0.614,
      "step": 45550
    },
    {
      "epoch": 9.764252036005143,
      "grad_norm": 0.4969044625759125,
      "learning_rate": 6.980997285326476e-06,
      "loss": 0.1605,
      "step": 45560
    },
    {
      "epoch": 9.766395199314188,
      "grad_norm": 0.501384437084198,
      "learning_rate": 6.97813973424775e-06,
      "loss": 0.9117,
      "step": 45570
    },
    {
      "epoch": 9.768538362623232,
      "grad_norm": 14.758011817932129,
      "learning_rate": 6.975282183169024e-06,
      "loss": 0.903,
      "step": 45580
    },
    {
      "epoch": 9.770681525932275,
      "grad_norm": 0.5473711490631104,
      "learning_rate": 6.972424632090299e-06,
      "loss": 0.7518,
      "step": 45590
    },
    {
      "epoch": 9.77282468924132,
      "grad_norm": 14.710383415222168,
      "learning_rate": 6.9695670810115735e-06,
      "loss": 0.4609,
      "step": 45600
    },
    {
      "epoch": 9.774967852550365,
      "grad_norm": 0.49108272790908813,
      "learning_rate": 6.966709529932848e-06,
      "loss": 0.3137,
      "step": 45610
    },
    {
      "epoch": 9.777111015859408,
      "grad_norm": 0.42505818605422974,
      "learning_rate": 6.963851978854122e-06,
      "loss": 0.6315,
      "step": 45620
    },
    {
      "epoch": 9.779254179168452,
      "grad_norm": 0.43193742632865906,
      "learning_rate": 6.960994427775397e-06,
      "loss": 0.6317,
      "step": 45630
    },
    {
      "epoch": 9.781397342477497,
      "grad_norm": 0.458230584859848,
      "learning_rate": 6.958136876696672e-06,
      "loss": 0.9418,
      "step": 45640
    },
    {
      "epoch": 9.78354050578654,
      "grad_norm": 0.46861499547958374,
      "learning_rate": 6.9552793256179465e-06,
      "loss": 0.6234,
      "step": 45650
    },
    {
      "epoch": 9.785683669095585,
      "grad_norm": 0.4711900055408478,
      "learning_rate": 6.9524217745392205e-06,
      "loss": 0.4674,
      "step": 45660
    },
    {
      "epoch": 9.78782683240463,
      "grad_norm": 0.4697539806365967,
      "learning_rate": 6.949564223460495e-06,
      "loss": 0.4625,
      "step": 45670
    },
    {
      "epoch": 9.789969995713673,
      "grad_norm": 29.64411163330078,
      "learning_rate": 6.946706672381769e-06,
      "loss": 1.0811,
      "step": 45680
    },
    {
      "epoch": 9.792113159022717,
      "grad_norm": 0.4954690933227539,
      "learning_rate": 6.943849121303043e-06,
      "loss": 1.0604,
      "step": 45690
    },
    {
      "epoch": 9.794256322331762,
      "grad_norm": 0.5403051972389221,
      "learning_rate": 6.940991570224318e-06,
      "loss": 0.6024,
      "step": 45700
    },
    {
      "epoch": 9.796399485640805,
      "grad_norm": 29.91994857788086,
      "learning_rate": 6.938134019145593e-06,
      "loss": 0.4567,
      "step": 45710
    },
    {
      "epoch": 9.79854264894985,
      "grad_norm": 0.520028829574585,
      "learning_rate": 6.935276468066867e-06,
      "loss": 1.1976,
      "step": 45720
    },
    {
      "epoch": 9.800685812258894,
      "grad_norm": 0.5533096790313721,
      "learning_rate": 6.932418916988141e-06,
      "loss": 0.8972,
      "step": 45730
    },
    {
      "epoch": 9.802828975567937,
      "grad_norm": 0.5748257637023926,
      "learning_rate": 6.929561365909416e-06,
      "loss": 0.7383,
      "step": 45740
    },
    {
      "epoch": 9.804972138876982,
      "grad_norm": 0.5648326873779297,
      "learning_rate": 6.926703814830691e-06,
      "loss": 0.3042,
      "step": 45750
    },
    {
      "epoch": 9.807115302186027,
      "grad_norm": 0.5623440146446228,
      "learning_rate": 6.923846263751966e-06,
      "loss": 1.0328,
      "step": 45760
    },
    {
      "epoch": 9.80925846549507,
      "grad_norm": 0.5533862113952637,
      "learning_rate": 6.92098871267324e-06,
      "loss": 0.5974,
      "step": 45770
    },
    {
      "epoch": 9.811401628804115,
      "grad_norm": 0.5519704818725586,
      "learning_rate": 6.918131161594514e-06,
      "loss": 0.4515,
      "step": 45780
    },
    {
      "epoch": 9.81354479211316,
      "grad_norm": 0.5441029071807861,
      "learning_rate": 6.915273610515788e-06,
      "loss": 0.4536,
      "step": 45790
    },
    {
      "epoch": 9.815687955422202,
      "grad_norm": 0.537102997303009,
      "learning_rate": 6.912416059437062e-06,
      "loss": 0.4532,
      "step": 45800
    },
    {
      "epoch": 9.817831118731247,
      "grad_norm": 0.48747822642326355,
      "learning_rate": 6.909558508358337e-06,
      "loss": 0.6131,
      "step": 45810
    },
    {
      "epoch": 9.819974282040292,
      "grad_norm": 0.521806538105011,
      "learning_rate": 6.906700957279612e-06,
      "loss": 0.4619,
      "step": 45820
    },
    {
      "epoch": 9.822117445349335,
      "grad_norm": 14.692182540893555,
      "learning_rate": 6.9038434062008866e-06,
      "loss": 0.3158,
      "step": 45830
    },
    {
      "epoch": 9.82426060865838,
      "grad_norm": 0.40643125772476196,
      "learning_rate": 6.9009858551221605e-06,
      "loss": 0.319,
      "step": 45840
    },
    {
      "epoch": 9.826403771967424,
      "grad_norm": 0.41855719685554504,
      "learning_rate": 6.898128304043435e-06,
      "loss": 0.7863,
      "step": 45850
    },
    {
      "epoch": 9.828546935276467,
      "grad_norm": 14.653916358947754,
      "learning_rate": 6.89527075296471e-06,
      "loss": 0.6311,
      "step": 45860
    },
    {
      "epoch": 9.830690098585512,
      "grad_norm": 14.888439178466797,
      "learning_rate": 6.892413201885985e-06,
      "loss": 0.7846,
      "step": 45870
    },
    {
      "epoch": 9.832833261894557,
      "grad_norm": 14.748038291931152,
      "learning_rate": 6.88955565080726e-06,
      "loss": 1.0866,
      "step": 45880
    },
    {
      "epoch": 9.8349764252036,
      "grad_norm": 0.47823747992515564,
      "learning_rate": 6.8866980997285335e-06,
      "loss": 0.3133,
      "step": 45890
    },
    {
      "epoch": 9.837119588512644,
      "grad_norm": 14.710927963256836,
      "learning_rate": 6.8838405486498074e-06,
      "loss": 1.0661,
      "step": 45900
    },
    {
      "epoch": 9.839262751821689,
      "grad_norm": 0.5076506733894348,
      "learning_rate": 6.880982997571081e-06,
      "loss": 0.3107,
      "step": 45910
    },
    {
      "epoch": 9.841405915130732,
      "grad_norm": 0.49249863624572754,
      "learning_rate": 6.878125446492356e-06,
      "loss": 0.3114,
      "step": 45920
    },
    {
      "epoch": 9.843549078439777,
      "grad_norm": 29.558271408081055,
      "learning_rate": 6.875267895413631e-06,
      "loss": 1.0571,
      "step": 45930
    },
    {
      "epoch": 9.845692241748822,
      "grad_norm": 0.47339221835136414,
      "learning_rate": 6.872410344334906e-06,
      "loss": 0.1609,
      "step": 45940
    },
    {
      "epoch": 9.847835405057864,
      "grad_norm": 14.697895050048828,
      "learning_rate": 6.8695527932561805e-06,
      "loss": 0.4657,
      "step": 45950
    },
    {
      "epoch": 9.84997856836691,
      "grad_norm": 15.728652000427246,
      "learning_rate": 6.866695242177454e-06,
      "loss": 1.3848,
      "step": 45960
    },
    {
      "epoch": 9.852121731675954,
      "grad_norm": 0.49065762758255005,
      "learning_rate": 6.863837691098729e-06,
      "loss": 0.4613,
      "step": 45970
    },
    {
      "epoch": 9.854264894984997,
      "grad_norm": 0.507149338722229,
      "learning_rate": 6.860980140020004e-06,
      "loss": 0.6103,
      "step": 45980
    },
    {
      "epoch": 9.856408058294042,
      "grad_norm": 0.5461278557777405,
      "learning_rate": 6.858122588941279e-06,
      "loss": 0.6133,
      "step": 45990
    },
    {
      "epoch": 9.858551221603086,
      "grad_norm": 15.33346176147461,
      "learning_rate": 6.855265037862552e-06,
      "loss": 1.0495,
      "step": 46000
    },
    {
      "epoch": 9.860694384912131,
      "grad_norm": 14.692132949829102,
      "learning_rate": 6.852407486783827e-06,
      "loss": 0.6064,
      "step": 46010
    },
    {
      "epoch": 9.862837548221174,
      "grad_norm": 15.237910270690918,
      "learning_rate": 6.849549935705101e-06,
      "loss": 0.4598,
      "step": 46020
    },
    {
      "epoch": 9.864980711530219,
      "grad_norm": 14.699630737304688,
      "learning_rate": 6.846692384626375e-06,
      "loss": 1.2125,
      "step": 46030
    },
    {
      "epoch": 9.867123874839264,
      "grad_norm": 0.5738311409950256,
      "learning_rate": 6.84383483354765e-06,
      "loss": 0.4537,
      "step": 46040
    },
    {
      "epoch": 9.869267038148307,
      "grad_norm": 14.560993194580078,
      "learning_rate": 6.840977282468925e-06,
      "loss": 0.1573,
      "step": 46050
    },
    {
      "epoch": 9.871410201457351,
      "grad_norm": 0.5062092542648315,
      "learning_rate": 6.8381197313902e-06,
      "loss": 0.9058,
      "step": 46060
    },
    {
      "epoch": 9.873553364766396,
      "grad_norm": 0.5159294605255127,
      "learning_rate": 6.8352621803114735e-06,
      "loss": 0.7451,
      "step": 46070
    },
    {
      "epoch": 9.875696528075439,
      "grad_norm": 0.5717717409133911,
      "learning_rate": 6.832404629232748e-06,
      "loss": 1.0368,
      "step": 46080
    },
    {
      "epoch": 9.877839691384484,
      "grad_norm": 30.171497344970703,
      "learning_rate": 6.829547078154023e-06,
      "loss": 0.8694,
      "step": 46090
    },
    {
      "epoch": 9.879982854693528,
      "grad_norm": 14.717863082885742,
      "learning_rate": 6.826689527075298e-06,
      "loss": 0.8557,
      "step": 46100
    },
    {
      "epoch": 9.882126018002571,
      "grad_norm": 0.7200395464897156,
      "learning_rate": 6.823831975996571e-06,
      "loss": 0.29,
      "step": 46110
    },
    {
      "epoch": 9.884269181311616,
      "grad_norm": 0.7357878684997559,
      "learning_rate": 6.820974424917846e-06,
      "loss": 0.8416,
      "step": 46120
    },
    {
      "epoch": 9.88641234462066,
      "grad_norm": 0.6514202356338501,
      "learning_rate": 6.8181168738391205e-06,
      "loss": 0.5746,
      "step": 46130
    },
    {
      "epoch": 9.888555507929704,
      "grad_norm": 14.56149959564209,
      "learning_rate": 6.8152593227603944e-06,
      "loss": 0.856,
      "step": 46140
    },
    {
      "epoch": 9.890698671238749,
      "grad_norm": 0.59876549243927,
      "learning_rate": 6.812401771681669e-06,
      "loss": 0.0137,
      "step": 46150
    },
    {
      "epoch": 9.892841834547793,
      "grad_norm": 14.573787689208984,
      "learning_rate": 6.809544220602944e-06,
      "loss": 0.5921,
      "step": 46160
    },
    {
      "epoch": 9.894984997856836,
      "grad_norm": 14.538543701171875,
      "learning_rate": 6.806686669524219e-06,
      "loss": 0.745,
      "step": 46170
    },
    {
      "epoch": 9.897128161165881,
      "grad_norm": 0.5499325394630432,
      "learning_rate": 6.803829118445493e-06,
      "loss": 0.6009,
      "step": 46180
    },
    {
      "epoch": 9.899271324474926,
      "grad_norm": 14.728902816772461,
      "learning_rate": 6.8009715673667675e-06,
      "loss": 0.4522,
      "step": 46190
    },
    {
      "epoch": 9.901414487783969,
      "grad_norm": 0.49584588408470154,
      "learning_rate": 6.798114016288042e-06,
      "loss": 0.3114,
      "step": 46200
    },
    {
      "epoch": 9.903557651093013,
      "grad_norm": 0.47653213143348694,
      "learning_rate": 6.795256465209317e-06,
      "loss": 0.1628,
      "step": 46210
    },
    {
      "epoch": 9.905700814402058,
      "grad_norm": 0.4816950261592865,
      "learning_rate": 6.79239891413059e-06,
      "loss": 1.0816,
      "step": 46220
    },
    {
      "epoch": 9.907843977711101,
      "grad_norm": 0.5182390809059143,
      "learning_rate": 6.789541363051865e-06,
      "loss": 1.078,
      "step": 46230
    },
    {
      "epoch": 9.909987141020146,
      "grad_norm": 14.634843826293945,
      "learning_rate": 6.78668381197314e-06,
      "loss": 0.4646,
      "step": 46240
    },
    {
      "epoch": 9.91213030432919,
      "grad_norm": 14.585319519042969,
      "learning_rate": 6.7838262608944136e-06,
      "loss": 0.469,
      "step": 46250
    },
    {
      "epoch": 9.914273467638234,
      "grad_norm": 0.533402681350708,
      "learning_rate": 6.780968709815688e-06,
      "loss": 0.6084,
      "step": 46260
    },
    {
      "epoch": 9.916416630947278,
      "grad_norm": 0.44319966435432434,
      "learning_rate": 6.778111158736963e-06,
      "loss": 0.4697,
      "step": 46270
    },
    {
      "epoch": 9.918559794256323,
      "grad_norm": 0.46560925245285034,
      "learning_rate": 6.775253607658238e-06,
      "loss": 0.6157,
      "step": 46280
    },
    {
      "epoch": 9.920702957565366,
      "grad_norm": 0.4758097231388092,
      "learning_rate": 6.772396056579512e-06,
      "loss": 0.6104,
      "step": 46290
    },
    {
      "epoch": 9.92284612087441,
      "grad_norm": 14.792354583740234,
      "learning_rate": 6.769538505500787e-06,
      "loss": 0.7639,
      "step": 46300
    },
    {
      "epoch": 9.924989284183455,
      "grad_norm": 0.5454085469245911,
      "learning_rate": 6.766680954422061e-06,
      "loss": 0.3155,
      "step": 46310
    },
    {
      "epoch": 9.927132447492498,
      "grad_norm": 15.445531845092773,
      "learning_rate": 6.763823403343336e-06,
      "loss": 0.4671,
      "step": 46320
    },
    {
      "epoch": 9.929275610801543,
      "grad_norm": 0.420511931180954,
      "learning_rate": 6.760965852264609e-06,
      "loss": 0.3187,
      "step": 46330
    },
    {
      "epoch": 9.931418774110588,
      "grad_norm": 14.763647079467773,
      "learning_rate": 6.758108301185884e-06,
      "loss": 0.6325,
      "step": 46340
    },
    {
      "epoch": 9.933561937419631,
      "grad_norm": 0.42386865615844727,
      "learning_rate": 6.755250750107159e-06,
      "loss": 0.6289,
      "step": 46350
    },
    {
      "epoch": 9.935705100728676,
      "grad_norm": 14.9033842086792,
      "learning_rate": 6.752393199028433e-06,
      "loss": 0.9509,
      "step": 46360
    },
    {
      "epoch": 9.93784826403772,
      "grad_norm": 0.47158077359199524,
      "learning_rate": 6.7495356479497075e-06,
      "loss": 0.9245,
      "step": 46370
    },
    {
      "epoch": 9.939991427346763,
      "grad_norm": 0.5062154531478882,
      "learning_rate": 6.746678096870982e-06,
      "loss": 0.7411,
      "step": 46380
    },
    {
      "epoch": 9.942134590655808,
      "grad_norm": 0.6912245750427246,
      "learning_rate": 6.743820545792257e-06,
      "loss": 0.7304,
      "step": 46390
    },
    {
      "epoch": 9.944277753964853,
      "grad_norm": 0.576816976070404,
      "learning_rate": 6.740962994713531e-06,
      "loss": 0.7244,
      "step": 46400
    },
    {
      "epoch": 9.946420917273896,
      "grad_norm": 29.533794403076172,
      "learning_rate": 6.738105443634806e-06,
      "loss": 1.2891,
      "step": 46410
    },
    {
      "epoch": 9.94856408058294,
      "grad_norm": 0.6874720454216003,
      "learning_rate": 6.7352478925560805e-06,
      "loss": 0.7024,
      "step": 46420
    },
    {
      "epoch": 9.950707243891985,
      "grad_norm": 0.662118136882782,
      "learning_rate": 6.732390341477354e-06,
      "loss": 0.5711,
      "step": 46430
    },
    {
      "epoch": 9.952850407201028,
      "grad_norm": 0.6612595915794373,
      "learning_rate": 6.729532790398628e-06,
      "loss": 0.5822,
      "step": 46440
    },
    {
      "epoch": 9.954993570510073,
      "grad_norm": 14.441389083862305,
      "learning_rate": 6.726675239319903e-06,
      "loss": 0.5829,
      "step": 46450
    },
    {
      "epoch": 9.957136733819118,
      "grad_norm": 0.6707622408866882,
      "learning_rate": 6.723817688241178e-06,
      "loss": 0.8385,
      "step": 46460
    },
    {
      "epoch": 9.95927989712816,
      "grad_norm": 14.673808097839355,
      "learning_rate": 6.720960137162452e-06,
      "loss": 0.5689,
      "step": 46470
    },
    {
      "epoch": 9.961423060437205,
      "grad_norm": 0.677082896232605,
      "learning_rate": 6.718102586083727e-06,
      "loss": 0.5805,
      "step": 46480
    },
    {
      "epoch": 9.96356622374625,
      "grad_norm": 14.423783302307129,
      "learning_rate": 6.715245035005001e-06,
      "loss": 0.5699,
      "step": 46490
    },
    {
      "epoch": 9.965709387055293,
      "grad_norm": 0.666662335395813,
      "learning_rate": 6.712387483926276e-06,
      "loss": 0.8463,
      "step": 46500
    },
    {
      "epoch": 9.967852550364338,
      "grad_norm": 0.8430702090263367,
      "learning_rate": 6.70952993284755e-06,
      "loss": 0.5864,
      "step": 46510
    },
    {
      "epoch": 9.969995713673383,
      "grad_norm": 0.7682989835739136,
      "learning_rate": 6.706672381768825e-06,
      "loss": 0.285,
      "step": 46520
    },
    {
      "epoch": 9.972138876982426,
      "grad_norm": 0.5823799967765808,
      "learning_rate": 6.7038148306901e-06,
      "loss": 0.3022,
      "step": 46530
    },
    {
      "epoch": 9.97428204029147,
      "grad_norm": 0.5823830962181091,
      "learning_rate": 6.700957279611373e-06,
      "loss": 0.2959,
      "step": 46540
    },
    {
      "epoch": 9.976425203600515,
      "grad_norm": 0.47819554805755615,
      "learning_rate": 6.6980997285326475e-06,
      "loss": 0.3078,
      "step": 46550
    },
    {
      "epoch": 9.978568366909558,
      "grad_norm": 0.453902930021286,
      "learning_rate": 6.695242177453922e-06,
      "loss": 0.1642,
      "step": 46560
    },
    {
      "epoch": 9.980711530218603,
      "grad_norm": 0.4388558268547058,
      "learning_rate": 6.692384626375197e-06,
      "loss": 0.9213,
      "step": 46570
    },
    {
      "epoch": 9.982854693527647,
      "grad_norm": 0.45495739579200745,
      "learning_rate": 6.689527075296471e-06,
      "loss": 0.7685,
      "step": 46580
    },
    {
      "epoch": 9.98499785683669,
      "grad_norm": 0.5401356816291809,
      "learning_rate": 6.686669524217746e-06,
      "loss": 1.2197,
      "step": 46590
    },
    {
      "epoch": 9.987141020145735,
      "grad_norm": 0.47066357731819153,
      "learning_rate": 6.6838119731390205e-06,
      "loss": 0.9062,
      "step": 46600
    },
    {
      "epoch": 9.98928418345478,
      "grad_norm": 14.646324157714844,
      "learning_rate": 6.680954422060295e-06,
      "loss": 0.8909,
      "step": 46610
    },
    {
      "epoch": 9.991427346763823,
      "grad_norm": 15.450798034667969,
      "learning_rate": 6.67809687098157e-06,
      "loss": 0.589,
      "step": 46620
    },
    {
      "epoch": 9.993570510072868,
      "grad_norm": 0.5677046179771423,
      "learning_rate": 6.675239319902844e-06,
      "loss": 0.8825,
      "step": 46630
    },
    {
      "epoch": 9.995713673381912,
      "grad_norm": 0.6079204082489014,
      "learning_rate": 6.672381768824119e-06,
      "loss": 0.4339,
      "step": 46640
    },
    {
      "epoch": 9.997856836690955,
      "grad_norm": 0.5524823665618896,
      "learning_rate": 6.669524217745392e-06,
      "loss": 0.8613,
      "step": 46650
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.5583052635192871,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.5887,
      "step": 46660
    },
    {
      "epoch": 10.0,
      "eval_accuracy": 0.8676666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.5727460384368896,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 396.242,
      "eval_samples_per_second": 7.571,
      "eval_steps_per_second": 2.524,
      "step": 46660
    },
    {
      "epoch": 10.002143163309045,
      "grad_norm": 0.6866861581802368,
      "learning_rate": 6.663809115587941e-06,
      "loss": 0.5755,
      "step": 46670
    },
    {
      "epoch": 10.004286326618088,
      "grad_norm": 14.54443359375,
      "learning_rate": 6.660951564509216e-06,
      "loss": 0.5813,
      "step": 46680
    },
    {
      "epoch": 10.006429489927132,
      "grad_norm": 0.5855585336685181,
      "learning_rate": 6.658094013430491e-06,
      "loss": 0.7115,
      "step": 46690
    },
    {
      "epoch": 10.008572653236177,
      "grad_norm": 14.587145805358887,
      "learning_rate": 6.655236462351765e-06,
      "loss": 0.578,
      "step": 46700
    },
    {
      "epoch": 10.01071581654522,
      "grad_norm": 14.679132461547852,
      "learning_rate": 6.65237891127304e-06,
      "loss": 1.276,
      "step": 46710
    },
    {
      "epoch": 10.012858979854265,
      "grad_norm": 0.6656410098075867,
      "learning_rate": 6.6495213601943145e-06,
      "loss": 0.1545,
      "step": 46720
    },
    {
      "epoch": 10.01500214316331,
      "grad_norm": 0.6699196696281433,
      "learning_rate": 6.646663809115589e-06,
      "loss": 0.692,
      "step": 46730
    },
    {
      "epoch": 10.017145306472353,
      "grad_norm": 44.49598693847656,
      "learning_rate": 6.643806258036863e-06,
      "loss": 1.3734,
      "step": 46740
    },
    {
      "epoch": 10.019288469781397,
      "grad_norm": 14.338173866271973,
      "learning_rate": 6.640948706958137e-06,
      "loss": 0.285,
      "step": 46750
    },
    {
      "epoch": 10.021431633090442,
      "grad_norm": 14.443552017211914,
      "learning_rate": 6.638091155879412e-06,
      "loss": 0.7149,
      "step": 46760
    },
    {
      "epoch": 10.023574796399485,
      "grad_norm": 14.36596393585205,
      "learning_rate": 6.635233604800686e-06,
      "loss": 0.8243,
      "step": 46770
    },
    {
      "epoch": 10.02571795970853,
      "grad_norm": 0.6113912463188171,
      "learning_rate": 6.6323760537219606e-06,
      "loss": 0.4121,
      "step": 46780
    },
    {
      "epoch": 10.027861123017574,
      "grad_norm": 0.6154240369796753,
      "learning_rate": 6.629518502643235e-06,
      "loss": 0.7174,
      "step": 46790
    },
    {
      "epoch": 10.030004286326617,
      "grad_norm": 0.6916561126708984,
      "learning_rate": 6.62666095156451e-06,
      "loss": 0.54,
      "step": 46800
    },
    {
      "epoch": 10.032147449635662,
      "grad_norm": 0.8206477165222168,
      "learning_rate": 6.623803400485784e-06,
      "loss": 0.5415,
      "step": 46810
    },
    {
      "epoch": 10.034290612944707,
      "grad_norm": 0.6207654476165771,
      "learning_rate": 6.620945849407059e-06,
      "loss": 1.233,
      "step": 46820
    },
    {
      "epoch": 10.03643377625375,
      "grad_norm": 0.5634042024612427,
      "learning_rate": 6.618088298328334e-06,
      "loss": 0.5799,
      "step": 46830
    },
    {
      "epoch": 10.038576939562795,
      "grad_norm": 0.6567341089248657,
      "learning_rate": 6.615230747249608e-06,
      "loss": 0.1422,
      "step": 46840
    },
    {
      "epoch": 10.04072010287184,
      "grad_norm": 0.5037777423858643,
      "learning_rate": 6.612373196170882e-06,
      "loss": 0.8906,
      "step": 46850
    },
    {
      "epoch": 10.042863266180882,
      "grad_norm": 0.5079036951065063,
      "learning_rate": 6.609515645092156e-06,
      "loss": 0.8647,
      "step": 46860
    },
    {
      "epoch": 10.045006429489927,
      "grad_norm": 0.5474971532821655,
      "learning_rate": 6.606658094013431e-06,
      "loss": 0.7293,
      "step": 46870
    },
    {
      "epoch": 10.047149592798972,
      "grad_norm": 0.6778799295425415,
      "learning_rate": 6.603800542934705e-06,
      "loss": 0.296,
      "step": 46880
    },
    {
      "epoch": 10.049292756108015,
      "grad_norm": 16.96355438232422,
      "learning_rate": 6.60094299185598e-06,
      "loss": 0.5634,
      "step": 46890
    },
    {
      "epoch": 10.05143591941706,
      "grad_norm": 0.6658197045326233,
      "learning_rate": 6.5980854407772545e-06,
      "loss": 0.7153,
      "step": 46900
    },
    {
      "epoch": 10.053579082726104,
      "grad_norm": 0.509799063205719,
      "learning_rate": 6.595227889698529e-06,
      "loss": 0.4105,
      "step": 46910
    },
    {
      "epoch": 10.055722246035147,
      "grad_norm": 0.5102371573448181,
      "learning_rate": 6.592370338619803e-06,
      "loss": 0.7151,
      "step": 46920
    },
    {
      "epoch": 10.057865409344192,
      "grad_norm": 0.6921285390853882,
      "learning_rate": 6.589512787541078e-06,
      "loss": 0.7078,
      "step": 46930
    },
    {
      "epoch": 10.060008572653237,
      "grad_norm": 15.174704551696777,
      "learning_rate": 6.586655236462353e-06,
      "loss": 0.705,
      "step": 46940
    },
    {
      "epoch": 10.06215173596228,
      "grad_norm": 14.559596061706543,
      "learning_rate": 6.5837976853836275e-06,
      "loss": 0.6117,
      "step": 46950
    },
    {
      "epoch": 10.064294899271324,
      "grad_norm": 0.49598485231399536,
      "learning_rate": 6.5809401343049014e-06,
      "loss": 0.452,
      "step": 46960
    },
    {
      "epoch": 10.066438062580369,
      "grad_norm": 29.67315101623535,
      "learning_rate": 6.578082583226175e-06,
      "loss": 1.2054,
      "step": 46970
    },
    {
      "epoch": 10.068581225889412,
      "grad_norm": 0.5112191438674927,
      "learning_rate": 6.57522503214745e-06,
      "loss": 0.161,
      "step": 46980
    },
    {
      "epoch": 10.070724389198457,
      "grad_norm": 0.46379169821739197,
      "learning_rate": 6.572367481068724e-06,
      "loss": 0.2947,
      "step": 46990
    },
    {
      "epoch": 10.072867552507502,
      "grad_norm": 0.3914722204208374,
      "learning_rate": 6.569509929989999e-06,
      "loss": 0.4734,
      "step": 47000
    },
    {
      "epoch": 10.075010715816545,
      "grad_norm": 14.691201210021973,
      "learning_rate": 6.566652378911274e-06,
      "loss": 1.0375,
      "step": 47010
    },
    {
      "epoch": 10.07715387912559,
      "grad_norm": 0.4213029444217682,
      "learning_rate": 6.563794827832548e-06,
      "loss": 0.725,
      "step": 47020
    },
    {
      "epoch": 10.079297042434634,
      "grad_norm": 0.44805800914764404,
      "learning_rate": 6.560937276753822e-06,
      "loss": 0.6275,
      "step": 47030
    },
    {
      "epoch": 10.081440205743677,
      "grad_norm": 0.49544450640678406,
      "learning_rate": 6.558079725675097e-06,
      "loss": 1.0418,
      "step": 47040
    },
    {
      "epoch": 10.083583369052722,
      "grad_norm": 0.5613812804222107,
      "learning_rate": 6.555222174596372e-06,
      "loss": 0.6065,
      "step": 47050
    },
    {
      "epoch": 10.085726532361766,
      "grad_norm": 0.47545644640922546,
      "learning_rate": 6.552364623517647e-06,
      "loss": 0.2756,
      "step": 47060
    },
    {
      "epoch": 10.08786969567081,
      "grad_norm": 14.8768310546875,
      "learning_rate": 6.5495070724389206e-06,
      "loss": 0.6343,
      "step": 47070
    },
    {
      "epoch": 10.090012858979854,
      "grad_norm": 31.839534759521484,
      "learning_rate": 6.5466495213601945e-06,
      "loss": 1.2432,
      "step": 47080
    },
    {
      "epoch": 10.092156022288899,
      "grad_norm": 29.509944915771484,
      "learning_rate": 6.543791970281469e-06,
      "loss": 0.4334,
      "step": 47090
    },
    {
      "epoch": 10.094299185597942,
      "grad_norm": 0.41637352108955383,
      "learning_rate": 6.540934419202743e-06,
      "loss": 0.835,
      "step": 47100
    },
    {
      "epoch": 10.096442348906987,
      "grad_norm": 14.906472206115723,
      "learning_rate": 6.538076868124018e-06,
      "loss": 0.5893,
      "step": 47110
    },
    {
      "epoch": 10.098585512216031,
      "grad_norm": 0.4597240090370178,
      "learning_rate": 6.535219317045293e-06,
      "loss": 0.4676,
      "step": 47120
    },
    {
      "epoch": 10.100728675525074,
      "grad_norm": 0.41729944944381714,
      "learning_rate": 6.5323617659665675e-06,
      "loss": 0.7101,
      "step": 47130
    },
    {
      "epoch": 10.102871838834119,
      "grad_norm": 14.706966400146484,
      "learning_rate": 6.5295042148878415e-06,
      "loss": 0.5446,
      "step": 47140
    },
    {
      "epoch": 10.105015002143164,
      "grad_norm": 30.679222106933594,
      "learning_rate": 6.526646663809116e-06,
      "loss": 0.6333,
      "step": 47150
    },
    {
      "epoch": 10.107158165452207,
      "grad_norm": 0.7944696545600891,
      "learning_rate": 6.523789112730391e-06,
      "loss": 0.6298,
      "step": 47160
    },
    {
      "epoch": 10.109301328761251,
      "grad_norm": 0.4541848599910736,
      "learning_rate": 6.520931561651666e-06,
      "loss": 0.9158,
      "step": 47170
    },
    {
      "epoch": 10.111444492070296,
      "grad_norm": 15.085149765014648,
      "learning_rate": 6.518074010572939e-06,
      "loss": 0.1732,
      "step": 47180
    },
    {
      "epoch": 10.11358765537934,
      "grad_norm": 28.8227596282959,
      "learning_rate": 6.515216459494214e-06,
      "loss": 1.0657,
      "step": 47190
    },
    {
      "epoch": 10.115730818688384,
      "grad_norm": 30.504751205444336,
      "learning_rate": 6.512358908415488e-06,
      "loss": 0.9008,
      "step": 47200
    },
    {
      "epoch": 10.117873981997429,
      "grad_norm": 15.40160846710205,
      "learning_rate": 6.509501357336762e-06,
      "loss": 1.0511,
      "step": 47210
    },
    {
      "epoch": 10.120017145306472,
      "grad_norm": 45.1986083984375,
      "learning_rate": 6.506643806258037e-06,
      "loss": 0.497,
      "step": 47220
    },
    {
      "epoch": 10.122160308615516,
      "grad_norm": 0.5646257996559143,
      "learning_rate": 6.503786255179312e-06,
      "loss": 0.6314,
      "step": 47230
    },
    {
      "epoch": 10.124303471924561,
      "grad_norm": 0.5214622020721436,
      "learning_rate": 6.500928704100587e-06,
      "loss": 0.6082,
      "step": 47240
    },
    {
      "epoch": 10.126446635233604,
      "grad_norm": 1.0510480403900146,
      "learning_rate": 6.498071153021861e-06,
      "loss": 0.8645,
      "step": 47250
    },
    {
      "epoch": 10.128589798542649,
      "grad_norm": 0.5249003171920776,
      "learning_rate": 6.495213601943135e-06,
      "loss": 0.86,
      "step": 47260
    },
    {
      "epoch": 10.130732961851693,
      "grad_norm": 0.46196356415748596,
      "learning_rate": 6.49235605086441e-06,
      "loss": 0.3582,
      "step": 47270
    },
    {
      "epoch": 10.132876125160736,
      "grad_norm": 14.357420921325684,
      "learning_rate": 6.489498499785685e-06,
      "loss": 0.8459,
      "step": 47280
    },
    {
      "epoch": 10.135019288469781,
      "grad_norm": 30.408084869384766,
      "learning_rate": 6.486640948706958e-06,
      "loss": 1.0311,
      "step": 47290
    },
    {
      "epoch": 10.137162451778826,
      "grad_norm": 0.7713356018066406,
      "learning_rate": 6.483783397628233e-06,
      "loss": 0.8587,
      "step": 47300
    },
    {
      "epoch": 10.139305615087869,
      "grad_norm": 0.6185476183891296,
      "learning_rate": 6.4809258465495076e-06,
      "loss": 0.3031,
      "step": 47310
    },
    {
      "epoch": 10.141448778396914,
      "grad_norm": 14.543540954589844,
      "learning_rate": 6.4780682954707815e-06,
      "loss": 0.8262,
      "step": 47320
    },
    {
      "epoch": 10.143591941705958,
      "grad_norm": 0.6420592665672302,
      "learning_rate": 6.475210744392056e-06,
      "loss": 1.1238,
      "step": 47330
    },
    {
      "epoch": 10.145735105015001,
      "grad_norm": 17.07795524597168,
      "learning_rate": 6.472353193313331e-06,
      "loss": 0.6662,
      "step": 47340
    },
    {
      "epoch": 10.147878268324046,
      "grad_norm": 0.644868016242981,
      "learning_rate": 6.469495642234606e-06,
      "loss": 0.5743,
      "step": 47350
    },
    {
      "epoch": 10.15002143163309,
      "grad_norm": 0.6731236577033997,
      "learning_rate": 6.46663809115588e-06,
      "loss": 0.3626,
      "step": 47360
    },
    {
      "epoch": 10.152164594942134,
      "grad_norm": 14.44929027557373,
      "learning_rate": 6.4637805400771545e-06,
      "loss": 0.8629,
      "step": 47370
    },
    {
      "epoch": 10.154307758251178,
      "grad_norm": 0.619978666305542,
      "learning_rate": 6.460922988998429e-06,
      "loss": 0.4019,
      "step": 47380
    },
    {
      "epoch": 10.156450921560223,
      "grad_norm": 0.6216813921928406,
      "learning_rate": 6.458065437919704e-06,
      "loss": 0.4092,
      "step": 47390
    },
    {
      "epoch": 10.158594084869266,
      "grad_norm": 1.0064867734909058,
      "learning_rate": 6.455207886840977e-06,
      "loss": 0.5464,
      "step": 47400
    },
    {
      "epoch": 10.160737248178311,
      "grad_norm": 0.48269733786582947,
      "learning_rate": 6.452350335762252e-06,
      "loss": 0.3772,
      "step": 47410
    },
    {
      "epoch": 10.162880411487356,
      "grad_norm": 0.49129071831703186,
      "learning_rate": 6.449492784683527e-06,
      "loss": 0.9337,
      "step": 47420
    },
    {
      "epoch": 10.165023574796399,
      "grad_norm": 15.331636428833008,
      "learning_rate": 6.446635233604801e-06,
      "loss": 0.6102,
      "step": 47430
    },
    {
      "epoch": 10.167166738105443,
      "grad_norm": 0.48800963163375854,
      "learning_rate": 6.443777682526075e-06,
      "loss": 0.4705,
      "step": 47440
    },
    {
      "epoch": 10.169309901414488,
      "grad_norm": 0.45153969526290894,
      "learning_rate": 6.44092013144735e-06,
      "loss": 0.4217,
      "step": 47450
    },
    {
      "epoch": 10.171453064723533,
      "grad_norm": 1.6748173236846924,
      "learning_rate": 6.438062580368625e-06,
      "loss": 0.1214,
      "step": 47460
    },
    {
      "epoch": 10.173596228032576,
      "grad_norm": 16.633394241333008,
      "learning_rate": 6.4352050292899e-06,
      "loss": 0.3365,
      "step": 47470
    },
    {
      "epoch": 10.17573939134162,
      "grad_norm": 0.18221525847911835,
      "learning_rate": 6.432347478211174e-06,
      "loss": 0.4806,
      "step": 47480
    },
    {
      "epoch": 10.177882554650665,
      "grad_norm": 16.982248306274414,
      "learning_rate": 6.4294899271324484e-06,
      "loss": 0.1963,
      "step": 47490
    },
    {
      "epoch": 10.180025717959708,
      "grad_norm": 0.2890636622905731,
      "learning_rate": 6.426632376053723e-06,
      "loss": 1.0139,
      "step": 47500
    },
    {
      "epoch": 10.182168881268753,
      "grad_norm": 14.848527908325195,
      "learning_rate": 6.423774824974996e-06,
      "loss": 0.6237,
      "step": 47510
    },
    {
      "epoch": 10.184312044577798,
      "grad_norm": 0.38824814558029175,
      "learning_rate": 6.420917273896271e-06,
      "loss": 0.941,
      "step": 47520
    },
    {
      "epoch": 10.18645520788684,
      "grad_norm": 0.3708713948726654,
      "learning_rate": 6.418059722817546e-06,
      "loss": 0.3271,
      "step": 47530
    },
    {
      "epoch": 10.188598371195885,
      "grad_norm": 0.36001551151275635,
      "learning_rate": 6.415202171738821e-06,
      "loss": 0.7205,
      "step": 47540
    },
    {
      "epoch": 10.19074153450493,
      "grad_norm": 14.909175872802734,
      "learning_rate": 6.4123446206600945e-06,
      "loss": 0.9168,
      "step": 47550
    },
    {
      "epoch": 10.192884697813973,
      "grad_norm": 0.38987988233566284,
      "learning_rate": 6.409487069581369e-06,
      "loss": 0.2733,
      "step": 47560
    },
    {
      "epoch": 10.195027861123018,
      "grad_norm": 0.859840452671051,
      "learning_rate": 6.406629518502644e-06,
      "loss": 0.3831,
      "step": 47570
    },
    {
      "epoch": 10.197171024432063,
      "grad_norm": 28.909420013427734,
      "learning_rate": 6.403771967423919e-06,
      "loss": 1.0143,
      "step": 47580
    },
    {
      "epoch": 10.199314187741106,
      "grad_norm": 0.36126208305358887,
      "learning_rate": 6.400914416345193e-06,
      "loss": 0.437,
      "step": 47590
    },
    {
      "epoch": 10.20145735105015,
      "grad_norm": 16.336013793945312,
      "learning_rate": 6.3980568652664676e-06,
      "loss": 0.3323,
      "step": 47600
    },
    {
      "epoch": 10.203600514359195,
      "grad_norm": 0.3531295657157898,
      "learning_rate": 6.3951993141877415e-06,
      "loss": 0.3323,
      "step": 47610
    },
    {
      "epoch": 10.205743677668238,
      "grad_norm": 0.3267170786857605,
      "learning_rate": 6.3923417631090154e-06,
      "loss": 0.4943,
      "step": 47620
    },
    {
      "epoch": 10.207886840977283,
      "grad_norm": 0.8177805542945862,
      "learning_rate": 6.38948421203029e-06,
      "loss": 0.9308,
      "step": 47630
    },
    {
      "epoch": 10.210030004286327,
      "grad_norm": 0.372159481048584,
      "learning_rate": 6.386626660951565e-06,
      "loss": 0.2738,
      "step": 47640
    },
    {
      "epoch": 10.21217316759537,
      "grad_norm": 0.36915701627731323,
      "learning_rate": 6.38376910987284e-06,
      "loss": 0.8663,
      "step": 47650
    },
    {
      "epoch": 10.214316330904415,
      "grad_norm": 0.3970595598220825,
      "learning_rate": 6.380911558794114e-06,
      "loss": 0.4315,
      "step": 47660
    },
    {
      "epoch": 10.21645949421346,
      "grad_norm": 0.4188832640647888,
      "learning_rate": 6.3780540077153885e-06,
      "loss": 0.6934,
      "step": 47670
    },
    {
      "epoch": 10.218602657522503,
      "grad_norm": 0.40078240633010864,
      "learning_rate": 6.375196456636663e-06,
      "loss": 0.2694,
      "step": 47680
    },
    {
      "epoch": 10.220745820831548,
      "grad_norm": 0.4025755226612091,
      "learning_rate": 6.372338905557938e-06,
      "loss": 0.4262,
      "step": 47690
    },
    {
      "epoch": 10.222888984140592,
      "grad_norm": 0.34601306915283203,
      "learning_rate": 6.369481354479212e-06,
      "loss": 0.3274,
      "step": 47700
    },
    {
      "epoch": 10.225032147449635,
      "grad_norm": 0.30674755573272705,
      "learning_rate": 6.366623803400487e-06,
      "loss": 0.2791,
      "step": 47710
    },
    {
      "epoch": 10.22717531075868,
      "grad_norm": 15.949673652648926,
      "learning_rate": 6.363766252321761e-06,
      "loss": 0.8975,
      "step": 47720
    },
    {
      "epoch": 10.229318474067725,
      "grad_norm": 1.3662394285202026,
      "learning_rate": 6.3609087012430346e-06,
      "loss": 0.7782,
      "step": 47730
    },
    {
      "epoch": 10.231461637376768,
      "grad_norm": 0.9459347724914551,
      "learning_rate": 6.358051150164309e-06,
      "loss": 0.8087,
      "step": 47740
    },
    {
      "epoch": 10.233604800685812,
      "grad_norm": 15.218539237976074,
      "learning_rate": 6.355193599085584e-06,
      "loss": 0.7974,
      "step": 47750
    },
    {
      "epoch": 10.235747963994857,
      "grad_norm": 0.4066946506500244,
      "learning_rate": 6.352336048006859e-06,
      "loss": 0.8958,
      "step": 47760
    },
    {
      "epoch": 10.2378911273039,
      "grad_norm": 0.46330344676971436,
      "learning_rate": 6.349478496928133e-06,
      "loss": 0.781,
      "step": 47770
    },
    {
      "epoch": 10.240034290612945,
      "grad_norm": 0.5689701437950134,
      "learning_rate": 6.346620945849408e-06,
      "loss": 0.5651,
      "step": 47780
    },
    {
      "epoch": 10.24217745392199,
      "grad_norm": 16.860767364501953,
      "learning_rate": 6.343763394770682e-06,
      "loss": 0.3106,
      "step": 47790
    },
    {
      "epoch": 10.244320617231033,
      "grad_norm": 1.0705716609954834,
      "learning_rate": 6.340905843691957e-06,
      "loss": 1.0323,
      "step": 47800
    },
    {
      "epoch": 10.246463780540077,
      "grad_norm": 1.2200106382369995,
      "learning_rate": 6.338048292613231e-06,
      "loss": 0.5576,
      "step": 47810
    },
    {
      "epoch": 10.248606943849122,
      "grad_norm": 0.4573771357536316,
      "learning_rate": 6.335190741534506e-06,
      "loss": 0.3055,
      "step": 47820
    },
    {
      "epoch": 10.250750107158165,
      "grad_norm": 0.4499812722206116,
      "learning_rate": 6.33233319045578e-06,
      "loss": 0.4703,
      "step": 47830
    },
    {
      "epoch": 10.25289327046721,
      "grad_norm": 31.472572326660156,
      "learning_rate": 6.329475639377054e-06,
      "loss": 0.6798,
      "step": 47840
    },
    {
      "epoch": 10.255036433776255,
      "grad_norm": 0.37945735454559326,
      "learning_rate": 6.3266180882983285e-06,
      "loss": 0.4979,
      "step": 47850
    },
    {
      "epoch": 10.257179597085297,
      "grad_norm": 0.4315929710865021,
      "learning_rate": 6.323760537219603e-06,
      "loss": 0.7441,
      "step": 47860
    },
    {
      "epoch": 10.259322760394342,
      "grad_norm": 16.97200584411621,
      "learning_rate": 6.320902986140878e-06,
      "loss": 0.8235,
      "step": 47870
    },
    {
      "epoch": 10.261465923703387,
      "grad_norm": 15.477592468261719,
      "learning_rate": 6.318045435062152e-06,
      "loss": 0.4102,
      "step": 47880
    },
    {
      "epoch": 10.26360908701243,
      "grad_norm": 15.560235977172852,
      "learning_rate": 6.315187883983427e-06,
      "loss": 0.4726,
      "step": 47890
    },
    {
      "epoch": 10.265752250321475,
      "grad_norm": 29.180118560791016,
      "learning_rate": 6.3123303329047015e-06,
      "loss": 1.091,
      "step": 47900
    },
    {
      "epoch": 10.26789541363052,
      "grad_norm": 15.161662101745605,
      "learning_rate": 6.309472781825976e-06,
      "loss": 0.5652,
      "step": 47910
    },
    {
      "epoch": 10.270038576939562,
      "grad_norm": 14.993860244750977,
      "learning_rate": 6.30661523074725e-06,
      "loss": 1.1722,
      "step": 47920
    },
    {
      "epoch": 10.272181740248607,
      "grad_norm": 14.854105949401855,
      "learning_rate": 6.303757679668525e-06,
      "loss": 0.7654,
      "step": 47930
    },
    {
      "epoch": 10.274324903557652,
      "grad_norm": 0.9263911247253418,
      "learning_rate": 6.300900128589799e-06,
      "loss": 0.6065,
      "step": 47940
    },
    {
      "epoch": 10.276468066866695,
      "grad_norm": 0.5794997215270996,
      "learning_rate": 6.298042577511073e-06,
      "loss": 0.3871,
      "step": 47950
    },
    {
      "epoch": 10.27861123017574,
      "grad_norm": 0.5654639005661011,
      "learning_rate": 6.295185026432348e-06,
      "loss": 0.4503,
      "step": 47960
    },
    {
      "epoch": 10.280754393484784,
      "grad_norm": 0.5309406518936157,
      "learning_rate": 6.292327475353622e-06,
      "loss": 0.4558,
      "step": 47970
    },
    {
      "epoch": 10.282897556793827,
      "grad_norm": 14.869596481323242,
      "learning_rate": 6.289469924274897e-06,
      "loss": 0.3228,
      "step": 47980
    },
    {
      "epoch": 10.285040720102872,
      "grad_norm": 0.3860797882080078,
      "learning_rate": 6.286612373196171e-06,
      "loss": 0.0103,
      "step": 47990
    },
    {
      "epoch": 10.287183883411917,
      "grad_norm": 14.6680269241333,
      "learning_rate": 6.283754822117446e-06,
      "loss": 0.7764,
      "step": 48000
    },
    {
      "epoch": 10.28932704672096,
      "grad_norm": 0.3807108998298645,
      "learning_rate": 6.280897271038721e-06,
      "loss": 0.3385,
      "step": 48010
    },
    {
      "epoch": 10.291470210030004,
      "grad_norm": 0.2247767299413681,
      "learning_rate": 6.2780397199599954e-06,
      "loss": 0.4853,
      "step": 48020
    },
    {
      "epoch": 10.29361337333905,
      "grad_norm": 17.439924240112305,
      "learning_rate": 6.275182168881269e-06,
      "loss": 0.9337,
      "step": 48030
    },
    {
      "epoch": 10.295756536648092,
      "grad_norm": 0.3119986951351166,
      "learning_rate": 6.272324617802543e-06,
      "loss": 0.491,
      "step": 48040
    },
    {
      "epoch": 10.297899699957137,
      "grad_norm": 0.33122411370277405,
      "learning_rate": 6.269467066723818e-06,
      "loss": 0.5673,
      "step": 48050
    },
    {
      "epoch": 10.300042863266182,
      "grad_norm": 0.4886883795261383,
      "learning_rate": 6.266609515645092e-06,
      "loss": 0.4201,
      "step": 48060
    },
    {
      "epoch": 10.302186026575225,
      "grad_norm": 15.704992294311523,
      "learning_rate": 6.263751964566367e-06,
      "loss": 0.7974,
      "step": 48070
    },
    {
      "epoch": 10.30432918988427,
      "grad_norm": 15.25996208190918,
      "learning_rate": 6.2608944134876415e-06,
      "loss": 0.5921,
      "step": 48080
    },
    {
      "epoch": 10.306472353193314,
      "grad_norm": 0.4200473725795746,
      "learning_rate": 6.258036862408916e-06,
      "loss": 0.5857,
      "step": 48090
    },
    {
      "epoch": 10.308615516502357,
      "grad_norm": 0.420249879360199,
      "learning_rate": 6.25517931133019e-06,
      "loss": 0.655,
      "step": 48100
    },
    {
      "epoch": 10.310758679811402,
      "grad_norm": 16.330257415771484,
      "learning_rate": 6.252321760251465e-06,
      "loss": 0.253,
      "step": 48110
    },
    {
      "epoch": 10.312901843120446,
      "grad_norm": 0.35244089365005493,
      "learning_rate": 6.24946420917274e-06,
      "loss": 0.4122,
      "step": 48120
    },
    {
      "epoch": 10.31504500642949,
      "grad_norm": 14.39731216430664,
      "learning_rate": 6.2466066580940146e-06,
      "loss": 0.3651,
      "step": 48130
    },
    {
      "epoch": 10.317188169738534,
      "grad_norm": 30.361677169799805,
      "learning_rate": 6.2437491070152885e-06,
      "loss": 1.0224,
      "step": 48140
    },
    {
      "epoch": 10.319331333047579,
      "grad_norm": 0.26267844438552856,
      "learning_rate": 6.2408915559365624e-06,
      "loss": 0.3172,
      "step": 48150
    },
    {
      "epoch": 10.321474496356622,
      "grad_norm": 0.334743857383728,
      "learning_rate": 6.238034004857837e-06,
      "loss": 0.3899,
      "step": 48160
    },
    {
      "epoch": 10.323617659665667,
      "grad_norm": 0.3420118987560272,
      "learning_rate": 6.235176453779111e-06,
      "loss": 0.8063,
      "step": 48170
    },
    {
      "epoch": 10.325760822974711,
      "grad_norm": 3.4984607696533203,
      "learning_rate": 6.232318902700386e-06,
      "loss": 0.7041,
      "step": 48180
    },
    {
      "epoch": 10.327903986283754,
      "grad_norm": 0.4147716760635376,
      "learning_rate": 6.229461351621661e-06,
      "loss": 0.7203,
      "step": 48190
    },
    {
      "epoch": 10.330047149592799,
      "grad_norm": 0.32398712635040283,
      "learning_rate": 6.2266038005429355e-06,
      "loss": 0.3257,
      "step": 48200
    },
    {
      "epoch": 10.332190312901844,
      "grad_norm": 0.38173866271972656,
      "learning_rate": 6.223746249464209e-06,
      "loss": 0.3473,
      "step": 48210
    },
    {
      "epoch": 10.334333476210887,
      "grad_norm": 0.283512145280838,
      "learning_rate": 6.220888698385484e-06,
      "loss": 0.668,
      "step": 48220
    },
    {
      "epoch": 10.336476639519931,
      "grad_norm": 0.3110886514186859,
      "learning_rate": 6.218031147306759e-06,
      "loss": 0.5591,
      "step": 48230
    },
    {
      "epoch": 10.338619802828976,
      "grad_norm": 13.653823852539062,
      "learning_rate": 6.215173596228034e-06,
      "loss": 0.5261,
      "step": 48240
    },
    {
      "epoch": 10.34076296613802,
      "grad_norm": 14.053011894226074,
      "learning_rate": 6.2123160451493085e-06,
      "loss": 0.9235,
      "step": 48250
    },
    {
      "epoch": 10.342906129447064,
      "grad_norm": 0.541888415813446,
      "learning_rate": 6.2094584940705816e-06,
      "loss": 0.5793,
      "step": 48260
    },
    {
      "epoch": 10.345049292756109,
      "grad_norm": 0.3951352536678314,
      "learning_rate": 6.206600942991856e-06,
      "loss": 0.3107,
      "step": 48270
    },
    {
      "epoch": 10.347192456065152,
      "grad_norm": 16.254575729370117,
      "learning_rate": 6.20374339191313e-06,
      "loss": 0.9394,
      "step": 48280
    },
    {
      "epoch": 10.349335619374196,
      "grad_norm": 0.3580709397792816,
      "learning_rate": 6.200885840834405e-06,
      "loss": 0.0902,
      "step": 48290
    },
    {
      "epoch": 10.351478782683241,
      "grad_norm": 0.3578201234340668,
      "learning_rate": 6.19802828975568e-06,
      "loss": 0.1795,
      "step": 48300
    },
    {
      "epoch": 10.353621945992284,
      "grad_norm": 0.2838297486305237,
      "learning_rate": 6.195170738676955e-06,
      "loss": 0.8002,
      "step": 48310
    },
    {
      "epoch": 10.355765109301329,
      "grad_norm": 1.294787049293518,
      "learning_rate": 6.192313187598229e-06,
      "loss": 0.492,
      "step": 48320
    },
    {
      "epoch": 10.357908272610374,
      "grad_norm": 0.2782493531703949,
      "learning_rate": 6.189455636519503e-06,
      "loss": 1.067,
      "step": 48330
    },
    {
      "epoch": 10.360051435919416,
      "grad_norm": 14.086385726928711,
      "learning_rate": 6.186598085440778e-06,
      "loss": 0.8978,
      "step": 48340
    },
    {
      "epoch": 10.362194599228461,
      "grad_norm": 0.5106996297836304,
      "learning_rate": 6.183740534362053e-06,
      "loss": 0.3362,
      "step": 48350
    },
    {
      "epoch": 10.364337762537506,
      "grad_norm": 14.952317237854004,
      "learning_rate": 6.180882983283326e-06,
      "loss": 1.2336,
      "step": 48360
    },
    {
      "epoch": 10.366480925846549,
      "grad_norm": 0.5926604270935059,
      "learning_rate": 6.178025432204601e-06,
      "loss": 0.7107,
      "step": 48370
    },
    {
      "epoch": 10.368624089155594,
      "grad_norm": 1.709649920463562,
      "learning_rate": 6.1751678811258755e-06,
      "loss": 0.8898,
      "step": 48380
    },
    {
      "epoch": 10.370767252464638,
      "grad_norm": 0.699279248714447,
      "learning_rate": 6.17231033004715e-06,
      "loss": 0.387,
      "step": 48390
    },
    {
      "epoch": 10.372910415773681,
      "grad_norm": 0.49917230010032654,
      "learning_rate": 6.169452778968424e-06,
      "loss": 0.7047,
      "step": 48400
    },
    {
      "epoch": 10.375053579082726,
      "grad_norm": 0.41634342074394226,
      "learning_rate": 6.166595227889699e-06,
      "loss": 0.5989,
      "step": 48410
    },
    {
      "epoch": 10.37719674239177,
      "grad_norm": 0.39058321714401245,
      "learning_rate": 6.163737676810974e-06,
      "loss": 0.822,
      "step": 48420
    },
    {
      "epoch": 10.379339905700814,
      "grad_norm": 14.7452974319458,
      "learning_rate": 6.1608801257322485e-06,
      "loss": 0.868,
      "step": 48430
    },
    {
      "epoch": 10.381483069009859,
      "grad_norm": 14.397710800170898,
      "learning_rate": 6.1580225746535224e-06,
      "loss": 0.818,
      "step": 48440
    },
    {
      "epoch": 10.383626232318903,
      "grad_norm": 15.284071922302246,
      "learning_rate": 6.155165023574797e-06,
      "loss": 0.5266,
      "step": 48450
    },
    {
      "epoch": 10.385769395627946,
      "grad_norm": 0.5349198579788208,
      "learning_rate": 6.152307472496072e-06,
      "loss": 0.3938,
      "step": 48460
    },
    {
      "epoch": 10.387912558936991,
      "grad_norm": 0.47313860058784485,
      "learning_rate": 6.149449921417345e-06,
      "loss": 0.3209,
      "step": 48470
    },
    {
      "epoch": 10.390055722246036,
      "grad_norm": 1.5297834873199463,
      "learning_rate": 6.14659237033862e-06,
      "loss": 0.6072,
      "step": 48480
    },
    {
      "epoch": 10.392198885555079,
      "grad_norm": 0.5111020803451538,
      "learning_rate": 6.143734819259895e-06,
      "loss": 0.3277,
      "step": 48490
    },
    {
      "epoch": 10.394342048864123,
      "grad_norm": 30.307958602905273,
      "learning_rate": 6.140877268181169e-06,
      "loss": 0.7961,
      "step": 48500
    },
    {
      "epoch": 10.396485212173168,
      "grad_norm": 0.5482204556465149,
      "learning_rate": 6.138019717102443e-06,
      "loss": 1.0507,
      "step": 48510
    },
    {
      "epoch": 10.398628375482211,
      "grad_norm": 14.62526798248291,
      "learning_rate": 6.135162166023718e-06,
      "loss": 0.3945,
      "step": 48520
    },
    {
      "epoch": 10.400771538791256,
      "grad_norm": 14.8619384765625,
      "learning_rate": 6.132304614944993e-06,
      "loss": 0.4117,
      "step": 48530
    },
    {
      "epoch": 10.4029147021003,
      "grad_norm": 14.804632186889648,
      "learning_rate": 6.129447063866268e-06,
      "loss": 0.8525,
      "step": 48540
    },
    {
      "epoch": 10.405057865409344,
      "grad_norm": 0.4674898386001587,
      "learning_rate": 6.126589512787542e-06,
      "loss": 0.3026,
      "step": 48550
    },
    {
      "epoch": 10.407201028718388,
      "grad_norm": 0.38762179017066956,
      "learning_rate": 6.123731961708816e-06,
      "loss": 0.4196,
      "step": 48560
    },
    {
      "epoch": 10.409344192027433,
      "grad_norm": 0.34248313307762146,
      "learning_rate": 6.120874410630091e-06,
      "loss": 0.2943,
      "step": 48570
    },
    {
      "epoch": 10.411487355336476,
      "grad_norm": 0.3394967317581177,
      "learning_rate": 6.118016859551364e-06,
      "loss": 0.8578,
      "step": 48580
    },
    {
      "epoch": 10.41363051864552,
      "grad_norm": 14.578595161437988,
      "learning_rate": 6.115159308472639e-06,
      "loss": 0.5075,
      "step": 48590
    },
    {
      "epoch": 10.415773681954565,
      "grad_norm": 0.5340147018432617,
      "learning_rate": 6.112301757393914e-06,
      "loss": 0.5782,
      "step": 48600
    },
    {
      "epoch": 10.417916845263608,
      "grad_norm": 1.460340142250061,
      "learning_rate": 6.1094442063151885e-06,
      "loss": 0.8555,
      "step": 48610
    },
    {
      "epoch": 10.420060008572653,
      "grad_norm": 1.4097189903259277,
      "learning_rate": 6.1065866552364625e-06,
      "loss": 0.4199,
      "step": 48620
    },
    {
      "epoch": 10.422203171881698,
      "grad_norm": 1.1238069534301758,
      "learning_rate": 6.103729104157737e-06,
      "loss": 0.4193,
      "step": 48630
    },
    {
      "epoch": 10.42434633519074,
      "grad_norm": 15.526927947998047,
      "learning_rate": 6.100871553079012e-06,
      "loss": 1.0154,
      "step": 48640
    },
    {
      "epoch": 10.426489498499786,
      "grad_norm": 0.5514688491821289,
      "learning_rate": 6.098014002000287e-06,
      "loss": 0.0127,
      "step": 48650
    },
    {
      "epoch": 10.42863266180883,
      "grad_norm": 14.739544868469238,
      "learning_rate": 6.095156450921561e-06,
      "loss": 0.8027,
      "step": 48660
    },
    {
      "epoch": 10.430775825117873,
      "grad_norm": 0.33397984504699707,
      "learning_rate": 6.0922988998428355e-06,
      "loss": 0.5428,
      "step": 48670
    },
    {
      "epoch": 10.432918988426918,
      "grad_norm": 1.2837607860565186,
      "learning_rate": 6.08944134876411e-06,
      "loss": 0.8994,
      "step": 48680
    },
    {
      "epoch": 10.435062151735963,
      "grad_norm": 0.48745375871658325,
      "learning_rate": 6.086583797685383e-06,
      "loss": 0.676,
      "step": 48690
    },
    {
      "epoch": 10.437205315045006,
      "grad_norm": 19.479612350463867,
      "learning_rate": 6.083726246606658e-06,
      "loss": 0.8175,
      "step": 48700
    },
    {
      "epoch": 10.43934847835405,
      "grad_norm": 0.5103787779808044,
      "learning_rate": 6.080868695527933e-06,
      "loss": 0.2612,
      "step": 48710
    },
    {
      "epoch": 10.441491641663095,
      "grad_norm": 0.44436776638031006,
      "learning_rate": 6.078011144449208e-06,
      "loss": 0.7178,
      "step": 48720
    },
    {
      "epoch": 10.443634804972138,
      "grad_norm": 15.674917221069336,
      "learning_rate": 6.075153593370482e-06,
      "loss": 0.6824,
      "step": 48730
    },
    {
      "epoch": 10.445777968281183,
      "grad_norm": 0.48101577162742615,
      "learning_rate": 6.072296042291756e-06,
      "loss": 0.8597,
      "step": 48740
    },
    {
      "epoch": 10.447921131590228,
      "grad_norm": 0.33811381459236145,
      "learning_rate": 6.069438491213031e-06,
      "loss": 0.3557,
      "step": 48750
    },
    {
      "epoch": 10.45006429489927,
      "grad_norm": 0.4259924590587616,
      "learning_rate": 6.066580940134306e-06,
      "loss": 0.6558,
      "step": 48760
    },
    {
      "epoch": 10.452207458208315,
      "grad_norm": 14.497655868530273,
      "learning_rate": 6.06372338905558e-06,
      "loss": 0.6436,
      "step": 48770
    },
    {
      "epoch": 10.45435062151736,
      "grad_norm": 14.96727180480957,
      "learning_rate": 6.060865837976855e-06,
      "loss": 1.1672,
      "step": 48780
    },
    {
      "epoch": 10.456493784826403,
      "grad_norm": 0.45119908452033997,
      "learning_rate": 6.0580082868981286e-06,
      "loss": 0.1616,
      "step": 48790
    },
    {
      "epoch": 10.458636948135448,
      "grad_norm": 0.35749074816703796,
      "learning_rate": 6.0551507358194025e-06,
      "loss": 0.1698,
      "step": 48800
    },
    {
      "epoch": 10.460780111444492,
      "grad_norm": 0.34460610151290894,
      "learning_rate": 6.052293184740677e-06,
      "loss": 0.3331,
      "step": 48810
    },
    {
      "epoch": 10.462923274753535,
      "grad_norm": 0.3158760666847229,
      "learning_rate": 6.049435633661952e-06,
      "loss": 0.4402,
      "step": 48820
    },
    {
      "epoch": 10.46506643806258,
      "grad_norm": 14.925772666931152,
      "learning_rate": 6.046578082583227e-06,
      "loss": 0.7864,
      "step": 48830
    },
    {
      "epoch": 10.467209601371625,
      "grad_norm": 0.2133154273033142,
      "learning_rate": 6.043720531504501e-06,
      "loss": 0.2069,
      "step": 48840
    },
    {
      "epoch": 10.469352764680668,
      "grad_norm": 0.9221999645233154,
      "learning_rate": 6.0408629804257755e-06,
      "loss": 0.5297,
      "step": 48850
    },
    {
      "epoch": 10.471495927989713,
      "grad_norm": 0.21818916499614716,
      "learning_rate": 6.03800542934705e-06,
      "loss": 0.2706,
      "step": 48860
    },
    {
      "epoch": 10.473639091298757,
      "grad_norm": 0.30013298988342285,
      "learning_rate": 6.035147878268325e-06,
      "loss": 0.6114,
      "step": 48870
    },
    {
      "epoch": 10.4757822546078,
      "grad_norm": 0.3756876587867737,
      "learning_rate": 6.032290327189599e-06,
      "loss": 0.8907,
      "step": 48880
    },
    {
      "epoch": 10.477925417916845,
      "grad_norm": 15.057267189025879,
      "learning_rate": 6.029432776110874e-06,
      "loss": 0.6864,
      "step": 48890
    },
    {
      "epoch": 10.48006858122589,
      "grad_norm": 0.396659255027771,
      "learning_rate": 6.026575225032148e-06,
      "loss": 0.1755,
      "step": 48900
    },
    {
      "epoch": 10.482211744534933,
      "grad_norm": 16.157184600830078,
      "learning_rate": 6.023717673953422e-06,
      "loss": 0.3121,
      "step": 48910
    },
    {
      "epoch": 10.484354907843978,
      "grad_norm": 0.2632286250591278,
      "learning_rate": 6.020860122874696e-06,
      "loss": 0.3345,
      "step": 48920
    },
    {
      "epoch": 10.486498071153022,
      "grad_norm": 14.06985855102539,
      "learning_rate": 6.018002571795971e-06,
      "loss": 0.7535,
      "step": 48930
    },
    {
      "epoch": 10.488641234462065,
      "grad_norm": 17.24837875366211,
      "learning_rate": 6.015145020717246e-06,
      "loss": 0.9106,
      "step": 48940
    },
    {
      "epoch": 10.49078439777111,
      "grad_norm": 0.853431224822998,
      "learning_rate": 6.01228746963852e-06,
      "loss": 0.5147,
      "step": 48950
    },
    {
      "epoch": 10.492927561080155,
      "grad_norm": 0.48962339758872986,
      "learning_rate": 6.009429918559795e-06,
      "loss": 0.9336,
      "step": 48960
    },
    {
      "epoch": 10.495070724389198,
      "grad_norm": 17.221317291259766,
      "learning_rate": 6.0065723674810694e-06,
      "loss": 0.9942,
      "step": 48970
    },
    {
      "epoch": 10.497213887698242,
      "grad_norm": 0.5941886901855469,
      "learning_rate": 6.003714816402344e-06,
      "loss": 0.3507,
      "step": 48980
    },
    {
      "epoch": 10.499357051007287,
      "grad_norm": 14.265260696411133,
      "learning_rate": 6.000857265323618e-06,
      "loss": 0.8889,
      "step": 48990
    },
    {
      "epoch": 10.50150021431633,
      "grad_norm": 0.36924561858177185,
      "learning_rate": 5.997999714244893e-06,
      "loss": 0.2746,
      "step": 49000
    },
    {
      "epoch": 10.503643377625375,
      "grad_norm": 15.667866706848145,
      "learning_rate": 5.995142163166167e-06,
      "loss": 0.4916,
      "step": 49010
    },
    {
      "epoch": 10.50578654093442,
      "grad_norm": 15.264894485473633,
      "learning_rate": 5.992284612087441e-06,
      "loss": 0.3836,
      "step": 49020
    },
    {
      "epoch": 10.507929704243463,
      "grad_norm": 1.3621892929077148,
      "learning_rate": 5.9894270610087156e-06,
      "loss": 0.9703,
      "step": 49030
    },
    {
      "epoch": 10.510072867552507,
      "grad_norm": 15.097563743591309,
      "learning_rate": 5.98656950992999e-06,
      "loss": 0.6242,
      "step": 49040
    },
    {
      "epoch": 10.512216030861552,
      "grad_norm": 15.334284782409668,
      "learning_rate": 5.983711958851265e-06,
      "loss": 0.5635,
      "step": 49050
    },
    {
      "epoch": 10.514359194170595,
      "grad_norm": 0.5418811440467834,
      "learning_rate": 5.980854407772539e-06,
      "loss": 0.4653,
      "step": 49060
    },
    {
      "epoch": 10.51650235747964,
      "grad_norm": 0.5044717788696289,
      "learning_rate": 5.977996856693814e-06,
      "loss": 0.3142,
      "step": 49070
    },
    {
      "epoch": 10.518645520788684,
      "grad_norm": 0.41081947088241577,
      "learning_rate": 5.975139305615089e-06,
      "loss": 0.7287,
      "step": 49080
    },
    {
      "epoch": 10.520788684097727,
      "grad_norm": 0.5232987403869629,
      "learning_rate": 5.972281754536363e-06,
      "loss": 0.533,
      "step": 49090
    },
    {
      "epoch": 10.522931847406772,
      "grad_norm": 0.634589672088623,
      "learning_rate": 5.969424203457638e-06,
      "loss": 0.5581,
      "step": 49100
    },
    {
      "epoch": 10.525075010715817,
      "grad_norm": 0.748995304107666,
      "learning_rate": 5.966566652378912e-06,
      "loss": 0.4218,
      "step": 49110
    },
    {
      "epoch": 10.52721817402486,
      "grad_norm": 0.7172545194625854,
      "learning_rate": 5.963709101300186e-06,
      "loss": 0.8785,
      "step": 49120
    },
    {
      "epoch": 10.529361337333905,
      "grad_norm": 30.166006088256836,
      "learning_rate": 5.96085155022146e-06,
      "loss": 0.4674,
      "step": 49130
    },
    {
      "epoch": 10.53150450064295,
      "grad_norm": 14.537803649902344,
      "learning_rate": 5.957993999142735e-06,
      "loss": 0.4273,
      "step": 49140
    },
    {
      "epoch": 10.533647663951992,
      "grad_norm": 0.3530399203300476,
      "learning_rate": 5.9551364480640095e-06,
      "loss": 0.2435,
      "step": 49150
    },
    {
      "epoch": 10.535790827261037,
      "grad_norm": 0.2640756368637085,
      "learning_rate": 5.952278896985284e-06,
      "loss": 0.5782,
      "step": 49160
    },
    {
      "epoch": 10.537933990570082,
      "grad_norm": 0.5497628450393677,
      "learning_rate": 5.949421345906559e-06,
      "loss": 0.5977,
      "step": 49170
    },
    {
      "epoch": 10.540077153879125,
      "grad_norm": 0.26105737686157227,
      "learning_rate": 5.946563794827833e-06,
      "loss": 0.5672,
      "step": 49180
    },
    {
      "epoch": 10.54222031718817,
      "grad_norm": 14.959027290344238,
      "learning_rate": 5.943706243749108e-06,
      "loss": 0.8399,
      "step": 49190
    },
    {
      "epoch": 10.544363480497214,
      "grad_norm": 0.23332838714122772,
      "learning_rate": 5.9408486926703825e-06,
      "loss": 0.3249,
      "step": 49200
    },
    {
      "epoch": 10.546506643806257,
      "grad_norm": 0.6437187790870667,
      "learning_rate": 5.937991141591657e-06,
      "loss": 0.7192,
      "step": 49210
    },
    {
      "epoch": 10.548649807115302,
      "grad_norm": 0.33640265464782715,
      "learning_rate": 5.93513359051293e-06,
      "loss": 0.375,
      "step": 49220
    },
    {
      "epoch": 10.550792970424347,
      "grad_norm": 23.096263885498047,
      "learning_rate": 5.932276039434205e-06,
      "loss": 0.3857,
      "step": 49230
    },
    {
      "epoch": 10.55293613373339,
      "grad_norm": 16.02924346923828,
      "learning_rate": 5.92941848835548e-06,
      "loss": 0.7824,
      "step": 49240
    },
    {
      "epoch": 10.555079297042434,
      "grad_norm": 18.359121322631836,
      "learning_rate": 5.926560937276754e-06,
      "loss": 0.3964,
      "step": 49250
    },
    {
      "epoch": 10.557222460351479,
      "grad_norm": 3.8500137329101562,
      "learning_rate": 5.923703386198029e-06,
      "loss": 0.5047,
      "step": 49260
    },
    {
      "epoch": 10.559365623660522,
      "grad_norm": 0.342092901468277,
      "learning_rate": 5.920845835119303e-06,
      "loss": 0.5427,
      "step": 49270
    },
    {
      "epoch": 10.561508786969567,
      "grad_norm": 0.20067822933197021,
      "learning_rate": 5.917988284040578e-06,
      "loss": 0.0188,
      "step": 49280
    },
    {
      "epoch": 10.563651950278611,
      "grad_norm": 0.1606258898973465,
      "learning_rate": 5.915130732961852e-06,
      "loss": 0.4347,
      "step": 49290
    },
    {
      "epoch": 10.565795113587654,
      "grad_norm": 0.2115272581577301,
      "learning_rate": 5.912273181883127e-06,
      "loss": 0.8305,
      "step": 49300
    },
    {
      "epoch": 10.5679382768967,
      "grad_norm": 0.22516115009784698,
      "learning_rate": 5.909415630804402e-06,
      "loss": 0.2673,
      "step": 49310
    },
    {
      "epoch": 10.570081440205744,
      "grad_norm": 0.20202486217021942,
      "learning_rate": 5.906558079725676e-06,
      "loss": 0.5038,
      "step": 49320
    },
    {
      "epoch": 10.572224603514789,
      "grad_norm": 0.2853347659111023,
      "learning_rate": 5.9037005286469495e-06,
      "loss": 0.4409,
      "step": 49330
    },
    {
      "epoch": 10.574367766823832,
      "grad_norm": 0.19006428122520447,
      "learning_rate": 5.900842977568224e-06,
      "loss": 0.2824,
      "step": 49340
    },
    {
      "epoch": 10.576510930132876,
      "grad_norm": 0.9858078956604004,
      "learning_rate": 5.897985426489499e-06,
      "loss": 0.6657,
      "step": 49350
    },
    {
      "epoch": 10.578654093441921,
      "grad_norm": 0.31116029620170593,
      "learning_rate": 5.895127875410773e-06,
      "loss": 0.4939,
      "step": 49360
    },
    {
      "epoch": 10.580797256750964,
      "grad_norm": 14.316237449645996,
      "learning_rate": 5.892270324332048e-06,
      "loss": 0.6588,
      "step": 49370
    },
    {
      "epoch": 10.582940420060009,
      "grad_norm": 0.4346104860305786,
      "learning_rate": 5.8894127732533225e-06,
      "loss": 0.4959,
      "step": 49380
    },
    {
      "epoch": 10.585083583369054,
      "grad_norm": 0.46241530776023865,
      "learning_rate": 5.886555222174597e-06,
      "loss": 0.3513,
      "step": 49390
    },
    {
      "epoch": 10.587226746678096,
      "grad_norm": 0.2188911885023117,
      "learning_rate": 5.883697671095871e-06,
      "loss": 0.3608,
      "step": 49400
    },
    {
      "epoch": 10.589369909987141,
      "grad_norm": 0.390349417924881,
      "learning_rate": 5.880840120017146e-06,
      "loss": 1.2103,
      "step": 49410
    },
    {
      "epoch": 10.591513073296186,
      "grad_norm": 0.5411498546600342,
      "learning_rate": 5.877982568938421e-06,
      "loss": 0.4818,
      "step": 49420
    },
    {
      "epoch": 10.593656236605229,
      "grad_norm": 0.6776316165924072,
      "learning_rate": 5.8751250178596956e-06,
      "loss": 0.1171,
      "step": 49430
    },
    {
      "epoch": 10.595799399914274,
      "grad_norm": 0.15333260595798492,
      "learning_rate": 5.872267466780969e-06,
      "loss": 0.8043,
      "step": 49440
    },
    {
      "epoch": 10.597942563223318,
      "grad_norm": 14.570418357849121,
      "learning_rate": 5.869409915702243e-06,
      "loss": 0.6125,
      "step": 49450
    },
    {
      "epoch": 10.600085726532361,
      "grad_norm": 17.18436050415039,
      "learning_rate": 5.866552364623518e-06,
      "loss": 0.6998,
      "step": 49460
    },
    {
      "epoch": 10.602228889841406,
      "grad_norm": 0.6324166655540466,
      "learning_rate": 5.863694813544792e-06,
      "loss": 0.4275,
      "step": 49470
    },
    {
      "epoch": 10.60437205315045,
      "grad_norm": 3.3907625675201416,
      "learning_rate": 5.860837262466067e-06,
      "loss": 0.5072,
      "step": 49480
    },
    {
      "epoch": 10.606515216459494,
      "grad_norm": 0.5499164462089539,
      "learning_rate": 5.857979711387342e-06,
      "loss": 0.8345,
      "step": 49490
    },
    {
      "epoch": 10.608658379768539,
      "grad_norm": 0.1563078910112381,
      "learning_rate": 5.8551221603086164e-06,
      "loss": 0.5368,
      "step": 49500
    },
    {
      "epoch": 10.610801543077583,
      "grad_norm": 0.5687077045440674,
      "learning_rate": 5.85226460922989e-06,
      "loss": 0.7554,
      "step": 49510
    },
    {
      "epoch": 10.612944706386626,
      "grad_norm": 30.679506301879883,
      "learning_rate": 5.849407058151165e-06,
      "loss": 1.0859,
      "step": 49520
    },
    {
      "epoch": 10.615087869695671,
      "grad_norm": 2.1700897216796875,
      "learning_rate": 5.84654950707244e-06,
      "loss": 0.6818,
      "step": 49530
    },
    {
      "epoch": 10.617231033004716,
      "grad_norm": 0.21392787992954254,
      "learning_rate": 5.843691955993715e-06,
      "loss": 0.3846,
      "step": 49540
    },
    {
      "epoch": 10.619374196313759,
      "grad_norm": 14.237523078918457,
      "learning_rate": 5.840834404914988e-06,
      "loss": 0.5392,
      "step": 49550
    },
    {
      "epoch": 10.621517359622803,
      "grad_norm": 28.933704376220703,
      "learning_rate": 5.8379768538362625e-06,
      "loss": 0.4777,
      "step": 49560
    },
    {
      "epoch": 10.623660522931848,
      "grad_norm": 12.137284278869629,
      "learning_rate": 5.835119302757537e-06,
      "loss": 0.3974,
      "step": 49570
    },
    {
      "epoch": 10.625803686240891,
      "grad_norm": 0.3300742506980896,
      "learning_rate": 5.832261751678811e-06,
      "loss": 0.3624,
      "step": 49580
    },
    {
      "epoch": 10.627946849549936,
      "grad_norm": 1.8136950731277466,
      "learning_rate": 5.829404200600086e-06,
      "loss": 0.3807,
      "step": 49590
    },
    {
      "epoch": 10.63009001285898,
      "grad_norm": 4.256978511810303,
      "learning_rate": 5.826546649521361e-06,
      "loss": 0.5582,
      "step": 49600
    },
    {
      "epoch": 10.632233176168024,
      "grad_norm": 2.3002052307128906,
      "learning_rate": 5.823689098442636e-06,
      "loss": 0.5564,
      "step": 49610
    },
    {
      "epoch": 10.634376339477068,
      "grad_norm": 0.8900678753852844,
      "learning_rate": 5.8208315473639095e-06,
      "loss": 0.7939,
      "step": 49620
    },
    {
      "epoch": 10.636519502786113,
      "grad_norm": 3.800428628921509,
      "learning_rate": 5.817973996285184e-06,
      "loss": 0.5554,
      "step": 49630
    },
    {
      "epoch": 10.638662666095156,
      "grad_norm": 0.45775261521339417,
      "learning_rate": 5.815116445206459e-06,
      "loss": 0.3409,
      "step": 49640
    },
    {
      "epoch": 10.6408058294042,
      "grad_norm": 74.07674407958984,
      "learning_rate": 5.812258894127732e-06,
      "loss": 0.2968,
      "step": 49650
    },
    {
      "epoch": 10.642948992713245,
      "grad_norm": 0.22997815907001495,
      "learning_rate": 5.809401343049007e-06,
      "loss": 0.1555,
      "step": 49660
    },
    {
      "epoch": 10.645092156022288,
      "grad_norm": 20.895212173461914,
      "learning_rate": 5.806543791970282e-06,
      "loss": 0.9215,
      "step": 49670
    },
    {
      "epoch": 10.647235319331333,
      "grad_norm": 0.22960922122001648,
      "learning_rate": 5.8036862408915565e-06,
      "loss": 0.4272,
      "step": 49680
    },
    {
      "epoch": 10.649378482640378,
      "grad_norm": 3.5294947624206543,
      "learning_rate": 5.80082868981283e-06,
      "loss": 0.2236,
      "step": 49690
    },
    {
      "epoch": 10.65152164594942,
      "grad_norm": 0.1833185851573944,
      "learning_rate": 5.797971138734105e-06,
      "loss": 0.4003,
      "step": 49700
    },
    {
      "epoch": 10.653664809258466,
      "grad_norm": 0.21328900754451752,
      "learning_rate": 5.79511358765538e-06,
      "loss": 0.2905,
      "step": 49710
    },
    {
      "epoch": 10.65580797256751,
      "grad_norm": 0.2489221692085266,
      "learning_rate": 5.792256036576655e-06,
      "loss": 0.2058,
      "step": 49720
    },
    {
      "epoch": 10.657951135876553,
      "grad_norm": 12.133004188537598,
      "learning_rate": 5.789398485497929e-06,
      "loss": 0.8092,
      "step": 49730
    },
    {
      "epoch": 10.660094299185598,
      "grad_norm": 0.4428240954875946,
      "learning_rate": 5.786540934419203e-06,
      "loss": 0.2554,
      "step": 49740
    },
    {
      "epoch": 10.662237462494643,
      "grad_norm": 34.90721130371094,
      "learning_rate": 5.783683383340478e-06,
      "loss": 0.927,
      "step": 49750
    },
    {
      "epoch": 10.664380625803686,
      "grad_norm": 0.39495426416397095,
      "learning_rate": 5.780825832261751e-06,
      "loss": 0.3608,
      "step": 49760
    },
    {
      "epoch": 10.66652378911273,
      "grad_norm": 45.072509765625,
      "learning_rate": 5.777968281183026e-06,
      "loss": 0.6012,
      "step": 49770
    },
    {
      "epoch": 10.668666952421775,
      "grad_norm": 0.44940415024757385,
      "learning_rate": 5.775110730104301e-06,
      "loss": 0.332,
      "step": 49780
    },
    {
      "epoch": 10.670810115730818,
      "grad_norm": 0.36009863018989563,
      "learning_rate": 5.772253179025576e-06,
      "loss": 0.03,
      "step": 49790
    },
    {
      "epoch": 10.672953279039863,
      "grad_norm": 21.4680118560791,
      "learning_rate": 5.7693956279468495e-06,
      "loss": 0.8568,
      "step": 49800
    },
    {
      "epoch": 10.675096442348908,
      "grad_norm": 0.13049925863742828,
      "learning_rate": 5.766538076868124e-06,
      "loss": 0.6453,
      "step": 49810
    },
    {
      "epoch": 10.67723960565795,
      "grad_norm": 1.5196352005004883,
      "learning_rate": 5.763680525789399e-06,
      "loss": 0.3895,
      "step": 49820
    },
    {
      "epoch": 10.679382768966995,
      "grad_norm": 2.7348852157592773,
      "learning_rate": 5.760822974710674e-06,
      "loss": 0.2659,
      "step": 49830
    },
    {
      "epoch": 10.68152593227604,
      "grad_norm": 0.09957101941108704,
      "learning_rate": 5.757965423631949e-06,
      "loss": 0.0269,
      "step": 49840
    },
    {
      "epoch": 10.683669095585083,
      "grad_norm": 14.103323936462402,
      "learning_rate": 5.7551078725532226e-06,
      "loss": 0.2513,
      "step": 49850
    },
    {
      "epoch": 10.685812258894128,
      "grad_norm": 1.152416706085205,
      "learning_rate": 5.752250321474497e-06,
      "loss": 0.2786,
      "step": 49860
    },
    {
      "epoch": 10.687955422203173,
      "grad_norm": 18.95588493347168,
      "learning_rate": 5.74939277039577e-06,
      "loss": 0.8894,
      "step": 49870
    },
    {
      "epoch": 10.690098585512215,
      "grad_norm": 0.13238415122032166,
      "learning_rate": 5.746535219317045e-06,
      "loss": 0.309,
      "step": 49880
    },
    {
      "epoch": 10.69224174882126,
      "grad_norm": 14.39708137512207,
      "learning_rate": 5.74367766823832e-06,
      "loss": 0.2546,
      "step": 49890
    },
    {
      "epoch": 10.694384912130305,
      "grad_norm": 12.59246826171875,
      "learning_rate": 5.740820117159595e-06,
      "loss": 0.3513,
      "step": 49900
    },
    {
      "epoch": 10.696528075439348,
      "grad_norm": 0.1385425329208374,
      "learning_rate": 5.7379625660808695e-06,
      "loss": 0.6246,
      "step": 49910
    },
    {
      "epoch": 10.698671238748393,
      "grad_norm": 0.17552976310253143,
      "learning_rate": 5.7351050150021434e-06,
      "loss": 0.5601,
      "step": 49920
    },
    {
      "epoch": 10.700814402057437,
      "grad_norm": 0.3427433967590332,
      "learning_rate": 5.732247463923418e-06,
      "loss": 0.1748,
      "step": 49930
    },
    {
      "epoch": 10.70295756536648,
      "grad_norm": 1.7808620929718018,
      "learning_rate": 5.729389912844693e-06,
      "loss": 0.5893,
      "step": 49940
    },
    {
      "epoch": 10.705100728675525,
      "grad_norm": 0.2647899389266968,
      "learning_rate": 5.726532361765968e-06,
      "loss": 0.5202,
      "step": 49950
    },
    {
      "epoch": 10.70724389198457,
      "grad_norm": 38.626033782958984,
      "learning_rate": 5.723674810687242e-06,
      "loss": 0.1617,
      "step": 49960
    },
    {
      "epoch": 10.709387055293613,
      "grad_norm": 28.459108352661133,
      "learning_rate": 5.720817259608516e-06,
      "loss": 0.6898,
      "step": 49970
    },
    {
      "epoch": 10.711530218602658,
      "grad_norm": 0.1828755885362625,
      "learning_rate": 5.71795970852979e-06,
      "loss": 0.626,
      "step": 49980
    },
    {
      "epoch": 10.713673381911702,
      "grad_norm": 0.5833923816680908,
      "learning_rate": 5.715102157451064e-06,
      "loss": 0.3738,
      "step": 49990
    },
    {
      "epoch": 10.715816545220745,
      "grad_norm": 11.329070091247559,
      "learning_rate": 5.712244606372339e-06,
      "loss": 0.5952,
      "step": 50000
    },
    {
      "epoch": 10.71795970852979,
      "grad_norm": 0.6719929575920105,
      "learning_rate": 5.709387055293614e-06,
      "loss": 0.3165,
      "step": 50010
    },
    {
      "epoch": 10.720102871838835,
      "grad_norm": 0.16041237115859985,
      "learning_rate": 5.706529504214889e-06,
      "loss": 0.2828,
      "step": 50020
    },
    {
      "epoch": 10.722246035147878,
      "grad_norm": 12.330520629882812,
      "learning_rate": 5.703671953136163e-06,
      "loss": 0.2901,
      "step": 50030
    },
    {
      "epoch": 10.724389198456922,
      "grad_norm": 0.08273886889219284,
      "learning_rate": 5.700814402057437e-06,
      "loss": 0.2844,
      "step": 50040
    },
    {
      "epoch": 10.726532361765967,
      "grad_norm": 0.07919033616781235,
      "learning_rate": 5.697956850978712e-06,
      "loss": 0.291,
      "step": 50050
    },
    {
      "epoch": 10.72867552507501,
      "grad_norm": 35.83502197265625,
      "learning_rate": 5.695099299899987e-06,
      "loss": 0.9329,
      "step": 50060
    },
    {
      "epoch": 10.730818688384055,
      "grad_norm": 0.08329827338457108,
      "learning_rate": 5.692241748821261e-06,
      "loss": 0.0996,
      "step": 50070
    },
    {
      "epoch": 10.7329618516931,
      "grad_norm": 73.14129638671875,
      "learning_rate": 5.689384197742535e-06,
      "loss": 0.3096,
      "step": 50080
    },
    {
      "epoch": 10.735105015002143,
      "grad_norm": 4.213139057159424,
      "learning_rate": 5.6865266466638095e-06,
      "loss": 0.2884,
      "step": 50090
    },
    {
      "epoch": 10.737248178311187,
      "grad_norm": 25.41496467590332,
      "learning_rate": 5.6836690955850835e-06,
      "loss": 0.5194,
      "step": 50100
    },
    {
      "epoch": 10.739391341620232,
      "grad_norm": 35.73131561279297,
      "learning_rate": 5.680811544506358e-06,
      "loss": 0.649,
      "step": 50110
    },
    {
      "epoch": 10.741534504929275,
      "grad_norm": 16.43263816833496,
      "learning_rate": 5.677953993427633e-06,
      "loss": 0.7538,
      "step": 50120
    },
    {
      "epoch": 10.74367766823832,
      "grad_norm": 9.961716651916504,
      "learning_rate": 5.675096442348908e-06,
      "loss": 0.3719,
      "step": 50130
    },
    {
      "epoch": 10.745820831547364,
      "grad_norm": 0.44007837772369385,
      "learning_rate": 5.672238891270182e-06,
      "loss": 0.3276,
      "step": 50140
    },
    {
      "epoch": 10.747963994856407,
      "grad_norm": 0.2355032116174698,
      "learning_rate": 5.6693813401914565e-06,
      "loss": 0.412,
      "step": 50150
    },
    {
      "epoch": 10.750107158165452,
      "grad_norm": 19.864412307739258,
      "learning_rate": 5.666523789112731e-06,
      "loss": 0.2375,
      "step": 50160
    },
    {
      "epoch": 10.752250321474497,
      "grad_norm": 0.1585996448993683,
      "learning_rate": 5.663666238034006e-06,
      "loss": 0.5092,
      "step": 50170
    },
    {
      "epoch": 10.75439348478354,
      "grad_norm": 0.3406478762626648,
      "learning_rate": 5.66080868695528e-06,
      "loss": 0.4752,
      "step": 50180
    },
    {
      "epoch": 10.756536648092585,
      "grad_norm": 0.14711327850818634,
      "learning_rate": 5.657951135876554e-06,
      "loss": 0.2449,
      "step": 50190
    },
    {
      "epoch": 10.75867981140163,
      "grad_norm": 0.12417709827423096,
      "learning_rate": 5.655093584797829e-06,
      "loss": 0.5032,
      "step": 50200
    },
    {
      "epoch": 10.760822974710672,
      "grad_norm": 19.648073196411133,
      "learning_rate": 5.652236033719103e-06,
      "loss": 0.4593,
      "step": 50210
    },
    {
      "epoch": 10.762966138019717,
      "grad_norm": 0.1695912629365921,
      "learning_rate": 5.649378482640377e-06,
      "loss": 0.1983,
      "step": 50220
    },
    {
      "epoch": 10.765109301328762,
      "grad_norm": 32.532745361328125,
      "learning_rate": 5.646520931561652e-06,
      "loss": 0.3569,
      "step": 50230
    },
    {
      "epoch": 10.767252464637805,
      "grad_norm": 1.9854031801223755,
      "learning_rate": 5.643663380482927e-06,
      "loss": 0.3296,
      "step": 50240
    },
    {
      "epoch": 10.76939562794685,
      "grad_norm": 0.6086340546607971,
      "learning_rate": 5.640805829404201e-06,
      "loss": 0.4929,
      "step": 50250
    },
    {
      "epoch": 10.771538791255894,
      "grad_norm": 9.62144947052002,
      "learning_rate": 5.637948278325476e-06,
      "loss": 0.5792,
      "step": 50260
    },
    {
      "epoch": 10.773681954564937,
      "grad_norm": 0.7779987454414368,
      "learning_rate": 5.63509072724675e-06,
      "loss": 0.6746,
      "step": 50270
    },
    {
      "epoch": 10.775825117873982,
      "grad_norm": 8.911500930786133,
      "learning_rate": 5.632233176168025e-06,
      "loss": 0.2162,
      "step": 50280
    },
    {
      "epoch": 10.777968281183027,
      "grad_norm": 0.9908723831176758,
      "learning_rate": 5.629375625089299e-06,
      "loss": 0.6189,
      "step": 50290
    },
    {
      "epoch": 10.78011144449207,
      "grad_norm": 10.290678024291992,
      "learning_rate": 5.626518074010573e-06,
      "loss": 0.6229,
      "step": 50300
    },
    {
      "epoch": 10.782254607801114,
      "grad_norm": 0.5984352827072144,
      "learning_rate": 5.623660522931848e-06,
      "loss": 0.7277,
      "step": 50310
    },
    {
      "epoch": 10.784397771110159,
      "grad_norm": 0.43616417050361633,
      "learning_rate": 5.620802971853122e-06,
      "loss": 0.1075,
      "step": 50320
    },
    {
      "epoch": 10.786540934419202,
      "grad_norm": 19.740657806396484,
      "learning_rate": 5.6179454207743965e-06,
      "loss": 0.6817,
      "step": 50330
    },
    {
      "epoch": 10.788684097728247,
      "grad_norm": 0.3664557933807373,
      "learning_rate": 5.615087869695671e-06,
      "loss": 0.0056,
      "step": 50340
    },
    {
      "epoch": 10.790827261037292,
      "grad_norm": 0.10388435423374176,
      "learning_rate": 5.612230318616946e-06,
      "loss": 0.6266,
      "step": 50350
    },
    {
      "epoch": 10.792970424346334,
      "grad_norm": 0.06943505257368088,
      "learning_rate": 5.60937276753822e-06,
      "loss": 0.6592,
      "step": 50360
    },
    {
      "epoch": 10.79511358765538,
      "grad_norm": 5.3902788162231445,
      "learning_rate": 5.606515216459495e-06,
      "loss": 0.5587,
      "step": 50370
    },
    {
      "epoch": 10.797256750964424,
      "grad_norm": 34.74327850341797,
      "learning_rate": 5.6036576653807696e-06,
      "loss": 0.8043,
      "step": 50380
    },
    {
      "epoch": 10.799399914273467,
      "grad_norm": 0.3519192337989807,
      "learning_rate": 5.600800114302044e-06,
      "loss": 0.1839,
      "step": 50390
    },
    {
      "epoch": 10.801543077582512,
      "grad_norm": 30.473175048828125,
      "learning_rate": 5.597942563223317e-06,
      "loss": 0.5754,
      "step": 50400
    },
    {
      "epoch": 10.803686240891556,
      "grad_norm": 16.61887550354004,
      "learning_rate": 5.595085012144592e-06,
      "loss": 0.4331,
      "step": 50410
    },
    {
      "epoch": 10.8058294042006,
      "grad_norm": 15.934179306030273,
      "learning_rate": 5.592227461065867e-06,
      "loss": 0.8804,
      "step": 50420
    },
    {
      "epoch": 10.807972567509644,
      "grad_norm": 0.312730997800827,
      "learning_rate": 5.589369909987141e-06,
      "loss": 0.059,
      "step": 50430
    },
    {
      "epoch": 10.810115730818689,
      "grad_norm": 0.1851704716682434,
      "learning_rate": 5.586512358908416e-06,
      "loss": 0.3801,
      "step": 50440
    },
    {
      "epoch": 10.812258894127732,
      "grad_norm": 0.4689352214336395,
      "learning_rate": 5.5836548078296904e-06,
      "loss": 0.6375,
      "step": 50450
    },
    {
      "epoch": 10.814402057436777,
      "grad_norm": 7.410476207733154,
      "learning_rate": 5.580797256750965e-06,
      "loss": 0.4584,
      "step": 50460
    },
    {
      "epoch": 10.816545220745821,
      "grad_norm": 5.139347553253174,
      "learning_rate": 5.577939705672239e-06,
      "loss": 0.4665,
      "step": 50470
    },
    {
      "epoch": 10.818688384054864,
      "grad_norm": 20.60079002380371,
      "learning_rate": 5.575082154593514e-06,
      "loss": 0.6141,
      "step": 50480
    },
    {
      "epoch": 10.820831547363909,
      "grad_norm": 0.4327506422996521,
      "learning_rate": 5.572224603514789e-06,
      "loss": 0.5441,
      "step": 50490
    },
    {
      "epoch": 10.822974710672954,
      "grad_norm": 6.214450359344482,
      "learning_rate": 5.5693670524360635e-06,
      "loss": 0.1941,
      "step": 50500
    },
    {
      "epoch": 10.825117873981997,
      "grad_norm": 0.20183438062667847,
      "learning_rate": 5.5665095013573366e-06,
      "loss": 0.5572,
      "step": 50510
    },
    {
      "epoch": 10.827261037291041,
      "grad_norm": 8.425580024719238,
      "learning_rate": 5.563651950278611e-06,
      "loss": 0.3225,
      "step": 50520
    },
    {
      "epoch": 10.829404200600086,
      "grad_norm": 0.6248738765716553,
      "learning_rate": 5.560794399199886e-06,
      "loss": 0.3905,
      "step": 50530
    },
    {
      "epoch": 10.831547363909129,
      "grad_norm": 0.09267190843820572,
      "learning_rate": 5.55793684812116e-06,
      "loss": 0.2457,
      "step": 50540
    },
    {
      "epoch": 10.833690527218174,
      "grad_norm": 0.5002108812332153,
      "learning_rate": 5.555079297042435e-06,
      "loss": 0.4447,
      "step": 50550
    },
    {
      "epoch": 10.835833690527219,
      "grad_norm": 22.42613983154297,
      "learning_rate": 5.55222174596371e-06,
      "loss": 0.4428,
      "step": 50560
    },
    {
      "epoch": 10.837976853836262,
      "grad_norm": 0.09597417712211609,
      "learning_rate": 5.549364194884984e-06,
      "loss": 0.2976,
      "step": 50570
    },
    {
      "epoch": 10.840120017145306,
      "grad_norm": 47.887420654296875,
      "learning_rate": 5.546506643806258e-06,
      "loss": 0.4373,
      "step": 50580
    },
    {
      "epoch": 10.842263180454351,
      "grad_norm": 2.097381353378296,
      "learning_rate": 5.543649092727533e-06,
      "loss": 0.7785,
      "step": 50590
    },
    {
      "epoch": 10.844406343763394,
      "grad_norm": 0.20746901631355286,
      "learning_rate": 5.540791541648808e-06,
      "loss": 0.6636,
      "step": 50600
    },
    {
      "epoch": 10.846549507072439,
      "grad_norm": 22.00235939025879,
      "learning_rate": 5.537933990570083e-06,
      "loss": 0.1953,
      "step": 50610
    },
    {
      "epoch": 10.848692670381483,
      "grad_norm": 64.238037109375,
      "learning_rate": 5.535076439491356e-06,
      "loss": 0.5386,
      "step": 50620
    },
    {
      "epoch": 10.850835833690526,
      "grad_norm": 5.907688617706299,
      "learning_rate": 5.5322188884126305e-06,
      "loss": 0.0759,
      "step": 50630
    },
    {
      "epoch": 10.852978996999571,
      "grad_norm": 0.4477914571762085,
      "learning_rate": 5.529361337333905e-06,
      "loss": 0.797,
      "step": 50640
    },
    {
      "epoch": 10.855122160308616,
      "grad_norm": 0.17786036431789398,
      "learning_rate": 5.526503786255179e-06,
      "loss": 0.0314,
      "step": 50650
    },
    {
      "epoch": 10.857265323617659,
      "grad_norm": 0.32263875007629395,
      "learning_rate": 5.523646235176454e-06,
      "loss": 0.5012,
      "step": 50660
    },
    {
      "epoch": 10.859408486926704,
      "grad_norm": 0.5109682679176331,
      "learning_rate": 5.520788684097729e-06,
      "loss": 0.6822,
      "step": 50670
    },
    {
      "epoch": 10.861551650235748,
      "grad_norm": 18.978160858154297,
      "learning_rate": 5.5179311330190035e-06,
      "loss": 0.2121,
      "step": 50680
    },
    {
      "epoch": 10.863694813544793,
      "grad_norm": 0.09565579891204834,
      "learning_rate": 5.515073581940278e-06,
      "loss": 0.2351,
      "step": 50690
    },
    {
      "epoch": 10.865837976853836,
      "grad_norm": 16.071399688720703,
      "learning_rate": 5.512216030861552e-06,
      "loss": 0.7873,
      "step": 50700
    },
    {
      "epoch": 10.86798114016288,
      "grad_norm": 0.3793753981590271,
      "learning_rate": 5.509358479782827e-06,
      "loss": 0.5095,
      "step": 50710
    },
    {
      "epoch": 10.870124303471925,
      "grad_norm": 0.11413727700710297,
      "learning_rate": 5.506500928704102e-06,
      "loss": 0.2662,
      "step": 50720
    },
    {
      "epoch": 10.872267466780968,
      "grad_norm": 0.15052978694438934,
      "learning_rate": 5.503643377625375e-06,
      "loss": 0.2704,
      "step": 50730
    },
    {
      "epoch": 10.874410630090013,
      "grad_norm": 0.14920130372047424,
      "learning_rate": 5.50078582654665e-06,
      "loss": 0.5151,
      "step": 50740
    },
    {
      "epoch": 10.876553793399058,
      "grad_norm": 0.07537063956260681,
      "learning_rate": 5.497928275467924e-06,
      "loss": 0.1961,
      "step": 50750
    },
    {
      "epoch": 10.8786969567081,
      "grad_norm": 0.06334111094474792,
      "learning_rate": 5.495070724389199e-06,
      "loss": 0.0318,
      "step": 50760
    },
    {
      "epoch": 10.880840120017146,
      "grad_norm": 0.19467946887016296,
      "learning_rate": 5.492213173310473e-06,
      "loss": 0.5406,
      "step": 50770
    },
    {
      "epoch": 10.88298328332619,
      "grad_norm": 0.09467009454965591,
      "learning_rate": 5.489355622231748e-06,
      "loss": 0.388,
      "step": 50780
    },
    {
      "epoch": 10.885126446635233,
      "grad_norm": 15.326672554016113,
      "learning_rate": 5.486498071153023e-06,
      "loss": 0.6098,
      "step": 50790
    },
    {
      "epoch": 10.887269609944278,
      "grad_norm": 12.800496101379395,
      "learning_rate": 5.483640520074297e-06,
      "loss": 0.5891,
      "step": 50800
    },
    {
      "epoch": 10.889412773253323,
      "grad_norm": 17.03118133544922,
      "learning_rate": 5.480782968995571e-06,
      "loss": 0.3618,
      "step": 50810
    },
    {
      "epoch": 10.891555936562366,
      "grad_norm": 19.094316482543945,
      "learning_rate": 5.477925417916846e-06,
      "loss": 0.2942,
      "step": 50820
    },
    {
      "epoch": 10.89369909987141,
      "grad_norm": 0.25945717096328735,
      "learning_rate": 5.47506786683812e-06,
      "loss": 0.4713,
      "step": 50830
    },
    {
      "epoch": 10.895842263180455,
      "grad_norm": 0.30828091502189636,
      "learning_rate": 5.472210315759394e-06,
      "loss": 0.1613,
      "step": 50840
    },
    {
      "epoch": 10.897985426489498,
      "grad_norm": 0.18665331602096558,
      "learning_rate": 5.469352764680669e-06,
      "loss": 0.3691,
      "step": 50850
    },
    {
      "epoch": 10.900128589798543,
      "grad_norm": 14.047239303588867,
      "learning_rate": 5.4664952136019435e-06,
      "loss": 0.3644,
      "step": 50860
    },
    {
      "epoch": 10.902271753107588,
      "grad_norm": 7.488735675811768,
      "learning_rate": 5.463637662523218e-06,
      "loss": 0.3106,
      "step": 50870
    },
    {
      "epoch": 10.90441491641663,
      "grad_norm": 0.14998680353164673,
      "learning_rate": 5.460780111444492e-06,
      "loss": 0.7924,
      "step": 50880
    },
    {
      "epoch": 10.906558079725675,
      "grad_norm": 7.91562557220459,
      "learning_rate": 5.457922560365767e-06,
      "loss": 0.3364,
      "step": 50890
    },
    {
      "epoch": 10.90870124303472,
      "grad_norm": 16.760578155517578,
      "learning_rate": 5.455065009287042e-06,
      "loss": 0.6747,
      "step": 50900
    },
    {
      "epoch": 10.910844406343763,
      "grad_norm": 21.600183486938477,
      "learning_rate": 5.4522074582083166e-06,
      "loss": 0.5565,
      "step": 50910
    },
    {
      "epoch": 10.912987569652808,
      "grad_norm": 31.293128967285156,
      "learning_rate": 5.4493499071295905e-06,
      "loss": 0.6958,
      "step": 50920
    },
    {
      "epoch": 10.915130732961853,
      "grad_norm": 0.1384207308292389,
      "learning_rate": 5.446492356050865e-06,
      "loss": 0.5413,
      "step": 50930
    },
    {
      "epoch": 10.917273896270896,
      "grad_norm": 0.3227235674858093,
      "learning_rate": 5.443634804972139e-06,
      "loss": 0.1614,
      "step": 50940
    },
    {
      "epoch": 10.91941705957994,
      "grad_norm": 11.780966758728027,
      "learning_rate": 5.440777253893413e-06,
      "loss": 0.0705,
      "step": 50950
    },
    {
      "epoch": 10.921560222888985,
      "grad_norm": 0.05437418073415756,
      "learning_rate": 5.437919702814688e-06,
      "loss": 0.299,
      "step": 50960
    },
    {
      "epoch": 10.923703386198028,
      "grad_norm": 0.2332676351070404,
      "learning_rate": 5.435062151735963e-06,
      "loss": 0.4753,
      "step": 50970
    },
    {
      "epoch": 10.925846549507073,
      "grad_norm": 0.06995254009962082,
      "learning_rate": 5.4322046006572374e-06,
      "loss": 0.5932,
      "step": 50980
    },
    {
      "epoch": 10.927989712816117,
      "grad_norm": 0.09638579189777374,
      "learning_rate": 5.429347049578511e-06,
      "loss": 0.3972,
      "step": 50990
    },
    {
      "epoch": 10.93013287612516,
      "grad_norm": 0.5526588559150696,
      "learning_rate": 5.426489498499786e-06,
      "loss": 0.1803,
      "step": 51000
    },
    {
      "epoch": 10.932276039434205,
      "grad_norm": 0.12154228985309601,
      "learning_rate": 5.423631947421061e-06,
      "loss": 0.3311,
      "step": 51010
    },
    {
      "epoch": 10.93441920274325,
      "grad_norm": 163.5345001220703,
      "learning_rate": 5.420774396342336e-06,
      "loss": 0.7046,
      "step": 51020
    },
    {
      "epoch": 10.936562366052293,
      "grad_norm": 0.1881655603647232,
      "learning_rate": 5.41791684526361e-06,
      "loss": 0.7626,
      "step": 51030
    },
    {
      "epoch": 10.938705529361338,
      "grad_norm": 17.39331817626953,
      "learning_rate": 5.415059294184884e-06,
      "loss": 0.4873,
      "step": 51040
    },
    {
      "epoch": 10.940848692670382,
      "grad_norm": 6.530455589294434,
      "learning_rate": 5.412201743106158e-06,
      "loss": 0.1444,
      "step": 51050
    },
    {
      "epoch": 10.942991855979425,
      "grad_norm": 29.52912712097168,
      "learning_rate": 5.409344192027432e-06,
      "loss": 0.1936,
      "step": 51060
    },
    {
      "epoch": 10.94513501928847,
      "grad_norm": 21.191123962402344,
      "learning_rate": 5.406486640948707e-06,
      "loss": 0.8714,
      "step": 51070
    },
    {
      "epoch": 10.947278182597515,
      "grad_norm": 0.3495256006717682,
      "learning_rate": 5.403629089869982e-06,
      "loss": 0.1505,
      "step": 51080
    },
    {
      "epoch": 10.949421345906558,
      "grad_norm": 4.588223457336426,
      "learning_rate": 5.400771538791257e-06,
      "loss": 0.7454,
      "step": 51090
    },
    {
      "epoch": 10.951564509215602,
      "grad_norm": 0.2248142510652542,
      "learning_rate": 5.3979139877125305e-06,
      "loss": 0.4591,
      "step": 51100
    },
    {
      "epoch": 10.953707672524647,
      "grad_norm": 0.2425553947687149,
      "learning_rate": 5.395056436633805e-06,
      "loss": 0.3632,
      "step": 51110
    },
    {
      "epoch": 10.95585083583369,
      "grad_norm": 188.56167602539062,
      "learning_rate": 5.39219888555508e-06,
      "loss": 0.736,
      "step": 51120
    },
    {
      "epoch": 10.957993999142735,
      "grad_norm": 1.440603256225586,
      "learning_rate": 5.389341334476355e-06,
      "loss": 0.5871,
      "step": 51130
    },
    {
      "epoch": 10.96013716245178,
      "grad_norm": 0.9488093256950378,
      "learning_rate": 5.386483783397629e-06,
      "loss": 0.4753,
      "step": 51140
    },
    {
      "epoch": 10.962280325760823,
      "grad_norm": 0.11518575996160507,
      "learning_rate": 5.3836262323189035e-06,
      "loss": 0.0316,
      "step": 51150
    },
    {
      "epoch": 10.964423489069867,
      "grad_norm": 0.164117693901062,
      "learning_rate": 5.3807686812401775e-06,
      "loss": 0.8594,
      "step": 51160
    },
    {
      "epoch": 10.966566652378912,
      "grad_norm": 0.2018084079027176,
      "learning_rate": 5.377911130161451e-06,
      "loss": 0.5747,
      "step": 51170
    },
    {
      "epoch": 10.968709815687955,
      "grad_norm": 15.683631896972656,
      "learning_rate": 5.375053579082726e-06,
      "loss": 0.3321,
      "step": 51180
    },
    {
      "epoch": 10.970852978997,
      "grad_norm": 4.428543567657471,
      "learning_rate": 5.372196028004001e-06,
      "loss": 0.6122,
      "step": 51190
    },
    {
      "epoch": 10.972996142306044,
      "grad_norm": 0.26066672801971436,
      "learning_rate": 5.369338476925276e-06,
      "loss": 0.7548,
      "step": 51200
    },
    {
      "epoch": 10.975139305615087,
      "grad_norm": 38.28887176513672,
      "learning_rate": 5.36648092584655e-06,
      "loss": 0.6427,
      "step": 51210
    },
    {
      "epoch": 10.977282468924132,
      "grad_norm": 0.5655263066291809,
      "learning_rate": 5.3636233747678244e-06,
      "loss": 0.1997,
      "step": 51220
    },
    {
      "epoch": 10.979425632233177,
      "grad_norm": 0.7361695170402527,
      "learning_rate": 5.360765823689099e-06,
      "loss": 0.5718,
      "step": 51230
    },
    {
      "epoch": 10.98156879554222,
      "grad_norm": 0.30108824372291565,
      "learning_rate": 5.357908272610374e-06,
      "loss": 0.5528,
      "step": 51240
    },
    {
      "epoch": 10.983711958851265,
      "grad_norm": 0.5724509954452515,
      "learning_rate": 5.355050721531648e-06,
      "loss": 0.5376,
      "step": 51250
    },
    {
      "epoch": 10.98585512216031,
      "grad_norm": 0.2911640703678131,
      "learning_rate": 5.352193170452922e-06,
      "loss": 0.2124,
      "step": 51260
    },
    {
      "epoch": 10.987998285469352,
      "grad_norm": 16.995616912841797,
      "learning_rate": 5.349335619374197e-06,
      "loss": 0.9351,
      "step": 51270
    },
    {
      "epoch": 10.990141448778397,
      "grad_norm": 0.25209859013557434,
      "learning_rate": 5.3464780682954705e-06,
      "loss": 0.1798,
      "step": 51280
    },
    {
      "epoch": 10.992284612087442,
      "grad_norm": 78.63339233398438,
      "learning_rate": 5.343620517216745e-06,
      "loss": 0.7326,
      "step": 51290
    },
    {
      "epoch": 10.994427775396485,
      "grad_norm": 24.665740966796875,
      "learning_rate": 5.34076296613802e-06,
      "loss": 0.4947,
      "step": 51300
    },
    {
      "epoch": 10.99657093870553,
      "grad_norm": 2.0812063217163086,
      "learning_rate": 5.337905415059295e-06,
      "loss": 0.4757,
      "step": 51310
    },
    {
      "epoch": 10.998714102014574,
      "grad_norm": 0.5966941714286804,
      "learning_rate": 5.335047863980569e-06,
      "loss": 0.5335,
      "step": 51320
    },
    {
      "epoch": 11.0,
      "eval_accuracy": 0.8913333333333333,
      "eval_f1": 0.4359861591695502,
      "eval_loss": 0.3988756835460663,
      "eval_precision": 0.6961325966850829,
      "eval_recall": 0.31738035264483627,
      "eval_runtime": 397.466,
      "eval_samples_per_second": 7.548,
      "eval_steps_per_second": 2.516,
      "step": 51326
    },
    {
      "epoch": 11.000857265323617,
      "grad_norm": 18.642478942871094,
      "learning_rate": 5.3321903129018436e-06,
      "loss": 0.8721,
      "step": 51330
    },
    {
      "epoch": 11.003000428632662,
      "grad_norm": 16.51458740234375,
      "learning_rate": 5.329332761823118e-06,
      "loss": 0.7373,
      "step": 51340
    },
    {
      "epoch": 11.005143591941707,
      "grad_norm": 0.7537529468536377,
      "learning_rate": 5.326475210744393e-06,
      "loss": 0.2343,
      "step": 51350
    },
    {
      "epoch": 11.00728675525075,
      "grad_norm": 0.33139583468437195,
      "learning_rate": 5.323617659665667e-06,
      "loss": 0.1423,
      "step": 51360
    },
    {
      "epoch": 11.009429918559794,
      "grad_norm": 14.075690269470215,
      "learning_rate": 5.320760108586941e-06,
      "loss": 0.2119,
      "step": 51370
    },
    {
      "epoch": 11.011573081868839,
      "grad_norm": 0.0903654620051384,
      "learning_rate": 5.317902557508216e-06,
      "loss": 0.4444,
      "step": 51380
    },
    {
      "epoch": 11.013716245177882,
      "grad_norm": 0.2717788815498352,
      "learning_rate": 5.31504500642949e-06,
      "loss": 0.3645,
      "step": 51390
    },
    {
      "epoch": 11.015859408486927,
      "grad_norm": 0.1040751188993454,
      "learning_rate": 5.3121874553507645e-06,
      "loss": 0.1795,
      "step": 51400
    },
    {
      "epoch": 11.018002571795972,
      "grad_norm": 20.39766502380371,
      "learning_rate": 5.309329904272039e-06,
      "loss": 0.2158,
      "step": 51410
    },
    {
      "epoch": 11.020145735105014,
      "grad_norm": 8.770403861999512,
      "learning_rate": 5.306472353193314e-06,
      "loss": 0.9999,
      "step": 51420
    },
    {
      "epoch": 11.02228889841406,
      "grad_norm": 20.63993263244629,
      "learning_rate": 5.303614802114588e-06,
      "loss": 0.3102,
      "step": 51430
    },
    {
      "epoch": 11.024432061723104,
      "grad_norm": 0.24553066492080688,
      "learning_rate": 5.300757251035863e-06,
      "loss": 0.4327,
      "step": 51440
    },
    {
      "epoch": 11.026575225032147,
      "grad_norm": 2.2671377658843994,
      "learning_rate": 5.2978996999571375e-06,
      "loss": 0.2277,
      "step": 51450
    },
    {
      "epoch": 11.028718388341192,
      "grad_norm": 27.0798397064209,
      "learning_rate": 5.295042148878412e-06,
      "loss": 0.5742,
      "step": 51460
    },
    {
      "epoch": 11.030861551650236,
      "grad_norm": 0.3182581663131714,
      "learning_rate": 5.292184597799687e-06,
      "loss": 0.0062,
      "step": 51470
    },
    {
      "epoch": 11.03300471495928,
      "grad_norm": 18.792997360229492,
      "learning_rate": 5.28932704672096e-06,
      "loss": 0.9923,
      "step": 51480
    },
    {
      "epoch": 11.035147878268324,
      "grad_norm": 25.283794403076172,
      "learning_rate": 5.286469495642235e-06,
      "loss": 0.5821,
      "step": 51490
    },
    {
      "epoch": 11.037291041577369,
      "grad_norm": 20.06623077392578,
      "learning_rate": 5.283611944563509e-06,
      "loss": 0.5838,
      "step": 51500
    },
    {
      "epoch": 11.039434204886412,
      "grad_norm": 0.2909708619117737,
      "learning_rate": 5.280754393484784e-06,
      "loss": 0.0809,
      "step": 51510
    },
    {
      "epoch": 11.041577368195457,
      "grad_norm": 0.43334639072418213,
      "learning_rate": 5.277896842406058e-06,
      "loss": 0.2238,
      "step": 51520
    },
    {
      "epoch": 11.043720531504501,
      "grad_norm": 0.7540340423583984,
      "learning_rate": 5.275039291327333e-06,
      "loss": 0.9445,
      "step": 51530
    },
    {
      "epoch": 11.045863694813544,
      "grad_norm": 199.1063995361328,
      "learning_rate": 5.272181740248608e-06,
      "loss": 0.2454,
      "step": 51540
    },
    {
      "epoch": 11.048006858122589,
      "grad_norm": 20.10054588317871,
      "learning_rate": 5.269324189169882e-06,
      "loss": 0.5087,
      "step": 51550
    },
    {
      "epoch": 11.050150021431634,
      "grad_norm": 2.8720386028289795,
      "learning_rate": 5.266466638091157e-06,
      "loss": 0.473,
      "step": 51560
    },
    {
      "epoch": 11.052293184740677,
      "grad_norm": 0.4266989529132843,
      "learning_rate": 5.263609087012431e-06,
      "loss": 0.1465,
      "step": 51570
    },
    {
      "epoch": 11.054436348049721,
      "grad_norm": 0.8644282221794128,
      "learning_rate": 5.260751535933706e-06,
      "loss": 0.7576,
      "step": 51580
    },
    {
      "epoch": 11.056579511358766,
      "grad_norm": 0.20503337681293488,
      "learning_rate": 5.257893984854979e-06,
      "loss": 0.8106,
      "step": 51590
    },
    {
      "epoch": 11.05872267466781,
      "grad_norm": 70.31414794921875,
      "learning_rate": 5.255036433776254e-06,
      "loss": 0.6818,
      "step": 51600
    },
    {
      "epoch": 11.060865837976854,
      "grad_norm": 0.20338493585586548,
      "learning_rate": 5.252178882697529e-06,
      "loss": 0.2321,
      "step": 51610
    },
    {
      "epoch": 11.063009001285899,
      "grad_norm": 0.37649106979370117,
      "learning_rate": 5.249321331618803e-06,
      "loss": 0.2392,
      "step": 51620
    },
    {
      "epoch": 11.065152164594942,
      "grad_norm": 0.3260091245174408,
      "learning_rate": 5.2464637805400775e-06,
      "loss": 0.4436,
      "step": 51630
    },
    {
      "epoch": 11.067295327903986,
      "grad_norm": 0.11416155099868774,
      "learning_rate": 5.243606229461352e-06,
      "loss": 0.4268,
      "step": 51640
    },
    {
      "epoch": 11.069438491213031,
      "grad_norm": 11.226609230041504,
      "learning_rate": 5.240748678382627e-06,
      "loss": 0.5499,
      "step": 51650
    },
    {
      "epoch": 11.071581654522074,
      "grad_norm": 16.834321975708008,
      "learning_rate": 5.237891127303901e-06,
      "loss": 0.6982,
      "step": 51660
    },
    {
      "epoch": 11.073724817831119,
      "grad_norm": 4.8434739112854,
      "learning_rate": 5.235033576225176e-06,
      "loss": 0.1173,
      "step": 51670
    },
    {
      "epoch": 11.075867981140163,
      "grad_norm": 0.1320183128118515,
      "learning_rate": 5.2321760251464505e-06,
      "loss": 0.4176,
      "step": 51680
    },
    {
      "epoch": 11.078011144449206,
      "grad_norm": 0.8710362315177917,
      "learning_rate": 5.229318474067724e-06,
      "loss": 0.2916,
      "step": 51690
    },
    {
      "epoch": 11.080154307758251,
      "grad_norm": 0.10783621668815613,
      "learning_rate": 5.226460922988998e-06,
      "loss": 0.2066,
      "step": 51700
    },
    {
      "epoch": 11.082297471067296,
      "grad_norm": 0.42624789476394653,
      "learning_rate": 5.223603371910273e-06,
      "loss": 0.7263,
      "step": 51710
    },
    {
      "epoch": 11.084440634376339,
      "grad_norm": 0.35356104373931885,
      "learning_rate": 5.220745820831548e-06,
      "loss": 0.6787,
      "step": 51720
    },
    {
      "epoch": 11.086583797685384,
      "grad_norm": 0.3843100368976593,
      "learning_rate": 5.217888269752822e-06,
      "loss": 0.6571,
      "step": 51730
    },
    {
      "epoch": 11.088726960994428,
      "grad_norm": 9.23711109161377,
      "learning_rate": 5.215030718674097e-06,
      "loss": 0.3653,
      "step": 51740
    },
    {
      "epoch": 11.090870124303471,
      "grad_norm": 0.08452976495027542,
      "learning_rate": 5.2121731675953714e-06,
      "loss": 0.128,
      "step": 51750
    },
    {
      "epoch": 11.093013287612516,
      "grad_norm": 0.5181097388267517,
      "learning_rate": 5.209315616516646e-06,
      "loss": 0.1367,
      "step": 51760
    },
    {
      "epoch": 11.09515645092156,
      "grad_norm": 35.32731246948242,
      "learning_rate": 5.20645806543792e-06,
      "loss": 0.5621,
      "step": 51770
    },
    {
      "epoch": 11.097299614230604,
      "grad_norm": 0.5835383534431458,
      "learning_rate": 5.203600514359195e-06,
      "loss": 0.2632,
      "step": 51780
    },
    {
      "epoch": 11.099442777539648,
      "grad_norm": 0.044241052120923996,
      "learning_rate": 5.20074296328047e-06,
      "loss": 0.0035,
      "step": 51790
    },
    {
      "epoch": 11.101585940848693,
      "grad_norm": 0.34421345591545105,
      "learning_rate": 5.197885412201743e-06,
      "loss": 0.9078,
      "step": 51800
    },
    {
      "epoch": 11.103729104157736,
      "grad_norm": 0.15106874704360962,
      "learning_rate": 5.1950278611230175e-06,
      "loss": 0.7662,
      "step": 51810
    },
    {
      "epoch": 11.105872267466781,
      "grad_norm": 0.4654647409915924,
      "learning_rate": 5.192170310044292e-06,
      "loss": 0.4057,
      "step": 51820
    },
    {
      "epoch": 11.108015430775826,
      "grad_norm": 0.2939540445804596,
      "learning_rate": 5.189312758965567e-06,
      "loss": 0.7447,
      "step": 51830
    },
    {
      "epoch": 11.110158594084869,
      "grad_norm": 0.36310675740242004,
      "learning_rate": 5.186455207886841e-06,
      "loss": 0.2911,
      "step": 51840
    },
    {
      "epoch": 11.112301757393913,
      "grad_norm": 0.3262637257575989,
      "learning_rate": 5.183597656808116e-06,
      "loss": 0.7571,
      "step": 51850
    },
    {
      "epoch": 11.114444920702958,
      "grad_norm": 24.189725875854492,
      "learning_rate": 5.1807401057293906e-06,
      "loss": 0.6707,
      "step": 51860
    },
    {
      "epoch": 11.116588084012001,
      "grad_norm": 1.3740864992141724,
      "learning_rate": 5.177882554650665e-06,
      "loss": 0.0907,
      "step": 51870
    },
    {
      "epoch": 11.118731247321046,
      "grad_norm": 18.534629821777344,
      "learning_rate": 5.175025003571939e-06,
      "loss": 0.7016,
      "step": 51880
    },
    {
      "epoch": 11.12087441063009,
      "grad_norm": 0.5461206436157227,
      "learning_rate": 5.172167452493214e-06,
      "loss": 0.9508,
      "step": 51890
    },
    {
      "epoch": 11.123017573939133,
      "grad_norm": 0.43278321623802185,
      "learning_rate": 5.169309901414489e-06,
      "loss": 0.2817,
      "step": 51900
    },
    {
      "epoch": 11.125160737248178,
      "grad_norm": 0.3842121362686157,
      "learning_rate": 5.166452350335762e-06,
      "loss": 0.4481,
      "step": 51910
    },
    {
      "epoch": 11.127303900557223,
      "grad_norm": 23.849878311157227,
      "learning_rate": 5.163594799257037e-06,
      "loss": 0.4648,
      "step": 51920
    },
    {
      "epoch": 11.129447063866266,
      "grad_norm": 4.6085944175720215,
      "learning_rate": 5.1607372481783115e-06,
      "loss": 0.794,
      "step": 51930
    },
    {
      "epoch": 11.13159022717531,
      "grad_norm": 18.725000381469727,
      "learning_rate": 5.157879697099586e-06,
      "loss": 0.4323,
      "step": 51940
    },
    {
      "epoch": 11.133733390484355,
      "grad_norm": 0.1071029007434845,
      "learning_rate": 5.15502214602086e-06,
      "loss": 0.3448,
      "step": 51950
    },
    {
      "epoch": 11.135876553793398,
      "grad_norm": 0.26307177543640137,
      "learning_rate": 5.152164594942135e-06,
      "loss": 0.5417,
      "step": 51960
    },
    {
      "epoch": 11.138019717102443,
      "grad_norm": 0.42495647072792053,
      "learning_rate": 5.14930704386341e-06,
      "loss": 0.302,
      "step": 51970
    },
    {
      "epoch": 11.140162880411488,
      "grad_norm": 26.00872802734375,
      "learning_rate": 5.1464494927846845e-06,
      "loss": 0.635,
      "step": 51980
    },
    {
      "epoch": 11.14230604372053,
      "grad_norm": 19.486995697021484,
      "learning_rate": 5.143591941705958e-06,
      "loss": 0.4875,
      "step": 51990
    },
    {
      "epoch": 11.144449207029576,
      "grad_norm": 23.477270126342773,
      "learning_rate": 5.140734390627233e-06,
      "loss": 0.1924,
      "step": 52000
    },
    {
      "epoch": 11.14659237033862,
      "grad_norm": 0.0945422351360321,
      "learning_rate": 5.137876839548507e-06,
      "loss": 0.3561,
      "step": 52010
    },
    {
      "epoch": 11.148735533647663,
      "grad_norm": 50.710060119628906,
      "learning_rate": 5.135019288469781e-06,
      "loss": 0.8781,
      "step": 52020
    },
    {
      "epoch": 11.150878696956708,
      "grad_norm": 17.55470085144043,
      "learning_rate": 5.132161737391056e-06,
      "loss": 0.3806,
      "step": 52030
    },
    {
      "epoch": 11.153021860265753,
      "grad_norm": 0.05886029452085495,
      "learning_rate": 5.129304186312331e-06,
      "loss": 0.1188,
      "step": 52040
    },
    {
      "epoch": 11.155165023574796,
      "grad_norm": 0.06474071741104126,
      "learning_rate": 5.126446635233605e-06,
      "loss": 0.3605,
      "step": 52050
    },
    {
      "epoch": 11.15730818688384,
      "grad_norm": 1.7728865146636963,
      "learning_rate": 5.123589084154879e-06,
      "loss": 0.4669,
      "step": 52060
    },
    {
      "epoch": 11.159451350192885,
      "grad_norm": 0.12151581794023514,
      "learning_rate": 5.120731533076154e-06,
      "loss": 0.3047,
      "step": 52070
    },
    {
      "epoch": 11.161594513501928,
      "grad_norm": 0.05787396430969238,
      "learning_rate": 5.117873981997429e-06,
      "loss": 0.4017,
      "step": 52080
    },
    {
      "epoch": 11.163737676810973,
      "grad_norm": 0.09129168838262558,
      "learning_rate": 5.115016430918704e-06,
      "loss": 1.1468,
      "step": 52090
    },
    {
      "epoch": 11.165880840120018,
      "grad_norm": 0.2072346806526184,
      "learning_rate": 5.1121588798399775e-06,
      "loss": 0.5587,
      "step": 52100
    },
    {
      "epoch": 11.16802400342906,
      "grad_norm": 0.4325050413608551,
      "learning_rate": 5.109301328761252e-06,
      "loss": 0.2901,
      "step": 52110
    },
    {
      "epoch": 11.170167166738105,
      "grad_norm": 27.941932678222656,
      "learning_rate": 5.106443777682526e-06,
      "loss": 0.2715,
      "step": 52120
    },
    {
      "epoch": 11.17231033004715,
      "grad_norm": 0.7729359269142151,
      "learning_rate": 5.1035862266038e-06,
      "loss": 0.458,
      "step": 52130
    },
    {
      "epoch": 11.174453493356193,
      "grad_norm": 0.12798820436000824,
      "learning_rate": 5.100728675525075e-06,
      "loss": 0.2729,
      "step": 52140
    },
    {
      "epoch": 11.176596656665238,
      "grad_norm": 1.3328239917755127,
      "learning_rate": 5.09787112444635e-06,
      "loss": 0.668,
      "step": 52150
    },
    {
      "epoch": 11.178739819974282,
      "grad_norm": 26.000186920166016,
      "learning_rate": 5.0950135733676245e-06,
      "loss": 0.6275,
      "step": 52160
    },
    {
      "epoch": 11.180882983283325,
      "grad_norm": 0.8301241397857666,
      "learning_rate": 5.0921560222888984e-06,
      "loss": 0.4211,
      "step": 52170
    },
    {
      "epoch": 11.18302614659237,
      "grad_norm": 22.561555862426758,
      "learning_rate": 5.089298471210173e-06,
      "loss": 0.281,
      "step": 52180
    },
    {
      "epoch": 11.185169309901415,
      "grad_norm": 0.13881255686283112,
      "learning_rate": 5.086440920131448e-06,
      "loss": 0.5136,
      "step": 52190
    },
    {
      "epoch": 11.187312473210458,
      "grad_norm": 0.3359568119049072,
      "learning_rate": 5.083583369052723e-06,
      "loss": 0.1089,
      "step": 52200
    },
    {
      "epoch": 11.189455636519503,
      "grad_norm": 0.4275173246860504,
      "learning_rate": 5.080725817973997e-06,
      "loss": 0.6384,
      "step": 52210
    },
    {
      "epoch": 11.191598799828547,
      "grad_norm": 0.19891144335269928,
      "learning_rate": 5.0778682668952715e-06,
      "loss": 0.3365,
      "step": 52220
    },
    {
      "epoch": 11.19374196313759,
      "grad_norm": 18.801645278930664,
      "learning_rate": 5.075010715816545e-06,
      "loss": 0.3434,
      "step": 52230
    },
    {
      "epoch": 11.195885126446635,
      "grad_norm": 0.3744833171367645,
      "learning_rate": 5.072153164737819e-06,
      "loss": 0.4397,
      "step": 52240
    },
    {
      "epoch": 11.19802828975568,
      "grad_norm": 0.3241428732872009,
      "learning_rate": 5.069295613659094e-06,
      "loss": 0.1859,
      "step": 52250
    },
    {
      "epoch": 11.200171453064723,
      "grad_norm": 0.4484424889087677,
      "learning_rate": 5.066438062580369e-06,
      "loss": 0.3938,
      "step": 52260
    },
    {
      "epoch": 11.202314616373767,
      "grad_norm": 0.1208222359418869,
      "learning_rate": 5.063580511501644e-06,
      "loss": 0.8657,
      "step": 52270
    },
    {
      "epoch": 11.204457779682812,
      "grad_norm": 33.59870529174805,
      "learning_rate": 5.0607229604229176e-06,
      "loss": 0.6509,
      "step": 52280
    },
    {
      "epoch": 11.206600942991855,
      "grad_norm": 0.13522300124168396,
      "learning_rate": 5.057865409344192e-06,
      "loss": 0.3151,
      "step": 52290
    },
    {
      "epoch": 11.2087441063009,
      "grad_norm": 66.20376586914062,
      "learning_rate": 5.055007858265467e-06,
      "loss": 0.4718,
      "step": 52300
    },
    {
      "epoch": 11.210887269609945,
      "grad_norm": 0.1431456059217453,
      "learning_rate": 5.052150307186742e-06,
      "loss": 0.3106,
      "step": 52310
    },
    {
      "epoch": 11.213030432918988,
      "grad_norm": 0.1841306984424591,
      "learning_rate": 5.049292756108017e-06,
      "loss": 0.8382,
      "step": 52320
    },
    {
      "epoch": 11.215173596228032,
      "grad_norm": 0.18704894185066223,
      "learning_rate": 5.046435205029291e-06,
      "loss": 0.39,
      "step": 52330
    },
    {
      "epoch": 11.217316759537077,
      "grad_norm": 0.1804528385400772,
      "learning_rate": 5.0435776539505645e-06,
      "loss": 0.3358,
      "step": 52340
    },
    {
      "epoch": 11.21945992284612,
      "grad_norm": 0.07539637386798859,
      "learning_rate": 5.0407201028718385e-06,
      "loss": 0.7402,
      "step": 52350
    },
    {
      "epoch": 11.221603086155165,
      "grad_norm": 21.32603645324707,
      "learning_rate": 5.037862551793113e-06,
      "loss": 0.55,
      "step": 52360
    },
    {
      "epoch": 11.22374624946421,
      "grad_norm": 21.720603942871094,
      "learning_rate": 5.035005000714388e-06,
      "loss": 0.2478,
      "step": 52370
    },
    {
      "epoch": 11.225889412773252,
      "grad_norm": 20.183589935302734,
      "learning_rate": 5.032147449635663e-06,
      "loss": 0.9714,
      "step": 52380
    },
    {
      "epoch": 11.228032576082297,
      "grad_norm": 0.12743528187274933,
      "learning_rate": 5.0292898985569376e-06,
      "loss": 0.4952,
      "step": 52390
    },
    {
      "epoch": 11.230175739391342,
      "grad_norm": 300.1671447753906,
      "learning_rate": 5.0264323474782115e-06,
      "loss": 1.0464,
      "step": 52400
    },
    {
      "epoch": 11.232318902700385,
      "grad_norm": 20.421438217163086,
      "learning_rate": 5.023574796399486e-06,
      "loss": 0.151,
      "step": 52410
    },
    {
      "epoch": 11.23446206600943,
      "grad_norm": 20.107892990112305,
      "learning_rate": 5.020717245320761e-06,
      "loss": 0.5465,
      "step": 52420
    },
    {
      "epoch": 11.236605229318474,
      "grad_norm": 0.35401326417922974,
      "learning_rate": 5.017859694242036e-06,
      "loss": 0.6618,
      "step": 52430
    },
    {
      "epoch": 11.238748392627517,
      "grad_norm": 29.858631134033203,
      "learning_rate": 5.015002143163309e-06,
      "loss": 0.3848,
      "step": 52440
    },
    {
      "epoch": 11.240891555936562,
      "grad_norm": 0.2365977019071579,
      "learning_rate": 5.012144592084584e-06,
      "loss": 0.5662,
      "step": 52450
    },
    {
      "epoch": 11.243034719245607,
      "grad_norm": 0.6301578879356384,
      "learning_rate": 5.0092870410058584e-06,
      "loss": 0.4487,
      "step": 52460
    },
    {
      "epoch": 11.24517788255465,
      "grad_norm": 0.0352400504052639,
      "learning_rate": 5.006429489927132e-06,
      "loss": 0.0583,
      "step": 52470
    },
    {
      "epoch": 11.247321045863695,
      "grad_norm": 27.523784637451172,
      "learning_rate": 5.003571938848407e-06,
      "loss": 0.3589,
      "step": 52480
    },
    {
      "epoch": 11.24946420917274,
      "grad_norm": 0.046885304152965546,
      "learning_rate": 5.000714387769682e-06,
      "loss": 0.1577,
      "step": 52490
    },
    {
      "epoch": 11.251607372481782,
      "grad_norm": 0.19470803439617157,
      "learning_rate": 4.997856836690957e-06,
      "loss": 0.5697,
      "step": 52500
    },
    {
      "epoch": 11.253750535790827,
      "grad_norm": 1.2868340015411377,
      "learning_rate": 4.994999285612231e-06,
      "loss": 0.4032,
      "step": 52510
    },
    {
      "epoch": 11.255893699099872,
      "grad_norm": 30.8380126953125,
      "learning_rate": 4.9921417345335046e-06,
      "loss": 0.4851,
      "step": 52520
    },
    {
      "epoch": 11.258036862408915,
      "grad_norm": 0.3955531716346741,
      "learning_rate": 4.989284183454779e-06,
      "loss": 0.522,
      "step": 52530
    },
    {
      "epoch": 11.26018002571796,
      "grad_norm": 0.5110606551170349,
      "learning_rate": 4.986426632376054e-06,
      "loss": 0.5414,
      "step": 52540
    },
    {
      "epoch": 11.262323189027004,
      "grad_norm": 0.4073362648487091,
      "learning_rate": 4.983569081297329e-06,
      "loss": 0.8011,
      "step": 52550
    },
    {
      "epoch": 11.264466352336047,
      "grad_norm": 18.30093765258789,
      "learning_rate": 4.980711530218603e-06,
      "loss": 0.4902,
      "step": 52560
    },
    {
      "epoch": 11.266609515645092,
      "grad_norm": 36.61170196533203,
      "learning_rate": 4.977853979139878e-06,
      "loss": 0.8432,
      "step": 52570
    },
    {
      "epoch": 11.268752678954137,
      "grad_norm": 0.41876548528671265,
      "learning_rate": 4.9749964280611515e-06,
      "loss": 0.2189,
      "step": 52580
    },
    {
      "epoch": 11.270895842263181,
      "grad_norm": 0.14719153940677643,
      "learning_rate": 4.972138876982426e-06,
      "loss": 0.5417,
      "step": 52590
    },
    {
      "epoch": 11.273039005572224,
      "grad_norm": 8.938794136047363,
      "learning_rate": 4.969281325903701e-06,
      "loss": 0.0097,
      "step": 52600
    },
    {
      "epoch": 11.275182168881269,
      "grad_norm": 4.2054853439331055,
      "learning_rate": 4.966423774824976e-06,
      "loss": 0.5933,
      "step": 52610
    },
    {
      "epoch": 11.277325332190314,
      "grad_norm": 0.05224327743053436,
      "learning_rate": 4.96356622374625e-06,
      "loss": 0.1419,
      "step": 52620
    },
    {
      "epoch": 11.279468495499357,
      "grad_norm": 0.22536177933216095,
      "learning_rate": 4.960708672667524e-06,
      "loss": 0.1737,
      "step": 52630
    },
    {
      "epoch": 11.281611658808401,
      "grad_norm": 0.029415922239422798,
      "learning_rate": 4.9578511215887985e-06,
      "loss": 0.7262,
      "step": 52640
    },
    {
      "epoch": 11.283754822117446,
      "grad_norm": 717.7014770507812,
      "learning_rate": 4.954993570510073e-06,
      "loss": 0.7034,
      "step": 52650
    },
    {
      "epoch": 11.28589798542649,
      "grad_norm": 27.58470916748047,
      "learning_rate": 4.952136019431348e-06,
      "loss": 0.5399,
      "step": 52660
    },
    {
      "epoch": 11.288041148735534,
      "grad_norm": 0.09843718260526657,
      "learning_rate": 4.949278468352623e-06,
      "loss": 0.1172,
      "step": 52670
    },
    {
      "epoch": 11.290184312044579,
      "grad_norm": 49.34559631347656,
      "learning_rate": 4.946420917273897e-06,
      "loss": 0.3547,
      "step": 52680
    },
    {
      "epoch": 11.292327475353622,
      "grad_norm": 0.358940452337265,
      "learning_rate": 4.943563366195171e-06,
      "loss": 0.6552,
      "step": 52690
    },
    {
      "epoch": 11.294470638662666,
      "grad_norm": 0.3249145746231079,
      "learning_rate": 4.9407058151164454e-06,
      "loss": 0.0074,
      "step": 52700
    },
    {
      "epoch": 11.296613801971711,
      "grad_norm": 40.396278381347656,
      "learning_rate": 4.93784826403772e-06,
      "loss": 0.719,
      "step": 52710
    },
    {
      "epoch": 11.298756965280754,
      "grad_norm": 0.052570704370737076,
      "learning_rate": 4.934990712958995e-06,
      "loss": 0.5224,
      "step": 52720
    },
    {
      "epoch": 11.300900128589799,
      "grad_norm": 41.09617614746094,
      "learning_rate": 4.932133161880269e-06,
      "loss": 1.1357,
      "step": 52730
    },
    {
      "epoch": 11.303043291898843,
      "grad_norm": 19.603797912597656,
      "learning_rate": 4.929275610801544e-06,
      "loss": 0.5754,
      "step": 52740
    },
    {
      "epoch": 11.305186455207886,
      "grad_norm": 19.247880935668945,
      "learning_rate": 4.926418059722818e-06,
      "loss": 0.2615,
      "step": 52750
    },
    {
      "epoch": 11.307329618516931,
      "grad_norm": 106.02925872802734,
      "learning_rate": 4.923560508644092e-06,
      "loss": 0.9003,
      "step": 52760
    },
    {
      "epoch": 11.309472781825976,
      "grad_norm": 0.8113073706626892,
      "learning_rate": 4.920702957565367e-06,
      "loss": 0.5546,
      "step": 52770
    },
    {
      "epoch": 11.311615945135019,
      "grad_norm": 0.4842173755168915,
      "learning_rate": 4.917845406486642e-06,
      "loss": 0.3225,
      "step": 52780
    },
    {
      "epoch": 11.313759108444064,
      "grad_norm": 22.19474983215332,
      "learning_rate": 4.914987855407916e-06,
      "loss": 0.2162,
      "step": 52790
    },
    {
      "epoch": 11.315902271753108,
      "grad_norm": 23.582260131835938,
      "learning_rate": 4.91213030432919e-06,
      "loss": 0.7214,
      "step": 52800
    },
    {
      "epoch": 11.318045435062151,
      "grad_norm": 0.46035706996917725,
      "learning_rate": 4.9092727532504646e-06,
      "loss": 0.2499,
      "step": 52810
    },
    {
      "epoch": 11.320188598371196,
      "grad_norm": 0.2526051104068756,
      "learning_rate": 4.906415202171739e-06,
      "loss": 0.4257,
      "step": 52820
    },
    {
      "epoch": 11.32233176168024,
      "grad_norm": 18.564289093017578,
      "learning_rate": 4.903557651093014e-06,
      "loss": 0.1156,
      "step": 52830
    },
    {
      "epoch": 11.324474924989284,
      "grad_norm": 147.68878173828125,
      "learning_rate": 4.900700100014288e-06,
      "loss": 0.7482,
      "step": 52840
    },
    {
      "epoch": 11.326618088298329,
      "grad_norm": 0.15726900100708008,
      "learning_rate": 4.897842548935563e-06,
      "loss": 0.0055,
      "step": 52850
    },
    {
      "epoch": 11.328761251607373,
      "grad_norm": 0.07187232375144958,
      "learning_rate": 4.894984997856837e-06,
      "loss": 0.4913,
      "step": 52860
    },
    {
      "epoch": 11.330904414916416,
      "grad_norm": 29.52058219909668,
      "learning_rate": 4.8921274467781115e-06,
      "loss": 0.5183,
      "step": 52870
    },
    {
      "epoch": 11.333047578225461,
      "grad_norm": 0.1597542017698288,
      "learning_rate": 4.889269895699386e-06,
      "loss": 0.5521,
      "step": 52880
    },
    {
      "epoch": 11.335190741534506,
      "grad_norm": 116.60428619384766,
      "learning_rate": 4.886412344620661e-06,
      "loss": 0.5091,
      "step": 52890
    },
    {
      "epoch": 11.337333904843549,
      "grad_norm": 0.5452592372894287,
      "learning_rate": 4.883554793541935e-06,
      "loss": 0.3523,
      "step": 52900
    },
    {
      "epoch": 11.339477068152593,
      "grad_norm": 0.09475979954004288,
      "learning_rate": 4.880697242463209e-06,
      "loss": 0.4135,
      "step": 52910
    },
    {
      "epoch": 11.341620231461638,
      "grad_norm": 24.07926368713379,
      "learning_rate": 4.877839691384484e-06,
      "loss": 0.7197,
      "step": 52920
    },
    {
      "epoch": 11.343763394770681,
      "grad_norm": 57.62022399902344,
      "learning_rate": 4.8749821403057585e-06,
      "loss": 0.5978,
      "step": 52930
    },
    {
      "epoch": 11.345906558079726,
      "grad_norm": 19.96505355834961,
      "learning_rate": 4.872124589227033e-06,
      "loss": 0.4838,
      "step": 52940
    },
    {
      "epoch": 11.34804972138877,
      "grad_norm": 0.20547261834144592,
      "learning_rate": 4.869267038148307e-06,
      "loss": 0.358,
      "step": 52950
    },
    {
      "epoch": 11.350192884697814,
      "grad_norm": 1.550036072731018,
      "learning_rate": 4.866409487069582e-06,
      "loss": 0.3182,
      "step": 52960
    },
    {
      "epoch": 11.352336048006858,
      "grad_norm": 0.5157474875450134,
      "learning_rate": 4.863551935990856e-06,
      "loss": 0.2285,
      "step": 52970
    },
    {
      "epoch": 11.354479211315903,
      "grad_norm": 0.41860082745552063,
      "learning_rate": 4.860694384912131e-06,
      "loss": 0.7631,
      "step": 52980
    },
    {
      "epoch": 11.356622374624946,
      "grad_norm": 59.41950607299805,
      "learning_rate": 4.8578368338334054e-06,
      "loss": 0.2848,
      "step": 52990
    },
    {
      "epoch": 11.35876553793399,
      "grad_norm": 0.029724769294261932,
      "learning_rate": 4.85497928275468e-06,
      "loss": 0.3603,
      "step": 53000
    },
    {
      "epoch": 11.360908701243035,
      "grad_norm": 21.56494903564453,
      "learning_rate": 4.852121731675954e-06,
      "loss": 0.8184,
      "step": 53010
    },
    {
      "epoch": 11.363051864552078,
      "grad_norm": 0.2621777355670929,
      "learning_rate": 4.849264180597228e-06,
      "loss": 0.3581,
      "step": 53020
    },
    {
      "epoch": 11.365195027861123,
      "grad_norm": 21.78750228881836,
      "learning_rate": 4.846406629518503e-06,
      "loss": 0.9629,
      "step": 53030
    },
    {
      "epoch": 11.367338191170168,
      "grad_norm": 24.507905960083008,
      "learning_rate": 4.843549078439778e-06,
      "loss": 0.5603,
      "step": 53040
    },
    {
      "epoch": 11.36948135447921,
      "grad_norm": 0.1614067107439041,
      "learning_rate": 4.840691527361052e-06,
      "loss": 0.1767,
      "step": 53050
    },
    {
      "epoch": 11.371624517788256,
      "grad_norm": 26.32438087463379,
      "learning_rate": 4.837833976282326e-06,
      "loss": 0.4915,
      "step": 53060
    },
    {
      "epoch": 11.3737676810973,
      "grad_norm": 0.08702412247657776,
      "learning_rate": 4.834976425203601e-06,
      "loss": 0.1654,
      "step": 53070
    },
    {
      "epoch": 11.375910844406343,
      "grad_norm": 0.13046422600746155,
      "learning_rate": 4.832118874124875e-06,
      "loss": 0.3641,
      "step": 53080
    },
    {
      "epoch": 11.378054007715388,
      "grad_norm": 0.07591903954744339,
      "learning_rate": 4.82926132304615e-06,
      "loss": 0.7873,
      "step": 53090
    },
    {
      "epoch": 11.380197171024433,
      "grad_norm": 0.7493208646774292,
      "learning_rate": 4.826403771967425e-06,
      "loss": 0.3399,
      "step": 53100
    },
    {
      "epoch": 11.382340334333476,
      "grad_norm": 20.310829162597656,
      "learning_rate": 4.8235462208886985e-06,
      "loss": 0.712,
      "step": 53110
    },
    {
      "epoch": 11.38448349764252,
      "grad_norm": 0.2517077326774597,
      "learning_rate": 4.820688669809973e-06,
      "loss": 0.4253,
      "step": 53120
    },
    {
      "epoch": 11.386626660951565,
      "grad_norm": 33.084285736083984,
      "learning_rate": 4.817831118731248e-06,
      "loss": 0.6586,
      "step": 53130
    },
    {
      "epoch": 11.388769824260608,
      "grad_norm": 36.93246841430664,
      "learning_rate": 4.814973567652522e-06,
      "loss": 1.1579,
      "step": 53140
    },
    {
      "epoch": 11.390912987569653,
      "grad_norm": 22.870433807373047,
      "learning_rate": 4.812116016573797e-06,
      "loss": 0.3112,
      "step": 53150
    },
    {
      "epoch": 11.393056150878698,
      "grad_norm": 0.6144183278083801,
      "learning_rate": 4.8092584654950715e-06,
      "loss": 0.4855,
      "step": 53160
    },
    {
      "epoch": 11.39519931418774,
      "grad_norm": 0.8261604905128479,
      "learning_rate": 4.8064009144163455e-06,
      "loss": 0.366,
      "step": 53170
    },
    {
      "epoch": 11.397342477496785,
      "grad_norm": 0.06145571917295456,
      "learning_rate": 4.80354336333762e-06,
      "loss": 0.4812,
      "step": 53180
    },
    {
      "epoch": 11.39948564080583,
      "grad_norm": 0.15581096708774567,
      "learning_rate": 4.800685812258894e-06,
      "loss": 0.3788,
      "step": 53190
    },
    {
      "epoch": 11.401628804114873,
      "grad_norm": 0.6966871023178101,
      "learning_rate": 4.797828261180169e-06,
      "loss": 0.1851,
      "step": 53200
    },
    {
      "epoch": 11.403771967423918,
      "grad_norm": 0.08590357005596161,
      "learning_rate": 4.794970710101444e-06,
      "loss": 0.0054,
      "step": 53210
    },
    {
      "epoch": 11.405915130732962,
      "grad_norm": 0.039207205176353455,
      "learning_rate": 4.792113159022718e-06,
      "loss": 0.3796,
      "step": 53220
    },
    {
      "epoch": 11.408058294042005,
      "grad_norm": 0.061460547149181366,
      "learning_rate": 4.7892556079439924e-06,
      "loss": 0.9623,
      "step": 53230
    },
    {
      "epoch": 11.41020145735105,
      "grad_norm": 0.05998879298567772,
      "learning_rate": 4.786398056865267e-06,
      "loss": 0.6506,
      "step": 53240
    },
    {
      "epoch": 11.412344620660095,
      "grad_norm": 1.2616971731185913,
      "learning_rate": 4.783540505786541e-06,
      "loss": 0.216,
      "step": 53250
    },
    {
      "epoch": 11.414487783969138,
      "grad_norm": 0.9234285354614258,
      "learning_rate": 4.780682954707816e-06,
      "loss": 0.5424,
      "step": 53260
    },
    {
      "epoch": 11.416630947278183,
      "grad_norm": 0.07202791422605515,
      "learning_rate": 4.777825403629091e-06,
      "loss": 0.3015,
      "step": 53270
    },
    {
      "epoch": 11.418774110587227,
      "grad_norm": 0.21053439378738403,
      "learning_rate": 4.774967852550365e-06,
      "loss": 0.5529,
      "step": 53280
    },
    {
      "epoch": 11.42091727389627,
      "grad_norm": 17.882272720336914,
      "learning_rate": 4.772110301471639e-06,
      "loss": 0.4148,
      "step": 53290
    },
    {
      "epoch": 11.423060437205315,
      "grad_norm": 19.35452651977539,
      "learning_rate": 4.769252750392913e-06,
      "loss": 0.7779,
      "step": 53300
    },
    {
      "epoch": 11.42520360051436,
      "grad_norm": 0.2137116640806198,
      "learning_rate": 4.766395199314188e-06,
      "loss": 0.3159,
      "step": 53310
    },
    {
      "epoch": 11.427346763823403,
      "grad_norm": 80.96038818359375,
      "learning_rate": 4.763537648235463e-06,
      "loss": 0.1082,
      "step": 53320
    },
    {
      "epoch": 11.429489927132447,
      "grad_norm": 0.18405170738697052,
      "learning_rate": 4.760680097156737e-06,
      "loss": 0.4579,
      "step": 53330
    },
    {
      "epoch": 11.431633090441492,
      "grad_norm": 27.686979293823242,
      "learning_rate": 4.7578225460780116e-06,
      "loss": 0.9507,
      "step": 53340
    },
    {
      "epoch": 11.433776253750535,
      "grad_norm": 0.22575496137142181,
      "learning_rate": 4.754964994999286e-06,
      "loss": 0.5254,
      "step": 53350
    },
    {
      "epoch": 11.43591941705958,
      "grad_norm": 36.491085052490234,
      "learning_rate": 4.75210744392056e-06,
      "loss": 0.1467,
      "step": 53360
    },
    {
      "epoch": 11.438062580368625,
      "grad_norm": 0.5381525754928589,
      "learning_rate": 4.749249892841835e-06,
      "loss": 0.6455,
      "step": 53370
    },
    {
      "epoch": 11.440205743677668,
      "grad_norm": 0.7325843572616577,
      "learning_rate": 4.746392341763109e-06,
      "loss": 0.8094,
      "step": 53380
    },
    {
      "epoch": 11.442348906986712,
      "grad_norm": 23.953596115112305,
      "learning_rate": 4.743534790684384e-06,
      "loss": 0.6689,
      "step": 53390
    },
    {
      "epoch": 11.444492070295757,
      "grad_norm": 0.7414406538009644,
      "learning_rate": 4.7406772396056585e-06,
      "loss": 0.3652,
      "step": 53400
    },
    {
      "epoch": 11.4466352336048,
      "grad_norm": 0.1885322779417038,
      "learning_rate": 4.7378196885269325e-06,
      "loss": 0.3007,
      "step": 53410
    },
    {
      "epoch": 11.448778396913845,
      "grad_norm": 3.3909261226654053,
      "learning_rate": 4.734962137448207e-06,
      "loss": 0.6584,
      "step": 53420
    },
    {
      "epoch": 11.45092156022289,
      "grad_norm": 0.28295838832855225,
      "learning_rate": 4.732104586369482e-06,
      "loss": 0.4916,
      "step": 53430
    },
    {
      "epoch": 11.453064723531933,
      "grad_norm": 0.15346841514110565,
      "learning_rate": 4.729247035290756e-06,
      "loss": 0.5133,
      "step": 53440
    },
    {
      "epoch": 11.455207886840977,
      "grad_norm": 0.895138680934906,
      "learning_rate": 4.726389484212031e-06,
      "loss": 0.2485,
      "step": 53450
    },
    {
      "epoch": 11.457351050150022,
      "grad_norm": 0.4344337582588196,
      "learning_rate": 4.7235319331333055e-06,
      "loss": 0.3568,
      "step": 53460
    },
    {
      "epoch": 11.459494213459065,
      "grad_norm": 0.18841663002967834,
      "learning_rate": 4.720674382054579e-06,
      "loss": 0.5323,
      "step": 53470
    },
    {
      "epoch": 11.46163737676811,
      "grad_norm": 21.142108917236328,
      "learning_rate": 4.717816830975854e-06,
      "loss": 0.829,
      "step": 53480
    },
    {
      "epoch": 11.463780540077154,
      "grad_norm": 96.31136322021484,
      "learning_rate": 4.714959279897128e-06,
      "loss": 0.3341,
      "step": 53490
    },
    {
      "epoch": 11.465923703386197,
      "grad_norm": 30.672258377075195,
      "learning_rate": 4.712101728818403e-06,
      "loss": 0.4552,
      "step": 53500
    },
    {
      "epoch": 11.468066866695242,
      "grad_norm": 0.4701205790042877,
      "learning_rate": 4.709244177739678e-06,
      "loss": 0.0069,
      "step": 53510
    },
    {
      "epoch": 11.470210030004287,
      "grad_norm": 0.06892849504947662,
      "learning_rate": 4.7063866266609524e-06,
      "loss": 0.3817,
      "step": 53520
    },
    {
      "epoch": 11.47235319331333,
      "grad_norm": 0.28190040588378906,
      "learning_rate": 4.703529075582226e-06,
      "loss": 0.8877,
      "step": 53530
    },
    {
      "epoch": 11.474496356622375,
      "grad_norm": 18.324769973754883,
      "learning_rate": 4.7006715245035e-06,
      "loss": 1.1523,
      "step": 53540
    },
    {
      "epoch": 11.47663951993142,
      "grad_norm": 0.601471483707428,
      "learning_rate": 4.697813973424775e-06,
      "loss": 0.0849,
      "step": 53550
    },
    {
      "epoch": 11.478782683240462,
      "grad_norm": 21.45638084411621,
      "learning_rate": 4.69495642234605e-06,
      "loss": 0.2941,
      "step": 53560
    },
    {
      "epoch": 11.480925846549507,
      "grad_norm": 34.93910598754883,
      "learning_rate": 4.692098871267325e-06,
      "loss": 0.1742,
      "step": 53570
    },
    {
      "epoch": 11.483069009858552,
      "grad_norm": 1.2871121168136597,
      "learning_rate": 4.6892413201885986e-06,
      "loss": 0.7495,
      "step": 53580
    },
    {
      "epoch": 11.485212173167595,
      "grad_norm": 0.22663205862045288,
      "learning_rate": 4.686383769109873e-06,
      "loss": 0.121,
      "step": 53590
    },
    {
      "epoch": 11.48735533647664,
      "grad_norm": 0.13899332284927368,
      "learning_rate": 4.683526218031147e-06,
      "loss": 0.825,
      "step": 53600
    },
    {
      "epoch": 11.489498499785684,
      "grad_norm": 0.19924092292785645,
      "learning_rate": 4.680668666952422e-06,
      "loss": 0.359,
      "step": 53610
    },
    {
      "epoch": 11.491641663094727,
      "grad_norm": 0.4414149820804596,
      "learning_rate": 4.677811115873697e-06,
      "loss": 0.401,
      "step": 53620
    },
    {
      "epoch": 11.493784826403772,
      "grad_norm": 0.5462128520011902,
      "learning_rate": 4.674953564794972e-06,
      "loss": 0.627,
      "step": 53630
    },
    {
      "epoch": 11.495927989712817,
      "grad_norm": 25.420814514160156,
      "learning_rate": 4.6720960137162455e-06,
      "loss": 0.8383,
      "step": 53640
    },
    {
      "epoch": 11.49807115302186,
      "grad_norm": 0.4317087233066559,
      "learning_rate": 4.6692384626375194e-06,
      "loss": 0.7174,
      "step": 53650
    },
    {
      "epoch": 11.500214316330904,
      "grad_norm": 0.5142440795898438,
      "learning_rate": 4.666380911558794e-06,
      "loss": 0.6,
      "step": 53660
    },
    {
      "epoch": 11.502357479639949,
      "grad_norm": 0.1014043316245079,
      "learning_rate": 4.663523360480069e-06,
      "loss": 0.2792,
      "step": 53670
    },
    {
      "epoch": 11.504500642948992,
      "grad_norm": 0.03720409795641899,
      "learning_rate": 4.660665809401344e-06,
      "loss": 0.6745,
      "step": 53680
    },
    {
      "epoch": 11.506643806258037,
      "grad_norm": 0.3000345528125763,
      "learning_rate": 4.657808258322618e-06,
      "loss": 0.4991,
      "step": 53690
    },
    {
      "epoch": 11.508786969567081,
      "grad_norm": 0.2707309424877167,
      "learning_rate": 4.6549507072438925e-06,
      "loss": 0.4101,
      "step": 53700
    },
    {
      "epoch": 11.510930132876124,
      "grad_norm": 0.1360214501619339,
      "learning_rate": 4.652093156165166e-06,
      "loss": 0.7152,
      "step": 53710
    },
    {
      "epoch": 11.51307329618517,
      "grad_norm": 24.359851837158203,
      "learning_rate": 4.649235605086441e-06,
      "loss": 0.8669,
      "step": 53720
    },
    {
      "epoch": 11.515216459494214,
      "grad_norm": 18.371971130371094,
      "learning_rate": 4.646378054007716e-06,
      "loss": 0.4226,
      "step": 53730
    },
    {
      "epoch": 11.517359622803257,
      "grad_norm": 0.5142045021057129,
      "learning_rate": 4.643520502928991e-06,
      "loss": 0.7925,
      "step": 53740
    },
    {
      "epoch": 11.519502786112302,
      "grad_norm": 2.284029722213745,
      "learning_rate": 4.640662951850265e-06,
      "loss": 0.5916,
      "step": 53750
    },
    {
      "epoch": 11.521645949421346,
      "grad_norm": 19.56418228149414,
      "learning_rate": 4.637805400771539e-06,
      "loss": 0.1538,
      "step": 53760
    },
    {
      "epoch": 11.52378911273039,
      "grad_norm": 0.19393311440944672,
      "learning_rate": 4.634947849692813e-06,
      "loss": 0.301,
      "step": 53770
    },
    {
      "epoch": 11.525932276039434,
      "grad_norm": 27.148630142211914,
      "learning_rate": 4.632090298614088e-06,
      "loss": 0.3906,
      "step": 53780
    },
    {
      "epoch": 11.528075439348479,
      "grad_norm": 0.7923312187194824,
      "learning_rate": 4.629232747535363e-06,
      "loss": 0.1965,
      "step": 53790
    },
    {
      "epoch": 11.530218602657522,
      "grad_norm": 0.38735270500183105,
      "learning_rate": 4.626375196456637e-06,
      "loss": 0.3429,
      "step": 53800
    },
    {
      "epoch": 11.532361765966566,
      "grad_norm": 0.46255984902381897,
      "learning_rate": 4.623517645377912e-06,
      "loss": 0.129,
      "step": 53810
    },
    {
      "epoch": 11.534504929275611,
      "grad_norm": 0.4387436509132385,
      "learning_rate": 4.6206600942991855e-06,
      "loss": 0.4255,
      "step": 53820
    },
    {
      "epoch": 11.536648092584654,
      "grad_norm": 0.15636244416236877,
      "learning_rate": 4.61780254322046e-06,
      "loss": 0.7973,
      "step": 53830
    },
    {
      "epoch": 11.538791255893699,
      "grad_norm": 0.2761574983596802,
      "learning_rate": 4.614944992141735e-06,
      "loss": 0.1586,
      "step": 53840
    },
    {
      "epoch": 11.540934419202744,
      "grad_norm": 0.031184250488877296,
      "learning_rate": 4.61208744106301e-06,
      "loss": 0.0024,
      "step": 53850
    },
    {
      "epoch": 11.543077582511787,
      "grad_norm": 0.0175987146794796,
      "learning_rate": 4.609229889984284e-06,
      "loss": 0.335,
      "step": 53860
    },
    {
      "epoch": 11.545220745820831,
      "grad_norm": 0.13035979866981506,
      "learning_rate": 4.606372338905558e-06,
      "loss": 0.179,
      "step": 53870
    },
    {
      "epoch": 11.547363909129876,
      "grad_norm": 0.46265530586242676,
      "learning_rate": 4.6035147878268325e-06,
      "loss": 0.0025,
      "step": 53880
    },
    {
      "epoch": 11.549507072438919,
      "grad_norm": 0.06287375837564468,
      "learning_rate": 4.600657236748107e-06,
      "loss": 0.0024,
      "step": 53890
    },
    {
      "epoch": 11.551650235747964,
      "grad_norm": 23.025142669677734,
      "learning_rate": 4.597799685669382e-06,
      "loss": 1.0074,
      "step": 53900
    },
    {
      "epoch": 11.553793399057009,
      "grad_norm": 0.11163330078125,
      "learning_rate": 4.594942134590657e-06,
      "loss": 0.6725,
      "step": 53910
    },
    {
      "epoch": 11.555936562366051,
      "grad_norm": 0.40119069814682007,
      "learning_rate": 4.592084583511931e-06,
      "loss": 0.3275,
      "step": 53920
    },
    {
      "epoch": 11.558079725675096,
      "grad_norm": 25.90144157409668,
      "learning_rate": 4.589227032433205e-06,
      "loss": 1.0833,
      "step": 53930
    },
    {
      "epoch": 11.560222888984141,
      "grad_norm": 1.1082096099853516,
      "learning_rate": 4.5863694813544795e-06,
      "loss": 0.4933,
      "step": 53940
    },
    {
      "epoch": 11.562366052293184,
      "grad_norm": 0.09122768044471741,
      "learning_rate": 4.583511930275754e-06,
      "loss": 0.1595,
      "step": 53950
    },
    {
      "epoch": 11.564509215602229,
      "grad_norm": 0.2322230041027069,
      "learning_rate": 4.580654379197029e-06,
      "loss": 0.4275,
      "step": 53960
    },
    {
      "epoch": 11.566652378911273,
      "grad_norm": 0.3345905840396881,
      "learning_rate": 4.577796828118303e-06,
      "loss": 0.3996,
      "step": 53970
    },
    {
      "epoch": 11.568795542220316,
      "grad_norm": 17.00615692138672,
      "learning_rate": 4.574939277039578e-06,
      "loss": 0.7103,
      "step": 53980
    },
    {
      "epoch": 11.570938705529361,
      "grad_norm": 0.0798906609416008,
      "learning_rate": 4.572081725960852e-06,
      "loss": 0.5138,
      "step": 53990
    },
    {
      "epoch": 11.573081868838406,
      "grad_norm": 36.57887268066406,
      "learning_rate": 4.569224174882126e-06,
      "loss": 0.9518,
      "step": 54000
    },
    {
      "epoch": 11.57522503214745,
      "grad_norm": 0.49001839756965637,
      "learning_rate": 4.566366623803401e-06,
      "loss": 0.7669,
      "step": 54010
    },
    {
      "epoch": 11.577368195456494,
      "grad_norm": 22.537193298339844,
      "learning_rate": 4.563509072724676e-06,
      "loss": 0.4442,
      "step": 54020
    },
    {
      "epoch": 11.579511358765538,
      "grad_norm": 453.6305236816406,
      "learning_rate": 4.56065152164595e-06,
      "loss": 0.2382,
      "step": 54030
    },
    {
      "epoch": 11.581654522074583,
      "grad_norm": 1.7521531581878662,
      "learning_rate": 4.557793970567224e-06,
      "loss": 1.044,
      "step": 54040
    },
    {
      "epoch": 11.583797685383626,
      "grad_norm": 18.172048568725586,
      "learning_rate": 4.554936419488499e-06,
      "loss": 0.4215,
      "step": 54050
    },
    {
      "epoch": 11.58594084869267,
      "grad_norm": 29.136825561523438,
      "learning_rate": 4.552078868409773e-06,
      "loss": 0.8398,
      "step": 54060
    },
    {
      "epoch": 11.588084012001715,
      "grad_norm": 18.343219757080078,
      "learning_rate": 4.549221317331048e-06,
      "loss": 0.5876,
      "step": 54070
    },
    {
      "epoch": 11.590227175310758,
      "grad_norm": 17.654258728027344,
      "learning_rate": 4.546363766252322e-06,
      "loss": 0.6724,
      "step": 54080
    },
    {
      "epoch": 11.592370338619803,
      "grad_norm": 0.28876838088035583,
      "learning_rate": 4.543506215173597e-06,
      "loss": 0.7163,
      "step": 54090
    },
    {
      "epoch": 11.594513501928848,
      "grad_norm": 0.5903988480567932,
      "learning_rate": 4.540648664094871e-06,
      "loss": 0.4312,
      "step": 54100
    },
    {
      "epoch": 11.59665666523789,
      "grad_norm": 0.05939582362771034,
      "learning_rate": 4.5377911130161456e-06,
      "loss": 0.7016,
      "step": 54110
    },
    {
      "epoch": 11.598799828546936,
      "grad_norm": 0.13444529473781586,
      "learning_rate": 4.53493356193742e-06,
      "loss": 0.2981,
      "step": 54120
    },
    {
      "epoch": 11.60094299185598,
      "grad_norm": 24.88164520263672,
      "learning_rate": 4.532076010858694e-06,
      "loss": 0.5751,
      "step": 54130
    },
    {
      "epoch": 11.603086155165023,
      "grad_norm": 20.208145141601562,
      "learning_rate": 4.529218459779969e-06,
      "loss": 0.6452,
      "step": 54140
    },
    {
      "epoch": 11.605229318474068,
      "grad_norm": 0.40746739506721497,
      "learning_rate": 4.526360908701243e-06,
      "loss": 0.6174,
      "step": 54150
    },
    {
      "epoch": 11.607372481783113,
      "grad_norm": 0.5542211532592773,
      "learning_rate": 4.523503357622518e-06,
      "loss": 0.1046,
      "step": 54160
    },
    {
      "epoch": 11.609515645092156,
      "grad_norm": 0.15583917498588562,
      "learning_rate": 4.5206458065437925e-06,
      "loss": 0.5556,
      "step": 54170
    },
    {
      "epoch": 11.6116588084012,
      "grad_norm": 0.7253438234329224,
      "learning_rate": 4.517788255465067e-06,
      "loss": 0.6602,
      "step": 54180
    },
    {
      "epoch": 11.613801971710245,
      "grad_norm": 1863.4129638671875,
      "learning_rate": 4.514930704386341e-06,
      "loss": 0.6017,
      "step": 54190
    },
    {
      "epoch": 11.615945135019288,
      "grad_norm": 0.2099527269601822,
      "learning_rate": 4.512073153307616e-06,
      "loss": 0.2148,
      "step": 54200
    },
    {
      "epoch": 11.618088298328333,
      "grad_norm": 0.5906544923782349,
      "learning_rate": 4.50921560222889e-06,
      "loss": 0.5383,
      "step": 54210
    },
    {
      "epoch": 11.620231461637378,
      "grad_norm": 0.19056177139282227,
      "learning_rate": 4.506358051150165e-06,
      "loss": 0.6412,
      "step": 54220
    },
    {
      "epoch": 11.62237462494642,
      "grad_norm": 0.10801111906766891,
      "learning_rate": 4.5035005000714395e-06,
      "loss": 0.436,
      "step": 54230
    },
    {
      "epoch": 11.624517788255465,
      "grad_norm": 0.0671578198671341,
      "learning_rate": 4.500642948992713e-06,
      "loss": 0.1347,
      "step": 54240
    },
    {
      "epoch": 11.62666095156451,
      "grad_norm": 0.057467710226774216,
      "learning_rate": 4.497785397913988e-06,
      "loss": 0.3079,
      "step": 54250
    },
    {
      "epoch": 11.628804114873553,
      "grad_norm": 0.14216406643390656,
      "learning_rate": 4.494927846835262e-06,
      "loss": 0.5633,
      "step": 54260
    },
    {
      "epoch": 11.630947278182598,
      "grad_norm": 0.05999823659658432,
      "learning_rate": 4.492070295756537e-06,
      "loss": 0.5168,
      "step": 54270
    },
    {
      "epoch": 11.633090441491643,
      "grad_norm": 19.199901580810547,
      "learning_rate": 4.489212744677812e-06,
      "loss": 0.5506,
      "step": 54280
    },
    {
      "epoch": 11.635233604800685,
      "grad_norm": 0.12305758148431778,
      "learning_rate": 4.4863551935990864e-06,
      "loss": 0.206,
      "step": 54290
    },
    {
      "epoch": 11.63737676810973,
      "grad_norm": 23.525527954101562,
      "learning_rate": 4.48349764252036e-06,
      "loss": 0.5965,
      "step": 54300
    },
    {
      "epoch": 11.639519931418775,
      "grad_norm": 0.8163224458694458,
      "learning_rate": 4.480640091441635e-06,
      "loss": 0.2498,
      "step": 54310
    },
    {
      "epoch": 11.641663094727818,
      "grad_norm": 21.053133010864258,
      "learning_rate": 4.477782540362909e-06,
      "loss": 0.3811,
      "step": 54320
    },
    {
      "epoch": 11.643806258036863,
      "grad_norm": 0.1647244542837143,
      "learning_rate": 4.474924989284184e-06,
      "loss": 0.2345,
      "step": 54330
    },
    {
      "epoch": 11.645949421345907,
      "grad_norm": 0.3726755380630493,
      "learning_rate": 4.472067438205459e-06,
      "loss": 0.6941,
      "step": 54340
    },
    {
      "epoch": 11.64809258465495,
      "grad_norm": 0.7581409811973572,
      "learning_rate": 4.4692098871267325e-06,
      "loss": 0.0141,
      "step": 54350
    },
    {
      "epoch": 11.650235747963995,
      "grad_norm": 20.63232421875,
      "learning_rate": 4.466352336048007e-06,
      "loss": 0.6295,
      "step": 54360
    },
    {
      "epoch": 11.65237891127304,
      "grad_norm": 0.34649595618247986,
      "learning_rate": 4.463494784969282e-06,
      "loss": 0.6829,
      "step": 54370
    },
    {
      "epoch": 11.654522074582083,
      "grad_norm": 22.763898849487305,
      "learning_rate": 4.460637233890556e-06,
      "loss": 0.9282,
      "step": 54380
    },
    {
      "epoch": 11.656665237891128,
      "grad_norm": 0.08902543783187866,
      "learning_rate": 4.457779682811831e-06,
      "loss": 0.0178,
      "step": 54390
    },
    {
      "epoch": 11.658808401200172,
      "grad_norm": 0.8858424425125122,
      "learning_rate": 4.454922131733105e-06,
      "loss": 0.2248,
      "step": 54400
    },
    {
      "epoch": 11.660951564509215,
      "grad_norm": 0.10658324509859085,
      "learning_rate": 4.4520645806543795e-06,
      "loss": 0.8258,
      "step": 54410
    },
    {
      "epoch": 11.66309472781826,
      "grad_norm": 0.4108704924583435,
      "learning_rate": 4.449207029575654e-06,
      "loss": 0.4353,
      "step": 54420
    },
    {
      "epoch": 11.665237891127305,
      "grad_norm": 0.46229082345962524,
      "learning_rate": 4.446349478496928e-06,
      "loss": 0.5722,
      "step": 54430
    },
    {
      "epoch": 11.667381054436348,
      "grad_norm": 0.4104573428630829,
      "learning_rate": 4.443491927418203e-06,
      "loss": 0.1242,
      "step": 54440
    },
    {
      "epoch": 11.669524217745392,
      "grad_norm": 25.9322452545166,
      "learning_rate": 4.440634376339478e-06,
      "loss": 0.7121,
      "step": 54450
    },
    {
      "epoch": 11.671667381054437,
      "grad_norm": 0.14070679247379303,
      "learning_rate": 4.437776825260752e-06,
      "loss": 0.7508,
      "step": 54460
    },
    {
      "epoch": 11.67381054436348,
      "grad_norm": 2.350090503692627,
      "learning_rate": 4.4349192741820265e-06,
      "loss": 0.7925,
      "step": 54470
    },
    {
      "epoch": 11.675953707672525,
      "grad_norm": 0.58116215467453,
      "learning_rate": 4.432061723103301e-06,
      "loss": 0.2588,
      "step": 54480
    },
    {
      "epoch": 11.67809687098157,
      "grad_norm": 0.12685257196426392,
      "learning_rate": 4.429204172024575e-06,
      "loss": 0.125,
      "step": 54490
    },
    {
      "epoch": 11.680240034290613,
      "grad_norm": 0.4587540030479431,
      "learning_rate": 4.42634662094585e-06,
      "loss": 0.345,
      "step": 54500
    },
    {
      "epoch": 11.682383197599657,
      "grad_norm": 0.07734602689743042,
      "learning_rate": 4.423489069867124e-06,
      "loss": 0.63,
      "step": 54510
    },
    {
      "epoch": 11.684526360908702,
      "grad_norm": 0.02411974035203457,
      "learning_rate": 4.420631518788399e-06,
      "loss": 0.3076,
      "step": 54520
    },
    {
      "epoch": 11.686669524217745,
      "grad_norm": 0.4898134768009186,
      "learning_rate": 4.417773967709673e-06,
      "loss": 0.1338,
      "step": 54530
    },
    {
      "epoch": 11.68881268752679,
      "grad_norm": 0.31534820795059204,
      "learning_rate": 4.414916416630947e-06,
      "loss": 0.2859,
      "step": 54540
    },
    {
      "epoch": 11.690955850835834,
      "grad_norm": 0.5558624863624573,
      "learning_rate": 4.412058865552222e-06,
      "loss": 0.7342,
      "step": 54550
    },
    {
      "epoch": 11.693099014144877,
      "grad_norm": 0.05161600932478905,
      "learning_rate": 4.409201314473496e-06,
      "loss": 0.376,
      "step": 54560
    },
    {
      "epoch": 11.695242177453922,
      "grad_norm": 65.95665740966797,
      "learning_rate": 4.406343763394771e-06,
      "loss": 0.7629,
      "step": 54570
    },
    {
      "epoch": 11.697385340762967,
      "grad_norm": 0.021452322602272034,
      "learning_rate": 4.403486212316046e-06,
      "loss": 0.0031,
      "step": 54580
    },
    {
      "epoch": 11.69952850407201,
      "grad_norm": 0.5103362202644348,
      "learning_rate": 4.40062866123732e-06,
      "loss": 0.1174,
      "step": 54590
    },
    {
      "epoch": 11.701671667381055,
      "grad_norm": 0.17747944593429565,
      "learning_rate": 4.397771110158594e-06,
      "loss": 0.4415,
      "step": 54600
    },
    {
      "epoch": 11.7038148306901,
      "grad_norm": 0.16154253482818604,
      "learning_rate": 4.394913559079869e-06,
      "loss": 0.3413,
      "step": 54610
    },
    {
      "epoch": 11.705957993999142,
      "grad_norm": 0.024333588778972626,
      "learning_rate": 4.392056008001143e-06,
      "loss": 0.4513,
      "step": 54620
    },
    {
      "epoch": 11.708101157308187,
      "grad_norm": 33.100738525390625,
      "learning_rate": 4.389198456922418e-06,
      "loss": 1.0919,
      "step": 54630
    },
    {
      "epoch": 11.710244320617232,
      "grad_norm": 0.5485649108886719,
      "learning_rate": 4.3863409058436925e-06,
      "loss": 0.313,
      "step": 54640
    },
    {
      "epoch": 11.712387483926275,
      "grad_norm": 1.0216115713119507,
      "learning_rate": 4.3834833547649665e-06,
      "loss": 0.256,
      "step": 54650
    },
    {
      "epoch": 11.71453064723532,
      "grad_norm": 0.20958393812179565,
      "learning_rate": 4.380625803686241e-06,
      "loss": 0.7314,
      "step": 54660
    },
    {
      "epoch": 11.716673810544364,
      "grad_norm": 1.1045870780944824,
      "learning_rate": 4.377768252607515e-06,
      "loss": 0.1448,
      "step": 54670
    },
    {
      "epoch": 11.718816973853407,
      "grad_norm": 0.6416274309158325,
      "learning_rate": 4.37491070152879e-06,
      "loss": 0.486,
      "step": 54680
    },
    {
      "epoch": 11.720960137162452,
      "grad_norm": 0.07430092990398407,
      "learning_rate": 4.372053150450065e-06,
      "loss": 0.272,
      "step": 54690
    },
    {
      "epoch": 11.723103300471497,
      "grad_norm": 0.103326715528965,
      "learning_rate": 4.3691955993713395e-06,
      "loss": 0.3708,
      "step": 54700
    },
    {
      "epoch": 11.72524646378054,
      "grad_norm": 0.0392279252409935,
      "learning_rate": 4.3663380482926134e-06,
      "loss": 0.0021,
      "step": 54710
    },
    {
      "epoch": 11.727389627089584,
      "grad_norm": 0.049295250326395035,
      "learning_rate": 4.363480497213887e-06,
      "loss": 0.5259,
      "step": 54720
    },
    {
      "epoch": 11.729532790398629,
      "grad_norm": 0.2622261941432953,
      "learning_rate": 4.360622946135162e-06,
      "loss": 0.0015,
      "step": 54730
    },
    {
      "epoch": 11.731675953707672,
      "grad_norm": 0.06780239194631577,
      "learning_rate": 4.357765395056437e-06,
      "loss": 0.5895,
      "step": 54740
    },
    {
      "epoch": 11.733819117016717,
      "grad_norm": 0.7748225927352905,
      "learning_rate": 4.354907843977712e-06,
      "loss": 0.5884,
      "step": 54750
    },
    {
      "epoch": 11.735962280325761,
      "grad_norm": 0.08539339154958725,
      "learning_rate": 4.3520502928989865e-06,
      "loss": 0.3005,
      "step": 54760
    },
    {
      "epoch": 11.738105443634804,
      "grad_norm": 18.821142196655273,
      "learning_rate": 4.34919274182026e-06,
      "loss": 0.2059,
      "step": 54770
    },
    {
      "epoch": 11.74024860694385,
      "grad_norm": 0.046307049691677094,
      "learning_rate": 4.346335190741534e-06,
      "loss": 0.6057,
      "step": 54780
    },
    {
      "epoch": 11.742391770252894,
      "grad_norm": 0.4020891785621643,
      "learning_rate": 4.343477639662809e-06,
      "loss": 0.4709,
      "step": 54790
    },
    {
      "epoch": 11.744534933561937,
      "grad_norm": 0.43330150842666626,
      "learning_rate": 4.340620088584084e-06,
      "loss": 0.2943,
      "step": 54800
    },
    {
      "epoch": 11.746678096870982,
      "grad_norm": 1.068933367729187,
      "learning_rate": 4.337762537505359e-06,
      "loss": 0.6375,
      "step": 54810
    },
    {
      "epoch": 11.748821260180026,
      "grad_norm": 20.713499069213867,
      "learning_rate": 4.3349049864266326e-06,
      "loss": 0.5658,
      "step": 54820
    },
    {
      "epoch": 11.75096442348907,
      "grad_norm": 0.07823752611875534,
      "learning_rate": 4.332047435347907e-06,
      "loss": 0.3253,
      "step": 54830
    },
    {
      "epoch": 11.753107586798114,
      "grad_norm": 0.04180452972650528,
      "learning_rate": 4.329189884269181e-06,
      "loss": 0.213,
      "step": 54840
    },
    {
      "epoch": 11.755250750107159,
      "grad_norm": 73.36038970947266,
      "learning_rate": 4.326332333190456e-06,
      "loss": 0.5473,
      "step": 54850
    },
    {
      "epoch": 11.757393913416202,
      "grad_norm": 0.05099518597126007,
      "learning_rate": 4.323474782111731e-06,
      "loss": 0.3369,
      "step": 54860
    },
    {
      "epoch": 11.759537076725247,
      "grad_norm": 32.11977767944336,
      "learning_rate": 4.320617231033006e-06,
      "loss": 0.6295,
      "step": 54870
    },
    {
      "epoch": 11.761680240034291,
      "grad_norm": 0.06117720529437065,
      "learning_rate": 4.3177596799542795e-06,
      "loss": 0.6093,
      "step": 54880
    },
    {
      "epoch": 11.763823403343334,
      "grad_norm": 0.6650792360305786,
      "learning_rate": 4.3149021288755535e-06,
      "loss": 0.6121,
      "step": 54890
    },
    {
      "epoch": 11.765966566652379,
      "grad_norm": 0.5851203799247742,
      "learning_rate": 4.312044577796828e-06,
      "loss": 0.2571,
      "step": 54900
    },
    {
      "epoch": 11.768109729961424,
      "grad_norm": 0.4935372769832611,
      "learning_rate": 4.309187026718103e-06,
      "loss": 0.4553,
      "step": 54910
    },
    {
      "epoch": 11.770252893270467,
      "grad_norm": 0.2908242642879486,
      "learning_rate": 4.306329475639378e-06,
      "loss": 0.4173,
      "step": 54920
    },
    {
      "epoch": 11.772396056579511,
      "grad_norm": 0.39242586493492126,
      "learning_rate": 4.303471924560652e-06,
      "loss": 0.4466,
      "step": 54930
    },
    {
      "epoch": 11.774539219888556,
      "grad_norm": 19.544546127319336,
      "learning_rate": 4.3006143734819265e-06,
      "loss": 0.3564,
      "step": 54940
    },
    {
      "epoch": 11.776682383197599,
      "grad_norm": 31.213504791259766,
      "learning_rate": 4.2977568224032e-06,
      "loss": 1.1735,
      "step": 54950
    },
    {
      "epoch": 11.778825546506644,
      "grad_norm": 0.4445593059062958,
      "learning_rate": 4.294899271324475e-06,
      "loss": 0.1285,
      "step": 54960
    },
    {
      "epoch": 11.780968709815689,
      "grad_norm": 21.374637603759766,
      "learning_rate": 4.29204172024575e-06,
      "loss": 0.7547,
      "step": 54970
    },
    {
      "epoch": 11.783111873124732,
      "grad_norm": 0.07499101758003235,
      "learning_rate": 4.289184169167025e-06,
      "loss": 0.1651,
      "step": 54980
    },
    {
      "epoch": 11.785255036433776,
      "grad_norm": 0.37651216983795166,
      "learning_rate": 4.286326618088299e-06,
      "loss": 0.2124,
      "step": 54990
    },
    {
      "epoch": 11.787398199742821,
      "grad_norm": 0.4099684953689575,
      "learning_rate": 4.283469067009573e-06,
      "loss": 0.4877,
      "step": 55000
    },
    {
      "epoch": 11.789541363051864,
      "grad_norm": 0.05310341715812683,
      "learning_rate": 4.280611515930847e-06,
      "loss": 0.4144,
      "step": 55010
    },
    {
      "epoch": 11.791684526360909,
      "grad_norm": 1.0220316648483276,
      "learning_rate": 4.277753964852122e-06,
      "loss": 0.1734,
      "step": 55020
    },
    {
      "epoch": 11.793827689669953,
      "grad_norm": 158.8153076171875,
      "learning_rate": 4.274896413773397e-06,
      "loss": 0.7566,
      "step": 55030
    },
    {
      "epoch": 11.795970852978996,
      "grad_norm": 0.41499483585357666,
      "learning_rate": 4.272038862694671e-06,
      "loss": 0.9575,
      "step": 55040
    },
    {
      "epoch": 11.798114016288041,
      "grad_norm": 0.09599173069000244,
      "learning_rate": 4.269181311615946e-06,
      "loss": 0.4781,
      "step": 55050
    },
    {
      "epoch": 11.800257179597086,
      "grad_norm": 0.104403056204319,
      "learning_rate": 4.2663237605372196e-06,
      "loss": 0.1399,
      "step": 55060
    },
    {
      "epoch": 11.802400342906129,
      "grad_norm": 31.621007919311523,
      "learning_rate": 4.263466209458494e-06,
      "loss": 0.859,
      "step": 55070
    },
    {
      "epoch": 11.804543506215174,
      "grad_norm": 25.18837547302246,
      "learning_rate": 4.260608658379769e-06,
      "loss": 0.2664,
      "step": 55080
    },
    {
      "epoch": 11.806686669524218,
      "grad_norm": 0.07096297293901443,
      "learning_rate": 4.257751107301044e-06,
      "loss": 0.4645,
      "step": 55090
    },
    {
      "epoch": 11.808829832833261,
      "grad_norm": 0.5773335099220276,
      "learning_rate": 4.254893556222318e-06,
      "loss": 0.2088,
      "step": 55100
    },
    {
      "epoch": 11.810972996142306,
      "grad_norm": 0.1102224811911583,
      "learning_rate": 4.252036005143592e-06,
      "loss": 0.8523,
      "step": 55110
    },
    {
      "epoch": 11.81311615945135,
      "grad_norm": 0.12350812554359436,
      "learning_rate": 4.2491784540648665e-06,
      "loss": 0.9185,
      "step": 55120
    },
    {
      "epoch": 11.815259322760394,
      "grad_norm": 0.1058594211935997,
      "learning_rate": 4.246320902986141e-06,
      "loss": 0.4871,
      "step": 55130
    },
    {
      "epoch": 11.817402486069438,
      "grad_norm": 0.16097433865070343,
      "learning_rate": 4.243463351907416e-06,
      "loss": 0.2454,
      "step": 55140
    },
    {
      "epoch": 11.819545649378483,
      "grad_norm": 0.11766878515481949,
      "learning_rate": 4.24060580082869e-06,
      "loss": 0.4875,
      "step": 55150
    },
    {
      "epoch": 11.821688812687526,
      "grad_norm": 18.670467376708984,
      "learning_rate": 4.237748249749965e-06,
      "loss": 0.2986,
      "step": 55160
    },
    {
      "epoch": 11.82383197599657,
      "grad_norm": 37.916019439697266,
      "learning_rate": 4.234890698671239e-06,
      "loss": 0.7196,
      "step": 55170
    },
    {
      "epoch": 11.825975139305616,
      "grad_norm": 0.5286905765533447,
      "learning_rate": 4.2320331475925135e-06,
      "loss": 0.7314,
      "step": 55180
    },
    {
      "epoch": 11.828118302614659,
      "grad_norm": 26.662744522094727,
      "learning_rate": 4.229175596513788e-06,
      "loss": 0.6832,
      "step": 55190
    },
    {
      "epoch": 11.830261465923703,
      "grad_norm": 0.4982393980026245,
      "learning_rate": 4.226318045435063e-06,
      "loss": 0.3123,
      "step": 55200
    },
    {
      "epoch": 11.832404629232748,
      "grad_norm": 26.698266983032227,
      "learning_rate": 4.223460494356337e-06,
      "loss": 0.5894,
      "step": 55210
    },
    {
      "epoch": 11.834547792541791,
      "grad_norm": 0.13481006026268005,
      "learning_rate": 4.220602943277612e-06,
      "loss": 0.444,
      "step": 55220
    },
    {
      "epoch": 11.836690955850836,
      "grad_norm": 30.261009216308594,
      "learning_rate": 4.217745392198886e-06,
      "loss": 0.3638,
      "step": 55230
    },
    {
      "epoch": 11.83883411915988,
      "grad_norm": 0.6629848480224609,
      "learning_rate": 4.2148878411201604e-06,
      "loss": 0.5307,
      "step": 55240
    },
    {
      "epoch": 11.840977282468923,
      "grad_norm": 0.5429121255874634,
      "learning_rate": 4.212030290041435e-06,
      "loss": 0.1705,
      "step": 55250
    },
    {
      "epoch": 11.843120445777968,
      "grad_norm": 42.706695556640625,
      "learning_rate": 4.209172738962709e-06,
      "loss": 0.8185,
      "step": 55260
    },
    {
      "epoch": 11.845263609087013,
      "grad_norm": 0.2915712296962738,
      "learning_rate": 4.206315187883984e-06,
      "loss": 0.2827,
      "step": 55270
    },
    {
      "epoch": 11.847406772396056,
      "grad_norm": 0.6146034002304077,
      "learning_rate": 4.203457636805258e-06,
      "loss": 0.88,
      "step": 55280
    },
    {
      "epoch": 11.8495499357051,
      "grad_norm": 0.129867285490036,
      "learning_rate": 4.200600085726533e-06,
      "loss": 0.6193,
      "step": 55290
    },
    {
      "epoch": 11.851693099014145,
      "grad_norm": 1.6259355545043945,
      "learning_rate": 4.197742534647807e-06,
      "loss": 0.257,
      "step": 55300
    },
    {
      "epoch": 11.853836262323188,
      "grad_norm": 0.10576904565095901,
      "learning_rate": 4.194884983569082e-06,
      "loss": 0.5533,
      "step": 55310
    },
    {
      "epoch": 11.855979425632233,
      "grad_norm": 0.6426755785942078,
      "learning_rate": 4.192027432490356e-06,
      "loss": 0.4311,
      "step": 55320
    },
    {
      "epoch": 11.858122588941278,
      "grad_norm": 0.10237257182598114,
      "learning_rate": 4.189169881411631e-06,
      "loss": 0.1586,
      "step": 55330
    },
    {
      "epoch": 11.86026575225032,
      "grad_norm": 0.5823113918304443,
      "learning_rate": 4.186312330332905e-06,
      "loss": 0.1469,
      "step": 55340
    },
    {
      "epoch": 11.862408915559365,
      "grad_norm": 18.2750244140625,
      "learning_rate": 4.1834547792541796e-06,
      "loss": 0.5129,
      "step": 55350
    },
    {
      "epoch": 11.86455207886841,
      "grad_norm": 27.013980865478516,
      "learning_rate": 4.180597228175454e-06,
      "loss": 0.4871,
      "step": 55360
    },
    {
      "epoch": 11.866695242177453,
      "grad_norm": 19.247634887695312,
      "learning_rate": 4.177739677096728e-06,
      "loss": 0.1284,
      "step": 55370
    },
    {
      "epoch": 11.868838405486498,
      "grad_norm": 0.4099797308444977,
      "learning_rate": 4.174882126018003e-06,
      "loss": 0.4132,
      "step": 55380
    },
    {
      "epoch": 11.870981568795543,
      "grad_norm": 0.20206131041049957,
      "learning_rate": 4.172024574939277e-06,
      "loss": 0.291,
      "step": 55390
    },
    {
      "epoch": 11.873124732104586,
      "grad_norm": 0.042004525661468506,
      "learning_rate": 4.169167023860552e-06,
      "loss": 0.1186,
      "step": 55400
    },
    {
      "epoch": 11.87526789541363,
      "grad_norm": 0.20277804136276245,
      "learning_rate": 4.1663094727818265e-06,
      "loss": 0.3284,
      "step": 55410
    },
    {
      "epoch": 11.877411058722675,
      "grad_norm": 29.81993865966797,
      "learning_rate": 4.1634519217031005e-06,
      "loss": 0.6562,
      "step": 55420
    },
    {
      "epoch": 11.879554222031718,
      "grad_norm": 0.14938154816627502,
      "learning_rate": 4.160594370624375e-06,
      "loss": 0.595,
      "step": 55430
    },
    {
      "epoch": 11.881697385340763,
      "grad_norm": 0.16556602716445923,
      "learning_rate": 4.15773681954565e-06,
      "loss": 0.0069,
      "step": 55440
    },
    {
      "epoch": 11.883840548649808,
      "grad_norm": 0.841144859790802,
      "learning_rate": 4.154879268466924e-06,
      "loss": 0.4264,
      "step": 55450
    },
    {
      "epoch": 11.88598371195885,
      "grad_norm": 40.4356575012207,
      "learning_rate": 4.152021717388199e-06,
      "loss": 0.5285,
      "step": 55460
    },
    {
      "epoch": 11.888126875267895,
      "grad_norm": 0.2753654420375824,
      "learning_rate": 4.1491641663094735e-06,
      "loss": 0.3113,
      "step": 55470
    },
    {
      "epoch": 11.89027003857694,
      "grad_norm": 0.021925870329141617,
      "learning_rate": 4.146306615230747e-06,
      "loss": 0.1348,
      "step": 55480
    },
    {
      "epoch": 11.892413201885983,
      "grad_norm": 0.15925589203834534,
      "learning_rate": 4.143449064152022e-06,
      "loss": 0.8933,
      "step": 55490
    },
    {
      "epoch": 11.894556365195028,
      "grad_norm": 23.025049209594727,
      "learning_rate": 4.140591513073296e-06,
      "loss": 0.5256,
      "step": 55500
    },
    {
      "epoch": 11.896699528504072,
      "grad_norm": 49.924774169921875,
      "learning_rate": 4.137733961994571e-06,
      "loss": 0.9118,
      "step": 55510
    },
    {
      "epoch": 11.898842691813115,
      "grad_norm": 0.09859127551317215,
      "learning_rate": 4.134876410915846e-06,
      "loss": 0.1063,
      "step": 55520
    },
    {
      "epoch": 11.90098585512216,
      "grad_norm": 19.45586395263672,
      "learning_rate": 4.13201885983712e-06,
      "loss": 0.7053,
      "step": 55530
    },
    {
      "epoch": 11.903129018431205,
      "grad_norm": 0.9174701571464539,
      "learning_rate": 4.129161308758394e-06,
      "loss": 0.5517,
      "step": 55540
    },
    {
      "epoch": 11.905272181740248,
      "grad_norm": 18.140111923217773,
      "learning_rate": 4.126303757679669e-06,
      "loss": 0.6984,
      "step": 55550
    },
    {
      "epoch": 11.907415345049293,
      "grad_norm": 0.6465668678283691,
      "learning_rate": 4.123446206600943e-06,
      "loss": 0.0086,
      "step": 55560
    },
    {
      "epoch": 11.909558508358337,
      "grad_norm": 40.03294372558594,
      "learning_rate": 4.120588655522218e-06,
      "loss": 0.5191,
      "step": 55570
    },
    {
      "epoch": 11.91170167166738,
      "grad_norm": 0.41278666257858276,
      "learning_rate": 4.117731104443492e-06,
      "loss": 0.2812,
      "step": 55580
    },
    {
      "epoch": 11.913844834976425,
      "grad_norm": 0.07542186975479126,
      "learning_rate": 4.1148735533647666e-06,
      "loss": 0.4313,
      "step": 55590
    },
    {
      "epoch": 11.91598799828547,
      "grad_norm": 0.5328169465065002,
      "learning_rate": 4.112016002286041e-06,
      "loss": 0.2141,
      "step": 55600
    },
    {
      "epoch": 11.918131161594513,
      "grad_norm": 0.212290957570076,
      "learning_rate": 4.109158451207316e-06,
      "loss": 0.5641,
      "step": 55610
    },
    {
      "epoch": 11.920274324903557,
      "grad_norm": 1.102143406867981,
      "learning_rate": 4.10630090012859e-06,
      "loss": 0.0078,
      "step": 55620
    },
    {
      "epoch": 11.922417488212602,
      "grad_norm": 0.17777428030967712,
      "learning_rate": 4.103443349049865e-06,
      "loss": 0.3024,
      "step": 55630
    },
    {
      "epoch": 11.924560651521645,
      "grad_norm": 46.925148010253906,
      "learning_rate": 4.100585797971139e-06,
      "loss": 0.9486,
      "step": 55640
    },
    {
      "epoch": 11.92670381483069,
      "grad_norm": 19.519756317138672,
      "learning_rate": 4.0977282468924135e-06,
      "loss": 0.4013,
      "step": 55650
    },
    {
      "epoch": 11.928846978139735,
      "grad_norm": 0.7442285418510437,
      "learning_rate": 4.094870695813688e-06,
      "loss": 0.1447,
      "step": 55660
    },
    {
      "epoch": 11.930990141448778,
      "grad_norm": 0.04985933005809784,
      "learning_rate": 4.092013144734962e-06,
      "loss": 0.2584,
      "step": 55670
    },
    {
      "epoch": 11.933133304757822,
      "grad_norm": 0.5322415828704834,
      "learning_rate": 4.089155593656237e-06,
      "loss": 0.4611,
      "step": 55680
    },
    {
      "epoch": 11.935276468066867,
      "grad_norm": 0.13291753828525543,
      "learning_rate": 4.086298042577511e-06,
      "loss": 0.2647,
      "step": 55690
    },
    {
      "epoch": 11.93741963137591,
      "grad_norm": 42.71376037597656,
      "learning_rate": 4.083440491498786e-06,
      "loss": 0.3344,
      "step": 55700
    },
    {
      "epoch": 11.939562794684955,
      "grad_norm": 0.054194800555706024,
      "learning_rate": 4.0805829404200605e-06,
      "loss": 0.2371,
      "step": 55710
    },
    {
      "epoch": 11.941705957994,
      "grad_norm": 0.40571874380111694,
      "learning_rate": 4.077725389341335e-06,
      "loss": 0.2751,
      "step": 55720
    },
    {
      "epoch": 11.943849121303042,
      "grad_norm": 37.96293640136719,
      "learning_rate": 4.074867838262609e-06,
      "loss": 0.6001,
      "step": 55730
    },
    {
      "epoch": 11.945992284612087,
      "grad_norm": 0.0903223529458046,
      "learning_rate": 4.072010287183883e-06,
      "loss": 0.6751,
      "step": 55740
    },
    {
      "epoch": 11.948135447921132,
      "grad_norm": 0.10941978543996811,
      "learning_rate": 4.069152736105158e-06,
      "loss": 0.6388,
      "step": 55750
    },
    {
      "epoch": 11.950278611230175,
      "grad_norm": 0.20150528848171234,
      "learning_rate": 4.066295185026433e-06,
      "loss": 0.1262,
      "step": 55760
    },
    {
      "epoch": 11.95242177453922,
      "grad_norm": 0.4454747140407562,
      "learning_rate": 4.0634376339477074e-06,
      "loss": 0.2447,
      "step": 55770
    },
    {
      "epoch": 11.954564937848264,
      "grad_norm": 0.1190660297870636,
      "learning_rate": 4.060580082868981e-06,
      "loss": 0.7772,
      "step": 55780
    },
    {
      "epoch": 11.956708101157307,
      "grad_norm": 0.07484084367752075,
      "learning_rate": 4.057722531790256e-06,
      "loss": 0.3765,
      "step": 55790
    },
    {
      "epoch": 11.958851264466352,
      "grad_norm": 0.05851127579808235,
      "learning_rate": 4.05486498071153e-06,
      "loss": 0.3959,
      "step": 55800
    },
    {
      "epoch": 11.960994427775397,
      "grad_norm": 56.08565902709961,
      "learning_rate": 4.052007429632805e-06,
      "loss": 0.3496,
      "step": 55810
    },
    {
      "epoch": 11.96313759108444,
      "grad_norm": 20.07707405090332,
      "learning_rate": 4.04914987855408e-06,
      "loss": 0.584,
      "step": 55820
    },
    {
      "epoch": 11.965280754393484,
      "grad_norm": 104.98836517333984,
      "learning_rate": 4.046292327475354e-06,
      "loss": 0.2285,
      "step": 55830
    },
    {
      "epoch": 11.96742391770253,
      "grad_norm": 16.757423400878906,
      "learning_rate": 4.043434776396628e-06,
      "loss": 0.3361,
      "step": 55840
    },
    {
      "epoch": 11.969567081011572,
      "grad_norm": 0.07598250359296799,
      "learning_rate": 4.040577225317902e-06,
      "loss": 0.1864,
      "step": 55850
    },
    {
      "epoch": 11.971710244320617,
      "grad_norm": 0.1652643084526062,
      "learning_rate": 4.037719674239177e-06,
      "loss": 0.5827,
      "step": 55860
    },
    {
      "epoch": 11.973853407629662,
      "grad_norm": 0.17980724573135376,
      "learning_rate": 4.034862123160452e-06,
      "loss": 0.1945,
      "step": 55870
    },
    {
      "epoch": 11.975996570938705,
      "grad_norm": 0.1344577968120575,
      "learning_rate": 4.0320045720817266e-06,
      "loss": 0.1665,
      "step": 55880
    },
    {
      "epoch": 11.97813973424775,
      "grad_norm": 0.22290471196174622,
      "learning_rate": 4.029147021003001e-06,
      "loss": 0.3296,
      "step": 55890
    },
    {
      "epoch": 11.980282897556794,
      "grad_norm": 0.14152707159519196,
      "learning_rate": 4.026289469924275e-06,
      "loss": 0.348,
      "step": 55900
    },
    {
      "epoch": 11.982426060865837,
      "grad_norm": 0.11192094534635544,
      "learning_rate": 4.023431918845549e-06,
      "loss": 0.3138,
      "step": 55910
    },
    {
      "epoch": 11.984569224174882,
      "grad_norm": 0.07305757701396942,
      "learning_rate": 4.020574367766824e-06,
      "loss": 0.3273,
      "step": 55920
    },
    {
      "epoch": 11.986712387483927,
      "grad_norm": 0.5965506434440613,
      "learning_rate": 4.017716816688099e-06,
      "loss": 0.0052,
      "step": 55930
    },
    {
      "epoch": 11.98885555079297,
      "grad_norm": 0.04917042329907417,
      "learning_rate": 4.0148592656093735e-06,
      "loss": 0.0022,
      "step": 55940
    },
    {
      "epoch": 11.990998714102014,
      "grad_norm": 0.8279473185539246,
      "learning_rate": 4.0120017145306475e-06,
      "loss": 0.7864,
      "step": 55950
    },
    {
      "epoch": 11.993141877411059,
      "grad_norm": 0.42490023374557495,
      "learning_rate": 4.009144163451922e-06,
      "loss": 0.9526,
      "step": 55960
    },
    {
      "epoch": 11.995285040720102,
      "grad_norm": 0.17674103379249573,
      "learning_rate": 4.006286612373196e-06,
      "loss": 0.4817,
      "step": 55970
    },
    {
      "epoch": 11.997428204029147,
      "grad_norm": 0.29806551337242126,
      "learning_rate": 4.003429061294471e-06,
      "loss": 0.3127,
      "step": 55980
    },
    {
      "epoch": 11.999571367338191,
      "grad_norm": 401.5059814453125,
      "learning_rate": 4.000571510215746e-06,
      "loss": 0.2919,
      "step": 55990
    },
    {
      "epoch": 12.0,
      "eval_accuracy": 0.902,
      "eval_f1": 0.5377358490566038,
      "eval_loss": 0.4562858045101166,
      "eval_precision": 0.7154811715481172,
      "eval_recall": 0.43073047858942065,
      "eval_runtime": 396.402,
      "eval_samples_per_second": 7.568,
      "eval_steps_per_second": 2.523,
      "step": 55992
    },
    {
      "epoch": 12.001714530647236,
      "grad_norm": 20.76740074157715,
      "learning_rate": 3.9977139591370205e-06,
      "loss": 0.8354,
      "step": 56000
    },
    {
      "epoch": 12.003857693956279,
      "grad_norm": 0.2471352070569992,
      "learning_rate": 3.994856408058294e-06,
      "loss": 0.0072,
      "step": 56010
    },
    {
      "epoch": 12.006000857265324,
      "grad_norm": 0.1373479664325714,
      "learning_rate": 3.991998856979568e-06,
      "loss": 0.003,
      "step": 56020
    },
    {
      "epoch": 12.008144020574369,
      "grad_norm": 0.42800477147102356,
      "learning_rate": 3.989141305900843e-06,
      "loss": 0.1509,
      "step": 56030
    },
    {
      "epoch": 12.010287183883412,
      "grad_norm": 108.8407974243164,
      "learning_rate": 3.986283754822118e-06,
      "loss": 0.4614,
      "step": 56040
    },
    {
      "epoch": 12.012430347192456,
      "grad_norm": 23.341596603393555,
      "learning_rate": 3.983426203743393e-06,
      "loss": 0.9946,
      "step": 56050
    },
    {
      "epoch": 12.014573510501501,
      "grad_norm": 1.0823171138763428,
      "learning_rate": 3.980568652664667e-06,
      "loss": 0.7435,
      "step": 56060
    },
    {
      "epoch": 12.016716673810544,
      "grad_norm": 8.325370788574219,
      "learning_rate": 3.977711101585941e-06,
      "loss": 0.1454,
      "step": 56070
    },
    {
      "epoch": 12.018859837119589,
      "grad_norm": 19.7778377532959,
      "learning_rate": 3.974853550507215e-06,
      "loss": 1.0257,
      "step": 56080
    },
    {
      "epoch": 12.021003000428633,
      "grad_norm": 0.2951834499835968,
      "learning_rate": 3.97199599942849e-06,
      "loss": 0.7355,
      "step": 56090
    },
    {
      "epoch": 12.023146163737676,
      "grad_norm": 0.11263178288936615,
      "learning_rate": 3.969138448349765e-06,
      "loss": 0.4985,
      "step": 56100
    },
    {
      "epoch": 12.025289327046721,
      "grad_norm": 0.5577880144119263,
      "learning_rate": 3.96628089727104e-06,
      "loss": 0.0061,
      "step": 56110
    },
    {
      "epoch": 12.027432490355766,
      "grad_norm": 0.31223371624946594,
      "learning_rate": 3.9634233461923136e-06,
      "loss": 0.46,
      "step": 56120
    },
    {
      "epoch": 12.029575653664809,
      "grad_norm": 30.838891983032227,
      "learning_rate": 3.9605657951135875e-06,
      "loss": 0.3384,
      "step": 56130
    },
    {
      "epoch": 12.031718816973854,
      "grad_norm": 0.19507180154323578,
      "learning_rate": 3.957708244034862e-06,
      "loss": 0.3194,
      "step": 56140
    },
    {
      "epoch": 12.033861980282898,
      "grad_norm": 0.5962699055671692,
      "learning_rate": 3.954850692956137e-06,
      "loss": 0.2641,
      "step": 56150
    },
    {
      "epoch": 12.036005143591941,
      "grad_norm": 0.054116882383823395,
      "learning_rate": 3.951993141877412e-06,
      "loss": 0.2222,
      "step": 56160
    },
    {
      "epoch": 12.038148306900986,
      "grad_norm": 0.23897071182727814,
      "learning_rate": 3.949135590798686e-06,
      "loss": 0.1468,
      "step": 56170
    },
    {
      "epoch": 12.04029147021003,
      "grad_norm": 0.1530226618051529,
      "learning_rate": 3.9462780397199605e-06,
      "loss": 0.5246,
      "step": 56180
    },
    {
      "epoch": 12.042434633519074,
      "grad_norm": 0.11529530584812164,
      "learning_rate": 3.9434204886412344e-06,
      "loss": 0.5182,
      "step": 56190
    },
    {
      "epoch": 12.044577796828118,
      "grad_norm": 0.34384289383888245,
      "learning_rate": 3.940562937562509e-06,
      "loss": 0.4846,
      "step": 56200
    },
    {
      "epoch": 12.046720960137163,
      "grad_norm": 0.03622480481863022,
      "learning_rate": 3.937705386483784e-06,
      "loss": 0.4268,
      "step": 56210
    },
    {
      "epoch": 12.048864123446206,
      "grad_norm": 0.3548199236392975,
      "learning_rate": 3.934847835405059e-06,
      "loss": 0.3392,
      "step": 56220
    },
    {
      "epoch": 12.051007286755251,
      "grad_norm": 211.43165588378906,
      "learning_rate": 3.931990284326333e-06,
      "loss": 0.4553,
      "step": 56230
    },
    {
      "epoch": 12.053150450064296,
      "grad_norm": 0.33303025364875793,
      "learning_rate": 3.929132733247607e-06,
      "loss": 0.4623,
      "step": 56240
    },
    {
      "epoch": 12.055293613373339,
      "grad_norm": 21.870384216308594,
      "learning_rate": 3.926275182168881e-06,
      "loss": 0.1604,
      "step": 56250
    },
    {
      "epoch": 12.057436776682383,
      "grad_norm": 0.24109622836112976,
      "learning_rate": 3.923417631090156e-06,
      "loss": 0.3907,
      "step": 56260
    },
    {
      "epoch": 12.059579939991428,
      "grad_norm": 0.12203242629766464,
      "learning_rate": 3.920560080011431e-06,
      "loss": 0.1987,
      "step": 56270
    },
    {
      "epoch": 12.061723103300471,
      "grad_norm": 0.38717758655548096,
      "learning_rate": 3.917702528932705e-06,
      "loss": 0.4895,
      "step": 56280
    },
    {
      "epoch": 12.063866266609516,
      "grad_norm": 0.23998549580574036,
      "learning_rate": 3.91484497785398e-06,
      "loss": 0.5746,
      "step": 56290
    },
    {
      "epoch": 12.06600942991856,
      "grad_norm": 0.5343297719955444,
      "learning_rate": 3.911987426775254e-06,
      "loss": 0.6062,
      "step": 56300
    },
    {
      "epoch": 12.068152593227603,
      "grad_norm": 0.02461051195859909,
      "learning_rate": 3.909129875696528e-06,
      "loss": 0.0055,
      "step": 56310
    },
    {
      "epoch": 12.070295756536648,
      "grad_norm": 24.12882423400879,
      "learning_rate": 3.906272324617803e-06,
      "loss": 0.7953,
      "step": 56320
    },
    {
      "epoch": 12.072438919845693,
      "grad_norm": 0.9915330410003662,
      "learning_rate": 3.903414773539078e-06,
      "loss": 0.6946,
      "step": 56330
    },
    {
      "epoch": 12.074582083154736,
      "grad_norm": 0.2735907733440399,
      "learning_rate": 3.900557222460352e-06,
      "loss": 0.2904,
      "step": 56340
    },
    {
      "epoch": 12.07672524646378,
      "grad_norm": 0.04671778157353401,
      "learning_rate": 3.897699671381627e-06,
      "loss": 0.134,
      "step": 56350
    },
    {
      "epoch": 12.078868409772825,
      "grad_norm": 0.15876644849777222,
      "learning_rate": 3.8948421203029005e-06,
      "loss": 0.7773,
      "step": 56360
    },
    {
      "epoch": 12.081011573081868,
      "grad_norm": 0.08469206839799881,
      "learning_rate": 3.891984569224175e-06,
      "loss": 0.4181,
      "step": 56370
    },
    {
      "epoch": 12.083154736390913,
      "grad_norm": 0.054952461272478104,
      "learning_rate": 3.88912701814545e-06,
      "loss": 0.0024,
      "step": 56380
    },
    {
      "epoch": 12.085297899699958,
      "grad_norm": 0.14657460153102875,
      "learning_rate": 3.886269467066724e-06,
      "loss": 0.2803,
      "step": 56390
    },
    {
      "epoch": 12.087441063009,
      "grad_norm": 0.18393073976039886,
      "learning_rate": 3.883411915987999e-06,
      "loss": 0.5418,
      "step": 56400
    },
    {
      "epoch": 12.089584226318046,
      "grad_norm": 0.07707107067108154,
      "learning_rate": 3.880554364909273e-06,
      "loss": 0.8254,
      "step": 56410
    },
    {
      "epoch": 12.09172738962709,
      "grad_norm": 0.16127870976924896,
      "learning_rate": 3.8776968138305475e-06,
      "loss": 0.008,
      "step": 56420
    },
    {
      "epoch": 12.093870552936133,
      "grad_norm": 3.2411434650421143,
      "learning_rate": 3.874839262751822e-06,
      "loss": 0.7905,
      "step": 56430
    },
    {
      "epoch": 12.096013716245178,
      "grad_norm": 25.53797721862793,
      "learning_rate": 3.871981711673096e-06,
      "loss": 0.3868,
      "step": 56440
    },
    {
      "epoch": 12.098156879554223,
      "grad_norm": 0.4016876220703125,
      "learning_rate": 3.869124160594371e-06,
      "loss": 0.1752,
      "step": 56450
    },
    {
      "epoch": 12.100300042863266,
      "grad_norm": 0.4767087697982788,
      "learning_rate": 3.866266609515646e-06,
      "loss": 0.2679,
      "step": 56460
    },
    {
      "epoch": 12.10244320617231,
      "grad_norm": 0.07751716673374176,
      "learning_rate": 3.86340905843692e-06,
      "loss": 0.8175,
      "step": 56470
    },
    {
      "epoch": 12.104586369481355,
      "grad_norm": 0.6437217593193054,
      "learning_rate": 3.8605515073581945e-06,
      "loss": 0.006,
      "step": 56480
    },
    {
      "epoch": 12.106729532790398,
      "grad_norm": 23.997447967529297,
      "learning_rate": 3.857693956279469e-06,
      "loss": 0.3831,
      "step": 56490
    },
    {
      "epoch": 12.108872696099443,
      "grad_norm": 0.3923758864402771,
      "learning_rate": 3.854836405200743e-06,
      "loss": 0.0044,
      "step": 56500
    },
    {
      "epoch": 12.111015859408488,
      "grad_norm": 0.12295576184988022,
      "learning_rate": 3.851978854122018e-06,
      "loss": 0.47,
      "step": 56510
    },
    {
      "epoch": 12.11315902271753,
      "grad_norm": 0.03028731793165207,
      "learning_rate": 3.849121303043292e-06,
      "loss": 0.1743,
      "step": 56520
    },
    {
      "epoch": 12.115302186026575,
      "grad_norm": 0.049046095460653305,
      "learning_rate": 3.846263751964567e-06,
      "loss": 0.3011,
      "step": 56530
    },
    {
      "epoch": 12.11744534933562,
      "grad_norm": 0.04464324191212654,
      "learning_rate": 3.843406200885841e-06,
      "loss": 0.004,
      "step": 56540
    },
    {
      "epoch": 12.119588512644663,
      "grad_norm": 0.060901936143636703,
      "learning_rate": 3.840548649807115e-06,
      "loss": 0.8424,
      "step": 56550
    },
    {
      "epoch": 12.121731675953708,
      "grad_norm": 0.06149990111589432,
      "learning_rate": 3.83769109872839e-06,
      "loss": 0.7694,
      "step": 56560
    },
    {
      "epoch": 12.123874839262752,
      "grad_norm": 23.085582733154297,
      "learning_rate": 3.834833547649665e-06,
      "loss": 0.9322,
      "step": 56570
    },
    {
      "epoch": 12.126018002571795,
      "grad_norm": 0.3061608672142029,
      "learning_rate": 3.831975996570939e-06,
      "loss": 0.4328,
      "step": 56580
    },
    {
      "epoch": 12.12816116588084,
      "grad_norm": 0.2464471310377121,
      "learning_rate": 3.829118445492214e-06,
      "loss": 0.1293,
      "step": 56590
    },
    {
      "epoch": 12.130304329189885,
      "grad_norm": 0.430433988571167,
      "learning_rate": 3.8262608944134875e-06,
      "loss": 0.3122,
      "step": 56600
    },
    {
      "epoch": 12.132447492498928,
      "grad_norm": 0.07174859941005707,
      "learning_rate": 3.823403343334762e-06,
      "loss": 0.2887,
      "step": 56610
    },
    {
      "epoch": 12.134590655807973,
      "grad_norm": 0.5976426601409912,
      "learning_rate": 3.820545792256037e-06,
      "loss": 0.004,
      "step": 56620
    },
    {
      "epoch": 12.136733819117017,
      "grad_norm": 0.02259250171482563,
      "learning_rate": 3.817688241177311e-06,
      "loss": 0.1555,
      "step": 56630
    },
    {
      "epoch": 12.13887698242606,
      "grad_norm": 0.1642853021621704,
      "learning_rate": 3.814830690098586e-06,
      "loss": 0.5902,
      "step": 56640
    },
    {
      "epoch": 12.141020145735105,
      "grad_norm": 0.05263034999370575,
      "learning_rate": 3.8119731390198606e-06,
      "loss": 0.2691,
      "step": 56650
    },
    {
      "epoch": 12.14316330904415,
      "grad_norm": 0.43541598320007324,
      "learning_rate": 3.8091155879411345e-06,
      "loss": 0.1731,
      "step": 56660
    },
    {
      "epoch": 12.145306472353193,
      "grad_norm": 0.07145562767982483,
      "learning_rate": 3.8062580368624093e-06,
      "loss": 0.0041,
      "step": 56670
    },
    {
      "epoch": 12.147449635662237,
      "grad_norm": 0.2522805631160736,
      "learning_rate": 3.8034004857836836e-06,
      "loss": 0.0041,
      "step": 56680
    },
    {
      "epoch": 12.149592798971282,
      "grad_norm": 0.058912135660648346,
      "learning_rate": 3.8005429347049584e-06,
      "loss": 0.413,
      "step": 56690
    },
    {
      "epoch": 12.151735962280325,
      "grad_norm": 0.47710365056991577,
      "learning_rate": 3.7976853836262327e-06,
      "loss": 0.1602,
      "step": 56700
    },
    {
      "epoch": 12.15387912558937,
      "grad_norm": 0.5915146470069885,
      "learning_rate": 3.7948278325475067e-06,
      "loss": 0.3965,
      "step": 56710
    },
    {
      "epoch": 12.156022288898415,
      "grad_norm": 0.22612380981445312,
      "learning_rate": 3.7919702814687814e-06,
      "loss": 0.7516,
      "step": 56720
    },
    {
      "epoch": 12.158165452207458,
      "grad_norm": 392.08978271484375,
      "learning_rate": 3.7891127303900562e-06,
      "loss": 0.2857,
      "step": 56730
    },
    {
      "epoch": 12.160308615516502,
      "grad_norm": 0.14416828751564026,
      "learning_rate": 3.7862551793113306e-06,
      "loss": 0.3772,
      "step": 56740
    },
    {
      "epoch": 12.162451778825547,
      "grad_norm": 0.07759120315313339,
      "learning_rate": 3.7833976282326053e-06,
      "loss": 0.3117,
      "step": 56750
    },
    {
      "epoch": 12.16459494213459,
      "grad_norm": 0.6660104393959045,
      "learning_rate": 3.7805400771538793e-06,
      "loss": 0.1449,
      "step": 56760
    },
    {
      "epoch": 12.166738105443635,
      "grad_norm": 0.10679510980844498,
      "learning_rate": 3.7776825260751536e-06,
      "loss": 0.3147,
      "step": 56770
    },
    {
      "epoch": 12.16888126875268,
      "grad_norm": 0.3205547630786896,
      "learning_rate": 3.7748249749964284e-06,
      "loss": 0.1854,
      "step": 56780
    },
    {
      "epoch": 12.171024432061722,
      "grad_norm": 0.0855119600892067,
      "learning_rate": 3.7719674239177027e-06,
      "loss": 0.4031,
      "step": 56790
    },
    {
      "epoch": 12.173167595370767,
      "grad_norm": 0.03019876405596733,
      "learning_rate": 3.7691098728389775e-06,
      "loss": 0.2074,
      "step": 56800
    },
    {
      "epoch": 12.175310758679812,
      "grad_norm": 0.07953885942697525,
      "learning_rate": 3.766252321760252e-06,
      "loss": 0.3166,
      "step": 56810
    },
    {
      "epoch": 12.177453921988855,
      "grad_norm": 0.05021461099386215,
      "learning_rate": 3.7633947706815262e-06,
      "loss": 0.4535,
      "step": 56820
    },
    {
      "epoch": 12.1795970852979,
      "grad_norm": 0.023196468129754066,
      "learning_rate": 3.7605372196028006e-06,
      "loss": 0.1663,
      "step": 56830
    },
    {
      "epoch": 12.181740248606944,
      "grad_norm": 0.20576906204223633,
      "learning_rate": 3.7576796685240754e-06,
      "loss": 0.4225,
      "step": 56840
    },
    {
      "epoch": 12.183883411915987,
      "grad_norm": 37.13924026489258,
      "learning_rate": 3.7548221174453497e-06,
      "loss": 0.6976,
      "step": 56850
    },
    {
      "epoch": 12.186026575225032,
      "grad_norm": 0.02715216763317585,
      "learning_rate": 3.7519645663666245e-06,
      "loss": 0.0039,
      "step": 56860
    },
    {
      "epoch": 12.188169738534077,
      "grad_norm": 0.404448539018631,
      "learning_rate": 3.7491070152878984e-06,
      "loss": 0.67,
      "step": 56870
    },
    {
      "epoch": 12.19031290184312,
      "grad_norm": 19.796098709106445,
      "learning_rate": 3.7462494642091728e-06,
      "loss": 0.7675,
      "step": 56880
    },
    {
      "epoch": 12.192456065152165,
      "grad_norm": 0.046754464507102966,
      "learning_rate": 3.7433919131304475e-06,
      "loss": 0.2151,
      "step": 56890
    },
    {
      "epoch": 12.19459922846121,
      "grad_norm": 0.028231315314769745,
      "learning_rate": 3.740534362051722e-06,
      "loss": 0.5794,
      "step": 56900
    },
    {
      "epoch": 12.196742391770252,
      "grad_norm": 0.4032542407512665,
      "learning_rate": 3.7376768109729967e-06,
      "loss": 0.1505,
      "step": 56910
    },
    {
      "epoch": 12.198885555079297,
      "grad_norm": 1.5940195322036743,
      "learning_rate": 3.734819259894271e-06,
      "loss": 0.358,
      "step": 56920
    },
    {
      "epoch": 12.201028718388342,
      "grad_norm": 0.046471092849969864,
      "learning_rate": 3.7319617088155454e-06,
      "loss": 0.3086,
      "step": 56930
    },
    {
      "epoch": 12.203171881697385,
      "grad_norm": 30.947994232177734,
      "learning_rate": 3.7291041577368197e-06,
      "loss": 0.7861,
      "step": 56940
    },
    {
      "epoch": 12.20531504500643,
      "grad_norm": 0.03769490867853165,
      "learning_rate": 3.7262466066580945e-06,
      "loss": 0.3393,
      "step": 56950
    },
    {
      "epoch": 12.207458208315474,
      "grad_norm": 0.06024874001741409,
      "learning_rate": 3.723389055579369e-06,
      "loss": 0.1403,
      "step": 56960
    },
    {
      "epoch": 12.209601371624517,
      "grad_norm": 0.03619801625609398,
      "learning_rate": 3.7205315045006436e-06,
      "loss": 0.4346,
      "step": 56970
    },
    {
      "epoch": 12.211744534933562,
      "grad_norm": 0.3036423623561859,
      "learning_rate": 3.7176739534219175e-06,
      "loss": 0.1579,
      "step": 56980
    },
    {
      "epoch": 12.213887698242607,
      "grad_norm": 27.951305389404297,
      "learning_rate": 3.714816402343192e-06,
      "loss": 0.4749,
      "step": 56990
    },
    {
      "epoch": 12.21603086155165,
      "grad_norm": 23.889707565307617,
      "learning_rate": 3.7119588512644667e-06,
      "loss": 0.5318,
      "step": 57000
    },
    {
      "epoch": 12.218174024860694,
      "grad_norm": 17.209135055541992,
      "learning_rate": 3.709101300185741e-06,
      "loss": 0.6292,
      "step": 57010
    },
    {
      "epoch": 12.220317188169739,
      "grad_norm": 0.24450773000717163,
      "learning_rate": 3.706243749107016e-06,
      "loss": 0.843,
      "step": 57020
    },
    {
      "epoch": 12.222460351478782,
      "grad_norm": 0.3348805010318756,
      "learning_rate": 3.7033861980282897e-06,
      "loss": 0.3738,
      "step": 57030
    },
    {
      "epoch": 12.224603514787827,
      "grad_norm": 0.18862822651863098,
      "learning_rate": 3.7005286469495645e-06,
      "loss": 0.0059,
      "step": 57040
    },
    {
      "epoch": 12.226746678096871,
      "grad_norm": 0.5630064606666565,
      "learning_rate": 3.697671095870839e-06,
      "loss": 0.674,
      "step": 57050
    },
    {
      "epoch": 12.228889841405914,
      "grad_norm": 1.1241612434387207,
      "learning_rate": 3.6948135447921136e-06,
      "loss": 0.3909,
      "step": 57060
    },
    {
      "epoch": 12.23103300471496,
      "grad_norm": 0.07475821673870087,
      "learning_rate": 3.691955993713388e-06,
      "loss": 0.3463,
      "step": 57070
    },
    {
      "epoch": 12.233176168024004,
      "grad_norm": 0.061294566839933395,
      "learning_rate": 3.6890984426346628e-06,
      "loss": 0.1411,
      "step": 57080
    },
    {
      "epoch": 12.235319331333047,
      "grad_norm": 0.19368620216846466,
      "learning_rate": 3.6862408915559367e-06,
      "loss": 0.8066,
      "step": 57090
    },
    {
      "epoch": 12.237462494642092,
      "grad_norm": 33.94865798950195,
      "learning_rate": 3.683383340477211e-06,
      "loss": 0.6728,
      "step": 57100
    },
    {
      "epoch": 12.239605657951136,
      "grad_norm": 0.2708829641342163,
      "learning_rate": 3.680525789398486e-06,
      "loss": 0.4024,
      "step": 57110
    },
    {
      "epoch": 12.24174882126018,
      "grad_norm": 19.40520668029785,
      "learning_rate": 3.6776682383197606e-06,
      "loss": 0.1197,
      "step": 57120
    },
    {
      "epoch": 12.243891984569224,
      "grad_norm": 63.745399475097656,
      "learning_rate": 3.674810687241035e-06,
      "loss": 0.8646,
      "step": 57130
    },
    {
      "epoch": 12.246035147878269,
      "grad_norm": 0.023975493386387825,
      "learning_rate": 3.671953136162309e-06,
      "loss": 0.2368,
      "step": 57140
    },
    {
      "epoch": 12.248178311187312,
      "grad_norm": 0.3365112841129303,
      "learning_rate": 3.6690955850835836e-06,
      "loss": 0.2497,
      "step": 57150
    },
    {
      "epoch": 12.250321474496356,
      "grad_norm": 0.08673137426376343,
      "learning_rate": 3.666238034004858e-06,
      "loss": 0.4361,
      "step": 57160
    },
    {
      "epoch": 12.252464637805401,
      "grad_norm": 0.10668640583753586,
      "learning_rate": 3.6633804829261328e-06,
      "loss": 0.6811,
      "step": 57170
    },
    {
      "epoch": 12.254607801114444,
      "grad_norm": 40.425540924072266,
      "learning_rate": 3.660522931847407e-06,
      "loss": 0.6215,
      "step": 57180
    },
    {
      "epoch": 12.256750964423489,
      "grad_norm": 0.051265984773635864,
      "learning_rate": 3.6576653807686815e-06,
      "loss": 0.4699,
      "step": 57190
    },
    {
      "epoch": 12.258894127732534,
      "grad_norm": 0.4323429465293884,
      "learning_rate": 3.654807829689956e-06,
      "loss": 0.389,
      "step": 57200
    },
    {
      "epoch": 12.261037291041577,
      "grad_norm": 0.11742199957370758,
      "learning_rate": 3.6519502786112306e-06,
      "loss": 0.164,
      "step": 57210
    },
    {
      "epoch": 12.263180454350621,
      "grad_norm": 0.23036690056324005,
      "learning_rate": 3.649092727532505e-06,
      "loss": 0.1517,
      "step": 57220
    },
    {
      "epoch": 12.265323617659666,
      "grad_norm": 0.13898660242557526,
      "learning_rate": 3.6462351764537797e-06,
      "loss": 0.315,
      "step": 57230
    },
    {
      "epoch": 12.267466780968709,
      "grad_norm": 0.03120397962629795,
      "learning_rate": 3.643377625375054e-06,
      "loss": 0.5036,
      "step": 57240
    },
    {
      "epoch": 12.269609944277754,
      "grad_norm": 0.6044878363609314,
      "learning_rate": 3.640520074296328e-06,
      "loss": 0.7562,
      "step": 57250
    },
    {
      "epoch": 12.271753107586798,
      "grad_norm": 0.31253236532211304,
      "learning_rate": 3.6376625232176028e-06,
      "loss": 0.1584,
      "step": 57260
    },
    {
      "epoch": 12.273896270895841,
      "grad_norm": 0.03684011846780777,
      "learning_rate": 3.634804972138877e-06,
      "loss": 0.2776,
      "step": 57270
    },
    {
      "epoch": 12.276039434204886,
      "grad_norm": 0.03852830454707146,
      "learning_rate": 3.631947421060152e-06,
      "loss": 0.6688,
      "step": 57280
    },
    {
      "epoch": 12.278182597513931,
      "grad_norm": 0.33974459767341614,
      "learning_rate": 3.6290898699814263e-06,
      "loss": 0.0034,
      "step": 57290
    },
    {
      "epoch": 12.280325760822974,
      "grad_norm": 0.12035339325666428,
      "learning_rate": 3.6262323189027006e-06,
      "loss": 0.6373,
      "step": 57300
    },
    {
      "epoch": 12.282468924132019,
      "grad_norm": 19.656755447387695,
      "learning_rate": 3.623374767823975e-06,
      "loss": 0.4269,
      "step": 57310
    },
    {
      "epoch": 12.284612087441063,
      "grad_norm": 0.4001055359840393,
      "learning_rate": 3.6205172167452497e-06,
      "loss": 0.5631,
      "step": 57320
    },
    {
      "epoch": 12.286755250750106,
      "grad_norm": 0.09623801708221436,
      "learning_rate": 3.617659665666524e-06,
      "loss": 0.4813,
      "step": 57330
    },
    {
      "epoch": 12.288898414059151,
      "grad_norm": 29.838712692260742,
      "learning_rate": 3.614802114587799e-06,
      "loss": 0.8241,
      "step": 57340
    },
    {
      "epoch": 12.291041577368196,
      "grad_norm": 0.03886107727885246,
      "learning_rate": 3.611944563509073e-06,
      "loss": 0.8023,
      "step": 57350
    },
    {
      "epoch": 12.293184740677239,
      "grad_norm": 0.33600524067878723,
      "learning_rate": 3.609087012430347e-06,
      "loss": 0.4037,
      "step": 57360
    },
    {
      "epoch": 12.295327903986284,
      "grad_norm": 0.16484485566616058,
      "learning_rate": 3.606229461351622e-06,
      "loss": 0.8595,
      "step": 57370
    },
    {
      "epoch": 12.297471067295328,
      "grad_norm": 124.77859497070312,
      "learning_rate": 3.6033719102728963e-06,
      "loss": 0.1131,
      "step": 57380
    },
    {
      "epoch": 12.299614230604371,
      "grad_norm": 0.04509524255990982,
      "learning_rate": 3.600514359194171e-06,
      "loss": 0.1677,
      "step": 57390
    },
    {
      "epoch": 12.301757393913416,
      "grad_norm": 41.537757873535156,
      "learning_rate": 3.5976568081154454e-06,
      "loss": 0.6893,
      "step": 57400
    },
    {
      "epoch": 12.30390055722246,
      "grad_norm": 0.6494842767715454,
      "learning_rate": 3.5947992570367198e-06,
      "loss": 0.48,
      "step": 57410
    },
    {
      "epoch": 12.306043720531505,
      "grad_norm": 0.4990748465061188,
      "learning_rate": 3.591941705957994e-06,
      "loss": 0.2905,
      "step": 57420
    },
    {
      "epoch": 12.308186883840548,
      "grad_norm": 0.29316991567611694,
      "learning_rate": 3.589084154879269e-06,
      "loss": 0.4385,
      "step": 57430
    },
    {
      "epoch": 12.310330047149593,
      "grad_norm": 0.28360646963119507,
      "learning_rate": 3.5862266038005432e-06,
      "loss": 0.526,
      "step": 57440
    },
    {
      "epoch": 12.312473210458638,
      "grad_norm": 0.2568691372871399,
      "learning_rate": 3.583369052721818e-06,
      "loss": 0.3445,
      "step": 57450
    },
    {
      "epoch": 12.31461637376768,
      "grad_norm": 0.18886125087738037,
      "learning_rate": 3.580511501643092e-06,
      "loss": 0.1618,
      "step": 57460
    },
    {
      "epoch": 12.316759537076726,
      "grad_norm": 22.180150985717773,
      "learning_rate": 3.5776539505643663e-06,
      "loss": 0.6346,
      "step": 57470
    },
    {
      "epoch": 12.31890270038577,
      "grad_norm": 0.2569485008716583,
      "learning_rate": 3.574796399485641e-06,
      "loss": 0.1587,
      "step": 57480
    },
    {
      "epoch": 12.321045863694813,
      "grad_norm": 0.14877855777740479,
      "learning_rate": 3.5719388484069154e-06,
      "loss": 0.1692,
      "step": 57490
    },
    {
      "epoch": 12.323189027003858,
      "grad_norm": 24.39802360534668,
      "learning_rate": 3.56908129732819e-06,
      "loss": 0.7638,
      "step": 57500
    },
    {
      "epoch": 12.325332190312903,
      "grad_norm": 0.42355969548225403,
      "learning_rate": 3.566223746249465e-06,
      "loss": 0.2408,
      "step": 57510
    },
    {
      "epoch": 12.327475353621946,
      "grad_norm": 0.043259382247924805,
      "learning_rate": 3.563366195170739e-06,
      "loss": 0.2431,
      "step": 57520
    },
    {
      "epoch": 12.32961851693099,
      "grad_norm": 1.0661983489990234,
      "learning_rate": 3.5605086440920133e-06,
      "loss": 0.738,
      "step": 57530
    },
    {
      "epoch": 12.331761680240035,
      "grad_norm": 0.02439565397799015,
      "learning_rate": 3.557651093013288e-06,
      "loss": 0.1254,
      "step": 57540
    },
    {
      "epoch": 12.333904843549078,
      "grad_norm": 20.14459800720215,
      "learning_rate": 3.5547935419345624e-06,
      "loss": 0.8053,
      "step": 57550
    },
    {
      "epoch": 12.336048006858123,
      "grad_norm": 0.15943770110607147,
      "learning_rate": 3.551935990855837e-06,
      "loss": 0.2348,
      "step": 57560
    },
    {
      "epoch": 12.338191170167168,
      "grad_norm": 0.9320935010910034,
      "learning_rate": 3.549078439777111e-06,
      "loss": 0.4395,
      "step": 57570
    },
    {
      "epoch": 12.34033433347621,
      "grad_norm": 346.2384033203125,
      "learning_rate": 3.546220888698386e-06,
      "loss": 0.4736,
      "step": 57580
    },
    {
      "epoch": 12.342477496785255,
      "grad_norm": 4.126060962677002,
      "learning_rate": 3.54336333761966e-06,
      "loss": 0.2361,
      "step": 57590
    },
    {
      "epoch": 12.3446206600943,
      "grad_norm": 0.024694398045539856,
      "learning_rate": 3.540505786540935e-06,
      "loss": 0.6705,
      "step": 57600
    },
    {
      "epoch": 12.346763823403343,
      "grad_norm": 0.4088965654373169,
      "learning_rate": 3.5376482354622093e-06,
      "loss": 1.0917,
      "step": 57610
    },
    {
      "epoch": 12.348906986712388,
      "grad_norm": 0.7327161431312561,
      "learning_rate": 3.5347906843834833e-06,
      "loss": 0.2616,
      "step": 57620
    },
    {
      "epoch": 12.351050150021432,
      "grad_norm": 23.848365783691406,
      "learning_rate": 3.531933133304758e-06,
      "loss": 0.3669,
      "step": 57630
    },
    {
      "epoch": 12.353193313330475,
      "grad_norm": 0.37268564105033875,
      "learning_rate": 3.5290755822260324e-06,
      "loss": 0.2411,
      "step": 57640
    },
    {
      "epoch": 12.35533647663952,
      "grad_norm": 17.158206939697266,
      "learning_rate": 3.526218031147307e-06,
      "loss": 0.2877,
      "step": 57650
    },
    {
      "epoch": 12.357479639948565,
      "grad_norm": 0.09235431998968124,
      "learning_rate": 3.5233604800685815e-06,
      "loss": 0.1511,
      "step": 57660
    },
    {
      "epoch": 12.359622803257608,
      "grad_norm": 0.031583432108163834,
      "learning_rate": 3.5205029289898563e-06,
      "loss": 0.002,
      "step": 57670
    },
    {
      "epoch": 12.361765966566653,
      "grad_norm": 0.011908281594514847,
      "learning_rate": 3.5176453779111302e-06,
      "loss": 0.7418,
      "step": 57680
    },
    {
      "epoch": 12.363909129875697,
      "grad_norm": 0.22688506543636322,
      "learning_rate": 3.514787826832405e-06,
      "loss": 0.1478,
      "step": 57690
    },
    {
      "epoch": 12.36605229318474,
      "grad_norm": 0.05241389572620392,
      "learning_rate": 3.5119302757536793e-06,
      "loss": 0.6637,
      "step": 57700
    },
    {
      "epoch": 12.368195456493785,
      "grad_norm": 36.414337158203125,
      "learning_rate": 3.509072724674954e-06,
      "loss": 0.7102,
      "step": 57710
    },
    {
      "epoch": 12.37033861980283,
      "grad_norm": 0.08078869432210922,
      "learning_rate": 3.5062151735962285e-06,
      "loss": 0.2686,
      "step": 57720
    },
    {
      "epoch": 12.372481783111873,
      "grad_norm": 0.8707069158554077,
      "learning_rate": 3.5033576225175024e-06,
      "loss": 0.3068,
      "step": 57730
    },
    {
      "epoch": 12.374624946420917,
      "grad_norm": 0.9596691131591797,
      "learning_rate": 3.500500071438777e-06,
      "loss": 0.2388,
      "step": 57740
    },
    {
      "epoch": 12.376768109729962,
      "grad_norm": 0.03009878098964691,
      "learning_rate": 3.4976425203600515e-06,
      "loss": 0.3996,
      "step": 57750
    },
    {
      "epoch": 12.378911273039005,
      "grad_norm": 0.23845724761486053,
      "learning_rate": 3.4947849692813263e-06,
      "loss": 0.4822,
      "step": 57760
    },
    {
      "epoch": 12.38105443634805,
      "grad_norm": 2.957549810409546,
      "learning_rate": 3.4919274182026007e-06,
      "loss": 0.006,
      "step": 57770
    },
    {
      "epoch": 12.383197599657095,
      "grad_norm": 17.559083938598633,
      "learning_rate": 3.489069867123875e-06,
      "loss": 0.5773,
      "step": 57780
    },
    {
      "epoch": 12.385340762966138,
      "grad_norm": 0.29022035002708435,
      "learning_rate": 3.4862123160451494e-06,
      "loss": 0.4687,
      "step": 57790
    },
    {
      "epoch": 12.387483926275182,
      "grad_norm": 2.138335943222046,
      "learning_rate": 3.483354764966424e-06,
      "loss": 0.1371,
      "step": 57800
    },
    {
      "epoch": 12.389627089584227,
      "grad_norm": 0.35211658477783203,
      "learning_rate": 3.4804972138876985e-06,
      "loss": 0.3462,
      "step": 57810
    },
    {
      "epoch": 12.39177025289327,
      "grad_norm": 0.01955842785537243,
      "learning_rate": 3.4776396628089733e-06,
      "loss": 0.0065,
      "step": 57820
    },
    {
      "epoch": 12.393913416202315,
      "grad_norm": 0.016624515876173973,
      "learning_rate": 3.4747821117302476e-06,
      "loss": 0.2546,
      "step": 57830
    },
    {
      "epoch": 12.39605657951136,
      "grad_norm": 19.709745407104492,
      "learning_rate": 3.4719245606515215e-06,
      "loss": 0.6201,
      "step": 57840
    },
    {
      "epoch": 12.398199742820402,
      "grad_norm": 0.39130645990371704,
      "learning_rate": 3.4690670095727963e-06,
      "loss": 0.2305,
      "step": 57850
    },
    {
      "epoch": 12.400342906129447,
      "grad_norm": 25.390871047973633,
      "learning_rate": 3.4662094584940707e-06,
      "loss": 0.2442,
      "step": 57860
    },
    {
      "epoch": 12.402486069438492,
      "grad_norm": 0.027424335479736328,
      "learning_rate": 3.4633519074153454e-06,
      "loss": 0.4609,
      "step": 57870
    },
    {
      "epoch": 12.404629232747535,
      "grad_norm": 0.026328228414058685,
      "learning_rate": 3.46049435633662e-06,
      "loss": 0.5064,
      "step": 57880
    },
    {
      "epoch": 12.40677239605658,
      "grad_norm": 2.1583380699157715,
      "learning_rate": 3.457636805257894e-06,
      "loss": 0.1439,
      "step": 57890
    },
    {
      "epoch": 12.408915559365624,
      "grad_norm": 0.0774306133389473,
      "learning_rate": 3.4547792541791685e-06,
      "loss": 0.8975,
      "step": 57900
    },
    {
      "epoch": 12.411058722674667,
      "grad_norm": 0.1390916109085083,
      "learning_rate": 3.4519217031004433e-06,
      "loss": 0.8219,
      "step": 57910
    },
    {
      "epoch": 12.413201885983712,
      "grad_norm": 0.1208345964550972,
      "learning_rate": 3.4490641520217176e-06,
      "loss": 0.6042,
      "step": 57920
    },
    {
      "epoch": 12.415345049292757,
      "grad_norm": 50.12019348144531,
      "learning_rate": 3.4462066009429924e-06,
      "loss": 0.554,
      "step": 57930
    },
    {
      "epoch": 12.4174882126018,
      "grad_norm": 0.8811611533164978,
      "learning_rate": 3.4433490498642668e-06,
      "loss": 0.3345,
      "step": 57940
    },
    {
      "epoch": 12.419631375910845,
      "grad_norm": 18.948383331298828,
      "learning_rate": 3.4404914987855407e-06,
      "loss": 0.3617,
      "step": 57950
    },
    {
      "epoch": 12.42177453921989,
      "grad_norm": 0.3103315830230713,
      "learning_rate": 3.4376339477068155e-06,
      "loss": 0.3933,
      "step": 57960
    },
    {
      "epoch": 12.423917702528932,
      "grad_norm": 0.07001059502363205,
      "learning_rate": 3.4347763966280902e-06,
      "loss": 0.2218,
      "step": 57970
    },
    {
      "epoch": 12.426060865837977,
      "grad_norm": 0.9512147903442383,
      "learning_rate": 3.4319188455493646e-06,
      "loss": 0.6699,
      "step": 57980
    },
    {
      "epoch": 12.428204029147022,
      "grad_norm": 17.393592834472656,
      "learning_rate": 3.4290612944706394e-06,
      "loss": 0.5486,
      "step": 57990
    },
    {
      "epoch": 12.430347192456065,
      "grad_norm": 0.7696019411087036,
      "learning_rate": 3.4262037433919133e-06,
      "loss": 0.421,
      "step": 58000
    },
    {
      "epoch": 12.43249035576511,
      "grad_norm": 0.5169152617454529,
      "learning_rate": 3.4233461923131876e-06,
      "loss": 0.0044,
      "step": 58010
    },
    {
      "epoch": 12.434633519074154,
      "grad_norm": 1.5923925638198853,
      "learning_rate": 3.4204886412344624e-06,
      "loss": 0.3797,
      "step": 58020
    },
    {
      "epoch": 12.436776682383197,
      "grad_norm": 0.012346478179097176,
      "learning_rate": 3.4176310901557368e-06,
      "loss": 0.3257,
      "step": 58030
    },
    {
      "epoch": 12.438919845692242,
      "grad_norm": 0.32576867938041687,
      "learning_rate": 3.4147735390770115e-06,
      "loss": 0.8453,
      "step": 58040
    },
    {
      "epoch": 12.441063009001287,
      "grad_norm": 0.30334603786468506,
      "learning_rate": 3.4119159879982855e-06,
      "loss": 0.1566,
      "step": 58050
    },
    {
      "epoch": 12.44320617231033,
      "grad_norm": 678.1458740234375,
      "learning_rate": 3.4090584369195602e-06,
      "loss": 0.1999,
      "step": 58060
    },
    {
      "epoch": 12.445349335619374,
      "grad_norm": 35.22085189819336,
      "learning_rate": 3.4062008858408346e-06,
      "loss": 0.7904,
      "step": 58070
    },
    {
      "epoch": 12.447492498928419,
      "grad_norm": 0.6733900904655457,
      "learning_rate": 3.4033433347621094e-06,
      "loss": 0.5985,
      "step": 58080
    },
    {
      "epoch": 12.449635662237462,
      "grad_norm": 0.936693012714386,
      "learning_rate": 3.4004857836833837e-06,
      "loss": 0.3,
      "step": 58090
    },
    {
      "epoch": 12.451778825546507,
      "grad_norm": 2.0920846462249756,
      "learning_rate": 3.3976282326046585e-06,
      "loss": 0.1617,
      "step": 58100
    },
    {
      "epoch": 12.453921988855551,
      "grad_norm": 37.030792236328125,
      "learning_rate": 3.3947706815259324e-06,
      "loss": 0.6349,
      "step": 58110
    },
    {
      "epoch": 12.456065152164594,
      "grad_norm": 0.2202831655740738,
      "learning_rate": 3.3919131304472068e-06,
      "loss": 0.3644,
      "step": 58120
    },
    {
      "epoch": 12.45820831547364,
      "grad_norm": 0.543205201625824,
      "learning_rate": 3.3890555793684816e-06,
      "loss": 0.3609,
      "step": 58130
    },
    {
      "epoch": 12.460351478782684,
      "grad_norm": 0.06221945583820343,
      "learning_rate": 3.386198028289756e-06,
      "loss": 0.1906,
      "step": 58140
    },
    {
      "epoch": 12.462494642091727,
      "grad_norm": 1.0586260557174683,
      "learning_rate": 3.3833404772110307e-06,
      "loss": 0.2742,
      "step": 58150
    },
    {
      "epoch": 12.464637805400772,
      "grad_norm": 19.822742462158203,
      "learning_rate": 3.3804829261323046e-06,
      "loss": 0.498,
      "step": 58160
    },
    {
      "epoch": 12.466780968709816,
      "grad_norm": 0.33303701877593994,
      "learning_rate": 3.3776253750535794e-06,
      "loss": 0.225,
      "step": 58170
    },
    {
      "epoch": 12.46892413201886,
      "grad_norm": 0.09401293098926544,
      "learning_rate": 3.3747678239748537e-06,
      "loss": 0.3804,
      "step": 58180
    },
    {
      "epoch": 12.471067295327904,
      "grad_norm": 0.2173878252506256,
      "learning_rate": 3.3719102728961285e-06,
      "loss": 0.3958,
      "step": 58190
    },
    {
      "epoch": 12.473210458636949,
      "grad_norm": 0.47711995244026184,
      "learning_rate": 3.369052721817403e-06,
      "loss": 0.6572,
      "step": 58200
    },
    {
      "epoch": 12.475353621945992,
      "grad_norm": 0.036758825182914734,
      "learning_rate": 3.366195170738677e-06,
      "loss": 0.1107,
      "step": 58210
    },
    {
      "epoch": 12.477496785255036,
      "grad_norm": 0.016215577721595764,
      "learning_rate": 3.3633376196599516e-06,
      "loss": 0.0749,
      "step": 58220
    },
    {
      "epoch": 12.479639948564081,
      "grad_norm": 0.019887352362275124,
      "learning_rate": 3.360480068581226e-06,
      "loss": 0.4103,
      "step": 58230
    },
    {
      "epoch": 12.481783111873124,
      "grad_norm": 0.17552098631858826,
      "learning_rate": 3.3576225175025007e-06,
      "loss": 0.5295,
      "step": 58240
    },
    {
      "epoch": 12.483926275182169,
      "grad_norm": 156.23143005371094,
      "learning_rate": 3.354764966423775e-06,
      "loss": 0.4127,
      "step": 58250
    },
    {
      "epoch": 12.486069438491214,
      "grad_norm": 0.04184333235025406,
      "learning_rate": 3.35190741534505e-06,
      "loss": 0.2635,
      "step": 58260
    },
    {
      "epoch": 12.488212601800257,
      "grad_norm": 0.30108770728111267,
      "learning_rate": 3.3490498642663238e-06,
      "loss": 0.4154,
      "step": 58270
    },
    {
      "epoch": 12.490355765109301,
      "grad_norm": 0.05074639990925789,
      "learning_rate": 3.3461923131875985e-06,
      "loss": 0.2567,
      "step": 58280
    },
    {
      "epoch": 12.492498928418346,
      "grad_norm": 0.01560735609382391,
      "learning_rate": 3.343334762108873e-06,
      "loss": 0.0023,
      "step": 58290
    },
    {
      "epoch": 12.494642091727389,
      "grad_norm": 0.6314940452575684,
      "learning_rate": 3.3404772110301477e-06,
      "loss": 0.5661,
      "step": 58300
    },
    {
      "epoch": 12.496785255036434,
      "grad_norm": 0.031414322555065155,
      "learning_rate": 3.337619659951422e-06,
      "loss": 0.4113,
      "step": 58310
    },
    {
      "epoch": 12.498928418345479,
      "grad_norm": 0.2288871705532074,
      "learning_rate": 3.334762108872696e-06,
      "loss": 0.1592,
      "step": 58320
    },
    {
      "epoch": 12.501071581654521,
      "grad_norm": 0.0649477168917656,
      "learning_rate": 3.3319045577939707e-06,
      "loss": 0.3834,
      "step": 58330
    },
    {
      "epoch": 12.503214744963566,
      "grad_norm": 0.14855492115020752,
      "learning_rate": 3.3290470067152455e-06,
      "loss": 0.0033,
      "step": 58340
    },
    {
      "epoch": 12.505357908272611,
      "grad_norm": 0.04204230755567551,
      "learning_rate": 3.32618945563652e-06,
      "loss": 0.4085,
      "step": 58350
    },
    {
      "epoch": 12.507501071581654,
      "grad_norm": 21.454944610595703,
      "learning_rate": 3.3233319045577946e-06,
      "loss": 0.2416,
      "step": 58360
    },
    {
      "epoch": 12.509644234890699,
      "grad_norm": 0.09963316470384598,
      "learning_rate": 3.3204743534790685e-06,
      "loss": 0.1731,
      "step": 58370
    },
    {
      "epoch": 12.511787398199743,
      "grad_norm": 0.14691467583179474,
      "learning_rate": 3.317616802400343e-06,
      "loss": 0.7979,
      "step": 58380
    },
    {
      "epoch": 12.513930561508786,
      "grad_norm": 91.43035125732422,
      "learning_rate": 3.3147592513216177e-06,
      "loss": 0.32,
      "step": 58390
    },
    {
      "epoch": 12.516073724817831,
      "grad_norm": 48.015289306640625,
      "learning_rate": 3.311901700242892e-06,
      "loss": 0.5056,
      "step": 58400
    },
    {
      "epoch": 12.518216888126876,
      "grad_norm": 0.2569895386695862,
      "learning_rate": 3.309044149164167e-06,
      "loss": 0.1775,
      "step": 58410
    },
    {
      "epoch": 12.520360051435919,
      "grad_norm": 0.05768226087093353,
      "learning_rate": 3.306186598085441e-06,
      "loss": 0.434,
      "step": 58420
    },
    {
      "epoch": 12.522503214744964,
      "grad_norm": 1.7732360363006592,
      "learning_rate": 3.3033290470067155e-06,
      "loss": 0.2985,
      "step": 58430
    },
    {
      "epoch": 12.524646378054008,
      "grad_norm": 0.8637948036193848,
      "learning_rate": 3.30047149592799e-06,
      "loss": 0.0023,
      "step": 58440
    },
    {
      "epoch": 12.526789541363051,
      "grad_norm": 89.12242126464844,
      "learning_rate": 3.2976139448492646e-06,
      "loss": 0.2938,
      "step": 58450
    },
    {
      "epoch": 12.528932704672096,
      "grad_norm": 68.51268005371094,
      "learning_rate": 3.294756393770539e-06,
      "loss": 0.4291,
      "step": 58460
    },
    {
      "epoch": 12.53107586798114,
      "grad_norm": 39.605621337890625,
      "learning_rate": 3.2918988426918138e-06,
      "loss": 0.369,
      "step": 58470
    },
    {
      "epoch": 12.533219031290184,
      "grad_norm": 0.027262113988399506,
      "learning_rate": 3.2890412916130877e-06,
      "loss": 0.2491,
      "step": 58480
    },
    {
      "epoch": 12.535362194599228,
      "grad_norm": 0.1375046670436859,
      "learning_rate": 3.286183740534362e-06,
      "loss": 0.2563,
      "step": 58490
    },
    {
      "epoch": 12.537505357908273,
      "grad_norm": 0.19091597199440002,
      "learning_rate": 3.283326189455637e-06,
      "loss": 0.8302,
      "step": 58500
    },
    {
      "epoch": 12.539648521217316,
      "grad_norm": 0.05079244449734688,
      "learning_rate": 3.280468638376911e-06,
      "loss": 0.0027,
      "step": 58510
    },
    {
      "epoch": 12.54179168452636,
      "grad_norm": 2.718334197998047,
      "learning_rate": 3.277611087298186e-06,
      "loss": 0.1358,
      "step": 58520
    },
    {
      "epoch": 12.543934847835406,
      "grad_norm": 0.15542283654212952,
      "learning_rate": 3.2747535362194603e-06,
      "loss": 0.4868,
      "step": 58530
    },
    {
      "epoch": 12.546078011144449,
      "grad_norm": 0.06586849689483643,
      "learning_rate": 3.2718959851407346e-06,
      "loss": 0.1266,
      "step": 58540
    },
    {
      "epoch": 12.548221174453493,
      "grad_norm": 0.122368223965168,
      "learning_rate": 3.269038434062009e-06,
      "loss": 0.726,
      "step": 58550
    },
    {
      "epoch": 12.550364337762538,
      "grad_norm": 0.1723128855228424,
      "learning_rate": 3.2661808829832838e-06,
      "loss": 0.2494,
      "step": 58560
    },
    {
      "epoch": 12.552507501071581,
      "grad_norm": 0.1353921741247177,
      "learning_rate": 3.263323331904558e-06,
      "loss": 0.3914,
      "step": 58570
    },
    {
      "epoch": 12.554650664380626,
      "grad_norm": 21.641462326049805,
      "learning_rate": 3.260465780825833e-06,
      "loss": 0.4616,
      "step": 58580
    },
    {
      "epoch": 12.55679382768967,
      "grad_norm": 0.03692149743437767,
      "learning_rate": 3.257608229747107e-06,
      "loss": 0.4762,
      "step": 58590
    },
    {
      "epoch": 12.558936990998713,
      "grad_norm": 0.689660906791687,
      "learning_rate": 3.254750678668381e-06,
      "loss": 0.1499,
      "step": 58600
    },
    {
      "epoch": 12.561080154307758,
      "grad_norm": 39.38768005371094,
      "learning_rate": 3.251893127589656e-06,
      "loss": 0.3328,
      "step": 58610
    },
    {
      "epoch": 12.563223317616803,
      "grad_norm": 0.04957792907953262,
      "learning_rate": 3.2490355765109303e-06,
      "loss": 0.205,
      "step": 58620
    },
    {
      "epoch": 12.565366480925846,
      "grad_norm": 0.07110143452882767,
      "learning_rate": 3.246178025432205e-06,
      "loss": 0.3225,
      "step": 58630
    },
    {
      "epoch": 12.56750964423489,
      "grad_norm": 0.21483050286769867,
      "learning_rate": 3.243320474353479e-06,
      "loss": 0.658,
      "step": 58640
    },
    {
      "epoch": 12.569652807543935,
      "grad_norm": 0.20334894955158234,
      "learning_rate": 3.2404629232747538e-06,
      "loss": 0.1523,
      "step": 58650
    },
    {
      "epoch": 12.571795970852978,
      "grad_norm": 0.18944238126277924,
      "learning_rate": 3.237605372196028e-06,
      "loss": 0.0122,
      "step": 58660
    },
    {
      "epoch": 12.573939134162023,
      "grad_norm": 0.014472133480012417,
      "learning_rate": 3.234747821117303e-06,
      "loss": 0.2677,
      "step": 58670
    },
    {
      "epoch": 12.576082297471068,
      "grad_norm": 0.05429670959711075,
      "learning_rate": 3.2318902700385773e-06,
      "loss": 0.2422,
      "step": 58680
    },
    {
      "epoch": 12.57822546078011,
      "grad_norm": 41.53065490722656,
      "learning_rate": 3.229032718959852e-06,
      "loss": 0.5279,
      "step": 58690
    },
    {
      "epoch": 12.580368624089155,
      "grad_norm": 0.3053058087825775,
      "learning_rate": 3.226175167881126e-06,
      "loss": 1.1495,
      "step": 58700
    },
    {
      "epoch": 12.5825117873982,
      "grad_norm": 0.3379322588443756,
      "learning_rate": 3.2233176168024003e-06,
      "loss": 0.3126,
      "step": 58710
    },
    {
      "epoch": 12.584654950707243,
      "grad_norm": 0.2068004310131073,
      "learning_rate": 3.220460065723675e-06,
      "loss": 0.1701,
      "step": 58720
    },
    {
      "epoch": 12.586798114016288,
      "grad_norm": 0.14288270473480225,
      "learning_rate": 3.21760251464495e-06,
      "loss": 0.2502,
      "step": 58730
    },
    {
      "epoch": 12.588941277325333,
      "grad_norm": 0.26233989000320435,
      "learning_rate": 3.2147449635662242e-06,
      "loss": 0.4007,
      "step": 58740
    },
    {
      "epoch": 12.591084440634376,
      "grad_norm": 0.1715548187494278,
      "learning_rate": 3.211887412487498e-06,
      "loss": 0.2673,
      "step": 58750
    },
    {
      "epoch": 12.59322760394342,
      "grad_norm": 0.13944080471992493,
      "learning_rate": 3.209029861408773e-06,
      "loss": 0.0047,
      "step": 58760
    },
    {
      "epoch": 12.595370767252465,
      "grad_norm": 0.03502575680613518,
      "learning_rate": 3.2061723103300473e-06,
      "loss": 0.3011,
      "step": 58770
    },
    {
      "epoch": 12.597513930561508,
      "grad_norm": 0.0630551353096962,
      "learning_rate": 3.203314759251322e-06,
      "loss": 0.5441,
      "step": 58780
    },
    {
      "epoch": 12.599657093870553,
      "grad_norm": 0.828948974609375,
      "learning_rate": 3.2004572081725964e-06,
      "loss": 0.3397,
      "step": 58790
    },
    {
      "epoch": 12.601800257179598,
      "grad_norm": 0.049840476363897324,
      "learning_rate": 3.1975996570938708e-06,
      "loss": 0.1452,
      "step": 58800
    },
    {
      "epoch": 12.60394342048864,
      "grad_norm": 0.38549190759658813,
      "learning_rate": 3.194742106015145e-06,
      "loss": 0.6129,
      "step": 58810
    },
    {
      "epoch": 12.606086583797685,
      "grad_norm": 0.17176032066345215,
      "learning_rate": 3.19188455493642e-06,
      "loss": 0.4474,
      "step": 58820
    },
    {
      "epoch": 12.60822974710673,
      "grad_norm": 19.392446517944336,
      "learning_rate": 3.1890270038576942e-06,
      "loss": 0.2441,
      "step": 58830
    },
    {
      "epoch": 12.610372910415773,
      "grad_norm": 0.08003075420856476,
      "learning_rate": 3.186169452778969e-06,
      "loss": 0.4099,
      "step": 58840
    },
    {
      "epoch": 12.612516073724818,
      "grad_norm": 46.88328170776367,
      "learning_rate": 3.1833119017002434e-06,
      "loss": 0.4577,
      "step": 58850
    },
    {
      "epoch": 12.614659237033862,
      "grad_norm": 0.026301192119717598,
      "learning_rate": 3.1804543506215173e-06,
      "loss": 0.1579,
      "step": 58860
    },
    {
      "epoch": 12.616802400342905,
      "grad_norm": 0.027873070910573006,
      "learning_rate": 3.177596799542792e-06,
      "loss": 0.3811,
      "step": 58870
    },
    {
      "epoch": 12.61894556365195,
      "grad_norm": 28.69539451599121,
      "learning_rate": 3.1747392484640664e-06,
      "loss": 0.3259,
      "step": 58880
    },
    {
      "epoch": 12.621088726960995,
      "grad_norm": 48.87506103515625,
      "learning_rate": 3.171881697385341e-06,
      "loss": 0.4377,
      "step": 58890
    },
    {
      "epoch": 12.623231890270038,
      "grad_norm": 0.07835812121629715,
      "learning_rate": 3.1690241463066155e-06,
      "loss": 0.4398,
      "step": 58900
    },
    {
      "epoch": 12.625375053579083,
      "grad_norm": 0.21549390256404877,
      "learning_rate": 3.16616659522789e-06,
      "loss": 0.1586,
      "step": 58910
    },
    {
      "epoch": 12.627518216888127,
      "grad_norm": 0.1749516874551773,
      "learning_rate": 3.1633090441491642e-06,
      "loss": 0.8796,
      "step": 58920
    },
    {
      "epoch": 12.62966138019717,
      "grad_norm": 0.0588412769138813,
      "learning_rate": 3.160451493070439e-06,
      "loss": 0.3038,
      "step": 58930
    },
    {
      "epoch": 12.631804543506215,
      "grad_norm": 61.4964485168457,
      "learning_rate": 3.1575939419917134e-06,
      "loss": 0.2939,
      "step": 58940
    },
    {
      "epoch": 12.63394770681526,
      "grad_norm": 0.18578609824180603,
      "learning_rate": 3.154736390912988e-06,
      "loss": 0.4109,
      "step": 58950
    },
    {
      "epoch": 12.636090870124303,
      "grad_norm": 0.22816675901412964,
      "learning_rate": 3.1518788398342625e-06,
      "loss": 0.0888,
      "step": 58960
    },
    {
      "epoch": 12.638234033433347,
      "grad_norm": 4.228776454925537,
      "learning_rate": 3.1490212887555364e-06,
      "loss": 0.2792,
      "step": 58970
    },
    {
      "epoch": 12.640377196742392,
      "grad_norm": 20.26662826538086,
      "learning_rate": 3.146163737676811e-06,
      "loss": 0.5081,
      "step": 58980
    },
    {
      "epoch": 12.642520360051435,
      "grad_norm": 36.75878143310547,
      "learning_rate": 3.1433061865980856e-06,
      "loss": 0.3648,
      "step": 58990
    },
    {
      "epoch": 12.64466352336048,
      "grad_norm": 0.02860965207219124,
      "learning_rate": 3.1404486355193603e-06,
      "loss": 0.9092,
      "step": 59000
    },
    {
      "epoch": 12.646806686669525,
      "grad_norm": 48.29483413696289,
      "learning_rate": 3.1375910844406347e-06,
      "loss": 0.4515,
      "step": 59010
    },
    {
      "epoch": 12.648949849978568,
      "grad_norm": 0.2675539255142212,
      "learning_rate": 3.134733533361909e-06,
      "loss": 0.2629,
      "step": 59020
    },
    {
      "epoch": 12.651093013287612,
      "grad_norm": 0.09410656988620758,
      "learning_rate": 3.1318759822831834e-06,
      "loss": 0.293,
      "step": 59030
    },
    {
      "epoch": 12.653236176596657,
      "grad_norm": 0.5307469964027405,
      "learning_rate": 3.129018431204458e-06,
      "loss": 0.2832,
      "step": 59040
    },
    {
      "epoch": 12.6553793399057,
      "grad_norm": 0.042499568313360214,
      "learning_rate": 3.1261608801257325e-06,
      "loss": 0.5418,
      "step": 59050
    },
    {
      "epoch": 12.657522503214745,
      "grad_norm": 0.06537605822086334,
      "learning_rate": 3.1233033290470073e-06,
      "loss": 0.132,
      "step": 59060
    },
    {
      "epoch": 12.65966566652379,
      "grad_norm": 2.8783786296844482,
      "learning_rate": 3.1204457779682812e-06,
      "loss": 0.1412,
      "step": 59070
    },
    {
      "epoch": 12.661808829832832,
      "grad_norm": 0.04326793551445007,
      "learning_rate": 3.1175882268895556e-06,
      "loss": 0.2166,
      "step": 59080
    },
    {
      "epoch": 12.663951993141877,
      "grad_norm": 0.007483142428100109,
      "learning_rate": 3.1147306758108303e-06,
      "loss": 0.2796,
      "step": 59090
    },
    {
      "epoch": 12.666095156450922,
      "grad_norm": 24.051666259765625,
      "learning_rate": 3.1118731247321047e-06,
      "loss": 0.7013,
      "step": 59100
    },
    {
      "epoch": 12.668238319759965,
      "grad_norm": 0.8242531418800354,
      "learning_rate": 3.1090155736533795e-06,
      "loss": 0.189,
      "step": 59110
    },
    {
      "epoch": 12.67038148306901,
      "grad_norm": 0.12243681401014328,
      "learning_rate": 3.1061580225746542e-06,
      "loss": 0.1298,
      "step": 59120
    },
    {
      "epoch": 12.672524646378054,
      "grad_norm": 34.99349594116211,
      "learning_rate": 3.103300471495928e-06,
      "loss": 0.5149,
      "step": 59130
    },
    {
      "epoch": 12.674667809687097,
      "grad_norm": 0.02735569328069687,
      "learning_rate": 3.1004429204172025e-06,
      "loss": 0.2185,
      "step": 59140
    },
    {
      "epoch": 12.676810972996142,
      "grad_norm": 0.04260564222931862,
      "learning_rate": 3.0975853693384773e-06,
      "loss": 0.0017,
      "step": 59150
    },
    {
      "epoch": 12.678954136305187,
      "grad_norm": 0.016496822237968445,
      "learning_rate": 3.0947278182597517e-06,
      "loss": 0.1759,
      "step": 59160
    },
    {
      "epoch": 12.68109729961423,
      "grad_norm": 0.10459613800048828,
      "learning_rate": 3.0918702671810264e-06,
      "loss": 0.272,
      "step": 59170
    },
    {
      "epoch": 12.683240462923274,
      "grad_norm": 0.02095085196197033,
      "learning_rate": 3.0890127161023004e-06,
      "loss": 0.3366,
      "step": 59180
    },
    {
      "epoch": 12.68538362623232,
      "grad_norm": 0.034865446388721466,
      "learning_rate": 3.086155165023575e-06,
      "loss": 0.2979,
      "step": 59190
    },
    {
      "epoch": 12.687526789541362,
      "grad_norm": 22.84747886657715,
      "learning_rate": 3.0832976139448495e-06,
      "loss": 0.5227,
      "step": 59200
    },
    {
      "epoch": 12.689669952850407,
      "grad_norm": 33.74331283569336,
      "learning_rate": 3.0804400628661243e-06,
      "loss": 0.7676,
      "step": 59210
    },
    {
      "epoch": 12.691813116159452,
      "grad_norm": 0.10352703928947449,
      "learning_rate": 3.0775825117873986e-06,
      "loss": 0.1306,
      "step": 59220
    },
    {
      "epoch": 12.693956279468495,
      "grad_norm": 31.578371047973633,
      "learning_rate": 3.0747249607086725e-06,
      "loss": 0.2939,
      "step": 59230
    },
    {
      "epoch": 12.69609944277754,
      "grad_norm": 44.89870071411133,
      "learning_rate": 3.0718674096299473e-06,
      "loss": 0.4886,
      "step": 59240
    },
    {
      "epoch": 12.698242606086584,
      "grad_norm": 17.84441566467285,
      "learning_rate": 3.0690098585512217e-06,
      "loss": 0.4498,
      "step": 59250
    },
    {
      "epoch": 12.700385769395627,
      "grad_norm": 0.11989355832338333,
      "learning_rate": 3.0661523074724964e-06,
      "loss": 0.11,
      "step": 59260
    },
    {
      "epoch": 12.702528932704672,
      "grad_norm": 0.25900763273239136,
      "learning_rate": 3.063294756393771e-06,
      "loss": 0.3541,
      "step": 59270
    },
    {
      "epoch": 12.704672096013716,
      "grad_norm": 0.19116520881652832,
      "learning_rate": 3.0604372053150456e-06,
      "loss": 0.003,
      "step": 59280
    },
    {
      "epoch": 12.70681525932276,
      "grad_norm": 0.0699337050318718,
      "learning_rate": 3.0575796542363195e-06,
      "loss": 0.2415,
      "step": 59290
    },
    {
      "epoch": 12.708958422631804,
      "grad_norm": 0.041540514677762985,
      "learning_rate": 3.0547221031575943e-06,
      "loss": 0.6778,
      "step": 59300
    },
    {
      "epoch": 12.711101585940849,
      "grad_norm": 0.19939620792865753,
      "learning_rate": 3.0518645520788686e-06,
      "loss": 0.4794,
      "step": 59310
    },
    {
      "epoch": 12.713244749249894,
      "grad_norm": 8.960962295532227,
      "learning_rate": 3.0490070010001434e-06,
      "loss": 0.5338,
      "step": 59320
    },
    {
      "epoch": 12.715387912558937,
      "grad_norm": 0.8012970089912415,
      "learning_rate": 3.0461494499214177e-06,
      "loss": 0.342,
      "step": 59330
    },
    {
      "epoch": 12.717531075867981,
      "grad_norm": 0.08947652578353882,
      "learning_rate": 3.0432918988426917e-06,
      "loss": 0.6775,
      "step": 59340
    },
    {
      "epoch": 12.719674239177026,
      "grad_norm": 0.038873426616191864,
      "learning_rate": 3.0404343477639665e-06,
      "loss": 0.1032,
      "step": 59350
    },
    {
      "epoch": 12.721817402486069,
      "grad_norm": 38.06959533691406,
      "learning_rate": 3.037576796685241e-06,
      "loss": 0.7891,
      "step": 59360
    },
    {
      "epoch": 12.723960565795114,
      "grad_norm": 0.12404051423072815,
      "learning_rate": 3.0347192456065156e-06,
      "loss": 0.4042,
      "step": 59370
    },
    {
      "epoch": 12.726103729104159,
      "grad_norm": 0.7283543348312378,
      "learning_rate": 3.03186169452779e-06,
      "loss": 0.4963,
      "step": 59380
    },
    {
      "epoch": 12.728246892413202,
      "grad_norm": 0.1453111320734024,
      "learning_rate": 3.0290041434490643e-06,
      "loss": 0.3806,
      "step": 59390
    },
    {
      "epoch": 12.730390055722246,
      "grad_norm": 0.01647396944463253,
      "learning_rate": 3.0261465923703386e-06,
      "loss": 0.4799,
      "step": 59400
    },
    {
      "epoch": 12.732533219031291,
      "grad_norm": 52.58444595336914,
      "learning_rate": 3.0232890412916134e-06,
      "loss": 0.2763,
      "step": 59410
    },
    {
      "epoch": 12.734676382340334,
      "grad_norm": 0.03979707136750221,
      "learning_rate": 3.0204314902128878e-06,
      "loss": 0.3538,
      "step": 59420
    },
    {
      "epoch": 12.736819545649379,
      "grad_norm": 0.07138793915510178,
      "learning_rate": 3.0175739391341625e-06,
      "loss": 0.1305,
      "step": 59430
    },
    {
      "epoch": 12.738962708958423,
      "grad_norm": 36.863555908203125,
      "learning_rate": 3.014716388055437e-06,
      "loss": 0.2762,
      "step": 59440
    },
    {
      "epoch": 12.741105872267466,
      "grad_norm": 32.50342559814453,
      "learning_rate": 3.011858836976711e-06,
      "loss": 0.4694,
      "step": 59450
    },
    {
      "epoch": 12.743249035576511,
      "grad_norm": 1.7448731660842896,
      "learning_rate": 3.0090012858979856e-06,
      "loss": 0.4823,
      "step": 59460
    },
    {
      "epoch": 12.745392198885556,
      "grad_norm": 34.80134582519531,
      "learning_rate": 3.00614373481926e-06,
      "loss": 0.1124,
      "step": 59470
    },
    {
      "epoch": 12.747535362194599,
      "grad_norm": 57.8275032043457,
      "learning_rate": 3.0032861837405347e-06,
      "loss": 0.4125,
      "step": 59480
    },
    {
      "epoch": 12.749678525503644,
      "grad_norm": 18.986040115356445,
      "learning_rate": 3.000428632661809e-06,
      "loss": 0.39,
      "step": 59490
    },
    {
      "epoch": 12.751821688812688,
      "grad_norm": 0.20833997428417206,
      "learning_rate": 2.9975710815830834e-06,
      "loss": 0.1907,
      "step": 59500
    },
    {
      "epoch": 12.753964852121731,
      "grad_norm": 0.02359246462583542,
      "learning_rate": 2.9947135305043578e-06,
      "loss": 0.1867,
      "step": 59510
    },
    {
      "epoch": 12.756108015430776,
      "grad_norm": 0.3246779143810272,
      "learning_rate": 2.9918559794256326e-06,
      "loss": 0.388,
      "step": 59520
    },
    {
      "epoch": 12.75825117873982,
      "grad_norm": 0.28873223066329956,
      "learning_rate": 2.988998428346907e-06,
      "loss": 0.3709,
      "step": 59530
    },
    {
      "epoch": 12.760394342048864,
      "grad_norm": 0.4990541338920593,
      "learning_rate": 2.9861408772681817e-06,
      "loss": 0.3456,
      "step": 59540
    },
    {
      "epoch": 12.762537505357908,
      "grad_norm": 0.03604015335440636,
      "learning_rate": 2.983283326189456e-06,
      "loss": 0.5145,
      "step": 59550
    },
    {
      "epoch": 12.764680668666953,
      "grad_norm": 0.08647074550390244,
      "learning_rate": 2.98042577511073e-06,
      "loss": 0.0045,
      "step": 59560
    },
    {
      "epoch": 12.766823831975996,
      "grad_norm": 0.03647977113723755,
      "learning_rate": 2.9775682240320047e-06,
      "loss": 0.3092,
      "step": 59570
    },
    {
      "epoch": 12.76896699528504,
      "grad_norm": 26.396482467651367,
      "learning_rate": 2.9747106729532795e-06,
      "loss": 0.9942,
      "step": 59580
    },
    {
      "epoch": 12.771110158594086,
      "grad_norm": 27.49713706970215,
      "learning_rate": 2.971853121874554e-06,
      "loss": 0.663,
      "step": 59590
    },
    {
      "epoch": 12.773253321903129,
      "grad_norm": 23.659090042114258,
      "learning_rate": 2.9689955707958286e-06,
      "loss": 0.3376,
      "step": 59600
    },
    {
      "epoch": 12.775396485212173,
      "grad_norm": 0.1222979947924614,
      "learning_rate": 2.9661380197171026e-06,
      "loss": 0.0077,
      "step": 59610
    },
    {
      "epoch": 12.777539648521218,
      "grad_norm": 0.14155827462673187,
      "learning_rate": 2.963280468638377e-06,
      "loss": 0.3717,
      "step": 59620
    },
    {
      "epoch": 12.779682811830261,
      "grad_norm": 0.05487792566418648,
      "learning_rate": 2.9604229175596517e-06,
      "loss": 0.3937,
      "step": 59630
    },
    {
      "epoch": 12.781825975139306,
      "grad_norm": 0.036910418421030045,
      "learning_rate": 2.957565366480926e-06,
      "loss": 0.2318,
      "step": 59640
    },
    {
      "epoch": 12.78396913844835,
      "grad_norm": 1.0540475845336914,
      "learning_rate": 2.954707815402201e-06,
      "loss": 0.4951,
      "step": 59650
    },
    {
      "epoch": 12.786112301757393,
      "grad_norm": 0.035662807524204254,
      "learning_rate": 2.9518502643234747e-06,
      "loss": 0.5753,
      "step": 59660
    },
    {
      "epoch": 12.788255465066438,
      "grad_norm": 1.9464304447174072,
      "learning_rate": 2.9489927132447495e-06,
      "loss": 0.3282,
      "step": 59670
    },
    {
      "epoch": 12.790398628375483,
      "grad_norm": 0.9953854084014893,
      "learning_rate": 2.946135162166024e-06,
      "loss": 0.1925,
      "step": 59680
    },
    {
      "epoch": 12.792541791684526,
      "grad_norm": 1.0920997858047485,
      "learning_rate": 2.9432776110872986e-06,
      "loss": 0.5978,
      "step": 59690
    },
    {
      "epoch": 12.79468495499357,
      "grad_norm": 0.26667889952659607,
      "learning_rate": 2.940420060008573e-06,
      "loss": 0.3745,
      "step": 59700
    },
    {
      "epoch": 12.796828118302615,
      "grad_norm": 0.1321765035390854,
      "learning_rate": 2.9375625089298478e-06,
      "loss": 0.2784,
      "step": 59710
    },
    {
      "epoch": 12.798971281611658,
      "grad_norm": 0.08601924777030945,
      "learning_rate": 2.9347049578511217e-06,
      "loss": 0.6855,
      "step": 59720
    },
    {
      "epoch": 12.801114444920703,
      "grad_norm": 745.5067749023438,
      "learning_rate": 2.931847406772396e-06,
      "loss": 0.2142,
      "step": 59730
    },
    {
      "epoch": 12.803257608229748,
      "grad_norm": 0.24411143362522125,
      "learning_rate": 2.928989855693671e-06,
      "loss": 0.0082,
      "step": 59740
    },
    {
      "epoch": 12.80540077153879,
      "grad_norm": 51.35760498046875,
      "learning_rate": 2.926132304614945e-06,
      "loss": 0.2134,
      "step": 59750
    },
    {
      "epoch": 12.807543934847835,
      "grad_norm": 60.48594665527344,
      "learning_rate": 2.92327475353622e-06,
      "loss": 0.6498,
      "step": 59760
    },
    {
      "epoch": 12.80968709815688,
      "grad_norm": 0.01566975563764572,
      "learning_rate": 2.920417202457494e-06,
      "loss": 0.4154,
      "step": 59770
    },
    {
      "epoch": 12.811830261465923,
      "grad_norm": 0.014983191154897213,
      "learning_rate": 2.9175596513787687e-06,
      "loss": 0.1382,
      "step": 59780
    },
    {
      "epoch": 12.813973424774968,
      "grad_norm": 0.024431470781564713,
      "learning_rate": 2.914702100300043e-06,
      "loss": 0.87,
      "step": 59790
    },
    {
      "epoch": 12.816116588084013,
      "grad_norm": 0.017049143090844154,
      "learning_rate": 2.911844549221318e-06,
      "loss": 0.3602,
      "step": 59800
    },
    {
      "epoch": 12.818259751393056,
      "grad_norm": 0.13474787771701813,
      "learning_rate": 2.908986998142592e-06,
      "loss": 0.4705,
      "step": 59810
    },
    {
      "epoch": 12.8204029147021,
      "grad_norm": 0.13643567264080048,
      "learning_rate": 2.906129447063866e-06,
      "loss": 0.2315,
      "step": 59820
    },
    {
      "epoch": 12.822546078011145,
      "grad_norm": 0.019934527575969696,
      "learning_rate": 2.903271895985141e-06,
      "loss": 0.0089,
      "step": 59830
    },
    {
      "epoch": 12.824689241320188,
      "grad_norm": 0.21982567012310028,
      "learning_rate": 2.900414344906415e-06,
      "loss": 0.2947,
      "step": 59840
    },
    {
      "epoch": 12.826832404629233,
      "grad_norm": 0.027400841936469078,
      "learning_rate": 2.89755679382769e-06,
      "loss": 0.1458,
      "step": 59850
    },
    {
      "epoch": 12.828975567938278,
      "grad_norm": 0.017247458919882774,
      "learning_rate": 2.8946992427489643e-06,
      "loss": 0.7704,
      "step": 59860
    },
    {
      "epoch": 12.83111873124732,
      "grad_norm": 0.4052078425884247,
      "learning_rate": 2.891841691670239e-06,
      "loss": 0.3279,
      "step": 59870
    },
    {
      "epoch": 12.833261894556365,
      "grad_norm": 36.42414855957031,
      "learning_rate": 2.888984140591513e-06,
      "loss": 0.5971,
      "step": 59880
    },
    {
      "epoch": 12.83540505786541,
      "grad_norm": 0.08631806820631027,
      "learning_rate": 2.886126589512788e-06,
      "loss": 0.4783,
      "step": 59890
    },
    {
      "epoch": 12.837548221174453,
      "grad_norm": 18.254653930664062,
      "learning_rate": 2.883269038434062e-06,
      "loss": 0.5696,
      "step": 59900
    },
    {
      "epoch": 12.839691384483498,
      "grad_norm": 0.5058167576789856,
      "learning_rate": 2.880411487355337e-06,
      "loss": 0.3831,
      "step": 59910
    },
    {
      "epoch": 12.841834547792542,
      "grad_norm": 0.3062784969806671,
      "learning_rate": 2.8775539362766113e-06,
      "loss": 0.4808,
      "step": 59920
    },
    {
      "epoch": 12.843977711101585,
      "grad_norm": 0.3945869207382202,
      "learning_rate": 2.874696385197885e-06,
      "loss": 0.2411,
      "step": 59930
    },
    {
      "epoch": 12.84612087441063,
      "grad_norm": 41.072776794433594,
      "learning_rate": 2.87183883411916e-06,
      "loss": 0.5058,
      "step": 59940
    },
    {
      "epoch": 12.848264037719675,
      "grad_norm": 0.275877445936203,
      "learning_rate": 2.8689812830404348e-06,
      "loss": 0.2008,
      "step": 59950
    },
    {
      "epoch": 12.850407201028718,
      "grad_norm": 155.04782104492188,
      "learning_rate": 2.866123731961709e-06,
      "loss": 0.3779,
      "step": 59960
    },
    {
      "epoch": 12.852550364337763,
      "grad_norm": 0.14622975885868073,
      "learning_rate": 2.863266180882984e-06,
      "loss": 0.2642,
      "step": 59970
    },
    {
      "epoch": 12.854693527646807,
      "grad_norm": 0.08423735201358795,
      "learning_rate": 2.860408629804258e-06,
      "loss": 0.4811,
      "step": 59980
    },
    {
      "epoch": 12.85683669095585,
      "grad_norm": 0.14534303545951843,
      "learning_rate": 2.857551078725532e-06,
      "loss": 0.3239,
      "step": 59990
    },
    {
      "epoch": 12.858979854264895,
      "grad_norm": 0.1473204344511032,
      "learning_rate": 2.854693527646807e-06,
      "loss": 0.3478,
      "step": 60000
    },
    {
      "epoch": 12.86112301757394,
      "grad_norm": 0.14986112713813782,
      "learning_rate": 2.8518359765680813e-06,
      "loss": 0.3463,
      "step": 60010
    },
    {
      "epoch": 12.863266180882983,
      "grad_norm": 0.27908384799957275,
      "learning_rate": 2.848978425489356e-06,
      "loss": 0.6609,
      "step": 60020
    },
    {
      "epoch": 12.865409344192027,
      "grad_norm": 73.55052185058594,
      "learning_rate": 2.8461208744106304e-06,
      "loss": 0.6845,
      "step": 60030
    },
    {
      "epoch": 12.867552507501072,
      "grad_norm": 0.43259865045547485,
      "learning_rate": 2.8432633233319048e-06,
      "loss": 0.3153,
      "step": 60040
    },
    {
      "epoch": 12.869695670810115,
      "grad_norm": 0.08606278151273727,
      "learning_rate": 2.840405772253179e-06,
      "loss": 0.5464,
      "step": 60050
    },
    {
      "epoch": 12.87183883411916,
      "grad_norm": 0.36571019887924194,
      "learning_rate": 2.837548221174454e-06,
      "loss": 0.4315,
      "step": 60060
    },
    {
      "epoch": 12.873981997428205,
      "grad_norm": 265.8512268066406,
      "learning_rate": 2.8346906700957283e-06,
      "loss": 0.3797,
      "step": 60070
    },
    {
      "epoch": 12.876125160737248,
      "grad_norm": 0.15627215802669525,
      "learning_rate": 2.831833119017003e-06,
      "loss": 0.3963,
      "step": 60080
    },
    {
      "epoch": 12.878268324046292,
      "grad_norm": 45.23920440673828,
      "learning_rate": 2.828975567938277e-06,
      "loss": 0.3846,
      "step": 60090
    },
    {
      "epoch": 12.880411487355337,
      "grad_norm": 30.678741455078125,
      "learning_rate": 2.8261180168595513e-06,
      "loss": 0.7769,
      "step": 60100
    },
    {
      "epoch": 12.88255465066438,
      "grad_norm": 0.48084476590156555,
      "learning_rate": 2.823260465780826e-06,
      "loss": 0.1924,
      "step": 60110
    },
    {
      "epoch": 12.884697813973425,
      "grad_norm": 33.5482177734375,
      "learning_rate": 2.8204029147021004e-06,
      "loss": 0.7311,
      "step": 60120
    },
    {
      "epoch": 12.88684097728247,
      "grad_norm": 0.6522722244262695,
      "learning_rate": 2.817545363623375e-06,
      "loss": 0.3158,
      "step": 60130
    },
    {
      "epoch": 12.888984140591512,
      "grad_norm": 0.06495600938796997,
      "learning_rate": 2.8146878125446496e-06,
      "loss": 0.2826,
      "step": 60140
    },
    {
      "epoch": 12.891127303900557,
      "grad_norm": 0.6201558113098145,
      "learning_rate": 2.811830261465924e-06,
      "loss": 0.1603,
      "step": 60150
    },
    {
      "epoch": 12.893270467209602,
      "grad_norm": 0.0179649256169796,
      "learning_rate": 2.8089727103871983e-06,
      "loss": 0.0033,
      "step": 60160
    },
    {
      "epoch": 12.895413630518645,
      "grad_norm": 0.2676880955696106,
      "learning_rate": 2.806115159308473e-06,
      "loss": 0.0776,
      "step": 60170
    },
    {
      "epoch": 12.89755679382769,
      "grad_norm": 0.058691028505563736,
      "learning_rate": 2.8032576082297474e-06,
      "loss": 0.1809,
      "step": 60180
    },
    {
      "epoch": 12.899699957136734,
      "grad_norm": 0.02630523033440113,
      "learning_rate": 2.800400057151022e-06,
      "loss": 0.1681,
      "step": 60190
    },
    {
      "epoch": 12.901843120445777,
      "grad_norm": 0.20636947453022003,
      "learning_rate": 2.797542506072296e-06,
      "loss": 0.3057,
      "step": 60200
    },
    {
      "epoch": 12.903986283754822,
      "grad_norm": 56.19807434082031,
      "learning_rate": 2.7946849549935704e-06,
      "loss": 1.2452,
      "step": 60210
    },
    {
      "epoch": 12.906129447063867,
      "grad_norm": 0.23179353773593903,
      "learning_rate": 2.7918274039148452e-06,
      "loss": 0.2234,
      "step": 60220
    },
    {
      "epoch": 12.90827261037291,
      "grad_norm": 0.3869569003582001,
      "learning_rate": 2.7889698528361196e-06,
      "loss": 0.0041,
      "step": 60230
    },
    {
      "epoch": 12.910415773681954,
      "grad_norm": 0.007012609392404556,
      "learning_rate": 2.7861123017573943e-06,
      "loss": 0.2797,
      "step": 60240
    },
    {
      "epoch": 12.912558936991,
      "grad_norm": 0.032425105571746826,
      "learning_rate": 2.7832547506786683e-06,
      "loss": 0.144,
      "step": 60250
    },
    {
      "epoch": 12.914702100300042,
      "grad_norm": 0.13453075289726257,
      "learning_rate": 2.780397199599943e-06,
      "loss": 0.0935,
      "step": 60260
    },
    {
      "epoch": 12.916845263609087,
      "grad_norm": 0.2995036542415619,
      "learning_rate": 2.7775396485212174e-06,
      "loss": 0.3903,
      "step": 60270
    },
    {
      "epoch": 12.918988426918132,
      "grad_norm": 27.007080078125,
      "learning_rate": 2.774682097442492e-06,
      "loss": 0.4021,
      "step": 60280
    },
    {
      "epoch": 12.921131590227175,
      "grad_norm": 0.016912110149860382,
      "learning_rate": 2.7718245463637665e-06,
      "loss": 0.5389,
      "step": 60290
    },
    {
      "epoch": 12.92327475353622,
      "grad_norm": 0.15931063890457153,
      "learning_rate": 2.7689669952850413e-06,
      "loss": 0.3406,
      "step": 60300
    },
    {
      "epoch": 12.925417916845264,
      "grad_norm": 0.0749708041548729,
      "learning_rate": 2.7661094442063152e-06,
      "loss": 0.0016,
      "step": 60310
    },
    {
      "epoch": 12.927561080154307,
      "grad_norm": 0.04188542813062668,
      "learning_rate": 2.7632518931275896e-06,
      "loss": 0.2648,
      "step": 60320
    },
    {
      "epoch": 12.929704243463352,
      "grad_norm": 0.1605994701385498,
      "learning_rate": 2.7603943420488644e-06,
      "loss": 0.1888,
      "step": 60330
    },
    {
      "epoch": 12.931847406772397,
      "grad_norm": 15.460987091064453,
      "learning_rate": 2.757536790970139e-06,
      "loss": 0.4796,
      "step": 60340
    },
    {
      "epoch": 12.93399057008144,
      "grad_norm": 0.2594586908817291,
      "learning_rate": 2.7546792398914135e-06,
      "loss": 0.1146,
      "step": 60350
    },
    {
      "epoch": 12.936133733390484,
      "grad_norm": 0.11821483075618744,
      "learning_rate": 2.7518216888126874e-06,
      "loss": 0.6612,
      "step": 60360
    },
    {
      "epoch": 12.938276896699529,
      "grad_norm": 0.6928992867469788,
      "learning_rate": 2.748964137733962e-06,
      "loss": 0.1383,
      "step": 60370
    },
    {
      "epoch": 12.940420060008572,
      "grad_norm": 0.12056678533554077,
      "learning_rate": 2.7461065866552365e-06,
      "loss": 0.2946,
      "step": 60380
    },
    {
      "epoch": 12.942563223317617,
      "grad_norm": 39.70492935180664,
      "learning_rate": 2.7432490355765113e-06,
      "loss": 0.3547,
      "step": 60390
    },
    {
      "epoch": 12.944706386626661,
      "grad_norm": 0.3164427876472473,
      "learning_rate": 2.7403914844977857e-06,
      "loss": 0.171,
      "step": 60400
    },
    {
      "epoch": 12.946849549935704,
      "grad_norm": 0.08649981766939163,
      "learning_rate": 2.73753393341906e-06,
      "loss": 0.2534,
      "step": 60410
    },
    {
      "epoch": 12.948992713244749,
      "grad_norm": 0.3116007149219513,
      "learning_rate": 2.7346763823403344e-06,
      "loss": 0.3789,
      "step": 60420
    },
    {
      "epoch": 12.951135876553794,
      "grad_norm": 0.13092757761478424,
      "learning_rate": 2.731818831261609e-06,
      "loss": 0.1439,
      "step": 60430
    },
    {
      "epoch": 12.953279039862837,
      "grad_norm": 0.11817757040262222,
      "learning_rate": 2.7289612801828835e-06,
      "loss": 0.3864,
      "step": 60440
    },
    {
      "epoch": 12.955422203171882,
      "grad_norm": 0.3285784423351288,
      "learning_rate": 2.7261037291041583e-06,
      "loss": 0.7236,
      "step": 60450
    },
    {
      "epoch": 12.957565366480926,
      "grad_norm": 0.015619752928614616,
      "learning_rate": 2.7232461780254326e-06,
      "loss": 0.5134,
      "step": 60460
    },
    {
      "epoch": 12.95970852978997,
      "grad_norm": 0.011650154367089272,
      "learning_rate": 2.7203886269467066e-06,
      "loss": 0.1056,
      "step": 60470
    },
    {
      "epoch": 12.961851693099014,
      "grad_norm": 1.262640118598938,
      "learning_rate": 2.7175310758679813e-06,
      "loss": 0.2965,
      "step": 60480
    },
    {
      "epoch": 12.963994856408059,
      "grad_norm": 37.722198486328125,
      "learning_rate": 2.7146735247892557e-06,
      "loss": 0.4353,
      "step": 60490
    },
    {
      "epoch": 12.966138019717102,
      "grad_norm": 17.98137092590332,
      "learning_rate": 2.7118159737105305e-06,
      "loss": 0.6824,
      "step": 60500
    },
    {
      "epoch": 12.968281183026146,
      "grad_norm": 0.010536862537264824,
      "learning_rate": 2.708958422631805e-06,
      "loss": 0.0037,
      "step": 60510
    },
    {
      "epoch": 12.970424346335191,
      "grad_norm": 0.08592745661735535,
      "learning_rate": 2.706100871553079e-06,
      "loss": 0.0926,
      "step": 60520
    },
    {
      "epoch": 12.972567509644234,
      "grad_norm": 0.16258402168750763,
      "learning_rate": 2.7032433204743535e-06,
      "loss": 0.8942,
      "step": 60530
    },
    {
      "epoch": 12.974710672953279,
      "grad_norm": 0.027108659967780113,
      "learning_rate": 2.7003857693956283e-06,
      "loss": 0.6998,
      "step": 60540
    },
    {
      "epoch": 12.976853836262324,
      "grad_norm": 19.727174758911133,
      "learning_rate": 2.6975282183169026e-06,
      "loss": 0.2159,
      "step": 60550
    },
    {
      "epoch": 12.978996999571367,
      "grad_norm": 0.018139686435461044,
      "learning_rate": 2.6946706672381774e-06,
      "loss": 0.2248,
      "step": 60560
    },
    {
      "epoch": 12.981140162880411,
      "grad_norm": 4.676388263702393,
      "learning_rate": 2.6918131161594518e-06,
      "loss": 0.759,
      "step": 60570
    },
    {
      "epoch": 12.983283326189456,
      "grad_norm": 0.035978734493255615,
      "learning_rate": 2.6889555650807257e-06,
      "loss": 0.4116,
      "step": 60580
    },
    {
      "epoch": 12.985426489498499,
      "grad_norm": 0.07627138495445251,
      "learning_rate": 2.6860980140020005e-06,
      "loss": 0.2831,
      "step": 60590
    },
    {
      "epoch": 12.987569652807544,
      "grad_norm": 1.1904383897781372,
      "learning_rate": 2.683240462923275e-06,
      "loss": 0.7351,
      "step": 60600
    },
    {
      "epoch": 12.989712816116588,
      "grad_norm": 0.12731127440929413,
      "learning_rate": 2.6803829118445496e-06,
      "loss": 0.8826,
      "step": 60610
    },
    {
      "epoch": 12.991855979425631,
      "grad_norm": 0.08308316022157669,
      "learning_rate": 2.677525360765824e-06,
      "loss": 0.5069,
      "step": 60620
    },
    {
      "epoch": 12.993999142734676,
      "grad_norm": 0.21260152757167816,
      "learning_rate": 2.6746678096870983e-06,
      "loss": 0.1274,
      "step": 60630
    },
    {
      "epoch": 12.996142306043721,
      "grad_norm": 0.2843083441257477,
      "learning_rate": 2.6718102586083727e-06,
      "loss": 0.34,
      "step": 60640
    },
    {
      "epoch": 12.998285469352764,
      "grad_norm": 0.13374370336532593,
      "learning_rate": 2.6689527075296474e-06,
      "loss": 0.3012,
      "step": 60650
    },
    {
      "epoch": 13.0,
      "eval_accuracy": 0.9213333333333333,
      "eval_f1": 0.6358024691358026,
      "eval_loss": 0.36979350447654724,
      "eval_precision": 0.8207171314741036,
      "eval_recall": 0.5188916876574308,
      "eval_runtime": 396.7705,
      "eval_samples_per_second": 7.561,
      "eval_steps_per_second": 2.52,
      "step": 60658
    },
    {
      "epoch": 13.000428632661809,
      "grad_norm": 0.36216482520103455,
      "learning_rate": 2.6660951564509218e-06,
      "loss": 0.299,
      "step": 60660
    },
    {
      "epoch": 13.002571795970853,
      "grad_norm": 0.34067970514297485,
      "learning_rate": 2.6632376053721966e-06,
      "loss": 0.4509,
      "step": 60670
    },
    {
      "epoch": 13.004714959279896,
      "grad_norm": 0.23624847829341888,
      "learning_rate": 2.6603800542934705e-06,
      "loss": 0.1291,
      "step": 60680
    },
    {
      "epoch": 13.006858122588941,
      "grad_norm": 0.4333120286464691,
      "learning_rate": 2.657522503214745e-06,
      "loss": 0.2207,
      "step": 60690
    },
    {
      "epoch": 13.009001285897986,
      "grad_norm": 0.2771322727203369,
      "learning_rate": 2.6546649521360196e-06,
      "loss": 0.3087,
      "step": 60700
    },
    {
      "epoch": 13.011144449207029,
      "grad_norm": 0.2906586825847626,
      "learning_rate": 2.651807401057294e-06,
      "loss": 0.3489,
      "step": 60710
    },
    {
      "epoch": 13.013287612516073,
      "grad_norm": 0.049828287214040756,
      "learning_rate": 2.6489498499785687e-06,
      "loss": 0.5612,
      "step": 60720
    },
    {
      "epoch": 13.015430775825118,
      "grad_norm": 31.418420791625977,
      "learning_rate": 2.6460922988998435e-06,
      "loss": 0.3989,
      "step": 60730
    },
    {
      "epoch": 13.017573939134161,
      "grad_norm": 0.036778680980205536,
      "learning_rate": 2.6432347478211174e-06,
      "loss": 0.4331,
      "step": 60740
    },
    {
      "epoch": 13.019717102443206,
      "grad_norm": 46.328678131103516,
      "learning_rate": 2.640377196742392e-06,
      "loss": 0.407,
      "step": 60750
    },
    {
      "epoch": 13.02186026575225,
      "grad_norm": 23.57794189453125,
      "learning_rate": 2.6375196456636666e-06,
      "loss": 0.5996,
      "step": 60760
    },
    {
      "epoch": 13.024003429061294,
      "grad_norm": 57.97854995727539,
      "learning_rate": 2.634662094584941e-06,
      "loss": 0.8471,
      "step": 60770
    },
    {
      "epoch": 13.026146592370338,
      "grad_norm": 0.13968394696712494,
      "learning_rate": 2.6318045435062157e-06,
      "loss": 0.1865,
      "step": 60780
    },
    {
      "epoch": 13.028289755679383,
      "grad_norm": 0.18240277469158173,
      "learning_rate": 2.6289469924274896e-06,
      "loss": 0.171,
      "step": 60790
    },
    {
      "epoch": 13.030432918988426,
      "grad_norm": 0.9327254295349121,
      "learning_rate": 2.6260894413487644e-06,
      "loss": 0.899,
      "step": 60800
    },
    {
      "epoch": 13.03257608229747,
      "grad_norm": 0.16429640352725983,
      "learning_rate": 2.6232318902700388e-06,
      "loss": 0.1636,
      "step": 60810
    },
    {
      "epoch": 13.034719245606516,
      "grad_norm": 0.24490436911582947,
      "learning_rate": 2.6203743391913135e-06,
      "loss": 0.0054,
      "step": 60820
    },
    {
      "epoch": 13.03686240891556,
      "grad_norm": 38.668067932128906,
      "learning_rate": 2.617516788112588e-06,
      "loss": 0.8285,
      "step": 60830
    },
    {
      "epoch": 13.039005572224603,
      "grad_norm": 40.9741096496582,
      "learning_rate": 2.614659237033862e-06,
      "loss": 0.3104,
      "step": 60840
    },
    {
      "epoch": 13.041148735533648,
      "grad_norm": 0.20483067631721497,
      "learning_rate": 2.6118016859551366e-06,
      "loss": 0.3596,
      "step": 60850
    },
    {
      "epoch": 13.043291898842693,
      "grad_norm": 0.8351525068283081,
      "learning_rate": 2.608944134876411e-06,
      "loss": 0.1438,
      "step": 60860
    },
    {
      "epoch": 13.045435062151736,
      "grad_norm": 1.0087268352508545,
      "learning_rate": 2.6060865837976857e-06,
      "loss": 0.449,
      "step": 60870
    },
    {
      "epoch": 13.04757822546078,
      "grad_norm": 0.517327606678009,
      "learning_rate": 2.60322903271896e-06,
      "loss": 0.4498,
      "step": 60880
    },
    {
      "epoch": 13.049721388769825,
      "grad_norm": 0.1839892566204071,
      "learning_rate": 2.600371481640235e-06,
      "loss": 0.2215,
      "step": 60890
    },
    {
      "epoch": 13.051864552078868,
      "grad_norm": 0.1706022322177887,
      "learning_rate": 2.5975139305615088e-06,
      "loss": 0.4032,
      "step": 60900
    },
    {
      "epoch": 13.054007715387913,
      "grad_norm": 0.0370110422372818,
      "learning_rate": 2.5946563794827835e-06,
      "loss": 0.4779,
      "step": 60910
    },
    {
      "epoch": 13.056150878696958,
      "grad_norm": 0.014596324414014816,
      "learning_rate": 2.591798828404058e-06,
      "loss": 0.003,
      "step": 60920
    },
    {
      "epoch": 13.058294042006,
      "grad_norm": 34.762874603271484,
      "learning_rate": 2.5889412773253327e-06,
      "loss": 0.7165,
      "step": 60930
    },
    {
      "epoch": 13.060437205315045,
      "grad_norm": 0.1850249171257019,
      "learning_rate": 2.586083726246607e-06,
      "loss": 0.7772,
      "step": 60940
    },
    {
      "epoch": 13.06258036862409,
      "grad_norm": 12.836642265319824,
      "learning_rate": 2.583226175167881e-06,
      "loss": 0.0678,
      "step": 60950
    },
    {
      "epoch": 13.064723531933133,
      "grad_norm": 0.003814803436398506,
      "learning_rate": 2.5803686240891557e-06,
      "loss": 0.1893,
      "step": 60960
    },
    {
      "epoch": 13.066866695242178,
      "grad_norm": 0.2098572701215744,
      "learning_rate": 2.57751107301043e-06,
      "loss": 0.117,
      "step": 60970
    },
    {
      "epoch": 13.069009858551222,
      "grad_norm": 81.48841857910156,
      "learning_rate": 2.574653521931705e-06,
      "loss": 0.7991,
      "step": 60980
    },
    {
      "epoch": 13.071153021860265,
      "grad_norm": 0.03264610469341278,
      "learning_rate": 2.571795970852979e-06,
      "loss": 0.3245,
      "step": 60990
    },
    {
      "epoch": 13.07329618516931,
      "grad_norm": 0.16448237001895905,
      "learning_rate": 2.5689384197742536e-06,
      "loss": 0.0995,
      "step": 61000
    },
    {
      "epoch": 13.075439348478355,
      "grad_norm": 11.277902603149414,
      "learning_rate": 2.566080868695528e-06,
      "loss": 0.2584,
      "step": 61010
    },
    {
      "epoch": 13.077582511787398,
      "grad_norm": 0.09693317860364914,
      "learning_rate": 2.5632233176168027e-06,
      "loss": 0.2741,
      "step": 61020
    },
    {
      "epoch": 13.079725675096443,
      "grad_norm": 0.3591187298297882,
      "learning_rate": 2.560365766538077e-06,
      "loss": 0.2781,
      "step": 61030
    },
    {
      "epoch": 13.081868838405487,
      "grad_norm": 0.0367007739841938,
      "learning_rate": 2.557508215459352e-06,
      "loss": 0.4829,
      "step": 61040
    },
    {
      "epoch": 13.08401200171453,
      "grad_norm": 0.1596725583076477,
      "learning_rate": 2.554650664380626e-06,
      "loss": 0.0903,
      "step": 61050
    },
    {
      "epoch": 13.086155165023575,
      "grad_norm": 70.18295288085938,
      "learning_rate": 2.5517931133019e-06,
      "loss": 0.721,
      "step": 61060
    },
    {
      "epoch": 13.08829832833262,
      "grad_norm": 0.01252490933984518,
      "learning_rate": 2.548935562223175e-06,
      "loss": 0.1409,
      "step": 61070
    },
    {
      "epoch": 13.090441491641663,
      "grad_norm": 0.0026472245808690786,
      "learning_rate": 2.5460780111444492e-06,
      "loss": 0.483,
      "step": 61080
    },
    {
      "epoch": 13.092584654950707,
      "grad_norm": 0.0064310641027987,
      "learning_rate": 2.543220460065724e-06,
      "loss": 0.1905,
      "step": 61090
    },
    {
      "epoch": 13.094727818259752,
      "grad_norm": 0.1490355134010315,
      "learning_rate": 2.5403629089869983e-06,
      "loss": 0.3027,
      "step": 61100
    },
    {
      "epoch": 13.096870981568795,
      "grad_norm": 0.329706609249115,
      "learning_rate": 2.5375053579082727e-06,
      "loss": 0.1582,
      "step": 61110
    },
    {
      "epoch": 13.09901414487784,
      "grad_norm": 12.252341270446777,
      "learning_rate": 2.534647806829547e-06,
      "loss": 0.1078,
      "step": 61120
    },
    {
      "epoch": 13.101157308186885,
      "grad_norm": 0.08416537940502167,
      "learning_rate": 2.531790255750822e-06,
      "loss": 0.4347,
      "step": 61130
    },
    {
      "epoch": 13.103300471495928,
      "grad_norm": 17.50356674194336,
      "learning_rate": 2.528932704672096e-06,
      "loss": 0.5487,
      "step": 61140
    },
    {
      "epoch": 13.105443634804972,
      "grad_norm": 0.13744018971920013,
      "learning_rate": 2.526075153593371e-06,
      "loss": 0.1238,
      "step": 61150
    },
    {
      "epoch": 13.107586798114017,
      "grad_norm": 0.0050811441615223885,
      "learning_rate": 2.5232176025146453e-06,
      "loss": 0.0039,
      "step": 61160
    },
    {
      "epoch": 13.10972996142306,
      "grad_norm": 0.11450255662202835,
      "learning_rate": 2.5203600514359192e-06,
      "loss": 0.2805,
      "step": 61170
    },
    {
      "epoch": 13.111873124732105,
      "grad_norm": 0.03743059188127518,
      "learning_rate": 2.517502500357194e-06,
      "loss": 0.5425,
      "step": 61180
    },
    {
      "epoch": 13.11401628804115,
      "grad_norm": 0.03556184098124504,
      "learning_rate": 2.5146449492784688e-06,
      "loss": 0.396,
      "step": 61190
    },
    {
      "epoch": 13.116159451350192,
      "grad_norm": 0.10138431936502457,
      "learning_rate": 2.511787398199743e-06,
      "loss": 0.3462,
      "step": 61200
    },
    {
      "epoch": 13.118302614659237,
      "grad_norm": 0.16339261829853058,
      "learning_rate": 2.508929847121018e-06,
      "loss": 0.4744,
      "step": 61210
    },
    {
      "epoch": 13.120445777968282,
      "grad_norm": 0.003766231471672654,
      "learning_rate": 2.506072296042292e-06,
      "loss": 0.1077,
      "step": 61220
    },
    {
      "epoch": 13.122588941277325,
      "grad_norm": 49.07525634765625,
      "learning_rate": 2.503214744963566e-06,
      "loss": 0.8538,
      "step": 61230
    },
    {
      "epoch": 13.12473210458637,
      "grad_norm": 0.01633632183074951,
      "learning_rate": 2.500357193884841e-06,
      "loss": 0.0938,
      "step": 61240
    },
    {
      "epoch": 13.126875267895414,
      "grad_norm": 0.05698154494166374,
      "learning_rate": 2.4974996428061153e-06,
      "loss": 0.3458,
      "step": 61250
    },
    {
      "epoch": 13.129018431204457,
      "grad_norm": 0.015029548667371273,
      "learning_rate": 2.4946420917273897e-06,
      "loss": 0.1391,
      "step": 61260
    },
    {
      "epoch": 13.131161594513502,
      "grad_norm": 0.15874184668064117,
      "learning_rate": 2.4917845406486644e-06,
      "loss": 0.2325,
      "step": 61270
    },
    {
      "epoch": 13.133304757822547,
      "grad_norm": 0.20286808907985687,
      "learning_rate": 2.488926989569939e-06,
      "loss": 0.4507,
      "step": 61280
    },
    {
      "epoch": 13.13544792113159,
      "grad_norm": 16.694116592407227,
      "learning_rate": 2.486069438491213e-06,
      "loss": 0.311,
      "step": 61290
    },
    {
      "epoch": 13.137591084440635,
      "grad_norm": 3.593414783477783,
      "learning_rate": 2.483211887412488e-06,
      "loss": 0.3203,
      "step": 61300
    },
    {
      "epoch": 13.13973424774968,
      "grad_norm": 0.20137643814086914,
      "learning_rate": 2.480354336333762e-06,
      "loss": 0.6729,
      "step": 61310
    },
    {
      "epoch": 13.141877411058722,
      "grad_norm": 2.5489542484283447,
      "learning_rate": 2.4774967852550366e-06,
      "loss": 0.1907,
      "step": 61320
    },
    {
      "epoch": 13.144020574367767,
      "grad_norm": 0.03714045509696007,
      "learning_rate": 2.4746392341763114e-06,
      "loss": 0.4754,
      "step": 61330
    },
    {
      "epoch": 13.146163737676812,
      "grad_norm": 0.2606804668903351,
      "learning_rate": 2.4717816830975853e-06,
      "loss": 0.3801,
      "step": 61340
    },
    {
      "epoch": 13.148306900985855,
      "grad_norm": 0.7490617632865906,
      "learning_rate": 2.46892413201886e-06,
      "loss": 0.6768,
      "step": 61350
    },
    {
      "epoch": 13.1504500642949,
      "grad_norm": 0.290359765291214,
      "learning_rate": 2.4660665809401345e-06,
      "loss": 0.1487,
      "step": 61360
    },
    {
      "epoch": 13.152593227603944,
      "grad_norm": 0.21673178672790527,
      "learning_rate": 2.463209029861409e-06,
      "loss": 0.3952,
      "step": 61370
    },
    {
      "epoch": 13.154736390912987,
      "grad_norm": 0.07962005585432053,
      "learning_rate": 2.4603514787826836e-06,
      "loss": 0.1567,
      "step": 61380
    },
    {
      "epoch": 13.156879554222032,
      "grad_norm": 0.025211643427610397,
      "learning_rate": 2.457493927703958e-06,
      "loss": 0.3225,
      "step": 61390
    },
    {
      "epoch": 13.159022717531077,
      "grad_norm": 0.06397722661495209,
      "learning_rate": 2.4546363766252323e-06,
      "loss": 0.3515,
      "step": 61400
    },
    {
      "epoch": 13.16116588084012,
      "grad_norm": 0.16775591671466827,
      "learning_rate": 2.451778825546507e-06,
      "loss": 0.0032,
      "step": 61410
    },
    {
      "epoch": 13.163309044149164,
      "grad_norm": 0.31873416900634766,
      "learning_rate": 2.4489212744677814e-06,
      "loss": 0.374,
      "step": 61420
    },
    {
      "epoch": 13.165452207458209,
      "grad_norm": 0.03616056963801384,
      "learning_rate": 2.4460637233890558e-06,
      "loss": 0.3447,
      "step": 61430
    },
    {
      "epoch": 13.167595370767252,
      "grad_norm": 18.750226974487305,
      "learning_rate": 2.4432061723103305e-06,
      "loss": 0.3102,
      "step": 61440
    },
    {
      "epoch": 13.169738534076297,
      "grad_norm": 0.011458505876362324,
      "learning_rate": 2.4403486212316045e-06,
      "loss": 0.2233,
      "step": 61450
    },
    {
      "epoch": 13.171881697385341,
      "grad_norm": 40.75403594970703,
      "learning_rate": 2.4374910701528792e-06,
      "loss": 0.6491,
      "step": 61460
    },
    {
      "epoch": 13.174024860694384,
      "grad_norm": 0.04996545612812042,
      "learning_rate": 2.4346335190741536e-06,
      "loss": 0.0982,
      "step": 61470
    },
    {
      "epoch": 13.17616802400343,
      "grad_norm": 0.03808126226067543,
      "learning_rate": 2.431775967995428e-06,
      "loss": 0.0015,
      "step": 61480
    },
    {
      "epoch": 13.178311187312474,
      "grad_norm": 46.74834442138672,
      "learning_rate": 2.4289184169167027e-06,
      "loss": 0.3599,
      "step": 61490
    },
    {
      "epoch": 13.180454350621517,
      "grad_norm": 0.15431730449199677,
      "learning_rate": 2.426060865837977e-06,
      "loss": 0.46,
      "step": 61500
    },
    {
      "epoch": 13.182597513930562,
      "grad_norm": 61.06310272216797,
      "learning_rate": 2.4232033147592514e-06,
      "loss": 1.1266,
      "step": 61510
    },
    {
      "epoch": 13.184740677239606,
      "grad_norm": 46.64698791503906,
      "learning_rate": 2.420345763680526e-06,
      "loss": 0.3871,
      "step": 61520
    },
    {
      "epoch": 13.18688384054865,
      "grad_norm": 0.02772442437708378,
      "learning_rate": 2.4174882126018006e-06,
      "loss": 0.5366,
      "step": 61530
    },
    {
      "epoch": 13.189027003857694,
      "grad_norm": 0.17902974784374237,
      "learning_rate": 2.414630661523075e-06,
      "loss": 0.229,
      "step": 61540
    },
    {
      "epoch": 13.191170167166739,
      "grad_norm": 0.2078992873430252,
      "learning_rate": 2.4117731104443493e-06,
      "loss": 0.4515,
      "step": 61550
    },
    {
      "epoch": 13.193313330475782,
      "grad_norm": 0.41173818707466125,
      "learning_rate": 2.408915559365624e-06,
      "loss": 0.6177,
      "step": 61560
    },
    {
      "epoch": 13.195456493784826,
      "grad_norm": 28.169204711914062,
      "learning_rate": 2.4060580082868984e-06,
      "loss": 0.9421,
      "step": 61570
    },
    {
      "epoch": 13.197599657093871,
      "grad_norm": 0.20858994126319885,
      "learning_rate": 2.4032004572081727e-06,
      "loss": 0.0898,
      "step": 61580
    },
    {
      "epoch": 13.199742820402914,
      "grad_norm": 23.28823471069336,
      "learning_rate": 2.400342906129447e-06,
      "loss": 0.3037,
      "step": 61590
    },
    {
      "epoch": 13.201885983711959,
      "grad_norm": 0.29804664850234985,
      "learning_rate": 2.397485355050722e-06,
      "loss": 0.2728,
      "step": 61600
    },
    {
      "epoch": 13.204029147021004,
      "grad_norm": 0.028269942849874496,
      "learning_rate": 2.3946278039719962e-06,
      "loss": 0.1388,
      "step": 61610
    },
    {
      "epoch": 13.206172310330047,
      "grad_norm": 0.29976221919059753,
      "learning_rate": 2.3917702528932706e-06,
      "loss": 0.201,
      "step": 61620
    },
    {
      "epoch": 13.208315473639091,
      "grad_norm": 0.2114841192960739,
      "learning_rate": 2.3889127018145453e-06,
      "loss": 0.2066,
      "step": 61630
    },
    {
      "epoch": 13.210458636948136,
      "grad_norm": 0.1292591691017151,
      "learning_rate": 2.3860551507358197e-06,
      "loss": 0.4143,
      "step": 61640
    },
    {
      "epoch": 13.212601800257179,
      "grad_norm": 37.932334899902344,
      "learning_rate": 2.383197599657094e-06,
      "loss": 0.2862,
      "step": 61650
    },
    {
      "epoch": 13.214744963566224,
      "grad_norm": 0.01867276430130005,
      "learning_rate": 2.3803400485783684e-06,
      "loss": 0.0022,
      "step": 61660
    },
    {
      "epoch": 13.216888126875268,
      "grad_norm": 0.014221828430891037,
      "learning_rate": 2.377482497499643e-06,
      "loss": 0.2962,
      "step": 61670
    },
    {
      "epoch": 13.219031290184311,
      "grad_norm": 0.28274717926979065,
      "learning_rate": 2.3746249464209175e-06,
      "loss": 0.0021,
      "step": 61680
    },
    {
      "epoch": 13.221174453493356,
      "grad_norm": 44.63345718383789,
      "learning_rate": 2.371767395342192e-06,
      "loss": 0.437,
      "step": 61690
    },
    {
      "epoch": 13.223317616802401,
      "grad_norm": 0.005959840025752783,
      "learning_rate": 2.3689098442634662e-06,
      "loss": 0.0005,
      "step": 61700
    },
    {
      "epoch": 13.225460780111444,
      "grad_norm": 0.005479519255459309,
      "learning_rate": 2.366052293184741e-06,
      "loss": 0.1941,
      "step": 61710
    },
    {
      "epoch": 13.227603943420489,
      "grad_norm": 33.82936477661133,
      "learning_rate": 2.3631947421060154e-06,
      "loss": 0.2034,
      "step": 61720
    },
    {
      "epoch": 13.229747106729533,
      "grad_norm": 53.646793365478516,
      "learning_rate": 2.3603371910272897e-06,
      "loss": 0.1131,
      "step": 61730
    },
    {
      "epoch": 13.231890270038576,
      "grad_norm": 0.17542912065982819,
      "learning_rate": 2.357479639948564e-06,
      "loss": 0.335,
      "step": 61740
    },
    {
      "epoch": 13.234033433347621,
      "grad_norm": 0.02886437438428402,
      "learning_rate": 2.354622088869839e-06,
      "loss": 0.1449,
      "step": 61750
    },
    {
      "epoch": 13.236176596656666,
      "grad_norm": 0.3543448746204376,
      "learning_rate": 2.351764537791113e-06,
      "loss": 0.1178,
      "step": 61760
    },
    {
      "epoch": 13.238319759965709,
      "grad_norm": 0.22045886516571045,
      "learning_rate": 2.3489069867123875e-06,
      "loss": 0.3628,
      "step": 61770
    },
    {
      "epoch": 13.240462923274753,
      "grad_norm": 0.013254575431346893,
      "learning_rate": 2.3460494356336623e-06,
      "loss": 0.152,
      "step": 61780
    },
    {
      "epoch": 13.242606086583798,
      "grad_norm": 0.13578321039676666,
      "learning_rate": 2.3431918845549367e-06,
      "loss": 0.3003,
      "step": 61790
    },
    {
      "epoch": 13.244749249892841,
      "grad_norm": 0.2195504754781723,
      "learning_rate": 2.340334333476211e-06,
      "loss": 0.1346,
      "step": 61800
    },
    {
      "epoch": 13.246892413201886,
      "grad_norm": 0.11800213158130646,
      "learning_rate": 2.337476782397486e-06,
      "loss": 0.1316,
      "step": 61810
    },
    {
      "epoch": 13.24903557651093,
      "grad_norm": 0.024321557953953743,
      "learning_rate": 2.3346192313187597e-06,
      "loss": 0.0019,
      "step": 61820
    },
    {
      "epoch": 13.251178739819974,
      "grad_norm": 0.05668514966964722,
      "learning_rate": 2.3317616802400345e-06,
      "loss": 0.2457,
      "step": 61830
    },
    {
      "epoch": 13.253321903129018,
      "grad_norm": 0.15062645077705383,
      "learning_rate": 2.328904129161309e-06,
      "loss": 0.285,
      "step": 61840
    },
    {
      "epoch": 13.255465066438063,
      "grad_norm": 20.560752868652344,
      "learning_rate": 2.326046578082583e-06,
      "loss": 0.5102,
      "step": 61850
    },
    {
      "epoch": 13.257608229747106,
      "grad_norm": 0.1442093849182129,
      "learning_rate": 2.323189027003858e-06,
      "loss": 0.2829,
      "step": 61860
    },
    {
      "epoch": 13.25975139305615,
      "grad_norm": 0.008021763525903225,
      "learning_rate": 2.3203314759251323e-06,
      "loss": 0.4388,
      "step": 61870
    },
    {
      "epoch": 13.261894556365196,
      "grad_norm": 0.011705270037055016,
      "learning_rate": 2.3174739248464067e-06,
      "loss": 0.0021,
      "step": 61880
    },
    {
      "epoch": 13.264037719674239,
      "grad_norm": 26.03560447692871,
      "learning_rate": 2.3146163737676815e-06,
      "loss": 0.6324,
      "step": 61890
    },
    {
      "epoch": 13.266180882983283,
      "grad_norm": 20.151973724365234,
      "learning_rate": 2.311758822688956e-06,
      "loss": 0.3959,
      "step": 61900
    },
    {
      "epoch": 13.268324046292328,
      "grad_norm": 32.70506286621094,
      "learning_rate": 2.30890127161023e-06,
      "loss": 0.5777,
      "step": 61910
    },
    {
      "epoch": 13.270467209601371,
      "grad_norm": 0.5827433466911316,
      "learning_rate": 2.306043720531505e-06,
      "loss": 0.1606,
      "step": 61920
    },
    {
      "epoch": 13.272610372910416,
      "grad_norm": 0.025675857439637184,
      "learning_rate": 2.303186169452779e-06,
      "loss": 0.6468,
      "step": 61930
    },
    {
      "epoch": 13.27475353621946,
      "grad_norm": 0.17465350031852722,
      "learning_rate": 2.3003286183740536e-06,
      "loss": 0.1839,
      "step": 61940
    },
    {
      "epoch": 13.276896699528503,
      "grad_norm": 0.16804125905036926,
      "learning_rate": 2.2974710672953284e-06,
      "loss": 0.1623,
      "step": 61950
    },
    {
      "epoch": 13.279039862837548,
      "grad_norm": 0.04096205532550812,
      "learning_rate": 2.2946135162166023e-06,
      "loss": 0.1345,
      "step": 61960
    },
    {
      "epoch": 13.281183026146593,
      "grad_norm": 0.08077558875083923,
      "learning_rate": 2.291755965137877e-06,
      "loss": 0.1208,
      "step": 61970
    },
    {
      "epoch": 13.283326189455636,
      "grad_norm": 0.16531571745872498,
      "learning_rate": 2.2888984140591515e-06,
      "loss": 0.3279,
      "step": 61980
    },
    {
      "epoch": 13.28546935276468,
      "grad_norm": 0.17224277555942535,
      "learning_rate": 2.286040862980426e-06,
      "loss": 0.003,
      "step": 61990
    },
    {
      "epoch": 13.287612516073725,
      "grad_norm": 0.08644182980060577,
      "learning_rate": 2.2831833119017006e-06,
      "loss": 0.1926,
      "step": 62000
    },
    {
      "epoch": 13.289755679382768,
      "grad_norm": 0.006412137765437365,
      "learning_rate": 2.280325760822975e-06,
      "loss": 0.7011,
      "step": 62010
    },
    {
      "epoch": 13.291898842691813,
      "grad_norm": 0.048414669930934906,
      "learning_rate": 2.2774682097442493e-06,
      "loss": 0.0026,
      "step": 62020
    },
    {
      "epoch": 13.294042006000858,
      "grad_norm": 21.703834533691406,
      "learning_rate": 2.274610658665524e-06,
      "loss": 0.2603,
      "step": 62030
    },
    {
      "epoch": 13.2961851693099,
      "grad_norm": 0.08668651431798935,
      "learning_rate": 2.2717531075867984e-06,
      "loss": 0.0011,
      "step": 62040
    },
    {
      "epoch": 13.298328332618945,
      "grad_norm": 9.096576690673828,
      "learning_rate": 2.2688955565080728e-06,
      "loss": 0.0038,
      "step": 62050
    },
    {
      "epoch": 13.30047149592799,
      "grad_norm": 0.14640721678733826,
      "learning_rate": 2.266038005429347e-06,
      "loss": 0.2323,
      "step": 62060
    },
    {
      "epoch": 13.302614659237033,
      "grad_norm": 0.9201529026031494,
      "learning_rate": 2.2631804543506215e-06,
      "loss": 0.7608,
      "step": 62070
    },
    {
      "epoch": 13.304757822546078,
      "grad_norm": 24.5172061920166,
      "learning_rate": 2.2603229032718963e-06,
      "loss": 0.4579,
      "step": 62080
    },
    {
      "epoch": 13.306900985855123,
      "grad_norm": 0.19130198657512665,
      "learning_rate": 2.2574653521931706e-06,
      "loss": 0.283,
      "step": 62090
    },
    {
      "epoch": 13.309044149164166,
      "grad_norm": 0.24856507778167725,
      "learning_rate": 2.254607801114445e-06,
      "loss": 0.2665,
      "step": 62100
    },
    {
      "epoch": 13.31118731247321,
      "grad_norm": 0.18567678332328796,
      "learning_rate": 2.2517502500357197e-06,
      "loss": 0.1746,
      "step": 62110
    },
    {
      "epoch": 13.313330475782255,
      "grad_norm": 0.017277220264077187,
      "learning_rate": 2.248892698956994e-06,
      "loss": 0.8147,
      "step": 62120
    },
    {
      "epoch": 13.315473639091298,
      "grad_norm": 0.07446121424436569,
      "learning_rate": 2.2460351478782684e-06,
      "loss": 0.3292,
      "step": 62130
    },
    {
      "epoch": 13.317616802400343,
      "grad_norm": 104.74173736572266,
      "learning_rate": 2.2431775967995432e-06,
      "loss": 0.3586,
      "step": 62140
    },
    {
      "epoch": 13.319759965709387,
      "grad_norm": 0.15758633613586426,
      "learning_rate": 2.2403200457208176e-06,
      "loss": 0.3435,
      "step": 62150
    },
    {
      "epoch": 13.32190312901843,
      "grad_norm": 0.0745520144701004,
      "learning_rate": 2.237462494642092e-06,
      "loss": 0.1693,
      "step": 62160
    },
    {
      "epoch": 13.324046292327475,
      "grad_norm": 0.03834894672036171,
      "learning_rate": 2.2346049435633663e-06,
      "loss": 0.0031,
      "step": 62170
    },
    {
      "epoch": 13.32618945563652,
      "grad_norm": 0.10693974047899246,
      "learning_rate": 2.231747392484641e-06,
      "loss": 0.0041,
      "step": 62180
    },
    {
      "epoch": 13.328332618945563,
      "grad_norm": 0.13865789771080017,
      "learning_rate": 2.2288898414059154e-06,
      "loss": 0.1713,
      "step": 62190
    },
    {
      "epoch": 13.330475782254608,
      "grad_norm": 0.04406603053212166,
      "learning_rate": 2.2260322903271897e-06,
      "loss": 0.5163,
      "step": 62200
    },
    {
      "epoch": 13.332618945563652,
      "grad_norm": 0.044753726571798325,
      "learning_rate": 2.223174739248464e-06,
      "loss": 0.163,
      "step": 62210
    },
    {
      "epoch": 13.334762108872695,
      "grad_norm": 0.12123329192399979,
      "learning_rate": 2.220317188169739e-06,
      "loss": 0.8813,
      "step": 62220
    },
    {
      "epoch": 13.33690527218174,
      "grad_norm": 0.016034115105867386,
      "learning_rate": 2.2174596370910132e-06,
      "loss": 0.1975,
      "step": 62230
    },
    {
      "epoch": 13.339048435490785,
      "grad_norm": 1.5097174644470215,
      "learning_rate": 2.2146020860122876e-06,
      "loss": 0.5021,
      "step": 62240
    },
    {
      "epoch": 13.341191598799828,
      "grad_norm": 3.133342981338501,
      "learning_rate": 2.211744534933562e-06,
      "loss": 0.1515,
      "step": 62250
    },
    {
      "epoch": 13.343334762108872,
      "grad_norm": 0.12784706056118011,
      "learning_rate": 2.2088869838548367e-06,
      "loss": 0.0034,
      "step": 62260
    },
    {
      "epoch": 13.345477925417917,
      "grad_norm": 0.0123090585693717,
      "learning_rate": 2.206029432776111e-06,
      "loss": 0.3418,
      "step": 62270
    },
    {
      "epoch": 13.34762108872696,
      "grad_norm": 0.1004980131983757,
      "learning_rate": 2.2031718816973854e-06,
      "loss": 0.4377,
      "step": 62280
    },
    {
      "epoch": 13.349764252036005,
      "grad_norm": 0.01681584306061268,
      "learning_rate": 2.20031433061866e-06,
      "loss": 0.1491,
      "step": 62290
    },
    {
      "epoch": 13.35190741534505,
      "grad_norm": 0.047652795910835266,
      "learning_rate": 2.1974567795399345e-06,
      "loss": 0.3647,
      "step": 62300
    },
    {
      "epoch": 13.354050578654093,
      "grad_norm": 2.4028449058532715,
      "learning_rate": 2.194599228461209e-06,
      "loss": 0.4173,
      "step": 62310
    },
    {
      "epoch": 13.356193741963137,
      "grad_norm": 0.1417827308177948,
      "learning_rate": 2.1917416773824832e-06,
      "loss": 0.2012,
      "step": 62320
    },
    {
      "epoch": 13.358336905272182,
      "grad_norm": 0.07520879805088043,
      "learning_rate": 2.1888841263037576e-06,
      "loss": 0.247,
      "step": 62330
    },
    {
      "epoch": 13.360480068581225,
      "grad_norm": 32.761497497558594,
      "learning_rate": 2.1860265752250324e-06,
      "loss": 0.1517,
      "step": 62340
    },
    {
      "epoch": 13.36262323189027,
      "grad_norm": 0.08989027887582779,
      "learning_rate": 2.1831690241463067e-06,
      "loss": 0.4236,
      "step": 62350
    },
    {
      "epoch": 13.364766395199315,
      "grad_norm": 0.48996374011039734,
      "learning_rate": 2.180311473067581e-06,
      "loss": 0.377,
      "step": 62360
    },
    {
      "epoch": 13.366909558508357,
      "grad_norm": 0.09873554855585098,
      "learning_rate": 2.177453921988856e-06,
      "loss": 0.3305,
      "step": 62370
    },
    {
      "epoch": 13.369052721817402,
      "grad_norm": 19.333486557006836,
      "learning_rate": 2.17459637091013e-06,
      "loss": 0.1623,
      "step": 62380
    },
    {
      "epoch": 13.371195885126447,
      "grad_norm": 0.009345706552267075,
      "learning_rate": 2.1717388198314045e-06,
      "loss": 0.3579,
      "step": 62390
    },
    {
      "epoch": 13.37333904843549,
      "grad_norm": 0.022335419431328773,
      "learning_rate": 2.1688812687526793e-06,
      "loss": 0.6094,
      "step": 62400
    },
    {
      "epoch": 13.375482211744535,
      "grad_norm": 0.0811801329255104,
      "learning_rate": 2.1660237176739537e-06,
      "loss": 0.2626,
      "step": 62410
    },
    {
      "epoch": 13.37762537505358,
      "grad_norm": 0.17704875767230988,
      "learning_rate": 2.163166166595228e-06,
      "loss": 0.9013,
      "step": 62420
    },
    {
      "epoch": 13.379768538362622,
      "grad_norm": 0.11780062317848206,
      "learning_rate": 2.160308615516503e-06,
      "loss": 0.1996,
      "step": 62430
    },
    {
      "epoch": 13.381911701671667,
      "grad_norm": 0.17539016902446747,
      "learning_rate": 2.1574510644377767e-06,
      "loss": 0.5568,
      "step": 62440
    },
    {
      "epoch": 13.384054864980712,
      "grad_norm": 0.28539660573005676,
      "learning_rate": 2.1545935133590515e-06,
      "loss": 0.3245,
      "step": 62450
    },
    {
      "epoch": 13.386198028289755,
      "grad_norm": 24.83902931213379,
      "learning_rate": 2.151735962280326e-06,
      "loss": 0.5279,
      "step": 62460
    },
    {
      "epoch": 13.3883411915988,
      "grad_norm": 0.22335955500602722,
      "learning_rate": 2.1488784112016e-06,
      "loss": 0.5303,
      "step": 62470
    },
    {
      "epoch": 13.390484354907844,
      "grad_norm": 0.024890350177884102,
      "learning_rate": 2.146020860122875e-06,
      "loss": 0.0027,
      "step": 62480
    },
    {
      "epoch": 13.392627518216887,
      "grad_norm": 0.07244385778903961,
      "learning_rate": 2.1431633090441493e-06,
      "loss": 0.6416,
      "step": 62490
    },
    {
      "epoch": 13.394770681525932,
      "grad_norm": 0.08339737355709076,
      "learning_rate": 2.1403057579654237e-06,
      "loss": 0.4099,
      "step": 62500
    },
    {
      "epoch": 13.396913844834977,
      "grad_norm": 0.38663238286972046,
      "learning_rate": 2.1374482068866985e-06,
      "loss": 0.5423,
      "step": 62510
    },
    {
      "epoch": 13.39905700814402,
      "grad_norm": 0.1955859661102295,
      "learning_rate": 2.134590655807973e-06,
      "loss": 0.2298,
      "step": 62520
    },
    {
      "epoch": 13.401200171453064,
      "grad_norm": 0.3132292330265045,
      "learning_rate": 2.131733104729247e-06,
      "loss": 0.0021,
      "step": 62530
    },
    {
      "epoch": 13.40334333476211,
      "grad_norm": 0.05984002724289894,
      "learning_rate": 2.128875553650522e-06,
      "loss": 0.3937,
      "step": 62540
    },
    {
      "epoch": 13.405486498071152,
      "grad_norm": 0.013971801847219467,
      "learning_rate": 2.126018002571796e-06,
      "loss": 0.6854,
      "step": 62550
    },
    {
      "epoch": 13.407629661380197,
      "grad_norm": 0.1314614713191986,
      "learning_rate": 2.1231604514930706e-06,
      "loss": 0.75,
      "step": 62560
    },
    {
      "epoch": 13.409772824689242,
      "grad_norm": 16.2158145904541,
      "learning_rate": 2.120302900414345e-06,
      "loss": 0.4422,
      "step": 62570
    },
    {
      "epoch": 13.411915987998286,
      "grad_norm": 0.3462487757205963,
      "learning_rate": 2.1174453493356193e-06,
      "loss": 0.3382,
      "step": 62580
    },
    {
      "epoch": 13.41405915130733,
      "grad_norm": 0.2184825837612152,
      "learning_rate": 2.114587798256894e-06,
      "loss": 0.2539,
      "step": 62590
    },
    {
      "epoch": 13.416202314616374,
      "grad_norm": 0.16078191995620728,
      "learning_rate": 2.1117302471781685e-06,
      "loss": 0.6032,
      "step": 62600
    },
    {
      "epoch": 13.418345477925419,
      "grad_norm": 23.758203506469727,
      "learning_rate": 2.108872696099443e-06,
      "loss": 0.4284,
      "step": 62610
    },
    {
      "epoch": 13.420488641234462,
      "grad_norm": 0.08322972059249878,
      "learning_rate": 2.1060151450207176e-06,
      "loss": 0.1262,
      "step": 62620
    },
    {
      "epoch": 13.422631804543506,
      "grad_norm": 0.33697906136512756,
      "learning_rate": 2.103157593941992e-06,
      "loss": 0.346,
      "step": 62630
    },
    {
      "epoch": 13.424774967852551,
      "grad_norm": 0.40155041217803955,
      "learning_rate": 2.1003000428632663e-06,
      "loss": 0.0043,
      "step": 62640
    },
    {
      "epoch": 13.426918131161594,
      "grad_norm": 0.014115653932094574,
      "learning_rate": 2.097442491784541e-06,
      "loss": 0.1591,
      "step": 62650
    },
    {
      "epoch": 13.429061294470639,
      "grad_norm": 3.3862104415893555,
      "learning_rate": 2.0945849407058154e-06,
      "loss": 0.158,
      "step": 62660
    },
    {
      "epoch": 13.431204457779684,
      "grad_norm": 0.009872755035758018,
      "learning_rate": 2.0917273896270898e-06,
      "loss": 0.7618,
      "step": 62670
    },
    {
      "epoch": 13.433347621088727,
      "grad_norm": 0.01469335425645113,
      "learning_rate": 2.088869838548364e-06,
      "loss": 0.3578,
      "step": 62680
    },
    {
      "epoch": 13.435490784397771,
      "grad_norm": 0.01870153471827507,
      "learning_rate": 2.0860122874696385e-06,
      "loss": 0.4335,
      "step": 62690
    },
    {
      "epoch": 13.437633947706816,
      "grad_norm": 0.23524102568626404,
      "learning_rate": 2.0831547363909133e-06,
      "loss": 0.1948,
      "step": 62700
    },
    {
      "epoch": 13.439777111015859,
      "grad_norm": 0.11665461957454681,
      "learning_rate": 2.0802971853121876e-06,
      "loss": 0.1604,
      "step": 62710
    },
    {
      "epoch": 13.441920274324904,
      "grad_norm": 0.028835685923695564,
      "learning_rate": 2.077439634233462e-06,
      "loss": 0.2957,
      "step": 62720
    },
    {
      "epoch": 13.444063437633949,
      "grad_norm": 0.015933582559227943,
      "learning_rate": 2.0745820831547367e-06,
      "loss": 0.0025,
      "step": 62730
    },
    {
      "epoch": 13.446206600942991,
      "grad_norm": 0.005151587538421154,
      "learning_rate": 2.071724532076011e-06,
      "loss": 0.153,
      "step": 62740
    },
    {
      "epoch": 13.448349764252036,
      "grad_norm": 0.008137756027281284,
      "learning_rate": 2.0688669809972854e-06,
      "loss": 0.6115,
      "step": 62750
    },
    {
      "epoch": 13.450492927561081,
      "grad_norm": 0.02805258147418499,
      "learning_rate": 2.06600942991856e-06,
      "loss": 0.2646,
      "step": 62760
    },
    {
      "epoch": 13.452636090870124,
      "grad_norm": 0.008766727522015572,
      "learning_rate": 2.0631518788398346e-06,
      "loss": 0.2706,
      "step": 62770
    },
    {
      "epoch": 13.454779254179169,
      "grad_norm": 0.17431971430778503,
      "learning_rate": 2.060294327761109e-06,
      "loss": 0.4186,
      "step": 62780
    },
    {
      "epoch": 13.456922417488213,
      "grad_norm": 32.58015441894531,
      "learning_rate": 2.0574367766823833e-06,
      "loss": 0.8678,
      "step": 62790
    },
    {
      "epoch": 13.459065580797256,
      "grad_norm": 0.013639886863529682,
      "learning_rate": 2.054579225603658e-06,
      "loss": 0.1706,
      "step": 62800
    },
    {
      "epoch": 13.461208744106301,
      "grad_norm": 0.022647535428404808,
      "learning_rate": 2.0517216745249324e-06,
      "loss": 0.187,
      "step": 62810
    },
    {
      "epoch": 13.463351907415346,
      "grad_norm": 0.331097275018692,
      "learning_rate": 2.0488641234462068e-06,
      "loss": 0.0035,
      "step": 62820
    },
    {
      "epoch": 13.465495070724389,
      "grad_norm": 0.08409009128808975,
      "learning_rate": 2.046006572367481e-06,
      "loss": 0.0045,
      "step": 62830
    },
    {
      "epoch": 13.467638234033434,
      "grad_norm": 0.0798419862985611,
      "learning_rate": 2.0431490212887555e-06,
      "loss": 0.18,
      "step": 62840
    },
    {
      "epoch": 13.469781397342478,
      "grad_norm": 0.041543811559677124,
      "learning_rate": 2.0402914702100302e-06,
      "loss": 0.0017,
      "step": 62850
    },
    {
      "epoch": 13.471924560651521,
      "grad_norm": 0.07992378622293472,
      "learning_rate": 2.0374339191313046e-06,
      "loss": 0.0014,
      "step": 62860
    },
    {
      "epoch": 13.474067723960566,
      "grad_norm": 31.478452682495117,
      "learning_rate": 2.034576368052579e-06,
      "loss": 0.8659,
      "step": 62870
    },
    {
      "epoch": 13.47621088726961,
      "grad_norm": 38.256649017333984,
      "learning_rate": 2.0317188169738537e-06,
      "loss": 0.7021,
      "step": 62880
    },
    {
      "epoch": 13.478354050578654,
      "grad_norm": 0.03218439966440201,
      "learning_rate": 2.028861265895128e-06,
      "loss": 0.2669,
      "step": 62890
    },
    {
      "epoch": 13.480497213887698,
      "grad_norm": 0.23416824638843536,
      "learning_rate": 2.0260037148164024e-06,
      "loss": 0.3752,
      "step": 62900
    },
    {
      "epoch": 13.482640377196743,
      "grad_norm": 0.316234827041626,
      "learning_rate": 2.023146163737677e-06,
      "loss": 0.0029,
      "step": 62910
    },
    {
      "epoch": 13.484783540505786,
      "grad_norm": 0.13757817447185516,
      "learning_rate": 2.020288612658951e-06,
      "loss": 0.2906,
      "step": 62920
    },
    {
      "epoch": 13.48692670381483,
      "grad_norm": 0.12093623727560043,
      "learning_rate": 2.017431061580226e-06,
      "loss": 0.2711,
      "step": 62930
    },
    {
      "epoch": 13.489069867123876,
      "grad_norm": 0.7721169590950012,
      "learning_rate": 2.0145735105015007e-06,
      "loss": 0.3065,
      "step": 62940
    },
    {
      "epoch": 13.491213030432919,
      "grad_norm": 442.1917419433594,
      "learning_rate": 2.0117159594227746e-06,
      "loss": 0.2165,
      "step": 62950
    },
    {
      "epoch": 13.493356193741963,
      "grad_norm": 0.2835347354412079,
      "learning_rate": 2.0088584083440494e-06,
      "loss": 0.7615,
      "step": 62960
    },
    {
      "epoch": 13.495499357051008,
      "grad_norm": 0.4883587658405304,
      "learning_rate": 2.0060008572653237e-06,
      "loss": 0.425,
      "step": 62970
    },
    {
      "epoch": 13.497642520360051,
      "grad_norm": 0.09086604416370392,
      "learning_rate": 2.003143306186598e-06,
      "loss": 0.1973,
      "step": 62980
    },
    {
      "epoch": 13.499785683669096,
      "grad_norm": 0.20521385967731476,
      "learning_rate": 2.000285755107873e-06,
      "loss": 0.27,
      "step": 62990
    },
    {
      "epoch": 13.50192884697814,
      "grad_norm": 0.08265277743339539,
      "learning_rate": 1.997428204029147e-06,
      "loss": 0.5464,
      "step": 63000
    },
    {
      "epoch": 13.504072010287183,
      "grad_norm": 0.15764360129833221,
      "learning_rate": 1.9945706529504216e-06,
      "loss": 0.1617,
      "step": 63010
    },
    {
      "epoch": 13.506215173596228,
      "grad_norm": 0.7148053646087646,
      "learning_rate": 1.9917131018716963e-06,
      "loss": 0.6091,
      "step": 63020
    },
    {
      "epoch": 13.508358336905273,
      "grad_norm": 24.590246200561523,
      "learning_rate": 1.9888555507929707e-06,
      "loss": 0.4627,
      "step": 63030
    },
    {
      "epoch": 13.510501500214316,
      "grad_norm": 0.14955775439739227,
      "learning_rate": 1.985997999714245e-06,
      "loss": 0.2,
      "step": 63040
    },
    {
      "epoch": 13.51264466352336,
      "grad_norm": 0.06466817110776901,
      "learning_rate": 1.98314044863552e-06,
      "loss": 0.0028,
      "step": 63050
    },
    {
      "epoch": 13.514787826832405,
      "grad_norm": 0.05998056009411812,
      "learning_rate": 1.9802828975567937e-06,
      "loss": 0.3162,
      "step": 63060
    },
    {
      "epoch": 13.516930990141448,
      "grad_norm": 23.219154357910156,
      "learning_rate": 1.9774253464780685e-06,
      "loss": 0.2846,
      "step": 63070
    },
    {
      "epoch": 13.519074153450493,
      "grad_norm": 0.0031492533162236214,
      "learning_rate": 1.974567795399343e-06,
      "loss": 0.0016,
      "step": 63080
    },
    {
      "epoch": 13.521217316759538,
      "grad_norm": 0.2650497555732727,
      "learning_rate": 1.9717102443206172e-06,
      "loss": 0.0017,
      "step": 63090
    },
    {
      "epoch": 13.52336048006858,
      "grad_norm": 0.11305495351552963,
      "learning_rate": 1.968852693241892e-06,
      "loss": 0.4259,
      "step": 63100
    },
    {
      "epoch": 13.525503643377625,
      "grad_norm": 0.5155910849571228,
      "learning_rate": 1.9659951421631663e-06,
      "loss": 0.0015,
      "step": 63110
    },
    {
      "epoch": 13.52764680668667,
      "grad_norm": 0.15957282483577728,
      "learning_rate": 1.9631375910844407e-06,
      "loss": 0.1586,
      "step": 63120
    },
    {
      "epoch": 13.529789969995713,
      "grad_norm": 20.2852840423584,
      "learning_rate": 1.9602800400057155e-06,
      "loss": 0.4492,
      "step": 63130
    },
    {
      "epoch": 13.531933133304758,
      "grad_norm": 0.04728284478187561,
      "learning_rate": 1.95742248892699e-06,
      "loss": 0.2546,
      "step": 63140
    },
    {
      "epoch": 13.534076296613803,
      "grad_norm": 40.087284088134766,
      "learning_rate": 1.954564937848264e-06,
      "loss": 0.5714,
      "step": 63150
    },
    {
      "epoch": 13.536219459922846,
      "grad_norm": 0.24527086317539215,
      "learning_rate": 1.951707386769539e-06,
      "loss": 0.4973,
      "step": 63160
    },
    {
      "epoch": 13.53836262323189,
      "grad_norm": 0.2713342010974884,
      "learning_rate": 1.9488498356908133e-06,
      "loss": 0.4806,
      "step": 63170
    },
    {
      "epoch": 13.540505786540935,
      "grad_norm": 0.1274631917476654,
      "learning_rate": 1.9459922846120877e-06,
      "loss": 0.1748,
      "step": 63180
    },
    {
      "epoch": 13.542648949849978,
      "grad_norm": 0.23539093136787415,
      "learning_rate": 1.943134733533362e-06,
      "loss": 0.3264,
      "step": 63190
    },
    {
      "epoch": 13.544792113159023,
      "grad_norm": 25.286401748657227,
      "learning_rate": 1.9402771824546364e-06,
      "loss": 0.1588,
      "step": 63200
    },
    {
      "epoch": 13.546935276468067,
      "grad_norm": 0.0451880544424057,
      "learning_rate": 1.937419631375911e-06,
      "loss": 0.4489,
      "step": 63210
    },
    {
      "epoch": 13.54907843977711,
      "grad_norm": 0.4664536714553833,
      "learning_rate": 1.9345620802971855e-06,
      "loss": 0.6333,
      "step": 63220
    },
    {
      "epoch": 13.551221603086155,
      "grad_norm": 322.88177490234375,
      "learning_rate": 1.93170452921846e-06,
      "loss": 0.3922,
      "step": 63230
    },
    {
      "epoch": 13.5533647663952,
      "grad_norm": 0.003267813241109252,
      "learning_rate": 1.9288469781397346e-06,
      "loss": 0.3553,
      "step": 63240
    },
    {
      "epoch": 13.555507929704243,
      "grad_norm": 2.5805201530456543,
      "learning_rate": 1.925989427061009e-06,
      "loss": 0.58,
      "step": 63250
    },
    {
      "epoch": 13.557651093013288,
      "grad_norm": 26.149124145507812,
      "learning_rate": 1.9231318759822833e-06,
      "loss": 0.3197,
      "step": 63260
    },
    {
      "epoch": 13.559794256322332,
      "grad_norm": 0.04631439968943596,
      "learning_rate": 1.9202743249035577e-06,
      "loss": 0.1637,
      "step": 63270
    },
    {
      "epoch": 13.561937419631375,
      "grad_norm": 1.6286989450454712,
      "learning_rate": 1.9174167738248324e-06,
      "loss": 0.4618,
      "step": 63280
    },
    {
      "epoch": 13.56408058294042,
      "grad_norm": 0.041611723601818085,
      "learning_rate": 1.914559222746107e-06,
      "loss": 0.2343,
      "step": 63290
    },
    {
      "epoch": 13.566223746249465,
      "grad_norm": 51.561309814453125,
      "learning_rate": 1.911701671667381e-06,
      "loss": 0.2984,
      "step": 63300
    },
    {
      "epoch": 13.568366909558508,
      "grad_norm": 0.01944403350353241,
      "learning_rate": 1.9088441205886555e-06,
      "loss": 0.3832,
      "step": 63310
    },
    {
      "epoch": 13.570510072867553,
      "grad_norm": 0.11378544569015503,
      "learning_rate": 1.9059865695099303e-06,
      "loss": 0.0025,
      "step": 63320
    },
    {
      "epoch": 13.572653236176597,
      "grad_norm": 42.28133010864258,
      "learning_rate": 1.9031290184312046e-06,
      "loss": 0.6517,
      "step": 63330
    },
    {
      "epoch": 13.57479639948564,
      "grad_norm": 0.1511676013469696,
      "learning_rate": 1.9002714673524792e-06,
      "loss": 0.5981,
      "step": 63340
    },
    {
      "epoch": 13.576939562794685,
      "grad_norm": 1.405861496925354,
      "learning_rate": 1.8974139162737533e-06,
      "loss": 0.2552,
      "step": 63350
    },
    {
      "epoch": 13.57908272610373,
      "grad_norm": 45.82246780395508,
      "learning_rate": 1.8945563651950281e-06,
      "loss": 0.6635,
      "step": 63360
    },
    {
      "epoch": 13.581225889412773,
      "grad_norm": 0.2920537292957306,
      "learning_rate": 1.8916988141163027e-06,
      "loss": 0.3443,
      "step": 63370
    },
    {
      "epoch": 13.583369052721817,
      "grad_norm": 17.828147888183594,
      "learning_rate": 1.8888412630375768e-06,
      "loss": 0.4854,
      "step": 63380
    },
    {
      "epoch": 13.585512216030862,
      "grad_norm": 0.42138880491256714,
      "learning_rate": 1.8859837119588514e-06,
      "loss": 0.2313,
      "step": 63390
    },
    {
      "epoch": 13.587655379339905,
      "grad_norm": 18.63639259338379,
      "learning_rate": 1.883126160880126e-06,
      "loss": 0.6133,
      "step": 63400
    },
    {
      "epoch": 13.58979854264895,
      "grad_norm": 0.22901558876037598,
      "learning_rate": 1.8802686098014003e-06,
      "loss": 0.5362,
      "step": 63410
    },
    {
      "epoch": 13.591941705957995,
      "grad_norm": 43.893741607666016,
      "learning_rate": 1.8774110587226749e-06,
      "loss": 0.6664,
      "step": 63420
    },
    {
      "epoch": 13.594084869267038,
      "grad_norm": 0.24638575315475464,
      "learning_rate": 1.8745535076439492e-06,
      "loss": 0.1226,
      "step": 63430
    },
    {
      "epoch": 13.596228032576082,
      "grad_norm": 0.1283842772245407,
      "learning_rate": 1.8716959565652238e-06,
      "loss": 0.1838,
      "step": 63440
    },
    {
      "epoch": 13.598371195885127,
      "grad_norm": 0.02167954109609127,
      "learning_rate": 1.8688384054864983e-06,
      "loss": 0.1844,
      "step": 63450
    },
    {
      "epoch": 13.60051435919417,
      "grad_norm": 0.028582196682691574,
      "learning_rate": 1.8659808544077727e-06,
      "loss": 0.1474,
      "step": 63460
    },
    {
      "epoch": 13.602657522503215,
      "grad_norm": 0.03282694146037102,
      "learning_rate": 1.8631233033290472e-06,
      "loss": 0.5231,
      "step": 63470
    },
    {
      "epoch": 13.60480068581226,
      "grad_norm": 0.05552142485976219,
      "learning_rate": 1.8602657522503218e-06,
      "loss": 0.0026,
      "step": 63480
    },
    {
      "epoch": 13.606943849121302,
      "grad_norm": 0.02347697876393795,
      "learning_rate": 1.857408201171596e-06,
      "loss": 0.2829,
      "step": 63490
    },
    {
      "epoch": 13.609087012430347,
      "grad_norm": 11959.431640625,
      "learning_rate": 1.8545506500928705e-06,
      "loss": 0.4497,
      "step": 63500
    },
    {
      "epoch": 13.611230175739392,
      "grad_norm": 21.502565383911133,
      "learning_rate": 1.8516930990141449e-06,
      "loss": 0.4659,
      "step": 63510
    },
    {
      "epoch": 13.613373339048435,
      "grad_norm": 0.09871932864189148,
      "learning_rate": 1.8488355479354194e-06,
      "loss": 0.0966,
      "step": 63520
    },
    {
      "epoch": 13.61551650235748,
      "grad_norm": 0.03464291989803314,
      "learning_rate": 1.845977996856694e-06,
      "loss": 0.49,
      "step": 63530
    },
    {
      "epoch": 13.617659665666524,
      "grad_norm": 33.238853454589844,
      "learning_rate": 1.8431204457779683e-06,
      "loss": 0.6658,
      "step": 63540
    },
    {
      "epoch": 13.619802828975567,
      "grad_norm": 1.273950457572937,
      "learning_rate": 1.840262894699243e-06,
      "loss": 0.0047,
      "step": 63550
    },
    {
      "epoch": 13.621945992284612,
      "grad_norm": 5255.5654296875,
      "learning_rate": 1.8374053436205175e-06,
      "loss": 0.2871,
      "step": 63560
    },
    {
      "epoch": 13.624089155593657,
      "grad_norm": 0.0565420538187027,
      "learning_rate": 1.8345477925417918e-06,
      "loss": 0.2002,
      "step": 63570
    },
    {
      "epoch": 13.6262323189027,
      "grad_norm": 0.295112282037735,
      "learning_rate": 1.8316902414630664e-06,
      "loss": 0.003,
      "step": 63580
    },
    {
      "epoch": 13.628375482211744,
      "grad_norm": 0.06065738573670387,
      "learning_rate": 1.8288326903843407e-06,
      "loss": 0.2116,
      "step": 63590
    },
    {
      "epoch": 13.63051864552079,
      "grad_norm": 0.018414892256259918,
      "learning_rate": 1.8259751393056153e-06,
      "loss": 0.0017,
      "step": 63600
    },
    {
      "epoch": 13.632661808829832,
      "grad_norm": 0.17066740989685059,
      "learning_rate": 1.8231175882268899e-06,
      "loss": 0.6273,
      "step": 63610
    },
    {
      "epoch": 13.634804972138877,
      "grad_norm": 19.093639373779297,
      "learning_rate": 1.820260037148164e-06,
      "loss": 0.6256,
      "step": 63620
    },
    {
      "epoch": 13.636948135447922,
      "grad_norm": 23.127634048461914,
      "learning_rate": 1.8174024860694386e-06,
      "loss": 0.2012,
      "step": 63630
    },
    {
      "epoch": 13.639091298756965,
      "grad_norm": 0.31028056144714355,
      "learning_rate": 1.8145449349907131e-06,
      "loss": 0.4178,
      "step": 63640
    },
    {
      "epoch": 13.64123446206601,
      "grad_norm": 0.178998202085495,
      "learning_rate": 1.8116873839119875e-06,
      "loss": 0.3648,
      "step": 63650
    },
    {
      "epoch": 13.643377625375054,
      "grad_norm": 0.13418999314308167,
      "learning_rate": 1.808829832833262e-06,
      "loss": 1.0132,
      "step": 63660
    },
    {
      "epoch": 13.645520788684097,
      "grad_norm": 0.33749866485595703,
      "learning_rate": 1.8059722817545364e-06,
      "loss": 0.3048,
      "step": 63670
    },
    {
      "epoch": 13.647663951993142,
      "grad_norm": 0.1992957592010498,
      "learning_rate": 1.803114730675811e-06,
      "loss": 0.1879,
      "step": 63680
    },
    {
      "epoch": 13.649807115302186,
      "grad_norm": 0.21819916367530823,
      "learning_rate": 1.8002571795970855e-06,
      "loss": 0.3437,
      "step": 63690
    },
    {
      "epoch": 13.65195027861123,
      "grad_norm": 0.33261048793792725,
      "learning_rate": 1.7973996285183599e-06,
      "loss": 0.1658,
      "step": 63700
    },
    {
      "epoch": 13.654093441920274,
      "grad_norm": 0.14814408123493195,
      "learning_rate": 1.7945420774396344e-06,
      "loss": 0.2328,
      "step": 63710
    },
    {
      "epoch": 13.656236605229319,
      "grad_norm": 1.1357333660125732,
      "learning_rate": 1.791684526360909e-06,
      "loss": 0.254,
      "step": 63720
    },
    {
      "epoch": 13.658379768538362,
      "grad_norm": 47.553401947021484,
      "learning_rate": 1.7888269752821831e-06,
      "loss": 0.2966,
      "step": 63730
    },
    {
      "epoch": 13.660522931847407,
      "grad_norm": 0.09565073996782303,
      "learning_rate": 1.7859694242034577e-06,
      "loss": 0.3473,
      "step": 63740
    },
    {
      "epoch": 13.662666095156451,
      "grad_norm": 20.397424697875977,
      "learning_rate": 1.7831118731247325e-06,
      "loss": 0.5291,
      "step": 63750
    },
    {
      "epoch": 13.664809258465494,
      "grad_norm": 0.4092143774032593,
      "learning_rate": 1.7802543220460066e-06,
      "loss": 1.1537,
      "step": 63760
    },
    {
      "epoch": 13.666952421774539,
      "grad_norm": 0.2858326733112335,
      "learning_rate": 1.7773967709672812e-06,
      "loss": 0.5635,
      "step": 63770
    },
    {
      "epoch": 13.669095585083584,
      "grad_norm": 0.2671366333961487,
      "learning_rate": 1.7745392198885555e-06,
      "loss": 0.1767,
      "step": 63780
    },
    {
      "epoch": 13.671238748392627,
      "grad_norm": 0.04813823848962784,
      "learning_rate": 1.77168166880983e-06,
      "loss": 0.2715,
      "step": 63790
    },
    {
      "epoch": 13.673381911701671,
      "grad_norm": 0.1564878225326538,
      "learning_rate": 1.7688241177311047e-06,
      "loss": 0.5111,
      "step": 63800
    },
    {
      "epoch": 13.675525075010716,
      "grad_norm": 110.38768005371094,
      "learning_rate": 1.765966566652379e-06,
      "loss": 0.3253,
      "step": 63810
    },
    {
      "epoch": 13.67766823831976,
      "grad_norm": 0.5029965043067932,
      "learning_rate": 1.7631090155736536e-06,
      "loss": 0.3736,
      "step": 63820
    },
    {
      "epoch": 13.679811401628804,
      "grad_norm": 62.180606842041016,
      "learning_rate": 1.7602514644949281e-06,
      "loss": 0.7835,
      "step": 63830
    },
    {
      "epoch": 13.681954564937849,
      "grad_norm": 0.17609186470508575,
      "learning_rate": 1.7573939134162025e-06,
      "loss": 0.223,
      "step": 63840
    },
    {
      "epoch": 13.684097728246892,
      "grad_norm": 81.67272186279297,
      "learning_rate": 1.754536362337477e-06,
      "loss": 0.3275,
      "step": 63850
    },
    {
      "epoch": 13.686240891555936,
      "grad_norm": 348.2485046386719,
      "learning_rate": 1.7516788112587512e-06,
      "loss": 0.6345,
      "step": 63860
    },
    {
      "epoch": 13.688384054864981,
      "grad_norm": 0.005074643529951572,
      "learning_rate": 1.7488212601800258e-06,
      "loss": 0.1488,
      "step": 63870
    },
    {
      "epoch": 13.690527218174024,
      "grad_norm": 0.003003435442224145,
      "learning_rate": 1.7459637091013003e-06,
      "loss": 0.3996,
      "step": 63880
    },
    {
      "epoch": 13.692670381483069,
      "grad_norm": 0.8081409931182861,
      "learning_rate": 1.7431061580225747e-06,
      "loss": 0.0034,
      "step": 63890
    },
    {
      "epoch": 13.694813544792114,
      "grad_norm": 0.12030599266290665,
      "learning_rate": 1.7402486069438492e-06,
      "loss": 0.2825,
      "step": 63900
    },
    {
      "epoch": 13.696956708101157,
      "grad_norm": 0.01674020290374756,
      "learning_rate": 1.7373910558651238e-06,
      "loss": 0.3343,
      "step": 63910
    },
    {
      "epoch": 13.699099871410201,
      "grad_norm": 0.022344423457980156,
      "learning_rate": 1.7345335047863982e-06,
      "loss": 0.6274,
      "step": 63920
    },
    {
      "epoch": 13.701243034719246,
      "grad_norm": 0.04432161524891853,
      "learning_rate": 1.7316759537076727e-06,
      "loss": 0.2706,
      "step": 63930
    },
    {
      "epoch": 13.703386198028289,
      "grad_norm": 0.0189143605530262,
      "learning_rate": 1.728818402628947e-06,
      "loss": 0.6384,
      "step": 63940
    },
    {
      "epoch": 13.705529361337334,
      "grad_norm": 0.1822976917028427,
      "learning_rate": 1.7259608515502216e-06,
      "loss": 0.2951,
      "step": 63950
    },
    {
      "epoch": 13.707672524646378,
      "grad_norm": 0.331560343503952,
      "learning_rate": 1.7231033004714962e-06,
      "loss": 0.2856,
      "step": 63960
    },
    {
      "epoch": 13.709815687955421,
      "grad_norm": 46.17148971557617,
      "learning_rate": 1.7202457493927703e-06,
      "loss": 0.3522,
      "step": 63970
    },
    {
      "epoch": 13.711958851264466,
      "grad_norm": 0.07667718827724457,
      "learning_rate": 1.7173881983140451e-06,
      "loss": 0.4428,
      "step": 63980
    },
    {
      "epoch": 13.71410201457351,
      "grad_norm": 0.052263569086790085,
      "learning_rate": 1.7145306472353197e-06,
      "loss": 0.2471,
      "step": 63990
    },
    {
      "epoch": 13.716245177882554,
      "grad_norm": 0.004731429275125265,
      "learning_rate": 1.7116730961565938e-06,
      "loss": 0.0023,
      "step": 64000
    },
    {
      "epoch": 13.718388341191599,
      "grad_norm": 0.010097420774400234,
      "learning_rate": 1.7088155450778684e-06,
      "loss": 0.0044,
      "step": 64010
    },
    {
      "epoch": 13.720531504500643,
      "grad_norm": 48.655616760253906,
      "learning_rate": 1.7059579939991427e-06,
      "loss": 0.4298,
      "step": 64020
    },
    {
      "epoch": 13.722674667809688,
      "grad_norm": 0.05698942393064499,
      "learning_rate": 1.7031004429204173e-06,
      "loss": 0.0033,
      "step": 64030
    },
    {
      "epoch": 13.724817831118731,
      "grad_norm": 0.12343563884496689,
      "learning_rate": 1.7002428918416919e-06,
      "loss": 0.2859,
      "step": 64040
    },
    {
      "epoch": 13.726960994427776,
      "grad_norm": 0.06095851585268974,
      "learning_rate": 1.6973853407629662e-06,
      "loss": 0.3511,
      "step": 64050
    },
    {
      "epoch": 13.72910415773682,
      "grad_norm": 0.22941172122955322,
      "learning_rate": 1.6945277896842408e-06,
      "loss": 0.0021,
      "step": 64060
    },
    {
      "epoch": 13.731247321045863,
      "grad_norm": 0.22870756685733795,
      "learning_rate": 1.6916702386055153e-06,
      "loss": 0.1195,
      "step": 64070
    },
    {
      "epoch": 13.733390484354908,
      "grad_norm": 0.006814687047153711,
      "learning_rate": 1.6888126875267897e-06,
      "loss": 0.0009,
      "step": 64080
    },
    {
      "epoch": 13.735533647663953,
      "grad_norm": 0.017086375504732132,
      "learning_rate": 1.6859551364480643e-06,
      "loss": 0.6689,
      "step": 64090
    },
    {
      "epoch": 13.737676810972996,
      "grad_norm": 0.029782146215438843,
      "learning_rate": 1.6830975853693384e-06,
      "loss": 0.3217,
      "step": 64100
    },
    {
      "epoch": 13.73981997428204,
      "grad_norm": 0.003918054047971964,
      "learning_rate": 1.680240034290613e-06,
      "loss": 0.1367,
      "step": 64110
    },
    {
      "epoch": 13.741963137591085,
      "grad_norm": 0.07078041881322861,
      "learning_rate": 1.6773824832118875e-06,
      "loss": 0.2185,
      "step": 64120
    },
    {
      "epoch": 13.744106300900128,
      "grad_norm": 0.0030572412069886923,
      "learning_rate": 1.6745249321331619e-06,
      "loss": 0.1571,
      "step": 64130
    },
    {
      "epoch": 13.746249464209173,
      "grad_norm": 0.04101564735174179,
      "learning_rate": 1.6716673810544364e-06,
      "loss": 0.5172,
      "step": 64140
    },
    {
      "epoch": 13.748392627518218,
      "grad_norm": 0.1044805571436882,
      "learning_rate": 1.668809829975711e-06,
      "loss": 0.2041,
      "step": 64150
    },
    {
      "epoch": 13.75053579082726,
      "grad_norm": 0.09181617200374603,
      "learning_rate": 1.6659522788969854e-06,
      "loss": 0.594,
      "step": 64160
    },
    {
      "epoch": 13.752678954136305,
      "grad_norm": 0.19197849929332733,
      "learning_rate": 1.66309472781826e-06,
      "loss": 0.1839,
      "step": 64170
    },
    {
      "epoch": 13.75482211744535,
      "grad_norm": 0.12008796632289886,
      "learning_rate": 1.6602371767395343e-06,
      "loss": 0.1664,
      "step": 64180
    },
    {
      "epoch": 13.756965280754393,
      "grad_norm": 0.20590610802173615,
      "learning_rate": 1.6573796256608088e-06,
      "loss": 0.0013,
      "step": 64190
    },
    {
      "epoch": 13.759108444063438,
      "grad_norm": 0.11722436547279358,
      "learning_rate": 1.6545220745820834e-06,
      "loss": 0.2777,
      "step": 64200
    },
    {
      "epoch": 13.761251607372483,
      "grad_norm": 48.01110076904297,
      "learning_rate": 1.6516645235033577e-06,
      "loss": 0.7428,
      "step": 64210
    },
    {
      "epoch": 13.763394770681526,
      "grad_norm": 67.71500396728516,
      "learning_rate": 1.6488069724246323e-06,
      "loss": 0.0049,
      "step": 64220
    },
    {
      "epoch": 13.76553793399057,
      "grad_norm": 0.21479257941246033,
      "learning_rate": 1.6459494213459069e-06,
      "loss": 0.4389,
      "step": 64230
    },
    {
      "epoch": 13.767681097299615,
      "grad_norm": 0.6460893750190735,
      "learning_rate": 1.643091870267181e-06,
      "loss": 0.1478,
      "step": 64240
    },
    {
      "epoch": 13.769824260608658,
      "grad_norm": 0.24996912479400635,
      "learning_rate": 1.6402343191884556e-06,
      "loss": 0.6596,
      "step": 64250
    },
    {
      "epoch": 13.771967423917703,
      "grad_norm": 0.009011662565171719,
      "learning_rate": 1.6373767681097301e-06,
      "loss": 0.0009,
      "step": 64260
    },
    {
      "epoch": 13.774110587226748,
      "grad_norm": 0.006508923135697842,
      "learning_rate": 1.6345192170310045e-06,
      "loss": 0.4255,
      "step": 64270
    },
    {
      "epoch": 13.77625375053579,
      "grad_norm": 0.019880902022123337,
      "learning_rate": 1.631661665952279e-06,
      "loss": 0.4798,
      "step": 64280
    },
    {
      "epoch": 13.778396913844835,
      "grad_norm": 18.551210403442383,
      "learning_rate": 1.6288041148735534e-06,
      "loss": 0.8274,
      "step": 64290
    },
    {
      "epoch": 13.78054007715388,
      "grad_norm": 0.17594322562217712,
      "learning_rate": 1.625946563794828e-06,
      "loss": 0.1689,
      "step": 64300
    },
    {
      "epoch": 13.782683240462923,
      "grad_norm": 0.01854824833571911,
      "learning_rate": 1.6230890127161025e-06,
      "loss": 0.0013,
      "step": 64310
    },
    {
      "epoch": 13.784826403771968,
      "grad_norm": 0.01336299441754818,
      "learning_rate": 1.6202314616373769e-06,
      "loss": 0.3562,
      "step": 64320
    },
    {
      "epoch": 13.786969567081012,
      "grad_norm": 36.715057373046875,
      "learning_rate": 1.6173739105586515e-06,
      "loss": 0.3228,
      "step": 64330
    },
    {
      "epoch": 13.789112730390055,
      "grad_norm": 0.03673527017235756,
      "learning_rate": 1.614516359479926e-06,
      "loss": 0.0013,
      "step": 64340
    },
    {
      "epoch": 13.7912558936991,
      "grad_norm": 26.47029685974121,
      "learning_rate": 1.6116588084012002e-06,
      "loss": 0.6094,
      "step": 64350
    },
    {
      "epoch": 13.793399057008145,
      "grad_norm": 0.09376238286495209,
      "learning_rate": 1.608801257322475e-06,
      "loss": 0.1483,
      "step": 64360
    },
    {
      "epoch": 13.795542220317188,
      "grad_norm": 0.026090169325470924,
      "learning_rate": 1.605943706243749e-06,
      "loss": 0.3755,
      "step": 64370
    },
    {
      "epoch": 13.797685383626233,
      "grad_norm": 649.30126953125,
      "learning_rate": 1.6030861551650236e-06,
      "loss": 0.1303,
      "step": 64380
    },
    {
      "epoch": 13.799828546935277,
      "grad_norm": 34.63368606567383,
      "learning_rate": 1.6002286040862982e-06,
      "loss": 0.3867,
      "step": 64390
    },
    {
      "epoch": 13.80197171024432,
      "grad_norm": 0.21655136346817017,
      "learning_rate": 1.5973710530075726e-06,
      "loss": 0.0025,
      "step": 64400
    },
    {
      "epoch": 13.804114873553365,
      "grad_norm": 0.012857579626142979,
      "learning_rate": 1.5945135019288471e-06,
      "loss": 0.5347,
      "step": 64410
    },
    {
      "epoch": 13.80625803686241,
      "grad_norm": 17.546672821044922,
      "learning_rate": 1.5916559508501217e-06,
      "loss": 0.49,
      "step": 64420
    },
    {
      "epoch": 13.808401200171453,
      "grad_norm": 0.22068826854228973,
      "learning_rate": 1.588798399771396e-06,
      "loss": 0.1146,
      "step": 64430
    },
    {
      "epoch": 13.810544363480497,
      "grad_norm": 0.007095535285770893,
      "learning_rate": 1.5859408486926706e-06,
      "loss": 0.2375,
      "step": 64440
    },
    {
      "epoch": 13.812687526789542,
      "grad_norm": 1.4751559495925903,
      "learning_rate": 1.583083297613945e-06,
      "loss": 0.2833,
      "step": 64450
    },
    {
      "epoch": 13.814830690098585,
      "grad_norm": 0.046163104474544525,
      "learning_rate": 1.5802257465352195e-06,
      "loss": 0.0015,
      "step": 64460
    },
    {
      "epoch": 13.81697385340763,
      "grad_norm": 0.062447648495435715,
      "learning_rate": 1.577368195456494e-06,
      "loss": 0.5817,
      "step": 64470
    },
    {
      "epoch": 13.819117016716675,
      "grad_norm": 0.21238912642002106,
      "learning_rate": 1.5745106443777682e-06,
      "loss": 0.3642,
      "step": 64480
    },
    {
      "epoch": 13.821260180025718,
      "grad_norm": 24.190486907958984,
      "learning_rate": 1.5716530932990428e-06,
      "loss": 0.2757,
      "step": 64490
    },
    {
      "epoch": 13.823403343334762,
      "grad_norm": 0.1540023535490036,
      "learning_rate": 1.5687955422203173e-06,
      "loss": 0.2728,
      "step": 64500
    },
    {
      "epoch": 13.825546506643807,
      "grad_norm": 70.99846649169922,
      "learning_rate": 1.5659379911415917e-06,
      "loss": 0.4722,
      "step": 64510
    },
    {
      "epoch": 13.82768966995285,
      "grad_norm": 0.2881041467189789,
      "learning_rate": 1.5630804400628663e-06,
      "loss": 0.1901,
      "step": 64520
    },
    {
      "epoch": 13.829832833261895,
      "grad_norm": 0.006010382901877165,
      "learning_rate": 1.5602228889841406e-06,
      "loss": 0.6373,
      "step": 64530
    },
    {
      "epoch": 13.83197599657094,
      "grad_norm": 183.8310089111328,
      "learning_rate": 1.5573653379054152e-06,
      "loss": 0.2629,
      "step": 64540
    },
    {
      "epoch": 13.834119159879982,
      "grad_norm": 0.00713645713403821,
      "learning_rate": 1.5545077868266897e-06,
      "loss": 0.4206,
      "step": 64550
    },
    {
      "epoch": 13.836262323189027,
      "grad_norm": 0.08591022342443466,
      "learning_rate": 1.551650235747964e-06,
      "loss": 0.1081,
      "step": 64560
    },
    {
      "epoch": 13.838405486498072,
      "grad_norm": 0.16473117470741272,
      "learning_rate": 1.5487926846692386e-06,
      "loss": 0.1391,
      "step": 64570
    },
    {
      "epoch": 13.840548649807115,
      "grad_norm": 0.09858856350183487,
      "learning_rate": 1.5459351335905132e-06,
      "loss": 0.1727,
      "step": 64580
    },
    {
      "epoch": 13.84269181311616,
      "grad_norm": 0.14434833824634552,
      "learning_rate": 1.5430775825117876e-06,
      "loss": 0.8338,
      "step": 64590
    },
    {
      "epoch": 13.844834976425204,
      "grad_norm": 0.14364300668239594,
      "learning_rate": 1.5402200314330621e-06,
      "loss": 0.4413,
      "step": 64600
    },
    {
      "epoch": 13.846978139734247,
      "grad_norm": 0.14882372319698334,
      "learning_rate": 1.5373624803543363e-06,
      "loss": 0.2834,
      "step": 64610
    },
    {
      "epoch": 13.849121303043292,
      "grad_norm": 0.15436606109142303,
      "learning_rate": 1.5345049292756108e-06,
      "loss": 0.1571,
      "step": 64620
    },
    {
      "epoch": 13.851264466352337,
      "grad_norm": 22.158002853393555,
      "learning_rate": 1.5316473781968854e-06,
      "loss": 0.6229,
      "step": 64630
    },
    {
      "epoch": 13.85340762966138,
      "grad_norm": 0.7955511212348938,
      "learning_rate": 1.5287898271181597e-06,
      "loss": 0.1698,
      "step": 64640
    },
    {
      "epoch": 13.855550792970424,
      "grad_norm": 0.12106889486312866,
      "learning_rate": 1.5259322760394343e-06,
      "loss": 0.1305,
      "step": 64650
    },
    {
      "epoch": 13.85769395627947,
      "grad_norm": 0.09856750816106796,
      "learning_rate": 1.5230747249607089e-06,
      "loss": 0.2814,
      "step": 64660
    },
    {
      "epoch": 13.859837119588512,
      "grad_norm": 0.331101655960083,
      "learning_rate": 1.5202171738819832e-06,
      "loss": 0.4741,
      "step": 64670
    },
    {
      "epoch": 13.861980282897557,
      "grad_norm": 0.008201535791158676,
      "learning_rate": 1.5173596228032578e-06,
      "loss": 0.4043,
      "step": 64680
    },
    {
      "epoch": 13.864123446206602,
      "grad_norm": 52.6568489074707,
      "learning_rate": 1.5145020717245321e-06,
      "loss": 0.1468,
      "step": 64690
    },
    {
      "epoch": 13.866266609515645,
      "grad_norm": 0.2884688973426819,
      "learning_rate": 1.5116445206458067e-06,
      "loss": 0.2848,
      "step": 64700
    },
    {
      "epoch": 13.86840977282469,
      "grad_norm": 0.0793396607041359,
      "learning_rate": 1.5087869695670813e-06,
      "loss": 0.1376,
      "step": 64710
    },
    {
      "epoch": 13.870552936133734,
      "grad_norm": 0.9764887690544128,
      "learning_rate": 1.5059294184883554e-06,
      "loss": 0.3294,
      "step": 64720
    },
    {
      "epoch": 13.872696099442777,
      "grad_norm": 0.05404284968972206,
      "learning_rate": 1.50307186740963e-06,
      "loss": 0.4049,
      "step": 64730
    },
    {
      "epoch": 13.874839262751822,
      "grad_norm": 0.07575439661741257,
      "learning_rate": 1.5002143163309045e-06,
      "loss": 0.394,
      "step": 64740
    },
    {
      "epoch": 13.876982426060867,
      "grad_norm": 0.047534383833408356,
      "learning_rate": 1.4973567652521789e-06,
      "loss": 0.0018,
      "step": 64750
    },
    {
      "epoch": 13.87912558936991,
      "grad_norm": 0.06480448693037033,
      "learning_rate": 1.4944992141734535e-06,
      "loss": 0.0016,
      "step": 64760
    },
    {
      "epoch": 13.881268752678954,
      "grad_norm": 0.12427885085344315,
      "learning_rate": 1.491641663094728e-06,
      "loss": 0.2404,
      "step": 64770
    },
    {
      "epoch": 13.883411915987999,
      "grad_norm": 193.2920379638672,
      "learning_rate": 1.4887841120160024e-06,
      "loss": 0.1815,
      "step": 64780
    },
    {
      "epoch": 13.885555079297042,
      "grad_norm": 0.00267154723405838,
      "learning_rate": 1.485926560937277e-06,
      "loss": 0.3097,
      "step": 64790
    },
    {
      "epoch": 13.887698242606087,
      "grad_norm": 0.030932573601603508,
      "learning_rate": 1.4830690098585513e-06,
      "loss": 0.0809,
      "step": 64800
    },
    {
      "epoch": 13.889841405915131,
      "grad_norm": 0.31094563007354736,
      "learning_rate": 1.4802114587798258e-06,
      "loss": 0.5175,
      "step": 64810
    },
    {
      "epoch": 13.891984569224174,
      "grad_norm": 0.47310951352119446,
      "learning_rate": 1.4773539077011004e-06,
      "loss": 0.5674,
      "step": 64820
    },
    {
      "epoch": 13.894127732533219,
      "grad_norm": 0.06967233121395111,
      "learning_rate": 1.4744963566223748e-06,
      "loss": 0.2678,
      "step": 64830
    },
    {
      "epoch": 13.896270895842264,
      "grad_norm": 0.03107392229139805,
      "learning_rate": 1.4716388055436493e-06,
      "loss": 0.0013,
      "step": 64840
    },
    {
      "epoch": 13.898414059151307,
      "grad_norm": 0.8303369283676147,
      "learning_rate": 1.4687812544649239e-06,
      "loss": 0.1565,
      "step": 64850
    },
    {
      "epoch": 13.900557222460352,
      "grad_norm": 143.0952606201172,
      "learning_rate": 1.465923703386198e-06,
      "loss": 0.8259,
      "step": 64860
    },
    {
      "epoch": 13.902700385769396,
      "grad_norm": 0.06516142189502716,
      "learning_rate": 1.4630661523074726e-06,
      "loss": 0.462,
      "step": 64870
    },
    {
      "epoch": 13.90484354907844,
      "grad_norm": 0.09163480997085571,
      "learning_rate": 1.460208601228747e-06,
      "loss": 0.252,
      "step": 64880
    },
    {
      "epoch": 13.906986712387484,
      "grad_norm": 0.009996727108955383,
      "learning_rate": 1.4573510501500215e-06,
      "loss": 0.3274,
      "step": 64890
    },
    {
      "epoch": 13.909129875696529,
      "grad_norm": 0.10729371011257172,
      "learning_rate": 1.454493499071296e-06,
      "loss": 0.0008,
      "step": 64900
    },
    {
      "epoch": 13.911273039005572,
      "grad_norm": 0.004385573789477348,
      "learning_rate": 1.4516359479925704e-06,
      "loss": 0.6317,
      "step": 64910
    },
    {
      "epoch": 13.913416202314616,
      "grad_norm": 0.004062514286488295,
      "learning_rate": 1.448778396913845e-06,
      "loss": 0.8698,
      "step": 64920
    },
    {
      "epoch": 13.915559365623661,
      "grad_norm": 0.07161445915699005,
      "learning_rate": 1.4459208458351195e-06,
      "loss": 0.0974,
      "step": 64930
    },
    {
      "epoch": 13.917702528932704,
      "grad_norm": 0.03348561376333237,
      "learning_rate": 1.443063294756394e-06,
      "loss": 0.1512,
      "step": 64940
    },
    {
      "epoch": 13.919845692241749,
      "grad_norm": 1.4276584386825562,
      "learning_rate": 1.4402057436776685e-06,
      "loss": 0.2857,
      "step": 64950
    },
    {
      "epoch": 13.921988855550794,
      "grad_norm": 23.00010871887207,
      "learning_rate": 1.4373481925989426e-06,
      "loss": 0.3159,
      "step": 64960
    },
    {
      "epoch": 13.924132018859837,
      "grad_norm": 0.019530728459358215,
      "learning_rate": 1.4344906415202174e-06,
      "loss": 0.5522,
      "step": 64970
    },
    {
      "epoch": 13.926275182168881,
      "grad_norm": 133.7267303466797,
      "learning_rate": 1.431633090441492e-06,
      "loss": 0.2738,
      "step": 64980
    },
    {
      "epoch": 13.928418345477926,
      "grad_norm": 0.08372180163860321,
      "learning_rate": 1.428775539362766e-06,
      "loss": 0.3054,
      "step": 64990
    },
    {
      "epoch": 13.930561508786969,
      "grad_norm": 0.011389464139938354,
      "learning_rate": 1.4259179882840406e-06,
      "loss": 0.2095,
      "step": 65000
    },
    {
      "epoch": 13.932704672096014,
      "grad_norm": 42.79734420776367,
      "learning_rate": 1.4230604372053152e-06,
      "loss": 0.5793,
      "step": 65010
    },
    {
      "epoch": 13.934847835405058,
      "grad_norm": 0.01581827737390995,
      "learning_rate": 1.4202028861265896e-06,
      "loss": 0.2235,
      "step": 65020
    },
    {
      "epoch": 13.936990998714101,
      "grad_norm": 0.32441702485084534,
      "learning_rate": 1.4173453350478641e-06,
      "loss": 0.1586,
      "step": 65030
    },
    {
      "epoch": 13.939134162023146,
      "grad_norm": 0.35116371512413025,
      "learning_rate": 1.4144877839691385e-06,
      "loss": 0.1883,
      "step": 65040
    },
    {
      "epoch": 13.94127732533219,
      "grad_norm": 1.1553735733032227,
      "learning_rate": 1.411630232890413e-06,
      "loss": 0.1265,
      "step": 65050
    },
    {
      "epoch": 13.943420488641234,
      "grad_norm": 0.004108094610273838,
      "learning_rate": 1.4087726818116876e-06,
      "loss": 0.3062,
      "step": 65060
    },
    {
      "epoch": 13.945563651950279,
      "grad_norm": 0.11902797222137451,
      "learning_rate": 1.405915130732962e-06,
      "loss": 0.5565,
      "step": 65070
    },
    {
      "epoch": 13.947706815259323,
      "grad_norm": 26.305509567260742,
      "learning_rate": 1.4030575796542365e-06,
      "loss": 0.6238,
      "step": 65080
    },
    {
      "epoch": 13.949849978568366,
      "grad_norm": 39.82407760620117,
      "learning_rate": 1.400200028575511e-06,
      "loss": 0.5515,
      "step": 65090
    },
    {
      "epoch": 13.951993141877411,
      "grad_norm": 1.880392074584961,
      "learning_rate": 1.3973424774967852e-06,
      "loss": 0.2466,
      "step": 65100
    },
    {
      "epoch": 13.954136305186456,
      "grad_norm": 0.018039878457784653,
      "learning_rate": 1.3944849264180598e-06,
      "loss": 0.56,
      "step": 65110
    },
    {
      "epoch": 13.956279468495499,
      "grad_norm": 29.705015182495117,
      "learning_rate": 1.3916273753393341e-06,
      "loss": 0.7094,
      "step": 65120
    },
    {
      "epoch": 13.958422631804543,
      "grad_norm": 0.015130439773201942,
      "learning_rate": 1.3887698242606087e-06,
      "loss": 0.1222,
      "step": 65130
    },
    {
      "epoch": 13.960565795113588,
      "grad_norm": 773.1797485351562,
      "learning_rate": 1.3859122731818833e-06,
      "loss": 0.1578,
      "step": 65140
    },
    {
      "epoch": 13.962708958422631,
      "grad_norm": 0.3563268482685089,
      "learning_rate": 1.3830547221031576e-06,
      "loss": 0.0047,
      "step": 65150
    },
    {
      "epoch": 13.964852121731676,
      "grad_norm": 0.006087971851229668,
      "learning_rate": 1.3801971710244322e-06,
      "loss": 0.2992,
      "step": 65160
    },
    {
      "epoch": 13.96699528504072,
      "grad_norm": 0.014505213126540184,
      "learning_rate": 1.3773396199457067e-06,
      "loss": 0.1915,
      "step": 65170
    },
    {
      "epoch": 13.969138448349764,
      "grad_norm": 0.08637532591819763,
      "learning_rate": 1.374482068866981e-06,
      "loss": 0.2141,
      "step": 65180
    },
    {
      "epoch": 13.971281611658808,
      "grad_norm": 0.010624175891280174,
      "learning_rate": 1.3716245177882557e-06,
      "loss": 0.0008,
      "step": 65190
    },
    {
      "epoch": 13.973424774967853,
      "grad_norm": 0.14449256658554077,
      "learning_rate": 1.36876696670953e-06,
      "loss": 0.001,
      "step": 65200
    },
    {
      "epoch": 13.975567938276896,
      "grad_norm": 0.15379540622234344,
      "learning_rate": 1.3659094156308046e-06,
      "loss": 0.309,
      "step": 65210
    },
    {
      "epoch": 13.97771110158594,
      "grad_norm": 0.02833441272377968,
      "learning_rate": 1.3630518645520791e-06,
      "loss": 0.0072,
      "step": 65220
    },
    {
      "epoch": 13.979854264894986,
      "grad_norm": 683.6254272460938,
      "learning_rate": 1.3601943134733533e-06,
      "loss": 0.576,
      "step": 65230
    },
    {
      "epoch": 13.981997428204028,
      "grad_norm": 0.09190607815980911,
      "learning_rate": 1.3573367623946278e-06,
      "loss": 0.7119,
      "step": 65240
    },
    {
      "epoch": 13.984140591513073,
      "grad_norm": 0.013186842203140259,
      "learning_rate": 1.3544792113159024e-06,
      "loss": 0.7089,
      "step": 65250
    },
    {
      "epoch": 13.986283754822118,
      "grad_norm": 0.21794211864471436,
      "learning_rate": 1.3516216602371768e-06,
      "loss": 0.314,
      "step": 65260
    },
    {
      "epoch": 13.988426918131161,
      "grad_norm": 0.08261553943157196,
      "learning_rate": 1.3487641091584513e-06,
      "loss": 0.4997,
      "step": 65270
    },
    {
      "epoch": 13.990570081440206,
      "grad_norm": 0.13265320658683777,
      "learning_rate": 1.3459065580797259e-06,
      "loss": 0.2289,
      "step": 65280
    },
    {
      "epoch": 13.99271324474925,
      "grad_norm": 0.024442313238978386,
      "learning_rate": 1.3430490070010002e-06,
      "loss": 0.0024,
      "step": 65290
    },
    {
      "epoch": 13.994856408058293,
      "grad_norm": 0.01497120875865221,
      "learning_rate": 1.3401914559222748e-06,
      "loss": 0.1275,
      "step": 65300
    },
    {
      "epoch": 13.996999571367338,
      "grad_norm": 0.004310042131692171,
      "learning_rate": 1.3373339048435492e-06,
      "loss": 0.3439,
      "step": 65310
    },
    {
      "epoch": 13.999142734676383,
      "grad_norm": 0.05076872184872627,
      "learning_rate": 1.3344763537648237e-06,
      "loss": 0.2825,
      "step": 65320
    },
    {
      "epoch": 14.0,
      "eval_accuracy": 0.93,
      "eval_f1": 0.68562874251497,
      "eval_loss": 0.35050204396247864,
      "eval_precision": 0.8450184501845018,
      "eval_recall": 0.5768261964735516,
      "eval_runtime": 396.9001,
      "eval_samples_per_second": 7.559,
      "eval_steps_per_second": 2.52,
      "step": 65324
    },
    {
      "epoch": 14.001285897985426,
      "grad_norm": 0.010852966457605362,
      "learning_rate": 1.3316188026860983e-06,
      "loss": 0.2986,
      "step": 65330
    },
    {
      "epoch": 14.00342906129447,
      "grad_norm": 0.028157174587249756,
      "learning_rate": 1.3287612516073724e-06,
      "loss": 0.002,
      "step": 65340
    },
    {
      "epoch": 14.005572224603515,
      "grad_norm": 0.011247060261666775,
      "learning_rate": 1.325903700528647e-06,
      "loss": 0.0018,
      "step": 65350
    },
    {
      "epoch": 14.007715387912558,
      "grad_norm": 0.007154142949730158,
      "learning_rate": 1.3230461494499218e-06,
      "loss": 0.1235,
      "step": 65360
    },
    {
      "epoch": 14.009858551221603,
      "grad_norm": 0.04439140111207962,
      "learning_rate": 1.320188598371196e-06,
      "loss": 0.0012,
      "step": 65370
    },
    {
      "epoch": 14.012001714530648,
      "grad_norm": 0.0447441004216671,
      "learning_rate": 1.3173310472924705e-06,
      "loss": 0.0058,
      "step": 65380
    },
    {
      "epoch": 14.01414487783969,
      "grad_norm": 0.006080804858356714,
      "learning_rate": 1.3144734962137448e-06,
      "loss": 0.4921,
      "step": 65390
    },
    {
      "epoch": 14.016288041148735,
      "grad_norm": 48.586334228515625,
      "learning_rate": 1.3116159451350194e-06,
      "loss": 0.3239,
      "step": 65400
    },
    {
      "epoch": 14.01843120445778,
      "grad_norm": 46.858299255371094,
      "learning_rate": 1.308758394056294e-06,
      "loss": 0.5567,
      "step": 65410
    },
    {
      "epoch": 14.020574367766823,
      "grad_norm": 0.3403284251689911,
      "learning_rate": 1.3059008429775683e-06,
      "loss": 0.2206,
      "step": 65420
    },
    {
      "epoch": 14.022717531075868,
      "grad_norm": 0.42540428042411804,
      "learning_rate": 1.3030432918988429e-06,
      "loss": 0.0734,
      "step": 65430
    },
    {
      "epoch": 14.024860694384913,
      "grad_norm": 0.07723087072372437,
      "learning_rate": 1.3001857408201174e-06,
      "loss": 0.1717,
      "step": 65440
    },
    {
      "epoch": 14.027003857693956,
      "grad_norm": 15.858704566955566,
      "learning_rate": 1.2973281897413918e-06,
      "loss": 0.1613,
      "step": 65450
    },
    {
      "epoch": 14.029147021003,
      "grad_norm": 0.2448035478591919,
      "learning_rate": 1.2944706386626663e-06,
      "loss": 0.1699,
      "step": 65460
    },
    {
      "epoch": 14.031290184312045,
      "grad_norm": 0.29393360018730164,
      "learning_rate": 1.2916130875839405e-06,
      "loss": 0.1405,
      "step": 65470
    },
    {
      "epoch": 14.033433347621088,
      "grad_norm": 0.004040218424052,
      "learning_rate": 1.288755536505215e-06,
      "loss": 0.5542,
      "step": 65480
    },
    {
      "epoch": 14.035576510930133,
      "grad_norm": 26.63715934753418,
      "learning_rate": 1.2858979854264896e-06,
      "loss": 0.132,
      "step": 65490
    },
    {
      "epoch": 14.037719674239177,
      "grad_norm": 0.3336094319820404,
      "learning_rate": 1.283040434347764e-06,
      "loss": 0.0013,
      "step": 65500
    },
    {
      "epoch": 14.03986283754822,
      "grad_norm": 0.02102181874215603,
      "learning_rate": 1.2801828832690385e-06,
      "loss": 0.3234,
      "step": 65510
    },
    {
      "epoch": 14.042006000857265,
      "grad_norm": 19.293907165527344,
      "learning_rate": 1.277325332190313e-06,
      "loss": 1.1278,
      "step": 65520
    },
    {
      "epoch": 14.04414916416631,
      "grad_norm": 41.38884353637695,
      "learning_rate": 1.2744677811115874e-06,
      "loss": 0.4979,
      "step": 65530
    },
    {
      "epoch": 14.046292327475353,
      "grad_norm": 0.1505611687898636,
      "learning_rate": 1.271610230032862e-06,
      "loss": 0.2489,
      "step": 65540
    },
    {
      "epoch": 14.048435490784398,
      "grad_norm": 19.316951751708984,
      "learning_rate": 1.2687526789541363e-06,
      "loss": 0.2697,
      "step": 65550
    },
    {
      "epoch": 14.050578654093442,
      "grad_norm": 0.6023668646812439,
      "learning_rate": 1.265895127875411e-06,
      "loss": 0.6845,
      "step": 65560
    },
    {
      "epoch": 14.052721817402485,
      "grad_norm": 0.22190050780773163,
      "learning_rate": 1.2630375767966855e-06,
      "loss": 0.2111,
      "step": 65570
    },
    {
      "epoch": 14.05486498071153,
      "grad_norm": 0.09712372720241547,
      "learning_rate": 1.2601800257179596e-06,
      "loss": 0.1278,
      "step": 65580
    },
    {
      "epoch": 14.057008144020575,
      "grad_norm": 139.75767517089844,
      "learning_rate": 1.2573224746392344e-06,
      "loss": 0.9608,
      "step": 65590
    },
    {
      "epoch": 14.059151307329618,
      "grad_norm": 0.015218710526823997,
      "learning_rate": 1.254464923560509e-06,
      "loss": 0.4443,
      "step": 65600
    },
    {
      "epoch": 14.061294470638662,
      "grad_norm": 0.21230065822601318,
      "learning_rate": 1.251607372481783e-06,
      "loss": 0.2667,
      "step": 65610
    },
    {
      "epoch": 14.063437633947707,
      "grad_norm": 0.059210482984781265,
      "learning_rate": 1.2487498214030577e-06,
      "loss": 0.6813,
      "step": 65620
    },
    {
      "epoch": 14.06558079725675,
      "grad_norm": 85.15019989013672,
      "learning_rate": 1.2458922703243322e-06,
      "loss": 0.2317,
      "step": 65630
    },
    {
      "epoch": 14.067723960565795,
      "grad_norm": 20.391454696655273,
      "learning_rate": 1.2430347192456066e-06,
      "loss": 0.1373,
      "step": 65640
    },
    {
      "epoch": 14.06986712387484,
      "grad_norm": 25.639392852783203,
      "learning_rate": 1.240177168166881e-06,
      "loss": 0.1441,
      "step": 65650
    },
    {
      "epoch": 14.072010287183883,
      "grad_norm": 0.1413176953792572,
      "learning_rate": 1.2373196170881557e-06,
      "loss": 0.0018,
      "step": 65660
    },
    {
      "epoch": 14.074153450492927,
      "grad_norm": 2.435859441757202,
      "learning_rate": 1.23446206600943e-06,
      "loss": 0.2004,
      "step": 65670
    },
    {
      "epoch": 14.076296613801972,
      "grad_norm": 0.16267289221286774,
      "learning_rate": 1.2316045149307044e-06,
      "loss": 0.1729,
      "step": 65680
    },
    {
      "epoch": 14.078439777111015,
      "grad_norm": 0.05856841802597046,
      "learning_rate": 1.228746963851979e-06,
      "loss": 0.4116,
      "step": 65690
    },
    {
      "epoch": 14.08058294042006,
      "grad_norm": 0.2556035816669464,
      "learning_rate": 1.2258894127732535e-06,
      "loss": 0.2547,
      "step": 65700
    },
    {
      "epoch": 14.082726103729104,
      "grad_norm": 0.33739370107650757,
      "learning_rate": 1.2230318616945279e-06,
      "loss": 0.0036,
      "step": 65710
    },
    {
      "epoch": 14.084869267038147,
      "grad_norm": 0.005361739080399275,
      "learning_rate": 1.2201743106158022e-06,
      "loss": 0.0916,
      "step": 65720
    },
    {
      "epoch": 14.087012430347192,
      "grad_norm": 0.007960427552461624,
      "learning_rate": 1.2173167595370768e-06,
      "loss": 0.4569,
      "step": 65730
    },
    {
      "epoch": 14.089155593656237,
      "grad_norm": 28.930631637573242,
      "learning_rate": 1.2144592084583514e-06,
      "loss": 0.3515,
      "step": 65740
    },
    {
      "epoch": 14.09129875696528,
      "grad_norm": 0.053928084671497345,
      "learning_rate": 1.2116016573796257e-06,
      "loss": 0.0028,
      "step": 65750
    },
    {
      "epoch": 14.093441920274325,
      "grad_norm": 0.0016163643449544907,
      "learning_rate": 1.2087441063009003e-06,
      "loss": 0.5437,
      "step": 65760
    },
    {
      "epoch": 14.09558508358337,
      "grad_norm": 0.5991620421409607,
      "learning_rate": 1.2058865552221746e-06,
      "loss": 0.3705,
      "step": 65770
    },
    {
      "epoch": 14.097728246892412,
      "grad_norm": 41.92947006225586,
      "learning_rate": 1.2030290041434492e-06,
      "loss": 0.735,
      "step": 65780
    },
    {
      "epoch": 14.099871410201457,
      "grad_norm": 0.5026365518569946,
      "learning_rate": 1.2001714530647235e-06,
      "loss": 0.0021,
      "step": 65790
    },
    {
      "epoch": 14.102014573510502,
      "grad_norm": 0.11423512548208237,
      "learning_rate": 1.1973139019859981e-06,
      "loss": 0.0103,
      "step": 65800
    },
    {
      "epoch": 14.104157736819545,
      "grad_norm": 0.3039794862270355,
      "learning_rate": 1.1944563509072727e-06,
      "loss": 0.4109,
      "step": 65810
    },
    {
      "epoch": 14.10630090012859,
      "grad_norm": 0.0334220752120018,
      "learning_rate": 1.191598799828547e-06,
      "loss": 0.1104,
      "step": 65820
    },
    {
      "epoch": 14.108444063437634,
      "grad_norm": 0.017472025007009506,
      "learning_rate": 1.1887412487498216e-06,
      "loss": 0.2745,
      "step": 65830
    },
    {
      "epoch": 14.110587226746677,
      "grad_norm": 0.1393854022026062,
      "learning_rate": 1.185883697671096e-06,
      "loss": 0.5318,
      "step": 65840
    },
    {
      "epoch": 14.112730390055722,
      "grad_norm": 0.2536756694316864,
      "learning_rate": 1.1830261465923705e-06,
      "loss": 0.3176,
      "step": 65850
    },
    {
      "epoch": 14.114873553364767,
      "grad_norm": 0.005858121905475855,
      "learning_rate": 1.1801685955136449e-06,
      "loss": 0.2683,
      "step": 65860
    },
    {
      "epoch": 14.117016716673811,
      "grad_norm": 0.19963864982128143,
      "learning_rate": 1.1773110444349194e-06,
      "loss": 0.0022,
      "step": 65870
    },
    {
      "epoch": 14.119159879982854,
      "grad_norm": 0.11117721349000931,
      "learning_rate": 1.1744534933561938e-06,
      "loss": 0.1476,
      "step": 65880
    },
    {
      "epoch": 14.1213030432919,
      "grad_norm": 0.9334828853607178,
      "learning_rate": 1.1715959422774683e-06,
      "loss": 0.2092,
      "step": 65890
    },
    {
      "epoch": 14.123446206600944,
      "grad_norm": 111.79581451416016,
      "learning_rate": 1.168738391198743e-06,
      "loss": 0.3439,
      "step": 65900
    },
    {
      "epoch": 14.125589369909987,
      "grad_norm": 53.97764587402344,
      "learning_rate": 1.1658808401200172e-06,
      "loss": 0.4931,
      "step": 65910
    },
    {
      "epoch": 14.127732533219032,
      "grad_norm": 0.2170313000679016,
      "learning_rate": 1.1630232890412916e-06,
      "loss": 0.4035,
      "step": 65920
    },
    {
      "epoch": 14.129875696528076,
      "grad_norm": 0.3511773645877838,
      "learning_rate": 1.1601657379625662e-06,
      "loss": 0.4079,
      "step": 65930
    },
    {
      "epoch": 14.13201885983712,
      "grad_norm": 24.187435150146484,
      "learning_rate": 1.1573081868838407e-06,
      "loss": 0.2892,
      "step": 65940
    },
    {
      "epoch": 14.134162023146164,
      "grad_norm": 0.05638334900140762,
      "learning_rate": 1.154450635805115e-06,
      "loss": 0.2711,
      "step": 65950
    },
    {
      "epoch": 14.136305186455209,
      "grad_norm": 1.4505335092544556,
      "learning_rate": 1.1515930847263894e-06,
      "loss": 0.1408,
      "step": 65960
    },
    {
      "epoch": 14.138448349764252,
      "grad_norm": 0.3970201015472412,
      "learning_rate": 1.1487355336476642e-06,
      "loss": 0.5391,
      "step": 65970
    },
    {
      "epoch": 14.140591513073296,
      "grad_norm": 0.15520980954170227,
      "learning_rate": 1.1458779825689386e-06,
      "loss": 0.0778,
      "step": 65980
    },
    {
      "epoch": 14.142734676382341,
      "grad_norm": 0.011100467294454575,
      "learning_rate": 1.143020431490213e-06,
      "loss": 0.0023,
      "step": 65990
    },
    {
      "epoch": 14.144877839691384,
      "grad_norm": 0.02409498021006584,
      "learning_rate": 1.1401628804114875e-06,
      "loss": 0.0027,
      "step": 66000
    },
    {
      "epoch": 14.147021003000429,
      "grad_norm": 0.1908387541770935,
      "learning_rate": 1.137305329332762e-06,
      "loss": 0.6842,
      "step": 66010
    },
    {
      "epoch": 14.149164166309474,
      "grad_norm": 0.017681730911135674,
      "learning_rate": 1.1344477782540364e-06,
      "loss": 0.1424,
      "step": 66020
    },
    {
      "epoch": 14.151307329618517,
      "grad_norm": 31.545522689819336,
      "learning_rate": 1.1315902271753107e-06,
      "loss": 0.2167,
      "step": 66030
    },
    {
      "epoch": 14.153450492927561,
      "grad_norm": 0.020181985571980476,
      "learning_rate": 1.1287326760965853e-06,
      "loss": 0.4481,
      "step": 66040
    },
    {
      "epoch": 14.155593656236606,
      "grad_norm": 0.0878540650010109,
      "learning_rate": 1.1258751250178599e-06,
      "loss": 0.0006,
      "step": 66050
    },
    {
      "epoch": 14.157736819545649,
      "grad_norm": 1.2002571821212769,
      "learning_rate": 1.1230175739391342e-06,
      "loss": 0.1997,
      "step": 66060
    },
    {
      "epoch": 14.159879982854694,
      "grad_norm": 19.082300186157227,
      "learning_rate": 1.1201600228604088e-06,
      "loss": 0.297,
      "step": 66070
    },
    {
      "epoch": 14.162023146163738,
      "grad_norm": 0.16453881561756134,
      "learning_rate": 1.1173024717816831e-06,
      "loss": 0.1825,
      "step": 66080
    },
    {
      "epoch": 14.164166309472781,
      "grad_norm": 0.03738265857100487,
      "learning_rate": 1.1144449207029577e-06,
      "loss": 0.5456,
      "step": 66090
    },
    {
      "epoch": 14.166309472781826,
      "grad_norm": 0.10534953325986862,
      "learning_rate": 1.111587369624232e-06,
      "loss": 0.4192,
      "step": 66100
    },
    {
      "epoch": 14.168452636090871,
      "grad_norm": 0.2144320160150528,
      "learning_rate": 1.1087298185455066e-06,
      "loss": 0.3714,
      "step": 66110
    },
    {
      "epoch": 14.170595799399914,
      "grad_norm": 36.50953674316406,
      "learning_rate": 1.105872267466781e-06,
      "loss": 0.7198,
      "step": 66120
    },
    {
      "epoch": 14.172738962708959,
      "grad_norm": 0.2305333912372589,
      "learning_rate": 1.1030147163880555e-06,
      "loss": 0.2419,
      "step": 66130
    },
    {
      "epoch": 14.174882126018003,
      "grad_norm": 0.01890694908797741,
      "learning_rate": 1.10015716530933e-06,
      "loss": 0.2752,
      "step": 66140
    },
    {
      "epoch": 14.177025289327046,
      "grad_norm": 0.0023261206224560738,
      "learning_rate": 1.0972996142306044e-06,
      "loss": 0.4851,
      "step": 66150
    },
    {
      "epoch": 14.179168452636091,
      "grad_norm": 0.03347976878285408,
      "learning_rate": 1.0944420631518788e-06,
      "loss": 0.386,
      "step": 66160
    },
    {
      "epoch": 14.181311615945136,
      "grad_norm": 0.5885609984397888,
      "learning_rate": 1.0915845120731534e-06,
      "loss": 0.0026,
      "step": 66170
    },
    {
      "epoch": 14.183454779254179,
      "grad_norm": 0.09154927730560303,
      "learning_rate": 1.088726960994428e-06,
      "loss": 0.4238,
      "step": 66180
    },
    {
      "epoch": 14.185597942563223,
      "grad_norm": 0.5877969264984131,
      "learning_rate": 1.0858694099157023e-06,
      "loss": 0.2925,
      "step": 66190
    },
    {
      "epoch": 14.187741105872268,
      "grad_norm": 0.0710543543100357,
      "learning_rate": 1.0830118588369768e-06,
      "loss": 0.0078,
      "step": 66200
    },
    {
      "epoch": 14.189884269181311,
      "grad_norm": 0.338131844997406,
      "learning_rate": 1.0801543077582514e-06,
      "loss": 0.3518,
      "step": 66210
    },
    {
      "epoch": 14.192027432490356,
      "grad_norm": 48.43996047973633,
      "learning_rate": 1.0772967566795258e-06,
      "loss": 0.231,
      "step": 66220
    },
    {
      "epoch": 14.1941705957994,
      "grad_norm": 49.641563415527344,
      "learning_rate": 1.0744392056008e-06,
      "loss": 0.294,
      "step": 66230
    },
    {
      "epoch": 14.196313759108444,
      "grad_norm": 11.410861015319824,
      "learning_rate": 1.0715816545220747e-06,
      "loss": 0.3555,
      "step": 66240
    },
    {
      "epoch": 14.198456922417488,
      "grad_norm": 19.706167221069336,
      "learning_rate": 1.0687241034433492e-06,
      "loss": 0.5038,
      "step": 66250
    },
    {
      "epoch": 14.200600085726533,
      "grad_norm": 0.010754043236374855,
      "learning_rate": 1.0658665523646236e-06,
      "loss": 0.3373,
      "step": 66260
    },
    {
      "epoch": 14.202743249035576,
      "grad_norm": 19.2293758392334,
      "learning_rate": 1.063009001285898e-06,
      "loss": 0.4689,
      "step": 66270
    },
    {
      "epoch": 14.20488641234462,
      "grad_norm": 0.10065137594938278,
      "learning_rate": 1.0601514502071725e-06,
      "loss": 0.4114,
      "step": 66280
    },
    {
      "epoch": 14.207029575653666,
      "grad_norm": 0.25647634267807007,
      "learning_rate": 1.057293899128447e-06,
      "loss": 0.4325,
      "step": 66290
    },
    {
      "epoch": 14.209172738962708,
      "grad_norm": 0.2755824327468872,
      "learning_rate": 1.0544363480497214e-06,
      "loss": 0.0023,
      "step": 66300
    },
    {
      "epoch": 14.211315902271753,
      "grad_norm": 28.03974151611328,
      "learning_rate": 1.051578796970996e-06,
      "loss": 0.6859,
      "step": 66310
    },
    {
      "epoch": 14.213459065580798,
      "grad_norm": 0.007828228175640106,
      "learning_rate": 1.0487212458922705e-06,
      "loss": 0.1283,
      "step": 66320
    },
    {
      "epoch": 14.215602228889841,
      "grad_norm": 0.012565452605485916,
      "learning_rate": 1.0458636948135449e-06,
      "loss": 0.1599,
      "step": 66330
    },
    {
      "epoch": 14.217745392198886,
      "grad_norm": 0.0801306664943695,
      "learning_rate": 1.0430061437348192e-06,
      "loss": 0.1576,
      "step": 66340
    },
    {
      "epoch": 14.21988855550793,
      "grad_norm": 0.040441758930683136,
      "learning_rate": 1.0401485926560938e-06,
      "loss": 0.0034,
      "step": 66350
    },
    {
      "epoch": 14.222031718816973,
      "grad_norm": 0.08774298429489136,
      "learning_rate": 1.0372910415773684e-06,
      "loss": 0.2987,
      "step": 66360
    },
    {
      "epoch": 14.224174882126018,
      "grad_norm": 29.88840675354004,
      "learning_rate": 1.0344334904986427e-06,
      "loss": 0.7811,
      "step": 66370
    },
    {
      "epoch": 14.226318045435063,
      "grad_norm": 1.4285762310028076,
      "learning_rate": 1.0315759394199173e-06,
      "loss": 0.0027,
      "step": 66380
    },
    {
      "epoch": 14.228461208744106,
      "grad_norm": 0.044357459992170334,
      "learning_rate": 1.0287183883411916e-06,
      "loss": 0.0061,
      "step": 66390
    },
    {
      "epoch": 14.23060437205315,
      "grad_norm": 0.2590073049068451,
      "learning_rate": 1.0258608372624662e-06,
      "loss": 0.3293,
      "step": 66400
    },
    {
      "epoch": 14.232747535362195,
      "grad_norm": 0.007395969703793526,
      "learning_rate": 1.0230032861837406e-06,
      "loss": 0.5748,
      "step": 66410
    },
    {
      "epoch": 14.234890698671238,
      "grad_norm": 0.010342364199459553,
      "learning_rate": 1.0201457351050151e-06,
      "loss": 0.0016,
      "step": 66420
    },
    {
      "epoch": 14.237033861980283,
      "grad_norm": 0.41030678153038025,
      "learning_rate": 1.0172881840262895e-06,
      "loss": 0.2616,
      "step": 66430
    },
    {
      "epoch": 14.239177025289328,
      "grad_norm": 34.488033294677734,
      "learning_rate": 1.014430632947564e-06,
      "loss": 0.6774,
      "step": 66440
    },
    {
      "epoch": 14.24132018859837,
      "grad_norm": 0.2689859867095947,
      "learning_rate": 1.0115730818688386e-06,
      "loss": 0.3272,
      "step": 66450
    },
    {
      "epoch": 14.243463351907415,
      "grad_norm": 0.028980208560824394,
      "learning_rate": 1.008715530790113e-06,
      "loss": 0.1676,
      "step": 66460
    },
    {
      "epoch": 14.24560651521646,
      "grad_norm": 0.2920677065849304,
      "learning_rate": 1.0058579797113873e-06,
      "loss": 0.375,
      "step": 66470
    },
    {
      "epoch": 14.247749678525503,
      "grad_norm": 0.06244920194149017,
      "learning_rate": 1.0030004286326619e-06,
      "loss": 0.1998,
      "step": 66480
    },
    {
      "epoch": 14.249892841834548,
      "grad_norm": 29.04716682434082,
      "learning_rate": 1.0001428775539364e-06,
      "loss": 0.7016,
      "step": 66490
    },
    {
      "epoch": 14.252036005143593,
      "grad_norm": 0.009241445921361446,
      "learning_rate": 9.972853264752108e-07,
      "loss": 0.3911,
      "step": 66500
    },
    {
      "epoch": 14.254179168452636,
      "grad_norm": 1.3777072429656982,
      "learning_rate": 9.944277753964853e-07,
      "loss": 0.3488,
      "step": 66510
    },
    {
      "epoch": 14.25632233176168,
      "grad_norm": 0.31006869673728943,
      "learning_rate": 9.9157022431776e-07,
      "loss": 0.1419,
      "step": 66520
    },
    {
      "epoch": 14.258465495070725,
      "grad_norm": 0.07948273420333862,
      "learning_rate": 9.887126732390343e-07,
      "loss": 0.3953,
      "step": 66530
    },
    {
      "epoch": 14.260608658379768,
      "grad_norm": 0.3076755702495575,
      "learning_rate": 9.858551221603086e-07,
      "loss": 0.1418,
      "step": 66540
    },
    {
      "epoch": 14.262751821688813,
      "grad_norm": 44.85912322998047,
      "learning_rate": 9.829975710815832e-07,
      "loss": 0.5989,
      "step": 66550
    },
    {
      "epoch": 14.264894984997857,
      "grad_norm": 0.1319109946489334,
      "learning_rate": 9.801400200028577e-07,
      "loss": 0.1336,
      "step": 66560
    },
    {
      "epoch": 14.2670381483069,
      "grad_norm": 0.02934197522699833,
      "learning_rate": 9.77282468924132e-07,
      "loss": 0.2667,
      "step": 66570
    },
    {
      "epoch": 14.269181311615945,
      "grad_norm": 0.02882745862007141,
      "learning_rate": 9.744249178454067e-07,
      "loss": 0.299,
      "step": 66580
    },
    {
      "epoch": 14.27132447492499,
      "grad_norm": 0.031498298048973083,
      "learning_rate": 9.71567366766681e-07,
      "loss": 0.6744,
      "step": 66590
    },
    {
      "epoch": 14.273467638234033,
      "grad_norm": 0.0072191799990832806,
      "learning_rate": 9.687098156879556e-07,
      "loss": 0.2989,
      "step": 66600
    },
    {
      "epoch": 14.275610801543078,
      "grad_norm": 24.336252212524414,
      "learning_rate": 9.6585226460923e-07,
      "loss": 0.5239,
      "step": 66610
    },
    {
      "epoch": 14.277753964852122,
      "grad_norm": 0.03398057818412781,
      "learning_rate": 9.629947135305045e-07,
      "loss": 0.2632,
      "step": 66620
    },
    {
      "epoch": 14.279897128161165,
      "grad_norm": 0.07068836688995361,
      "learning_rate": 9.601371624517788e-07,
      "loss": 0.3279,
      "step": 66630
    },
    {
      "epoch": 14.28204029147021,
      "grad_norm": 34.18705749511719,
      "learning_rate": 9.572796113730534e-07,
      "loss": 0.2727,
      "step": 66640
    },
    {
      "epoch": 14.284183454779255,
      "grad_norm": 0.2353004813194275,
      "learning_rate": 9.544220602943278e-07,
      "loss": 0.0036,
      "step": 66650
    },
    {
      "epoch": 14.286326618088298,
      "grad_norm": 0.051328521221876144,
      "learning_rate": 9.515645092156023e-07,
      "loss": 0.3134,
      "step": 66660
    },
    {
      "epoch": 14.288469781397342,
      "grad_norm": 71.82561492919922,
      "learning_rate": 9.487069581368767e-07,
      "loss": 0.1233,
      "step": 66670
    },
    {
      "epoch": 14.290612944706387,
      "grad_norm": 0.25630947947502136,
      "learning_rate": 9.458494070581513e-07,
      "loss": 0.3693,
      "step": 66680
    },
    {
      "epoch": 14.29275610801543,
      "grad_norm": 0.02443883754312992,
      "learning_rate": 9.429918559794257e-07,
      "loss": 0.1443,
      "step": 66690
    },
    {
      "epoch": 14.294899271324475,
      "grad_norm": 0.17951542139053345,
      "learning_rate": 9.401343049007001e-07,
      "loss": 0.0028,
      "step": 66700
    },
    {
      "epoch": 14.29704243463352,
      "grad_norm": 0.18164291977882385,
      "learning_rate": 9.372767538219746e-07,
      "loss": 0.48,
      "step": 66710
    },
    {
      "epoch": 14.299185597942563,
      "grad_norm": 0.20533528923988342,
      "learning_rate": 9.344192027432492e-07,
      "loss": 0.1491,
      "step": 66720
    },
    {
      "epoch": 14.301328761251607,
      "grad_norm": 38.32244110107422,
      "learning_rate": 9.315616516645236e-07,
      "loss": 0.2254,
      "step": 66730
    },
    {
      "epoch": 14.303471924560652,
      "grad_norm": 0.0435134693980217,
      "learning_rate": 9.28704100585798e-07,
      "loss": 0.0026,
      "step": 66740
    },
    {
      "epoch": 14.305615087869695,
      "grad_norm": 28.59488296508789,
      "learning_rate": 9.258465495070724e-07,
      "loss": 0.3912,
      "step": 66750
    },
    {
      "epoch": 14.30775825117874,
      "grad_norm": 0.36412912607192993,
      "learning_rate": 9.22988998428347e-07,
      "loss": 0.1601,
      "step": 66760
    },
    {
      "epoch": 14.309901414487785,
      "grad_norm": 0.0896959975361824,
      "learning_rate": 9.201314473496215e-07,
      "loss": 0.197,
      "step": 66770
    },
    {
      "epoch": 14.312044577796827,
      "grad_norm": 0.3511470556259155,
      "learning_rate": 9.172738962708959e-07,
      "loss": 0.2095,
      "step": 66780
    },
    {
      "epoch": 14.314187741105872,
      "grad_norm": 0.3489140570163727,
      "learning_rate": 9.144163451921704e-07,
      "loss": 0.1349,
      "step": 66790
    },
    {
      "epoch": 14.316330904414917,
      "grad_norm": 0.2323273867368698,
      "learning_rate": 9.115587941134449e-07,
      "loss": 0.1721,
      "step": 66800
    },
    {
      "epoch": 14.31847406772396,
      "grad_norm": 0.09290438890457153,
      "learning_rate": 9.087012430347193e-07,
      "loss": 0.2363,
      "step": 66810
    },
    {
      "epoch": 14.320617231033005,
      "grad_norm": 0.0033887019380927086,
      "learning_rate": 9.058436919559937e-07,
      "loss": 0.1869,
      "step": 66820
    },
    {
      "epoch": 14.32276039434205,
      "grad_norm": 0.08856051415205002,
      "learning_rate": 9.029861408772682e-07,
      "loss": 0.1362,
      "step": 66830
    },
    {
      "epoch": 14.324903557651092,
      "grad_norm": 0.01943136565387249,
      "learning_rate": 9.001285897985428e-07,
      "loss": 0.2891,
      "step": 66840
    },
    {
      "epoch": 14.327046720960137,
      "grad_norm": 0.25629693269729614,
      "learning_rate": 8.972710387198172e-07,
      "loss": 0.5172,
      "step": 66850
    },
    {
      "epoch": 14.329189884269182,
      "grad_norm": 69.31571197509766,
      "learning_rate": 8.944134876410916e-07,
      "loss": 0.292,
      "step": 66860
    },
    {
      "epoch": 14.331333047578225,
      "grad_norm": 0.11358404159545898,
      "learning_rate": 8.915559365623662e-07,
      "loss": 0.3245,
      "step": 66870
    },
    {
      "epoch": 14.33347621088727,
      "grad_norm": 20.228702545166016,
      "learning_rate": 8.886983854836406e-07,
      "loss": 0.3626,
      "step": 66880
    },
    {
      "epoch": 14.335619374196314,
      "grad_norm": 0.05008826032280922,
      "learning_rate": 8.85840834404915e-07,
      "loss": 0.0011,
      "step": 66890
    },
    {
      "epoch": 14.337762537505357,
      "grad_norm": 0.15297487378120422,
      "learning_rate": 8.829832833261895e-07,
      "loss": 0.0009,
      "step": 66900
    },
    {
      "epoch": 14.339905700814402,
      "grad_norm": 0.011714876629412174,
      "learning_rate": 8.801257322474641e-07,
      "loss": 0.0014,
      "step": 66910
    },
    {
      "epoch": 14.342048864123447,
      "grad_norm": 0.010720230638980865,
      "learning_rate": 8.772681811687385e-07,
      "loss": 0.1397,
      "step": 66920
    },
    {
      "epoch": 14.34419202743249,
      "grad_norm": 0.028189625591039658,
      "learning_rate": 8.744106300900129e-07,
      "loss": 0.3497,
      "step": 66930
    },
    {
      "epoch": 14.346335190741534,
      "grad_norm": 0.02884761616587639,
      "learning_rate": 8.715530790112873e-07,
      "loss": 0.2179,
      "step": 66940
    },
    {
      "epoch": 14.34847835405058,
      "grad_norm": 0.18758486211299896,
      "learning_rate": 8.686955279325619e-07,
      "loss": 0.5813,
      "step": 66950
    },
    {
      "epoch": 14.350621517359622,
      "grad_norm": 0.06956463307142258,
      "learning_rate": 8.658379768538364e-07,
      "loss": 0.6386,
      "step": 66960
    },
    {
      "epoch": 14.352764680668667,
      "grad_norm": 31.66288185119629,
      "learning_rate": 8.629804257751108e-07,
      "loss": 0.5818,
      "step": 66970
    },
    {
      "epoch": 14.354907843977712,
      "grad_norm": 63.456817626953125,
      "learning_rate": 8.601228746963852e-07,
      "loss": 0.6116,
      "step": 66980
    },
    {
      "epoch": 14.357051007286755,
      "grad_norm": 0.44461676478385925,
      "learning_rate": 8.572653236176598e-07,
      "loss": 0.0031,
      "step": 66990
    },
    {
      "epoch": 14.3591941705958,
      "grad_norm": 0.64334636926651,
      "learning_rate": 8.544077725389342e-07,
      "loss": 0.0458,
      "step": 67000
    },
    {
      "epoch": 14.361337333904844,
      "grad_norm": 0.03644460439682007,
      "learning_rate": 8.515502214602087e-07,
      "loss": 0.0026,
      "step": 67010
    },
    {
      "epoch": 14.363480497213887,
      "grad_norm": 0.062125351279973984,
      "learning_rate": 8.486926703814831e-07,
      "loss": 0.0026,
      "step": 67020
    },
    {
      "epoch": 14.365623660522932,
      "grad_norm": 117.50517272949219,
      "learning_rate": 8.458351193027577e-07,
      "loss": 0.3595,
      "step": 67030
    },
    {
      "epoch": 14.367766823831976,
      "grad_norm": 0.07780800759792328,
      "learning_rate": 8.429775682240321e-07,
      "loss": 0.3744,
      "step": 67040
    },
    {
      "epoch": 14.36990998714102,
      "grad_norm": 18.94264793395996,
      "learning_rate": 8.401200171453065e-07,
      "loss": 0.1931,
      "step": 67050
    },
    {
      "epoch": 14.372053150450064,
      "grad_norm": 20.25790786743164,
      "learning_rate": 8.372624660665809e-07,
      "loss": 0.8643,
      "step": 67060
    },
    {
      "epoch": 14.374196313759109,
      "grad_norm": 0.004938105586916208,
      "learning_rate": 8.344049149878555e-07,
      "loss": 0.4497,
      "step": 67070
    },
    {
      "epoch": 14.376339477068152,
      "grad_norm": 0.09009716659784317,
      "learning_rate": 8.3154736390913e-07,
      "loss": 0.361,
      "step": 67080
    },
    {
      "epoch": 14.378482640377197,
      "grad_norm": 18.692888259887695,
      "learning_rate": 8.286898128304044e-07,
      "loss": 0.4431,
      "step": 67090
    },
    {
      "epoch": 14.380625803686241,
      "grad_norm": 35.8492431640625,
      "learning_rate": 8.258322617516789e-07,
      "loss": 0.7605,
      "step": 67100
    },
    {
      "epoch": 14.382768966995284,
      "grad_norm": 0.2138936072587967,
      "learning_rate": 8.229747106729534e-07,
      "loss": 0.0016,
      "step": 67110
    },
    {
      "epoch": 14.384912130304329,
      "grad_norm": 0.3046920597553253,
      "learning_rate": 8.201171595942278e-07,
      "loss": 0.3009,
      "step": 67120
    },
    {
      "epoch": 14.387055293613374,
      "grad_norm": 0.3528105318546295,
      "learning_rate": 8.172596085155022e-07,
      "loss": 0.0016,
      "step": 67130
    },
    {
      "epoch": 14.389198456922417,
      "grad_norm": 0.03436771035194397,
      "learning_rate": 8.144020574367767e-07,
      "loss": 0.0022,
      "step": 67140
    },
    {
      "epoch": 14.391341620231461,
      "grad_norm": 0.13220499455928802,
      "learning_rate": 8.115445063580513e-07,
      "loss": 0.2302,
      "step": 67150
    },
    {
      "epoch": 14.393484783540506,
      "grad_norm": 0.0924568772315979,
      "learning_rate": 8.086869552793257e-07,
      "loss": 0.5823,
      "step": 67160
    },
    {
      "epoch": 14.39562794684955,
      "grad_norm": 23.445096969604492,
      "learning_rate": 8.058294042006001e-07,
      "loss": 0.5394,
      "step": 67170
    },
    {
      "epoch": 14.397771110158594,
      "grad_norm": 0.305443674325943,
      "learning_rate": 8.029718531218745e-07,
      "loss": 0.2916,
      "step": 67180
    },
    {
      "epoch": 14.399914273467639,
      "grad_norm": 0.02631901577115059,
      "learning_rate": 8.001143020431491e-07,
      "loss": 0.2863,
      "step": 67190
    },
    {
      "epoch": 14.402057436776682,
      "grad_norm": 0.6890023946762085,
      "learning_rate": 7.972567509644236e-07,
      "loss": 0.1634,
      "step": 67200
    },
    {
      "epoch": 14.404200600085726,
      "grad_norm": 0.0933784618973732,
      "learning_rate": 7.94399199885698e-07,
      "loss": 0.0697,
      "step": 67210
    },
    {
      "epoch": 14.406343763394771,
      "grad_norm": 0.08896654099225998,
      "learning_rate": 7.915416488069725e-07,
      "loss": 0.2197,
      "step": 67220
    },
    {
      "epoch": 14.408486926703814,
      "grad_norm": 2.3125178813934326,
      "learning_rate": 7.88684097728247e-07,
      "loss": 0.4209,
      "step": 67230
    },
    {
      "epoch": 14.410630090012859,
      "grad_norm": 0.022075744345784187,
      "learning_rate": 7.858265466495214e-07,
      "loss": 0.1542,
      "step": 67240
    },
    {
      "epoch": 14.412773253321904,
      "grad_norm": 0.16551269590854645,
      "learning_rate": 7.829689955707958e-07,
      "loss": 0.1416,
      "step": 67250
    },
    {
      "epoch": 14.414916416630946,
      "grad_norm": 0.01750667579472065,
      "learning_rate": 7.801114444920703e-07,
      "loss": 0.1572,
      "step": 67260
    },
    {
      "epoch": 14.417059579939991,
      "grad_norm": 63.201053619384766,
      "learning_rate": 7.772538934133449e-07,
      "loss": 0.3958,
      "step": 67270
    },
    {
      "epoch": 14.419202743249036,
      "grad_norm": 47.9764289855957,
      "learning_rate": 7.743963423346193e-07,
      "loss": 0.5342,
      "step": 67280
    },
    {
      "epoch": 14.421345906558079,
      "grad_norm": 0.002504708943888545,
      "learning_rate": 7.715387912558938e-07,
      "loss": 0.6032,
      "step": 67290
    },
    {
      "epoch": 14.423489069867124,
      "grad_norm": 0.15858520567417145,
      "learning_rate": 7.686812401771681e-07,
      "loss": 0.3528,
      "step": 67300
    },
    {
      "epoch": 14.425632233176168,
      "grad_norm": 0.008412730880081654,
      "learning_rate": 7.658236890984427e-07,
      "loss": 0.4307,
      "step": 67310
    },
    {
      "epoch": 14.427775396485211,
      "grad_norm": 0.07988500595092773,
      "learning_rate": 7.629661380197172e-07,
      "loss": 0.3075,
      "step": 67320
    },
    {
      "epoch": 14.429918559794256,
      "grad_norm": 823.898681640625,
      "learning_rate": 7.601085869409916e-07,
      "loss": 0.2092,
      "step": 67330
    },
    {
      "epoch": 14.4320617231033,
      "grad_norm": 335.1797790527344,
      "learning_rate": 7.572510358622661e-07,
      "loss": 0.0982,
      "step": 67340
    },
    {
      "epoch": 14.434204886412344,
      "grad_norm": 51.065147399902344,
      "learning_rate": 7.543934847835406e-07,
      "loss": 0.3206,
      "step": 67350
    },
    {
      "epoch": 14.436348049721389,
      "grad_norm": 0.03559170290827751,
      "learning_rate": 7.51535933704815e-07,
      "loss": 0.2693,
      "step": 67360
    },
    {
      "epoch": 14.438491213030433,
      "grad_norm": 0.024398041889071465,
      "learning_rate": 7.486783826260894e-07,
      "loss": 0.0016,
      "step": 67370
    },
    {
      "epoch": 14.440634376339476,
      "grad_norm": 0.076994888484478,
      "learning_rate": 7.45820831547364e-07,
      "loss": 0.5852,
      "step": 67380
    },
    {
      "epoch": 14.442777539648521,
      "grad_norm": 21.553508758544922,
      "learning_rate": 7.429632804686385e-07,
      "loss": 0.4608,
      "step": 67390
    },
    {
      "epoch": 14.444920702957566,
      "grad_norm": 0.19640786945819855,
      "learning_rate": 7.401057293899129e-07,
      "loss": 0.1563,
      "step": 67400
    },
    {
      "epoch": 14.447063866266609,
      "grad_norm": 0.1575411558151245,
      "learning_rate": 7.372481783111874e-07,
      "loss": 0.5031,
      "step": 67410
    },
    {
      "epoch": 14.449207029575653,
      "grad_norm": 1133.161376953125,
      "learning_rate": 7.343906272324619e-07,
      "loss": 0.0286,
      "step": 67420
    },
    {
      "epoch": 14.451350192884698,
      "grad_norm": 0.006861585192382336,
      "learning_rate": 7.315330761537363e-07,
      "loss": 0.3208,
      "step": 67430
    },
    {
      "epoch": 14.453493356193743,
      "grad_norm": 2.9777307510375977,
      "learning_rate": 7.286755250750108e-07,
      "loss": 0.0038,
      "step": 67440
    },
    {
      "epoch": 14.455636519502786,
      "grad_norm": 18.055416107177734,
      "learning_rate": 7.258179739962852e-07,
      "loss": 0.6717,
      "step": 67450
    },
    {
      "epoch": 14.45777968281183,
      "grad_norm": 0.14212659001350403,
      "learning_rate": 7.229604229175598e-07,
      "loss": 0.0019,
      "step": 67460
    },
    {
      "epoch": 14.459922846120875,
      "grad_norm": 0.2540214955806732,
      "learning_rate": 7.201028718388342e-07,
      "loss": 0.1973,
      "step": 67470
    },
    {
      "epoch": 14.462066009429918,
      "grad_norm": 1.7864608764648438,
      "learning_rate": 7.172453207601087e-07,
      "loss": 0.4602,
      "step": 67480
    },
    {
      "epoch": 14.464209172738963,
      "grad_norm": 4.0835862159729,
      "learning_rate": 7.14387769681383e-07,
      "loss": 0.1833,
      "step": 67490
    },
    {
      "epoch": 14.466352336048008,
      "grad_norm": 22.671289443969727,
      "learning_rate": 7.115302186026576e-07,
      "loss": 0.3,
      "step": 67500
    },
    {
      "epoch": 14.46849549935705,
      "grad_norm": 0.1263788640499115,
      "learning_rate": 7.086726675239321e-07,
      "loss": 0.5288,
      "step": 67510
    },
    {
      "epoch": 14.470638662666095,
      "grad_norm": 0.3402549624443054,
      "learning_rate": 7.058151164452065e-07,
      "loss": 0.4699,
      "step": 67520
    },
    {
      "epoch": 14.47278182597514,
      "grad_norm": 45.89751052856445,
      "learning_rate": 7.02957565366481e-07,
      "loss": 0.3602,
      "step": 67530
    },
    {
      "epoch": 14.474924989284183,
      "grad_norm": 0.004127867985516787,
      "learning_rate": 7.001000142877555e-07,
      "loss": 0.1888,
      "step": 67540
    },
    {
      "epoch": 14.477068152593228,
      "grad_norm": 0.17156322300434113,
      "learning_rate": 6.972424632090299e-07,
      "loss": 0.1516,
      "step": 67550
    },
    {
      "epoch": 14.479211315902273,
      "grad_norm": 0.037878986448049545,
      "learning_rate": 6.943849121303044e-07,
      "loss": 0.3467,
      "step": 67560
    },
    {
      "epoch": 14.481354479211316,
      "grad_norm": 0.15507128834724426,
      "learning_rate": 6.915273610515788e-07,
      "loss": 0.3998,
      "step": 67570
    },
    {
      "epoch": 14.48349764252036,
      "grad_norm": 1.1302685737609863,
      "learning_rate": 6.886698099728534e-07,
      "loss": 0.1743,
      "step": 67580
    },
    {
      "epoch": 14.485640805829405,
      "grad_norm": 0.13328050076961517,
      "learning_rate": 6.858122588941278e-07,
      "loss": 0.1856,
      "step": 67590
    },
    {
      "epoch": 14.487783969138448,
      "grad_norm": 35.82609176635742,
      "learning_rate": 6.829547078154023e-07,
      "loss": 0.3384,
      "step": 67600
    },
    {
      "epoch": 14.489927132447493,
      "grad_norm": 0.014019929803907871,
      "learning_rate": 6.800971567366766e-07,
      "loss": 0.3722,
      "step": 67610
    },
    {
      "epoch": 14.492070295756537,
      "grad_norm": 0.05468840152025223,
      "learning_rate": 6.772396056579512e-07,
      "loss": 0.1498,
      "step": 67620
    },
    {
      "epoch": 14.49421345906558,
      "grad_norm": 0.0253423061221838,
      "learning_rate": 6.743820545792257e-07,
      "loss": 0.0015,
      "step": 67630
    },
    {
      "epoch": 14.496356622374625,
      "grad_norm": 0.043403640389442444,
      "learning_rate": 6.715245035005001e-07,
      "loss": 0.3466,
      "step": 67640
    },
    {
      "epoch": 14.49849978568367,
      "grad_norm": 0.20845934748649597,
      "learning_rate": 6.686669524217746e-07,
      "loss": 0.3805,
      "step": 67650
    },
    {
      "epoch": 14.500642948992713,
      "grad_norm": 0.004762673284858465,
      "learning_rate": 6.658094013430491e-07,
      "loss": 0.1638,
      "step": 67660
    },
    {
      "epoch": 14.502786112301758,
      "grad_norm": 32.801841735839844,
      "learning_rate": 6.629518502643235e-07,
      "loss": 0.3895,
      "step": 67670
    },
    {
      "epoch": 14.504929275610802,
      "grad_norm": 1.005165696144104,
      "learning_rate": 6.60094299185598e-07,
      "loss": 0.1484,
      "step": 67680
    },
    {
      "epoch": 14.507072438919845,
      "grad_norm": 24.916738510131836,
      "learning_rate": 6.572367481068724e-07,
      "loss": 0.1255,
      "step": 67690
    },
    {
      "epoch": 14.50921560222889,
      "grad_norm": 0.09652819484472275,
      "learning_rate": 6.54379197028147e-07,
      "loss": 0.3077,
      "step": 67700
    },
    {
      "epoch": 14.511358765537935,
      "grad_norm": 0.11578090488910675,
      "learning_rate": 6.515216459494214e-07,
      "loss": 0.5746,
      "step": 67710
    },
    {
      "epoch": 14.513501928846978,
      "grad_norm": 0.001773881260305643,
      "learning_rate": 6.486640948706959e-07,
      "loss": 0.001,
      "step": 67720
    },
    {
      "epoch": 14.515645092156022,
      "grad_norm": 0.08382698893547058,
      "learning_rate": 6.458065437919702e-07,
      "loss": 0.0024,
      "step": 67730
    },
    {
      "epoch": 14.517788255465067,
      "grad_norm": 22.112789154052734,
      "learning_rate": 6.429489927132448e-07,
      "loss": 0.7896,
      "step": 67740
    },
    {
      "epoch": 14.51993141877411,
      "grad_norm": 0.3233095109462738,
      "learning_rate": 6.400914416345193e-07,
      "loss": 0.2804,
      "step": 67750
    },
    {
      "epoch": 14.522074582083155,
      "grad_norm": 0.009135117754340172,
      "learning_rate": 6.372338905557937e-07,
      "loss": 0.2809,
      "step": 67760
    },
    {
      "epoch": 14.5242177453922,
      "grad_norm": 0.36295071244239807,
      "learning_rate": 6.343763394770682e-07,
      "loss": 0.6212,
      "step": 67770
    },
    {
      "epoch": 14.526360908701243,
      "grad_norm": 0.052019521594047546,
      "learning_rate": 6.315187883983427e-07,
      "loss": 0.1723,
      "step": 67780
    },
    {
      "epoch": 14.528504072010287,
      "grad_norm": 308.60052490234375,
      "learning_rate": 6.286612373196172e-07,
      "loss": 0.5163,
      "step": 67790
    },
    {
      "epoch": 14.530647235319332,
      "grad_norm": 0.05495928227901459,
      "learning_rate": 6.258036862408915e-07,
      "loss": 0.0019,
      "step": 67800
    },
    {
      "epoch": 14.532790398628375,
      "grad_norm": 0.027984604239463806,
      "learning_rate": 6.229461351621661e-07,
      "loss": 0.0027,
      "step": 67810
    },
    {
      "epoch": 14.53493356193742,
      "grad_norm": 0.03953941911458969,
      "learning_rate": 6.200885840834405e-07,
      "loss": 0.0029,
      "step": 67820
    },
    {
      "epoch": 14.537076725246465,
      "grad_norm": 0.3278460204601288,
      "learning_rate": 6.17231033004715e-07,
      "loss": 0.3146,
      "step": 67830
    },
    {
      "epoch": 14.539219888555508,
      "grad_norm": 0.039092667400836945,
      "learning_rate": 6.143734819259895e-07,
      "loss": 0.3165,
      "step": 67840
    },
    {
      "epoch": 14.541363051864552,
      "grad_norm": 0.08559413999319077,
      "learning_rate": 6.115159308472639e-07,
      "loss": 0.3434,
      "step": 67850
    },
    {
      "epoch": 14.543506215173597,
      "grad_norm": 0.11489653587341309,
      "learning_rate": 6.086583797685384e-07,
      "loss": 0.1253,
      "step": 67860
    },
    {
      "epoch": 14.54564937848264,
      "grad_norm": 0.009756308980286121,
      "learning_rate": 6.058008286898129e-07,
      "loss": 0.334,
      "step": 67870
    },
    {
      "epoch": 14.547792541791685,
      "grad_norm": 0.16460907459259033,
      "learning_rate": 6.029432776110873e-07,
      "loss": 0.1941,
      "step": 67880
    },
    {
      "epoch": 14.54993570510073,
      "grad_norm": 0.10454177856445312,
      "learning_rate": 6.000857265323618e-07,
      "loss": 0.0008,
      "step": 67890
    },
    {
      "epoch": 14.552078868409772,
      "grad_norm": 0.18870317935943604,
      "learning_rate": 5.972281754536363e-07,
      "loss": 0.0021,
      "step": 67900
    },
    {
      "epoch": 14.554222031718817,
      "grad_norm": 0.6064856648445129,
      "learning_rate": 5.943706243749108e-07,
      "loss": 0.3594,
      "step": 67910
    },
    {
      "epoch": 14.556365195027862,
      "grad_norm": 0.002872324315831065,
      "learning_rate": 5.915130732961853e-07,
      "loss": 0.1648,
      "step": 67920
    },
    {
      "epoch": 14.558508358336905,
      "grad_norm": 82.92362213134766,
      "learning_rate": 5.886555222174597e-07,
      "loss": 0.6079,
      "step": 67930
    },
    {
      "epoch": 14.56065152164595,
      "grad_norm": 0.289776086807251,
      "learning_rate": 5.857979711387342e-07,
      "loss": 0.1185,
      "step": 67940
    },
    {
      "epoch": 14.562794684954994,
      "grad_norm": 0.01335130911320448,
      "learning_rate": 5.829404200600086e-07,
      "loss": 0.001,
      "step": 67950
    },
    {
      "epoch": 14.564937848264037,
      "grad_norm": 0.0022128012496978045,
      "learning_rate": 5.800828689812831e-07,
      "loss": 0.0023,
      "step": 67960
    },
    {
      "epoch": 14.567081011573082,
      "grad_norm": 0.4968724846839905,
      "learning_rate": 5.772253179025575e-07,
      "loss": 0.0825,
      "step": 67970
    },
    {
      "epoch": 14.569224174882127,
      "grad_norm": 0.0466047003865242,
      "learning_rate": 5.743677668238321e-07,
      "loss": 0.3237,
      "step": 67980
    },
    {
      "epoch": 14.57136733819117,
      "grad_norm": 130.51348876953125,
      "learning_rate": 5.715102157451065e-07,
      "loss": 0.7413,
      "step": 67990
    },
    {
      "epoch": 14.573510501500214,
      "grad_norm": 35.00090026855469,
      "learning_rate": 5.68652664666381e-07,
      "loss": 0.2977,
      "step": 68000
    },
    {
      "epoch": 14.57565366480926,
      "grad_norm": 0.005570189096033573,
      "learning_rate": 5.657951135876554e-07,
      "loss": 0.1655,
      "step": 68010
    },
    {
      "epoch": 14.577796828118302,
      "grad_norm": 0.07329919934272766,
      "learning_rate": 5.629375625089299e-07,
      "loss": 0.4753,
      "step": 68020
    },
    {
      "epoch": 14.579939991427347,
      "grad_norm": 0.019113773480057716,
      "learning_rate": 5.600800114302044e-07,
      "loss": 0.1536,
      "step": 68030
    },
    {
      "epoch": 14.582083154736392,
      "grad_norm": 0.03598545119166374,
      "learning_rate": 5.572224603514788e-07,
      "loss": 0.3022,
      "step": 68040
    },
    {
      "epoch": 14.584226318045435,
      "grad_norm": 0.9581397771835327,
      "learning_rate": 5.543649092727533e-07,
      "loss": 0.3571,
      "step": 68050
    },
    {
      "epoch": 14.58636948135448,
      "grad_norm": 1.1258859634399414,
      "learning_rate": 5.515073581940278e-07,
      "loss": 0.1965,
      "step": 68060
    },
    {
      "epoch": 14.588512644663524,
      "grad_norm": 0.2794433832168579,
      "learning_rate": 5.486498071153022e-07,
      "loss": 0.1551,
      "step": 68070
    },
    {
      "epoch": 14.590655807972567,
      "grad_norm": 1.1400911808013916,
      "learning_rate": 5.457922560365767e-07,
      "loss": 0.1258,
      "step": 68080
    },
    {
      "epoch": 14.592798971281612,
      "grad_norm": 0.15636934340000153,
      "learning_rate": 5.429347049578511e-07,
      "loss": 0.4643,
      "step": 68090
    },
    {
      "epoch": 14.594942134590656,
      "grad_norm": 0.23554658889770508,
      "learning_rate": 5.400771538791257e-07,
      "loss": 0.4965,
      "step": 68100
    },
    {
      "epoch": 14.5970852978997,
      "grad_norm": 0.05707453936338425,
      "learning_rate": 5.372196028004e-07,
      "loss": 0.4622,
      "step": 68110
    },
    {
      "epoch": 14.599228461208744,
      "grad_norm": 0.036039937287569046,
      "learning_rate": 5.343620517216746e-07,
      "loss": 0.1616,
      "step": 68120
    },
    {
      "epoch": 14.601371624517789,
      "grad_norm": 0.27723944187164307,
      "learning_rate": 5.31504500642949e-07,
      "loss": 0.3247,
      "step": 68130
    },
    {
      "epoch": 14.603514787826832,
      "grad_norm": 0.1341763436794281,
      "learning_rate": 5.286469495642235e-07,
      "loss": 0.1726,
      "step": 68140
    },
    {
      "epoch": 14.605657951135877,
      "grad_norm": 0.055789198726415634,
      "learning_rate": 5.25789398485498e-07,
      "loss": 0.4112,
      "step": 68150
    },
    {
      "epoch": 14.607801114444921,
      "grad_norm": 19.302921295166016,
      "learning_rate": 5.229318474067724e-07,
      "loss": 0.382,
      "step": 68160
    },
    {
      "epoch": 14.609944277753964,
      "grad_norm": 61.874568939208984,
      "learning_rate": 5.200742963280469e-07,
      "loss": 0.1336,
      "step": 68170
    },
    {
      "epoch": 14.612087441063009,
      "grad_norm": 0.2964783310890198,
      "learning_rate": 5.172167452493214e-07,
      "loss": 0.3149,
      "step": 68180
    },
    {
      "epoch": 14.614230604372054,
      "grad_norm": 0.02161179669201374,
      "learning_rate": 5.143591941705958e-07,
      "loss": 0.001,
      "step": 68190
    },
    {
      "epoch": 14.616373767681097,
      "grad_norm": 0.02908160164952278,
      "learning_rate": 5.115016430918703e-07,
      "loss": 0.3699,
      "step": 68200
    },
    {
      "epoch": 14.618516930990141,
      "grad_norm": 45.35731506347656,
      "learning_rate": 5.086440920131447e-07,
      "loss": 0.6479,
      "step": 68210
    },
    {
      "epoch": 14.620660094299186,
      "grad_norm": 0.5889105796813965,
      "learning_rate": 5.057865409344193e-07,
      "loss": 0.0035,
      "step": 68220
    },
    {
      "epoch": 14.62280325760823,
      "grad_norm": 57.17509460449219,
      "learning_rate": 5.029289898556937e-07,
      "loss": 0.2835,
      "step": 68230
    },
    {
      "epoch": 14.624946420917274,
      "grad_norm": 83.34739685058594,
      "learning_rate": 5.000714387769682e-07,
      "loss": 0.4437,
      "step": 68240
    },
    {
      "epoch": 14.627089584226319,
      "grad_norm": 0.02281496487557888,
      "learning_rate": 4.972138876982427e-07,
      "loss": 0.0885,
      "step": 68250
    },
    {
      "epoch": 14.629232747535362,
      "grad_norm": 0.8947411775588989,
      "learning_rate": 4.943563366195171e-07,
      "loss": 0.1676,
      "step": 68260
    },
    {
      "epoch": 14.631375910844406,
      "grad_norm": 0.06294761598110199,
      "learning_rate": 4.914987855407916e-07,
      "loss": 0.0016,
      "step": 68270
    },
    {
      "epoch": 14.633519074153451,
      "grad_norm": 0.0747889056801796,
      "learning_rate": 4.88641234462066e-07,
      "loss": 0.1987,
      "step": 68280
    },
    {
      "epoch": 14.635662237462494,
      "grad_norm": 0.4347287118434906,
      "learning_rate": 4.857836833833405e-07,
      "loss": 0.002,
      "step": 68290
    },
    {
      "epoch": 14.637805400771539,
      "grad_norm": 0.15936650335788727,
      "learning_rate": 4.82926132304615e-07,
      "loss": 0.1561,
      "step": 68300
    },
    {
      "epoch": 14.639948564080584,
      "grad_norm": 41.333160400390625,
      "learning_rate": 4.800685812258894e-07,
      "loss": 0.572,
      "step": 68310
    },
    {
      "epoch": 14.642091727389626,
      "grad_norm": 0.001860251184552908,
      "learning_rate": 4.772110301471639e-07,
      "loss": 0.3391,
      "step": 68320
    },
    {
      "epoch": 14.644234890698671,
      "grad_norm": 0.013029037043452263,
      "learning_rate": 4.7435347906843833e-07,
      "loss": 0.0017,
      "step": 68330
    },
    {
      "epoch": 14.646378054007716,
      "grad_norm": 0.02319089137017727,
      "learning_rate": 4.7149592798971284e-07,
      "loss": 0.2759,
      "step": 68340
    },
    {
      "epoch": 14.648521217316759,
      "grad_norm": 0.03004557266831398,
      "learning_rate": 4.686383769109873e-07,
      "loss": 0.1505,
      "step": 68350
    },
    {
      "epoch": 14.650664380625804,
      "grad_norm": 0.3208467662334442,
      "learning_rate": 4.657808258322618e-07,
      "loss": 0.1526,
      "step": 68360
    },
    {
      "epoch": 14.652807543934848,
      "grad_norm": 0.005054148845374584,
      "learning_rate": 4.629232747535362e-07,
      "loss": 0.0007,
      "step": 68370
    },
    {
      "epoch": 14.654950707243891,
      "grad_norm": 0.19474360346794128,
      "learning_rate": 4.6006572367481073e-07,
      "loss": 0.1276,
      "step": 68380
    },
    {
      "epoch": 14.657093870552936,
      "grad_norm": 0.002877451479434967,
      "learning_rate": 4.572081725960852e-07,
      "loss": 0.807,
      "step": 68390
    },
    {
      "epoch": 14.65923703386198,
      "grad_norm": 0.2001684159040451,
      "learning_rate": 4.5435062151735964e-07,
      "loss": 0.7867,
      "step": 68400
    },
    {
      "epoch": 14.661380197171024,
      "grad_norm": 0.23218145966529846,
      "learning_rate": 4.514930704386341e-07,
      "loss": 0.6033,
      "step": 68410
    },
    {
      "epoch": 14.663523360480069,
      "grad_norm": 0.35440540313720703,
      "learning_rate": 4.486355193599086e-07,
      "loss": 0.1163,
      "step": 68420
    },
    {
      "epoch": 14.665666523789113,
      "grad_norm": 30.077648162841797,
      "learning_rate": 4.457779682811831e-07,
      "loss": 0.2443,
      "step": 68430
    },
    {
      "epoch": 14.667809687098156,
      "grad_norm": 0.02271971106529236,
      "learning_rate": 4.429204172024575e-07,
      "loss": 0.6395,
      "step": 68440
    },
    {
      "epoch": 14.669952850407201,
      "grad_norm": 0.013013984076678753,
      "learning_rate": 4.4006286612373204e-07,
      "loss": 0.0012,
      "step": 68450
    },
    {
      "epoch": 14.672096013716246,
      "grad_norm": 40.76285934448242,
      "learning_rate": 4.3720531504500644e-07,
      "loss": 0.732,
      "step": 68460
    },
    {
      "epoch": 14.674239177025289,
      "grad_norm": 0.11149286478757858,
      "learning_rate": 4.3434776396628095e-07,
      "loss": 0.3393,
      "step": 68470
    },
    {
      "epoch": 14.676382340334333,
      "grad_norm": 0.1281530261039734,
      "learning_rate": 4.314902128875554e-07,
      "loss": 0.4475,
      "step": 68480
    },
    {
      "epoch": 14.678525503643378,
      "grad_norm": 0.21161971986293793,
      "learning_rate": 4.286326618088299e-07,
      "loss": 0.3394,
      "step": 68490
    },
    {
      "epoch": 14.680668666952421,
      "grad_norm": 0.025282884016633034,
      "learning_rate": 4.257751107301043e-07,
      "loss": 0.1428,
      "step": 68500
    },
    {
      "epoch": 14.682811830261466,
      "grad_norm": 6633.01171875,
      "learning_rate": 4.2291755965137884e-07,
      "loss": 0.212,
      "step": 68510
    },
    {
      "epoch": 14.68495499357051,
      "grad_norm": 0.0378231555223465,
      "learning_rate": 4.2006000857265324e-07,
      "loss": 0.2665,
      "step": 68520
    },
    {
      "epoch": 14.687098156879554,
      "grad_norm": 0.05819692090153694,
      "learning_rate": 4.1720245749392775e-07,
      "loss": 0.1506,
      "step": 68530
    },
    {
      "epoch": 14.689241320188598,
      "grad_norm": 0.14098893105983734,
      "learning_rate": 4.143449064152022e-07,
      "loss": 0.7564,
      "step": 68540
    },
    {
      "epoch": 14.691384483497643,
      "grad_norm": 0.31826192140579224,
      "learning_rate": 4.114873553364767e-07,
      "loss": 0.1805,
      "step": 68550
    },
    {
      "epoch": 14.693527646806686,
      "grad_norm": 0.10652844607830048,
      "learning_rate": 4.086298042577511e-07,
      "loss": 0.1219,
      "step": 68560
    },
    {
      "epoch": 14.69567081011573,
      "grad_norm": 0.10526374727487564,
      "learning_rate": 4.0577225317902563e-07,
      "loss": 0.5415,
      "step": 68570
    },
    {
      "epoch": 14.697813973424775,
      "grad_norm": 0.4235844314098358,
      "learning_rate": 4.0291470210030004e-07,
      "loss": 0.1218,
      "step": 68580
    },
    {
      "epoch": 14.699957136733818,
      "grad_norm": 0.192838653922081,
      "learning_rate": 4.0005715102157455e-07,
      "loss": 0.1672,
      "step": 68590
    },
    {
      "epoch": 14.702100300042863,
      "grad_norm": 0.30285167694091797,
      "learning_rate": 3.97199599942849e-07,
      "loss": 0.1958,
      "step": 68600
    },
    {
      "epoch": 14.704243463351908,
      "grad_norm": 0.044833745807409286,
      "learning_rate": 3.943420488641235e-07,
      "loss": 0.6865,
      "step": 68610
    },
    {
      "epoch": 14.70638662666095,
      "grad_norm": 0.09531339257955551,
      "learning_rate": 3.914844977853979e-07,
      "loss": 0.1932,
      "step": 68620
    },
    {
      "epoch": 14.708529789969996,
      "grad_norm": 0.04211083799600601,
      "learning_rate": 3.8862694670667243e-07,
      "loss": 0.6744,
      "step": 68630
    },
    {
      "epoch": 14.71067295327904,
      "grad_norm": 0.12512141466140747,
      "learning_rate": 3.857693956279469e-07,
      "loss": 0.0025,
      "step": 68640
    },
    {
      "epoch": 14.712816116588083,
      "grad_norm": 0.011575153097510338,
      "learning_rate": 3.8291184454922135e-07,
      "loss": 0.0516,
      "step": 68650
    },
    {
      "epoch": 14.714959279897128,
      "grad_norm": 0.011650250293314457,
      "learning_rate": 3.800542934704958e-07,
      "loss": 0.3717,
      "step": 68660
    },
    {
      "epoch": 14.717102443206173,
      "grad_norm": 22.145845413208008,
      "learning_rate": 3.771967423917703e-07,
      "loss": 0.8342,
      "step": 68670
    },
    {
      "epoch": 14.719245606515216,
      "grad_norm": 0.22804570198059082,
      "learning_rate": 3.743391913130447e-07,
      "loss": 0.1577,
      "step": 68680
    },
    {
      "epoch": 14.72138876982426,
      "grad_norm": 1.440163016319275,
      "learning_rate": 3.7148164023431923e-07,
      "loss": 0.3082,
      "step": 68690
    },
    {
      "epoch": 14.723531933133305,
      "grad_norm": 47.02030563354492,
      "learning_rate": 3.686240891555937e-07,
      "loss": 0.4033,
      "step": 68700
    },
    {
      "epoch": 14.725675096442348,
      "grad_norm": 0.43388885259628296,
      "learning_rate": 3.6576653807686815e-07,
      "loss": 0.2339,
      "step": 68710
    },
    {
      "epoch": 14.727818259751393,
      "grad_norm": 0.027130115777254105,
      "learning_rate": 3.629089869981426e-07,
      "loss": 0.1192,
      "step": 68720
    },
    {
      "epoch": 14.729961423060438,
      "grad_norm": 0.0301045048981905,
      "learning_rate": 3.600514359194171e-07,
      "loss": 0.0011,
      "step": 68730
    },
    {
      "epoch": 14.73210458636948,
      "grad_norm": 0.1293473243713379,
      "learning_rate": 3.571938848406915e-07,
      "loss": 0.2407,
      "step": 68740
    },
    {
      "epoch": 14.734247749678525,
      "grad_norm": 0.16526862978935242,
      "learning_rate": 3.5433633376196603e-07,
      "loss": 0.001,
      "step": 68750
    },
    {
      "epoch": 14.73639091298757,
      "grad_norm": 0.112234927713871,
      "learning_rate": 3.514787826832405e-07,
      "loss": 0.3446,
      "step": 68760
    },
    {
      "epoch": 14.738534076296613,
      "grad_norm": 0.13528650999069214,
      "learning_rate": 3.4862123160451495e-07,
      "loss": 0.4615,
      "step": 68770
    },
    {
      "epoch": 14.740677239605658,
      "grad_norm": 0.10400743782520294,
      "learning_rate": 3.457636805257894e-07,
      "loss": 0.2147,
      "step": 68780
    },
    {
      "epoch": 14.742820402914703,
      "grad_norm": 46.64667892456055,
      "learning_rate": 3.429061294470639e-07,
      "loss": 0.1819,
      "step": 68790
    },
    {
      "epoch": 14.744963566223745,
      "grad_norm": 0.3004337549209595,
      "learning_rate": 3.400485783683383e-07,
      "loss": 0.3261,
      "step": 68800
    },
    {
      "epoch": 14.74710672953279,
      "grad_norm": 0.008659467101097107,
      "learning_rate": 3.3719102728961283e-07,
      "loss": 0.2118,
      "step": 68810
    },
    {
      "epoch": 14.749249892841835,
      "grad_norm": 1.3463648557662964,
      "learning_rate": 3.343334762108873e-07,
      "loss": 0.3452,
      "step": 68820
    },
    {
      "epoch": 14.751393056150878,
      "grad_norm": 34.02888107299805,
      "learning_rate": 3.3147592513216175e-07,
      "loss": 0.73,
      "step": 68830
    },
    {
      "epoch": 14.753536219459923,
      "grad_norm": 30.281179428100586,
      "learning_rate": 3.286183740534362e-07,
      "loss": 0.9277,
      "step": 68840
    },
    {
      "epoch": 14.755679382768967,
      "grad_norm": 0.04224979132413864,
      "learning_rate": 3.257608229747107e-07,
      "loss": 0.0018,
      "step": 68850
    },
    {
      "epoch": 14.75782254607801,
      "grad_norm": 0.04341841861605644,
      "learning_rate": 3.229032718959851e-07,
      "loss": 0.0017,
      "step": 68860
    },
    {
      "epoch": 14.759965709387055,
      "grad_norm": 0.15005524456501007,
      "learning_rate": 3.2004572081725963e-07,
      "loss": 0.4223,
      "step": 68870
    },
    {
      "epoch": 14.7621088726961,
      "grad_norm": 32.1170768737793,
      "learning_rate": 3.171881697385341e-07,
      "loss": 0.5622,
      "step": 68880
    },
    {
      "epoch": 14.764252036005143,
      "grad_norm": 0.1866893768310547,
      "learning_rate": 3.143306186598086e-07,
      "loss": 0.1828,
      "step": 68890
    },
    {
      "epoch": 14.766395199314188,
      "grad_norm": 0.32750454545021057,
      "learning_rate": 3.1147306758108306e-07,
      "loss": 0.1186,
      "step": 68900
    },
    {
      "epoch": 14.768538362623232,
      "grad_norm": 0.004965316504240036,
      "learning_rate": 3.086155165023575e-07,
      "loss": 0.7627,
      "step": 68910
    },
    {
      "epoch": 14.770681525932275,
      "grad_norm": 23.83538055419922,
      "learning_rate": 3.0575796542363197e-07,
      "loss": 0.4739,
      "step": 68920
    },
    {
      "epoch": 14.77282468924132,
      "grad_norm": 27.29030418395996,
      "learning_rate": 3.0290041434490643e-07,
      "loss": 0.4454,
      "step": 68930
    },
    {
      "epoch": 14.774967852550365,
      "grad_norm": 0.0072637321427464485,
      "learning_rate": 3.000428632661809e-07,
      "loss": 0.1422,
      "step": 68940
    },
    {
      "epoch": 14.777111015859408,
      "grad_norm": 0.365782767534256,
      "learning_rate": 2.971853121874554e-07,
      "loss": 0.1452,
      "step": 68950
    },
    {
      "epoch": 14.779254179168452,
      "grad_norm": 0.9083355069160461,
      "learning_rate": 2.9432776110872985e-07,
      "loss": 0.5415,
      "step": 68960
    },
    {
      "epoch": 14.781397342477497,
      "grad_norm": 0.11584937572479248,
      "learning_rate": 2.914702100300043e-07,
      "loss": 0.002,
      "step": 68970
    },
    {
      "epoch": 14.78354050578654,
      "grad_norm": 0.017046529799699783,
      "learning_rate": 2.8861265895127877e-07,
      "loss": 0.1641,
      "step": 68980
    },
    {
      "epoch": 14.785683669095585,
      "grad_norm": 0.043109502643346786,
      "learning_rate": 2.8575510787255323e-07,
      "loss": 0.1508,
      "step": 68990
    },
    {
      "epoch": 14.78782683240463,
      "grad_norm": 0.20558308064937592,
      "learning_rate": 2.828975567938277e-07,
      "loss": 0.3272,
      "step": 69000
    },
    {
      "epoch": 14.789969995713673,
      "grad_norm": 0.08072800189256668,
      "learning_rate": 2.800400057151022e-07,
      "loss": 0.4839,
      "step": 69010
    },
    {
      "epoch": 14.792113159022717,
      "grad_norm": 1.7219339609146118,
      "learning_rate": 2.7718245463637665e-07,
      "loss": 0.4421,
      "step": 69020
    },
    {
      "epoch": 14.794256322331762,
      "grad_norm": 0.2907206416130066,
      "learning_rate": 2.743249035576511e-07,
      "loss": 0.1393,
      "step": 69030
    },
    {
      "epoch": 14.796399485640805,
      "grad_norm": 0.011670459993183613,
      "learning_rate": 2.7146735247892557e-07,
      "loss": 0.4562,
      "step": 69040
    },
    {
      "epoch": 14.79854264894985,
      "grad_norm": 0.14775322377681732,
      "learning_rate": 2.686098014002e-07,
      "loss": 0.1828,
      "step": 69050
    },
    {
      "epoch": 14.800685812258894,
      "grad_norm": 0.032987941056489944,
      "learning_rate": 2.657522503214745e-07,
      "loss": 0.0016,
      "step": 69060
    },
    {
      "epoch": 14.802828975567937,
      "grad_norm": 0.15797555446624756,
      "learning_rate": 2.62894699242749e-07,
      "loss": 0.1937,
      "step": 69070
    },
    {
      "epoch": 14.804972138876982,
      "grad_norm": 0.009688927792012691,
      "learning_rate": 2.6003714816402345e-07,
      "loss": 0.1539,
      "step": 69080
    },
    {
      "epoch": 14.807115302186027,
      "grad_norm": 0.19099701941013336,
      "learning_rate": 2.571795970852979e-07,
      "loss": 0.0028,
      "step": 69090
    },
    {
      "epoch": 14.80925846549507,
      "grad_norm": 0.006481995340436697,
      "learning_rate": 2.5432204600657237e-07,
      "loss": 0.3788,
      "step": 69100
    },
    {
      "epoch": 14.811401628804115,
      "grad_norm": 0.05661342293024063,
      "learning_rate": 2.514644949278468e-07,
      "loss": 0.2067,
      "step": 69110
    },
    {
      "epoch": 14.81354479211316,
      "grad_norm": 0.19478556513786316,
      "learning_rate": 2.4860694384912134e-07,
      "loss": 0.1625,
      "step": 69120
    },
    {
      "epoch": 14.815687955422202,
      "grad_norm": 0.42325031757354736,
      "learning_rate": 2.457493927703958e-07,
      "loss": 0.5278,
      "step": 69130
    },
    {
      "epoch": 14.817831118731247,
      "grad_norm": 22.455387115478516,
      "learning_rate": 2.4289184169167025e-07,
      "loss": 0.3154,
      "step": 69140
    },
    {
      "epoch": 14.819974282040292,
      "grad_norm": 0.29692697525024414,
      "learning_rate": 2.400342906129447e-07,
      "loss": 0.373,
      "step": 69150
    },
    {
      "epoch": 14.822117445349335,
      "grad_norm": 0.30276912450790405,
      "learning_rate": 2.3717673953421917e-07,
      "loss": 0.1519,
      "step": 69160
    },
    {
      "epoch": 14.82426060865838,
      "grad_norm": 0.0666588768362999,
      "learning_rate": 2.3431918845549365e-07,
      "loss": 0.1524,
      "step": 69170
    },
    {
      "epoch": 14.826403771967424,
      "grad_norm": 0.008061741478741169,
      "learning_rate": 2.314616373767681e-07,
      "loss": 0.0038,
      "step": 69180
    },
    {
      "epoch": 14.828546935276467,
      "grad_norm": 35.229217529296875,
      "learning_rate": 2.286040862980426e-07,
      "loss": 0.213,
      "step": 69190
    },
    {
      "epoch": 14.830690098585512,
      "grad_norm": 0.005527265835553408,
      "learning_rate": 2.2574653521931705e-07,
      "loss": 0.4029,
      "step": 69200
    },
    {
      "epoch": 14.832833261894557,
      "grad_norm": 0.20867247879505157,
      "learning_rate": 2.2288898414059156e-07,
      "loss": 0.1783,
      "step": 69210
    },
    {
      "epoch": 14.8349764252036,
      "grad_norm": 0.08055831491947174,
      "learning_rate": 2.2003143306186602e-07,
      "loss": 0.2939,
      "step": 69220
    },
    {
      "epoch": 14.837119588512644,
      "grad_norm": 0.04868378862738609,
      "learning_rate": 2.1717388198314048e-07,
      "loss": 0.709,
      "step": 69230
    },
    {
      "epoch": 14.839262751821689,
      "grad_norm": 41.26314926147461,
      "learning_rate": 2.1431633090441496e-07,
      "loss": 0.3238,
      "step": 69240
    },
    {
      "epoch": 14.841405915130732,
      "grad_norm": 0.11081032454967499,
      "learning_rate": 2.1145877982568942e-07,
      "loss": 0.002,
      "step": 69250
    },
    {
      "epoch": 14.843549078439777,
      "grad_norm": 0.10356014966964722,
      "learning_rate": 2.0860122874696388e-07,
      "loss": 0.4084,
      "step": 69260
    },
    {
      "epoch": 14.845692241748822,
      "grad_norm": 0.08549094945192337,
      "learning_rate": 2.0574367766823836e-07,
      "loss": 0.1548,
      "step": 69270
    },
    {
      "epoch": 14.847835405057864,
      "grad_norm": 0.07128454744815826,
      "learning_rate": 2.0288612658951282e-07,
      "loss": 0.2752,
      "step": 69280
    },
    {
      "epoch": 14.84997856836691,
      "grad_norm": 0.03865884616971016,
      "learning_rate": 2.0002857551078727e-07,
      "loss": 0.1326,
      "step": 69290
    },
    {
      "epoch": 14.852121731675954,
      "grad_norm": 0.04288952425122261,
      "learning_rate": 1.9717102443206176e-07,
      "loss": 0.3692,
      "step": 69300
    },
    {
      "epoch": 14.854264894984997,
      "grad_norm": 46.886539459228516,
      "learning_rate": 1.9431347335333622e-07,
      "loss": 0.3549,
      "step": 69310
    },
    {
      "epoch": 14.856408058294042,
      "grad_norm": 0.15748314559459686,
      "learning_rate": 1.9145592227461067e-07,
      "loss": 0.5833,
      "step": 69320
    },
    {
      "epoch": 14.858551221603086,
      "grad_norm": 0.3670676350593567,
      "learning_rate": 1.8859837119588516e-07,
      "loss": 0.2894,
      "step": 69330
    },
    {
      "epoch": 14.860694384912131,
      "grad_norm": 0.09528834372758865,
      "learning_rate": 1.8574082011715962e-07,
      "loss": 0.1881,
      "step": 69340
    },
    {
      "epoch": 14.862837548221174,
      "grad_norm": 1.5035938024520874,
      "learning_rate": 1.8288326903843407e-07,
      "loss": 0.3851,
      "step": 69350
    },
    {
      "epoch": 14.864980711530219,
      "grad_norm": 0.055718034505844116,
      "learning_rate": 1.8002571795970856e-07,
      "loss": 0.002,
      "step": 69360
    },
    {
      "epoch": 14.867123874839264,
      "grad_norm": 26.718059539794922,
      "learning_rate": 1.7716816688098302e-07,
      "loss": 0.7088,
      "step": 69370
    },
    {
      "epoch": 14.869267038148307,
      "grad_norm": 0.17564859986305237,
      "learning_rate": 1.7431061580225747e-07,
      "loss": 0.3481,
      "step": 69380
    },
    {
      "epoch": 14.871410201457351,
      "grad_norm": 0.07273969054222107,
      "learning_rate": 1.7145306472353196e-07,
      "loss": 0.2469,
      "step": 69390
    },
    {
      "epoch": 14.873553364766396,
      "grad_norm": 19.27792739868164,
      "learning_rate": 1.6859551364480642e-07,
      "loss": 0.2962,
      "step": 69400
    },
    {
      "epoch": 14.875696528075439,
      "grad_norm": 0.059819918125867844,
      "learning_rate": 1.6573796256608087e-07,
      "loss": 0.2891,
      "step": 69410
    },
    {
      "epoch": 14.877839691384484,
      "grad_norm": 0.09654181450605392,
      "learning_rate": 1.6288041148735536e-07,
      "loss": 0.2481,
      "step": 69420
    },
    {
      "epoch": 14.879982854693528,
      "grad_norm": 0.00751750823110342,
      "learning_rate": 1.6002286040862981e-07,
      "loss": 0.1893,
      "step": 69430
    },
    {
      "epoch": 14.882126018002571,
      "grad_norm": 69.54998779296875,
      "learning_rate": 1.571653093299043e-07,
      "loss": 0.4564,
      "step": 69440
    },
    {
      "epoch": 14.884269181311616,
      "grad_norm": 0.07364871352910995,
      "learning_rate": 1.5430775825117876e-07,
      "loss": 0.0032,
      "step": 69450
    },
    {
      "epoch": 14.88641234462066,
      "grad_norm": 0.2997933626174927,
      "learning_rate": 1.5145020717245321e-07,
      "loss": 0.3852,
      "step": 69460
    },
    {
      "epoch": 14.888555507929704,
      "grad_norm": 0.019661083817481995,
      "learning_rate": 1.485926560937277e-07,
      "loss": 0.1602,
      "step": 69470
    },
    {
      "epoch": 14.890698671238749,
      "grad_norm": 0.1561650186777115,
      "learning_rate": 1.4573510501500216e-07,
      "loss": 0.2893,
      "step": 69480
    },
    {
      "epoch": 14.892841834547793,
      "grad_norm": 0.14974825084209442,
      "learning_rate": 1.4287755393627661e-07,
      "loss": 0.6131,
      "step": 69490
    },
    {
      "epoch": 14.894984997856836,
      "grad_norm": 0.008728574961423874,
      "learning_rate": 1.400200028575511e-07,
      "loss": 0.1571,
      "step": 69500
    },
    {
      "epoch": 14.897128161165881,
      "grad_norm": 0.22353918850421906,
      "learning_rate": 1.3716245177882556e-07,
      "loss": 0.1372,
      "step": 69510
    },
    {
      "epoch": 14.899271324474926,
      "grad_norm": 52.06662368774414,
      "learning_rate": 1.343049007001e-07,
      "loss": 0.922,
      "step": 69520
    },
    {
      "epoch": 14.901414487783969,
      "grad_norm": 0.08575482666492462,
      "learning_rate": 1.314473496213745e-07,
      "loss": 0.1231,
      "step": 69530
    },
    {
      "epoch": 14.903557651093013,
      "grad_norm": 0.1151726171374321,
      "learning_rate": 1.2858979854264895e-07,
      "loss": 0.1758,
      "step": 69540
    },
    {
      "epoch": 14.905700814402058,
      "grad_norm": 133.17074584960938,
      "learning_rate": 1.257322474639234e-07,
      "loss": 0.6169,
      "step": 69550
    },
    {
      "epoch": 14.907843977711101,
      "grad_norm": 1.793115496635437,
      "learning_rate": 1.228746963851979e-07,
      "loss": 0.8008,
      "step": 69560
    },
    {
      "epoch": 14.909987141020146,
      "grad_norm": 149.0121307373047,
      "learning_rate": 1.2001714530647235e-07,
      "loss": 0.0037,
      "step": 69570
    },
    {
      "epoch": 14.91213030432919,
      "grad_norm": 0.010625717230141163,
      "learning_rate": 1.1715959422774683e-07,
      "loss": 0.1587,
      "step": 69580
    },
    {
      "epoch": 14.914273467638234,
      "grad_norm": 0.2484702169895172,
      "learning_rate": 1.143020431490213e-07,
      "loss": 0.1585,
      "step": 69590
    },
    {
      "epoch": 14.916416630947278,
      "grad_norm": 0.09550641477108002,
      "learning_rate": 1.1144449207029578e-07,
      "loss": 0.5429,
      "step": 69600
    },
    {
      "epoch": 14.918559794256323,
      "grad_norm": 0.02113647572696209,
      "learning_rate": 1.0858694099157024e-07,
      "loss": 0.131,
      "step": 69610
    },
    {
      "epoch": 14.920702957565366,
      "grad_norm": 0.1539728343486786,
      "learning_rate": 1.0572938991284471e-07,
      "loss": 0.1238,
      "step": 69620
    },
    {
      "epoch": 14.92284612087441,
      "grad_norm": 0.1836346983909607,
      "learning_rate": 1.0287183883411918e-07,
      "loss": 0.3227,
      "step": 69630
    },
    {
      "epoch": 14.924989284183455,
      "grad_norm": 21.88917350769043,
      "learning_rate": 1.0001428775539364e-07,
      "loss": 0.76,
      "step": 69640
    },
    {
      "epoch": 14.927132447492498,
      "grad_norm": 0.2078884392976761,
      "learning_rate": 9.715673667666811e-08,
      "loss": 0.0016,
      "step": 69650
    },
    {
      "epoch": 14.929275610801543,
      "grad_norm": 0.06801139563322067,
      "learning_rate": 9.429918559794258e-08,
      "loss": 0.2705,
      "step": 69660
    },
    {
      "epoch": 14.931418774110588,
      "grad_norm": 0.25406116247177124,
      "learning_rate": 9.144163451921704e-08,
      "loss": 0.152,
      "step": 69670
    },
    {
      "epoch": 14.933561937419631,
      "grad_norm": 44.50493621826172,
      "learning_rate": 8.858408344049151e-08,
      "loss": 0.4758,
      "step": 69680
    },
    {
      "epoch": 14.935705100728676,
      "grad_norm": 0.02537211775779724,
      "learning_rate": 8.572653236176598e-08,
      "loss": 0.6885,
      "step": 69690
    },
    {
      "epoch": 14.93784826403772,
      "grad_norm": 0.24374845623970032,
      "learning_rate": 8.286898128304044e-08,
      "loss": 0.6702,
      "step": 69700
    },
    {
      "epoch": 14.939991427346763,
      "grad_norm": 0.012810304760932922,
      "learning_rate": 8.001143020431491e-08,
      "loss": 0.0727,
      "step": 69710
    },
    {
      "epoch": 14.942134590655808,
      "grad_norm": 0.05713671073317528,
      "learning_rate": 7.715387912558938e-08,
      "loss": 0.4842,
      "step": 69720
    },
    {
      "epoch": 14.944277753964853,
      "grad_norm": 42.9324836730957,
      "learning_rate": 7.429632804686385e-08,
      "loss": 0.298,
      "step": 69730
    },
    {
      "epoch": 14.946420917273896,
      "grad_norm": 0.015738125890493393,
      "learning_rate": 7.143877696813831e-08,
      "loss": 0.2929,
      "step": 69740
    },
    {
      "epoch": 14.94856408058294,
      "grad_norm": 19.448476791381836,
      "learning_rate": 6.858122588941278e-08,
      "loss": 0.4306,
      "step": 69750
    },
    {
      "epoch": 14.950707243891985,
      "grad_norm": 69.3829116821289,
      "learning_rate": 6.572367481068725e-08,
      "loss": 0.1753,
      "step": 69760
    },
    {
      "epoch": 14.952850407201028,
      "grad_norm": 0.08419627696275711,
      "learning_rate": 6.28661237319617e-08,
      "loss": 0.1541,
      "step": 69770
    },
    {
      "epoch": 14.954993570510073,
      "grad_norm": 1.3860502243041992,
      "learning_rate": 6.000857265323618e-08,
      "loss": 0.1578,
      "step": 69780
    },
    {
      "epoch": 14.957136733819118,
      "grad_norm": 0.06433504074811935,
      "learning_rate": 5.715102157451065e-08,
      "loss": 0.3488,
      "step": 69790
    },
    {
      "epoch": 14.95927989712816,
      "grad_norm": 0.017481811344623566,
      "learning_rate": 5.429347049578512e-08,
      "loss": 0.1387,
      "step": 69800
    },
    {
      "epoch": 14.961423060437205,
      "grad_norm": 45.574405670166016,
      "learning_rate": 5.143591941705959e-08,
      "loss": 0.4409,
      "step": 69810
    },
    {
      "epoch": 14.96356622374625,
      "grad_norm": 0.38548657298088074,
      "learning_rate": 4.8578368338334054e-08,
      "loss": 0.2026,
      "step": 69820
    },
    {
      "epoch": 14.965709387055293,
      "grad_norm": 0.019632115960121155,
      "learning_rate": 4.572081725960852e-08,
      "loss": 0.1692,
      "step": 69830
    },
    {
      "epoch": 14.967852550364338,
      "grad_norm": 193.88145446777344,
      "learning_rate": 4.286326618088299e-08,
      "loss": 0.2999,
      "step": 69840
    },
    {
      "epoch": 14.969995713673383,
      "grad_norm": 0.14683087170124054,
      "learning_rate": 4.0005715102157454e-08,
      "loss": 0.0039,
      "step": 69850
    },
    {
      "epoch": 14.972138876982426,
      "grad_norm": 26.266529083251953,
      "learning_rate": 3.7148164023431925e-08,
      "loss": 0.3803,
      "step": 69860
    },
    {
      "epoch": 14.97428204029147,
      "grad_norm": 92.62579345703125,
      "learning_rate": 3.429061294470639e-08,
      "loss": 0.8108,
      "step": 69870
    },
    {
      "epoch": 14.976425203600515,
      "grad_norm": 0.11019810289144516,
      "learning_rate": 3.143306186598085e-08,
      "loss": 0.0034,
      "step": 69880
    },
    {
      "epoch": 14.978568366909558,
      "grad_norm": 0.0640702024102211,
      "learning_rate": 2.8575510787255324e-08,
      "loss": 0.9872,
      "step": 69890
    },
    {
      "epoch": 14.980711530218603,
      "grad_norm": 0.013103041797876358,
      "learning_rate": 2.5717959708529795e-08,
      "loss": 0.4553,
      "step": 69900
    },
    {
      "epoch": 14.982854693527647,
      "grad_norm": 0.018405698239803314,
      "learning_rate": 2.286040862980426e-08,
      "loss": 0.2775,
      "step": 69910
    },
    {
      "epoch": 14.98499785683669,
      "grad_norm": 0.24754735827445984,
      "learning_rate": 2.0002857551078727e-08,
      "loss": 1.2626,
      "step": 69920
    },
    {
      "epoch": 14.987141020145735,
      "grad_norm": 0.39150169491767883,
      "learning_rate": 1.7145306472353194e-08,
      "loss": 0.1701,
      "step": 69930
    },
    {
      "epoch": 14.98928418345478,
      "grad_norm": 0.24883940815925598,
      "learning_rate": 1.4287755393627662e-08,
      "loss": 0.7937,
      "step": 69940
    },
    {
      "epoch": 14.991427346763823,
      "grad_norm": 0.012466236017644405,
      "learning_rate": 1.143020431490213e-08,
      "loss": 0.6638,
      "step": 69950
    },
    {
      "epoch": 14.993570510072868,
      "grad_norm": 0.23525503277778625,
      "learning_rate": 8.572653236176597e-09,
      "loss": 0.4271,
      "step": 69960
    },
    {
      "epoch": 14.995713673381912,
      "grad_norm": 0.20323976874351501,
      "learning_rate": 5.715102157451065e-09,
      "loss": 0.003,
      "step": 69970
    },
    {
      "epoch": 14.997856836690955,
      "grad_norm": 28.479644775390625,
      "learning_rate": 2.8575510787255324e-09,
      "loss": 0.3138,
      "step": 69980
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.20360146462917328,
      "learning_rate": 0.0,
      "loss": 0.5866,
      "step": 69990
    },
    {
      "epoch": 15.0,
      "eval_accuracy": 0.936,
      "eval_f1": 0.7384196185286105,
      "eval_loss": 0.3370334208011627,
      "eval_precision": 0.8041543026706232,
      "eval_recall": 0.6826196473551638,
      "eval_runtime": 397.2635,
      "eval_samples_per_second": 7.552,
      "eval_steps_per_second": 2.517,
      "step": 69990
    }
  ],
  "logging_steps": 10,
  "max_steps": 69990,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.7336673131551936e+17,
  "train_batch_size": 3,
  "trial_name": null,
  "trial_params": null
}
