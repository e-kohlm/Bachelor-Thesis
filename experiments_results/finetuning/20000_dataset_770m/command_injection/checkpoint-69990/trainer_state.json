{
  "best_metric": 0.8748317631224766,
  "best_model_checkpoint": "../saved_models/command_injection_770/checkpoint-69990",
  "epoch": 15.0,
  "eval_steps": 500,
  "global_step": 69990,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0002143163309044149,
      "grad_norm": 59.94352340698242,
      "learning_rate": 1.999971424489213e-05,
      "loss": 0.5027,
      "step": 1
    },
    {
      "epoch": 0.0021431633090441492,
      "grad_norm": 8.541203498840332,
      "learning_rate": 1.9997142448921276e-05,
      "loss": 0.6819,
      "step": 10
    },
    {
      "epoch": 0.0042863266180882984,
      "grad_norm": 1.9705119132995605,
      "learning_rate": 1.999428489784255e-05,
      "loss": 0.4244,
      "step": 20
    },
    {
      "epoch": 0.006429489927132447,
      "grad_norm": 0.24258345365524292,
      "learning_rate": 1.9991427346763827e-05,
      "loss": 0.9108,
      "step": 30
    },
    {
      "epoch": 0.008572653236176597,
      "grad_norm": 152.70489501953125,
      "learning_rate": 1.99885697956851e-05,
      "loss": 1.4826,
      "step": 40
    },
    {
      "epoch": 0.010715816545220747,
      "grad_norm": 330.6939392089844,
      "learning_rate": 1.9985712244606375e-05,
      "loss": 1.0137,
      "step": 50
    },
    {
      "epoch": 0.012858979854264894,
      "grad_norm": 0.03333823010325432,
      "learning_rate": 1.998285469352765e-05,
      "loss": 0.3712,
      "step": 60
    },
    {
      "epoch": 0.015002143163309044,
      "grad_norm": 88.17810821533203,
      "learning_rate": 1.9979997142448923e-05,
      "loss": 2.2933,
      "step": 70
    },
    {
      "epoch": 0.017145306472353194,
      "grad_norm": 3.1338791847229004,
      "learning_rate": 1.99771395913702e-05,
      "loss": 0.515,
      "step": 80
    },
    {
      "epoch": 0.01928846978139734,
      "grad_norm": 79.5361557006836,
      "learning_rate": 1.9974282040291474e-05,
      "loss": 1.4212,
      "step": 90
    },
    {
      "epoch": 0.021431633090441493,
      "grad_norm": 30.2512149810791,
      "learning_rate": 1.9971424489212748e-05,
      "loss": 1.0407,
      "step": 100
    },
    {
      "epoch": 0.02357479639948564,
      "grad_norm": 11.404745101928711,
      "learning_rate": 1.9968566938134018e-05,
      "loss": 0.394,
      "step": 110
    },
    {
      "epoch": 0.02571795970852979,
      "grad_norm": 117.69586944580078,
      "learning_rate": 1.9965709387055296e-05,
      "loss": 1.857,
      "step": 120
    },
    {
      "epoch": 0.02786112301757394,
      "grad_norm": 5.395991802215576,
      "learning_rate": 1.996285183597657e-05,
      "loss": 1.1586,
      "step": 130
    },
    {
      "epoch": 0.03000428632661809,
      "grad_norm": 2.024226665496826,
      "learning_rate": 1.9959994284897843e-05,
      "loss": 0.3955,
      "step": 140
    },
    {
      "epoch": 0.03214744963566224,
      "grad_norm": 64.69168090820312,
      "learning_rate": 1.9957136733819117e-05,
      "loss": 0.3887,
      "step": 150
    },
    {
      "epoch": 0.03429061294470639,
      "grad_norm": 61.827964782714844,
      "learning_rate": 1.995427918274039e-05,
      "loss": 1.8314,
      "step": 160
    },
    {
      "epoch": 0.036433776253750536,
      "grad_norm": 3.634791612625122,
      "learning_rate": 1.995142163166167e-05,
      "loss": 1.5263,
      "step": 170
    },
    {
      "epoch": 0.03857693956279468,
      "grad_norm": 62.55475616455078,
      "learning_rate": 1.9948564080582943e-05,
      "loss": 0.7913,
      "step": 180
    },
    {
      "epoch": 0.04072010287183883,
      "grad_norm": 54.6801643371582,
      "learning_rate": 1.9945706529504216e-05,
      "loss": 1.1467,
      "step": 190
    },
    {
      "epoch": 0.042863266180882986,
      "grad_norm": 0.969495952129364,
      "learning_rate": 1.994284897842549e-05,
      "loss": 0.8278,
      "step": 200
    },
    {
      "epoch": 0.045006429489927134,
      "grad_norm": 0.6565989255905151,
      "learning_rate": 1.9939991427346764e-05,
      "loss": 0.5082,
      "step": 210
    },
    {
      "epoch": 0.04714959279897128,
      "grad_norm": 1.123185396194458,
      "learning_rate": 1.993713387626804e-05,
      "loss": 1.3255,
      "step": 220
    },
    {
      "epoch": 0.04929275610801543,
      "grad_norm": 0.058902040123939514,
      "learning_rate": 1.9934276325189316e-05,
      "loss": 0.2664,
      "step": 230
    },
    {
      "epoch": 0.05143591941705958,
      "grad_norm": 82.68439483642578,
      "learning_rate": 1.993141877411059e-05,
      "loss": 1.3172,
      "step": 240
    },
    {
      "epoch": 0.053579082726103726,
      "grad_norm": 0.10068267583847046,
      "learning_rate": 1.9928561223031863e-05,
      "loss": 0.4186,
      "step": 250
    },
    {
      "epoch": 0.05572224603514788,
      "grad_norm": 55.704017639160156,
      "learning_rate": 1.9925703671953137e-05,
      "loss": 1.1259,
      "step": 260
    },
    {
      "epoch": 0.05786540934419203,
      "grad_norm": 0.24716100096702576,
      "learning_rate": 1.992284612087441e-05,
      "loss": 0.16,
      "step": 270
    },
    {
      "epoch": 0.06000857265323618,
      "grad_norm": 0.02398895099759102,
      "learning_rate": 1.991998856979569e-05,
      "loss": 0.5224,
      "step": 280
    },
    {
      "epoch": 0.062151735962280324,
      "grad_norm": 52.17587661743164,
      "learning_rate": 1.9917131018716962e-05,
      "loss": 1.4897,
      "step": 290
    },
    {
      "epoch": 0.06429489927132448,
      "grad_norm": 0.17695732414722443,
      "learning_rate": 1.9914273467638236e-05,
      "loss": 0.4129,
      "step": 300
    },
    {
      "epoch": 0.06643806258036862,
      "grad_norm": 87.30659484863281,
      "learning_rate": 1.991141591655951e-05,
      "loss": 1.1565,
      "step": 310
    },
    {
      "epoch": 0.06858122588941278,
      "grad_norm": 0.12250661849975586,
      "learning_rate": 1.9908558365480784e-05,
      "loss": 0.2266,
      "step": 320
    },
    {
      "epoch": 0.07072438919845692,
      "grad_norm": 0.2367817908525467,
      "learning_rate": 1.9905700814402058e-05,
      "loss": 0.8392,
      "step": 330
    },
    {
      "epoch": 0.07286755250750107,
      "grad_norm": 0.18909072875976562,
      "learning_rate": 1.9902843263323332e-05,
      "loss": 0.5332,
      "step": 340
    },
    {
      "epoch": 0.07501071581654523,
      "grad_norm": 0.0727391391992569,
      "learning_rate": 1.9899985712244606e-05,
      "loss": 0.8545,
      "step": 350
    },
    {
      "epoch": 0.07715387912558937,
      "grad_norm": 0.6485118865966797,
      "learning_rate": 1.9897128161165883e-05,
      "loss": 0.8591,
      "step": 360
    },
    {
      "epoch": 0.07929704243463352,
      "grad_norm": 0.07010781019926071,
      "learning_rate": 1.9894270610087157e-05,
      "loss": 0.1871,
      "step": 370
    },
    {
      "epoch": 0.08144020574367766,
      "grad_norm": 0.04004431515932083,
      "learning_rate": 1.989141305900843e-05,
      "loss": 0.2376,
      "step": 380
    },
    {
      "epoch": 0.08358336905272182,
      "grad_norm": 0.06289482861757278,
      "learning_rate": 1.9888555507929705e-05,
      "loss": 0.7508,
      "step": 390
    },
    {
      "epoch": 0.08572653236176597,
      "grad_norm": 36.62271499633789,
      "learning_rate": 1.988569795685098e-05,
      "loss": 0.5945,
      "step": 400
    },
    {
      "epoch": 0.08786969567081011,
      "grad_norm": 38.88352584838867,
      "learning_rate": 1.9882840405772253e-05,
      "loss": 1.0645,
      "step": 410
    },
    {
      "epoch": 0.09001285897985427,
      "grad_norm": 51.960880279541016,
      "learning_rate": 1.987998285469353e-05,
      "loss": 0.5684,
      "step": 420
    },
    {
      "epoch": 0.09215602228889841,
      "grad_norm": 56.93783187866211,
      "learning_rate": 1.9877125303614804e-05,
      "loss": 1.2756,
      "step": 430
    },
    {
      "epoch": 0.09429918559794256,
      "grad_norm": 0.1311175674200058,
      "learning_rate": 1.9874267752536078e-05,
      "loss": 0.2348,
      "step": 440
    },
    {
      "epoch": 0.09644234890698672,
      "grad_norm": 0.06735213100910187,
      "learning_rate": 1.9871410201457352e-05,
      "loss": 0.4327,
      "step": 450
    },
    {
      "epoch": 0.09858551221603086,
      "grad_norm": 0.22459201514720917,
      "learning_rate": 1.9868552650378626e-05,
      "loss": 0.942,
      "step": 460
    },
    {
      "epoch": 0.10072867552507501,
      "grad_norm": 0.31448492407798767,
      "learning_rate": 1.9865695099299903e-05,
      "loss": 0.8325,
      "step": 470
    },
    {
      "epoch": 0.10287183883411916,
      "grad_norm": 59.63381576538086,
      "learning_rate": 1.9862837548221177e-05,
      "loss": 0.7193,
      "step": 480
    },
    {
      "epoch": 0.10501500214316331,
      "grad_norm": 33.38119125366211,
      "learning_rate": 1.985997999714245e-05,
      "loss": 1.268,
      "step": 490
    },
    {
      "epoch": 0.10715816545220745,
      "grad_norm": 39.48326110839844,
      "learning_rate": 1.9857122446063725e-05,
      "loss": 0.7738,
      "step": 500
    },
    {
      "epoch": 0.1093013287612516,
      "grad_norm": 0.35890188813209534,
      "learning_rate": 1.9854264894985e-05,
      "loss": 0.1637,
      "step": 510
    },
    {
      "epoch": 0.11144449207029576,
      "grad_norm": 35.81284713745117,
      "learning_rate": 1.9851407343906276e-05,
      "loss": 0.5334,
      "step": 520
    },
    {
      "epoch": 0.1135876553793399,
      "grad_norm": 0.19879542291164398,
      "learning_rate": 1.984854979282755e-05,
      "loss": 1.164,
      "step": 530
    },
    {
      "epoch": 0.11573081868838406,
      "grad_norm": 0.3548402190208435,
      "learning_rate": 1.984569224174882e-05,
      "loss": 0.4943,
      "step": 540
    },
    {
      "epoch": 0.1178739819974282,
      "grad_norm": 0.17079363763332367,
      "learning_rate": 1.9842834690670095e-05,
      "loss": 0.344,
      "step": 550
    },
    {
      "epoch": 0.12001714530647235,
      "grad_norm": 34.07929229736328,
      "learning_rate": 1.9839977139591372e-05,
      "loss": 1.3039,
      "step": 560
    },
    {
      "epoch": 0.12216030861551651,
      "grad_norm": 62.90230178833008,
      "learning_rate": 1.9837119588512646e-05,
      "loss": 0.6626,
      "step": 570
    },
    {
      "epoch": 0.12430347192456065,
      "grad_norm": 41.96652603149414,
      "learning_rate": 1.983426203743392e-05,
      "loss": 1.0627,
      "step": 580
    },
    {
      "epoch": 0.1264466352336048,
      "grad_norm": 0.21331344544887543,
      "learning_rate": 1.9831404486355194e-05,
      "loss": 0.1546,
      "step": 590
    },
    {
      "epoch": 0.12858979854264896,
      "grad_norm": 0.06252337247133255,
      "learning_rate": 1.9828546935276468e-05,
      "loss": 0.2715,
      "step": 600
    },
    {
      "epoch": 0.13073296185169309,
      "grad_norm": 0.15603139996528625,
      "learning_rate": 1.9825689384197745e-05,
      "loss": 1.343,
      "step": 610
    },
    {
      "epoch": 0.13287612516073724,
      "grad_norm": 52.36104965209961,
      "learning_rate": 1.982283183311902e-05,
      "loss": 1.067,
      "step": 620
    },
    {
      "epoch": 0.1350192884697814,
      "grad_norm": 0.17495019733905792,
      "learning_rate": 1.9819974282040293e-05,
      "loss": 0.3895,
      "step": 630
    },
    {
      "epoch": 0.13716245177882555,
      "grad_norm": 0.7612299919128418,
      "learning_rate": 1.9817116730961567e-05,
      "loss": 0.8131,
      "step": 640
    },
    {
      "epoch": 0.1393056150878697,
      "grad_norm": 42.329219818115234,
      "learning_rate": 1.981425917988284e-05,
      "loss": 1.0096,
      "step": 650
    },
    {
      "epoch": 0.14144877839691383,
      "grad_norm": 55.93172836303711,
      "learning_rate": 1.9811401628804118e-05,
      "loss": 0.7942,
      "step": 660
    },
    {
      "epoch": 0.143591941705958,
      "grad_norm": 2.4928667545318604,
      "learning_rate": 1.9808544077725392e-05,
      "loss": 0.3037,
      "step": 670
    },
    {
      "epoch": 0.14573510501500214,
      "grad_norm": 0.38759365677833557,
      "learning_rate": 1.9805686526646666e-05,
      "loss": 0.6666,
      "step": 680
    },
    {
      "epoch": 0.1478782683240463,
      "grad_norm": 0.16253173351287842,
      "learning_rate": 1.980282897556794e-05,
      "loss": 0.1925,
      "step": 690
    },
    {
      "epoch": 0.15002143163309045,
      "grad_norm": 37.09601593017578,
      "learning_rate": 1.9799971424489214e-05,
      "loss": 0.2223,
      "step": 700
    },
    {
      "epoch": 0.15216459494213458,
      "grad_norm": 0.09729636460542679,
      "learning_rate": 1.979711387341049e-05,
      "loss": 0.7289,
      "step": 710
    },
    {
      "epoch": 0.15430775825117873,
      "grad_norm": 0.11854725331068039,
      "learning_rate": 1.9794256322331765e-05,
      "loss": 0.6814,
      "step": 720
    },
    {
      "epoch": 0.1564509215602229,
      "grad_norm": 0.22306081652641296,
      "learning_rate": 1.979139877125304e-05,
      "loss": 1.0305,
      "step": 730
    },
    {
      "epoch": 0.15859408486926704,
      "grad_norm": 51.882869720458984,
      "learning_rate": 1.9788541220174313e-05,
      "loss": 1.5324,
      "step": 740
    },
    {
      "epoch": 0.1607372481783112,
      "grad_norm": 47.51191329956055,
      "learning_rate": 1.9785683669095587e-05,
      "loss": 0.551,
      "step": 750
    },
    {
      "epoch": 0.16288041148735533,
      "grad_norm": 23.68375015258789,
      "learning_rate": 1.978282611801686e-05,
      "loss": 0.7545,
      "step": 760
    },
    {
      "epoch": 0.16502357479639948,
      "grad_norm": 9.351754188537598,
      "learning_rate": 1.9779968566938135e-05,
      "loss": 0.4199,
      "step": 770
    },
    {
      "epoch": 0.16716673810544364,
      "grad_norm": 0.08670070767402649,
      "learning_rate": 1.977711101585941e-05,
      "loss": 0.8055,
      "step": 780
    },
    {
      "epoch": 0.1693099014144878,
      "grad_norm": 51.22738265991211,
      "learning_rate": 1.9774253464780683e-05,
      "loss": 0.5868,
      "step": 790
    },
    {
      "epoch": 0.17145306472353194,
      "grad_norm": 32.25762939453125,
      "learning_rate": 1.977139591370196e-05,
      "loss": 0.9474,
      "step": 800
    },
    {
      "epoch": 0.17359622803257607,
      "grad_norm": 36.85539627075195,
      "learning_rate": 1.9768538362623234e-05,
      "loss": 0.4976,
      "step": 810
    },
    {
      "epoch": 0.17573939134162023,
      "grad_norm": 0.6332084536552429,
      "learning_rate": 1.9765680811544508e-05,
      "loss": 0.899,
      "step": 820
    },
    {
      "epoch": 0.17788255465066438,
      "grad_norm": 54.40570068359375,
      "learning_rate": 1.9762823260465782e-05,
      "loss": 0.9944,
      "step": 830
    },
    {
      "epoch": 0.18002571795970854,
      "grad_norm": 26.199739456176758,
      "learning_rate": 1.9759965709387056e-05,
      "loss": 0.6495,
      "step": 840
    },
    {
      "epoch": 0.1821688812687527,
      "grad_norm": 0.16784560680389404,
      "learning_rate": 1.9757108158308333e-05,
      "loss": 0.1632,
      "step": 850
    },
    {
      "epoch": 0.18431204457779682,
      "grad_norm": 0.13218453526496887,
      "learning_rate": 1.9754250607229607e-05,
      "loss": 0.841,
      "step": 860
    },
    {
      "epoch": 0.18645520788684097,
      "grad_norm": 0.18052658438682556,
      "learning_rate": 1.975139305615088e-05,
      "loss": 0.3577,
      "step": 870
    },
    {
      "epoch": 0.18859837119588513,
      "grad_norm": 0.1473010927438736,
      "learning_rate": 1.9748535505072155e-05,
      "loss": 0.4439,
      "step": 880
    },
    {
      "epoch": 0.19074153450492928,
      "grad_norm": 0.13906066119670868,
      "learning_rate": 1.974567795399343e-05,
      "loss": 0.5813,
      "step": 890
    },
    {
      "epoch": 0.19288469781397344,
      "grad_norm": 0.19176188111305237,
      "learning_rate": 1.9742820402914703e-05,
      "loss": 0.5434,
      "step": 900
    },
    {
      "epoch": 0.19502786112301757,
      "grad_norm": 0.15436631441116333,
      "learning_rate": 1.973996285183598e-05,
      "loss": 0.4469,
      "step": 910
    },
    {
      "epoch": 0.19717102443206172,
      "grad_norm": 36.89311981201172,
      "learning_rate": 1.9737105300757254e-05,
      "loss": 0.5745,
      "step": 920
    },
    {
      "epoch": 0.19931418774110587,
      "grad_norm": 30.19377326965332,
      "learning_rate": 1.9734247749678528e-05,
      "loss": 1.0484,
      "step": 930
    },
    {
      "epoch": 0.20145735105015003,
      "grad_norm": 0.7054452300071716,
      "learning_rate": 1.9731390198599802e-05,
      "loss": 0.3057,
      "step": 940
    },
    {
      "epoch": 0.20360051435919416,
      "grad_norm": 0.08305909484624863,
      "learning_rate": 1.9728532647521076e-05,
      "loss": 0.4048,
      "step": 950
    },
    {
      "epoch": 0.2057436776682383,
      "grad_norm": 28.025365829467773,
      "learning_rate": 1.9725675096442353e-05,
      "loss": 0.6964,
      "step": 960
    },
    {
      "epoch": 0.20788684097728247,
      "grad_norm": 0.17803093791007996,
      "learning_rate": 1.9722817545363624e-05,
      "loss": 0.6486,
      "step": 970
    },
    {
      "epoch": 0.21003000428632662,
      "grad_norm": 0.15796959400177002,
      "learning_rate": 1.9719959994284897e-05,
      "loss": 0.2072,
      "step": 980
    },
    {
      "epoch": 0.21217316759537078,
      "grad_norm": 0.12178654223680496,
      "learning_rate": 1.9717102443206175e-05,
      "loss": 0.7971,
      "step": 990
    },
    {
      "epoch": 0.2143163309044149,
      "grad_norm": 29.904645919799805,
      "learning_rate": 1.971424489212745e-05,
      "loss": 0.2216,
      "step": 1000
    },
    {
      "epoch": 0.21645949421345906,
      "grad_norm": 54.377315521240234,
      "learning_rate": 1.9711387341048723e-05,
      "loss": 0.6256,
      "step": 1010
    },
    {
      "epoch": 0.2186026575225032,
      "grad_norm": 0.2068239003419876,
      "learning_rate": 1.9708529789969997e-05,
      "loss": 0.4092,
      "step": 1020
    },
    {
      "epoch": 0.22074582083154737,
      "grad_norm": 48.938629150390625,
      "learning_rate": 1.970567223889127e-05,
      "loss": 1.1509,
      "step": 1030
    },
    {
      "epoch": 0.22288898414059152,
      "grad_norm": 0.943647027015686,
      "learning_rate": 1.9702814687812544e-05,
      "loss": 0.8222,
      "step": 1040
    },
    {
      "epoch": 0.22503214744963565,
      "grad_norm": 0.5401085019111633,
      "learning_rate": 1.9699957136733822e-05,
      "loss": 0.5015,
      "step": 1050
    },
    {
      "epoch": 0.2271753107586798,
      "grad_norm": 0.1611509770154953,
      "learning_rate": 1.9697099585655096e-05,
      "loss": 0.7157,
      "step": 1060
    },
    {
      "epoch": 0.22931847406772396,
      "grad_norm": 0.15432125329971313,
      "learning_rate": 1.969424203457637e-05,
      "loss": 0.3954,
      "step": 1070
    },
    {
      "epoch": 0.23146163737676811,
      "grad_norm": 0.5657260417938232,
      "learning_rate": 1.9691384483497643e-05,
      "loss": 0.9314,
      "step": 1080
    },
    {
      "epoch": 0.23360480068581227,
      "grad_norm": 2.3801684379577637,
      "learning_rate": 1.9688526932418917e-05,
      "loss": 0.6703,
      "step": 1090
    },
    {
      "epoch": 0.2357479639948564,
      "grad_norm": 34.78993225097656,
      "learning_rate": 1.9685669381340195e-05,
      "loss": 0.5162,
      "step": 1100
    },
    {
      "epoch": 0.23789112730390055,
      "grad_norm": 0.17952266335487366,
      "learning_rate": 1.968281183026147e-05,
      "loss": 1.022,
      "step": 1110
    },
    {
      "epoch": 0.2400342906129447,
      "grad_norm": 0.691577672958374,
      "learning_rate": 1.9679954279182743e-05,
      "loss": 0.2642,
      "step": 1120
    },
    {
      "epoch": 0.24217745392198886,
      "grad_norm": 33.05949020385742,
      "learning_rate": 1.9677096728104017e-05,
      "loss": 0.424,
      "step": 1130
    },
    {
      "epoch": 0.24432061723103302,
      "grad_norm": 0.2084202617406845,
      "learning_rate": 1.967423917702529e-05,
      "loss": 1.0444,
      "step": 1140
    },
    {
      "epoch": 0.24646378054007714,
      "grad_norm": 0.10016199201345444,
      "learning_rate": 1.9671381625946568e-05,
      "loss": 0.6005,
      "step": 1150
    },
    {
      "epoch": 0.2486069438491213,
      "grad_norm": 0.1902170479297638,
      "learning_rate": 1.966852407486784e-05,
      "loss": 0.1873,
      "step": 1160
    },
    {
      "epoch": 0.2507501071581654,
      "grad_norm": 0.0533028170466423,
      "learning_rate": 1.9665666523789116e-05,
      "loss": 0.5164,
      "step": 1170
    },
    {
      "epoch": 0.2528932704672096,
      "grad_norm": 53.392547607421875,
      "learning_rate": 1.966280897271039e-05,
      "loss": 0.8841,
      "step": 1180
    },
    {
      "epoch": 0.25503643377625373,
      "grad_norm": 0.2586987614631653,
      "learning_rate": 1.9659951421631663e-05,
      "loss": 0.3944,
      "step": 1190
    },
    {
      "epoch": 0.2571795970852979,
      "grad_norm": 28.303674697875977,
      "learning_rate": 1.9657093870552937e-05,
      "loss": 1.1818,
      "step": 1200
    },
    {
      "epoch": 0.25932276039434204,
      "grad_norm": 1.381087303161621,
      "learning_rate": 1.965423631947421e-05,
      "loss": 0.6512,
      "step": 1210
    },
    {
      "epoch": 0.26146592370338617,
      "grad_norm": 1.9669146537780762,
      "learning_rate": 1.9651378768395485e-05,
      "loss": 0.8276,
      "step": 1220
    },
    {
      "epoch": 0.26360908701243035,
      "grad_norm": 28.962783813476562,
      "learning_rate": 1.964852121731676e-05,
      "loss": 0.7711,
      "step": 1230
    },
    {
      "epoch": 0.2657522503214745,
      "grad_norm": 0.09972409904003143,
      "learning_rate": 1.9645663666238037e-05,
      "loss": 0.2062,
      "step": 1240
    },
    {
      "epoch": 0.26789541363051866,
      "grad_norm": 0.1783711165189743,
      "learning_rate": 1.964280611515931e-05,
      "loss": 0.3944,
      "step": 1250
    },
    {
      "epoch": 0.2700385769395628,
      "grad_norm": 0.23015178740024567,
      "learning_rate": 1.9639948564080584e-05,
      "loss": 0.9122,
      "step": 1260
    },
    {
      "epoch": 0.2721817402486069,
      "grad_norm": 0.6896770596504211,
      "learning_rate": 1.9637091013001858e-05,
      "loss": 0.6086,
      "step": 1270
    },
    {
      "epoch": 0.2743249035576511,
      "grad_norm": 0.27946868538856506,
      "learning_rate": 1.9634233461923132e-05,
      "loss": 0.4708,
      "step": 1280
    },
    {
      "epoch": 0.27646806686669523,
      "grad_norm": 0.2929776608943939,
      "learning_rate": 1.963137591084441e-05,
      "loss": 0.929,
      "step": 1290
    },
    {
      "epoch": 0.2786112301757394,
      "grad_norm": 1.607917070388794,
      "learning_rate": 1.9628518359765683e-05,
      "loss": 0.7858,
      "step": 1300
    },
    {
      "epoch": 0.28075439348478354,
      "grad_norm": 2.5430445671081543,
      "learning_rate": 1.9625660808686957e-05,
      "loss": 0.42,
      "step": 1310
    },
    {
      "epoch": 0.28289755679382766,
      "grad_norm": 0.18608644604682922,
      "learning_rate": 1.962280325760823e-05,
      "loss": 0.3688,
      "step": 1320
    },
    {
      "epoch": 0.28504072010287185,
      "grad_norm": 32.868743896484375,
      "learning_rate": 1.9619945706529505e-05,
      "loss": 0.6474,
      "step": 1330
    },
    {
      "epoch": 0.287183883411916,
      "grad_norm": 0.2491440623998642,
      "learning_rate": 1.961708815545078e-05,
      "loss": 0.205,
      "step": 1340
    },
    {
      "epoch": 0.28932704672096016,
      "grad_norm": 0.7051647305488586,
      "learning_rate": 1.9614230604372056e-05,
      "loss": 0.7596,
      "step": 1350
    },
    {
      "epoch": 0.2914702100300043,
      "grad_norm": 0.6372995972633362,
      "learning_rate": 1.961137305329333e-05,
      "loss": 0.7446,
      "step": 1360
    },
    {
      "epoch": 0.2936133733390484,
      "grad_norm": 0.049649592489004135,
      "learning_rate": 1.9608515502214604e-05,
      "loss": 0.2772,
      "step": 1370
    },
    {
      "epoch": 0.2957565366480926,
      "grad_norm": 0.017543204128742218,
      "learning_rate": 1.9605657951135878e-05,
      "loss": 1.0557,
      "step": 1380
    },
    {
      "epoch": 0.2978996999571367,
      "grad_norm": 0.3465307652950287,
      "learning_rate": 1.9602800400057152e-05,
      "loss": 0.8007,
      "step": 1390
    },
    {
      "epoch": 0.3000428632661809,
      "grad_norm": 26.816083908081055,
      "learning_rate": 1.9599942848978426e-05,
      "loss": 0.9577,
      "step": 1400
    },
    {
      "epoch": 0.30218602657522503,
      "grad_norm": 0.14250272512435913,
      "learning_rate": 1.95970852978997e-05,
      "loss": 0.3277,
      "step": 1410
    },
    {
      "epoch": 0.30432918988426916,
      "grad_norm": 0.11933379620313644,
      "learning_rate": 1.9594227746820974e-05,
      "loss": 0.7021,
      "step": 1420
    },
    {
      "epoch": 0.30647235319331334,
      "grad_norm": 0.19095124304294586,
      "learning_rate": 1.959137019574225e-05,
      "loss": 0.5926,
      "step": 1430
    },
    {
      "epoch": 0.30861551650235747,
      "grad_norm": 0.07544314116239548,
      "learning_rate": 1.9588512644663525e-05,
      "loss": 0.1652,
      "step": 1440
    },
    {
      "epoch": 0.31075867981140165,
      "grad_norm": 26.609943389892578,
      "learning_rate": 1.95856550935848e-05,
      "loss": 0.5073,
      "step": 1450
    },
    {
      "epoch": 0.3129018431204458,
      "grad_norm": 0.24494823813438416,
      "learning_rate": 1.9582797542506073e-05,
      "loss": 0.8065,
      "step": 1460
    },
    {
      "epoch": 0.3150450064294899,
      "grad_norm": 25.148412704467773,
      "learning_rate": 1.9579939991427347e-05,
      "loss": 1.3483,
      "step": 1470
    },
    {
      "epoch": 0.3171881697385341,
      "grad_norm": 20.578516006469727,
      "learning_rate": 1.957708244034862e-05,
      "loss": 0.5465,
      "step": 1480
    },
    {
      "epoch": 0.3193313330475782,
      "grad_norm": 31.1469669342041,
      "learning_rate": 1.9574224889269898e-05,
      "loss": 0.2806,
      "step": 1490
    },
    {
      "epoch": 0.3214744963566224,
      "grad_norm": 26.41072654724121,
      "learning_rate": 1.9571367338191172e-05,
      "loss": 0.5427,
      "step": 1500
    },
    {
      "epoch": 0.3236176596656665,
      "grad_norm": 26.328527450561523,
      "learning_rate": 1.9568509787112446e-05,
      "loss": 0.6711,
      "step": 1510
    },
    {
      "epoch": 0.32576082297471065,
      "grad_norm": 0.17107197642326355,
      "learning_rate": 1.956565223603372e-05,
      "loss": 0.1822,
      "step": 1520
    },
    {
      "epoch": 0.32790398628375483,
      "grad_norm": 0.14054642617702484,
      "learning_rate": 1.9562794684954994e-05,
      "loss": 0.2326,
      "step": 1530
    },
    {
      "epoch": 0.33004714959279896,
      "grad_norm": 0.18527741730213165,
      "learning_rate": 1.955993713387627e-05,
      "loss": 0.5731,
      "step": 1540
    },
    {
      "epoch": 0.33219031290184314,
      "grad_norm": 31.123767852783203,
      "learning_rate": 1.9557079582797545e-05,
      "loss": 0.169,
      "step": 1550
    },
    {
      "epoch": 0.33433347621088727,
      "grad_norm": 0.31714028120040894,
      "learning_rate": 1.955422203171882e-05,
      "loss": 0.2375,
      "step": 1560
    },
    {
      "epoch": 0.3364766395199314,
      "grad_norm": 27.47846221923828,
      "learning_rate": 1.9551364480640093e-05,
      "loss": 0.3993,
      "step": 1570
    },
    {
      "epoch": 0.3386198028289756,
      "grad_norm": 0.28195127844810486,
      "learning_rate": 1.9548506929561367e-05,
      "loss": 0.4309,
      "step": 1580
    },
    {
      "epoch": 0.3407629661380197,
      "grad_norm": 0.16276909410953522,
      "learning_rate": 1.9545649378482644e-05,
      "loss": 0.5529,
      "step": 1590
    },
    {
      "epoch": 0.3429061294470639,
      "grad_norm": 0.2216489464044571,
      "learning_rate": 1.9542791827403918e-05,
      "loss": 0.5442,
      "step": 1600
    },
    {
      "epoch": 0.345049292756108,
      "grad_norm": 0.07747692614793777,
      "learning_rate": 1.9539934276325192e-05,
      "loss": 0.4457,
      "step": 1610
    },
    {
      "epoch": 0.34719245606515214,
      "grad_norm": 27.016748428344727,
      "learning_rate": 1.9537076725246463e-05,
      "loss": 0.7458,
      "step": 1620
    },
    {
      "epoch": 0.3493356193741963,
      "grad_norm": 28.077239990234375,
      "learning_rate": 1.953421917416774e-05,
      "loss": 0.9219,
      "step": 1630
    },
    {
      "epoch": 0.35147878268324045,
      "grad_norm": 0.450302392244339,
      "learning_rate": 1.9531361623089014e-05,
      "loss": 0.2531,
      "step": 1640
    },
    {
      "epoch": 0.35362194599228464,
      "grad_norm": 0.09114199876785278,
      "learning_rate": 1.9528504072010288e-05,
      "loss": 0.492,
      "step": 1650
    },
    {
      "epoch": 0.35576510930132876,
      "grad_norm": 0.07395178824663162,
      "learning_rate": 1.9525646520931562e-05,
      "loss": 0.4129,
      "step": 1660
    },
    {
      "epoch": 0.3579082726103729,
      "grad_norm": 39.77848815917969,
      "learning_rate": 1.9522788969852836e-05,
      "loss": 0.9879,
      "step": 1670
    },
    {
      "epoch": 0.3600514359194171,
      "grad_norm": 0.7637739777565002,
      "learning_rate": 1.9519931418774113e-05,
      "loss": 0.3164,
      "step": 1680
    },
    {
      "epoch": 0.3621945992284612,
      "grad_norm": 31.948657989501953,
      "learning_rate": 1.9517073867695387e-05,
      "loss": 0.6032,
      "step": 1690
    },
    {
      "epoch": 0.3643377625375054,
      "grad_norm": 51.355796813964844,
      "learning_rate": 1.951421631661666e-05,
      "loss": 1.0343,
      "step": 1700
    },
    {
      "epoch": 0.3664809258465495,
      "grad_norm": 25.692981719970703,
      "learning_rate": 1.9511358765537935e-05,
      "loss": 0.9508,
      "step": 1710
    },
    {
      "epoch": 0.36862408915559364,
      "grad_norm": 7.907735824584961,
      "learning_rate": 1.950850121445921e-05,
      "loss": 0.6144,
      "step": 1720
    },
    {
      "epoch": 0.3707672524646378,
      "grad_norm": 39.18416976928711,
      "learning_rate": 1.9505643663380486e-05,
      "loss": 0.5167,
      "step": 1730
    },
    {
      "epoch": 0.37291041577368195,
      "grad_norm": 48.359893798828125,
      "learning_rate": 1.950278611230176e-05,
      "loss": 0.2659,
      "step": 1740
    },
    {
      "epoch": 0.37505357908272613,
      "grad_norm": 133.1476287841797,
      "learning_rate": 1.9499928561223034e-05,
      "loss": 0.5372,
      "step": 1750
    },
    {
      "epoch": 0.37719674239177026,
      "grad_norm": 12.488383293151855,
      "learning_rate": 1.9497071010144308e-05,
      "loss": 0.2929,
      "step": 1760
    },
    {
      "epoch": 0.3793399057008144,
      "grad_norm": 0.02364927902817726,
      "learning_rate": 1.9494213459065582e-05,
      "loss": 0.6584,
      "step": 1770
    },
    {
      "epoch": 0.38148306900985857,
      "grad_norm": 0.06788099557161331,
      "learning_rate": 1.949135590798686e-05,
      "loss": 0.9029,
      "step": 1780
    },
    {
      "epoch": 0.3836262323189027,
      "grad_norm": 37.04975891113281,
      "learning_rate": 1.9488498356908133e-05,
      "loss": 0.9315,
      "step": 1790
    },
    {
      "epoch": 0.3857693956279469,
      "grad_norm": 34.09807586669922,
      "learning_rate": 1.9485640805829407e-05,
      "loss": 0.6207,
      "step": 1800
    },
    {
      "epoch": 0.387912558936991,
      "grad_norm": 1.2708507776260376,
      "learning_rate": 1.948278325475068e-05,
      "loss": 0.7332,
      "step": 1810
    },
    {
      "epoch": 0.39005572224603513,
      "grad_norm": 5.736875534057617,
      "learning_rate": 1.9479925703671955e-05,
      "loss": 0.8625,
      "step": 1820
    },
    {
      "epoch": 0.3921988855550793,
      "grad_norm": 3.044238805770874,
      "learning_rate": 1.947706815259323e-05,
      "loss": 0.1943,
      "step": 1830
    },
    {
      "epoch": 0.39434204886412344,
      "grad_norm": 0.11376398056745529,
      "learning_rate": 1.9474210601514503e-05,
      "loss": 0.6453,
      "step": 1840
    },
    {
      "epoch": 0.3964852121731676,
      "grad_norm": 38.93380355834961,
      "learning_rate": 1.9471353050435777e-05,
      "loss": 1.8817,
      "step": 1850
    },
    {
      "epoch": 0.39862837548221175,
      "grad_norm": 0.1077784076333046,
      "learning_rate": 1.946849549935705e-05,
      "loss": 0.7157,
      "step": 1860
    },
    {
      "epoch": 0.4007715387912559,
      "grad_norm": 0.6978878378868103,
      "learning_rate": 1.9465637948278328e-05,
      "loss": 0.7242,
      "step": 1870
    },
    {
      "epoch": 0.40291470210030006,
      "grad_norm": 45.0114860534668,
      "learning_rate": 1.9462780397199602e-05,
      "loss": 0.6937,
      "step": 1880
    },
    {
      "epoch": 0.4050578654093442,
      "grad_norm": 40.30503845214844,
      "learning_rate": 1.9459922846120876e-05,
      "loss": 0.7484,
      "step": 1890
    },
    {
      "epoch": 0.4072010287183883,
      "grad_norm": 36.92446517944336,
      "learning_rate": 1.945706529504215e-05,
      "loss": 0.5341,
      "step": 1900
    },
    {
      "epoch": 0.4093441920274325,
      "grad_norm": 35.585872650146484,
      "learning_rate": 1.9454207743963424e-05,
      "loss": 0.356,
      "step": 1910
    },
    {
      "epoch": 0.4114873553364766,
      "grad_norm": 41.178375244140625,
      "learning_rate": 1.94513501928847e-05,
      "loss": 0.5763,
      "step": 1920
    },
    {
      "epoch": 0.4136305186455208,
      "grad_norm": 0.05457086116075516,
      "learning_rate": 1.9448492641805975e-05,
      "loss": 0.2528,
      "step": 1930
    },
    {
      "epoch": 0.41577368195456493,
      "grad_norm": 46.69797897338867,
      "learning_rate": 1.944563509072725e-05,
      "loss": 1.2712,
      "step": 1940
    },
    {
      "epoch": 0.41791684526360906,
      "grad_norm": 0.6628954410552979,
      "learning_rate": 1.9442777539648523e-05,
      "loss": 0.74,
      "step": 1950
    },
    {
      "epoch": 0.42006000857265324,
      "grad_norm": 23.76328468322754,
      "learning_rate": 1.9439919988569797e-05,
      "loss": 0.2381,
      "step": 1960
    },
    {
      "epoch": 0.42220317188169737,
      "grad_norm": 0.05680809170007706,
      "learning_rate": 1.943706243749107e-05,
      "loss": 0.2504,
      "step": 1970
    },
    {
      "epoch": 0.42434633519074155,
      "grad_norm": 59.93622589111328,
      "learning_rate": 1.9434204886412348e-05,
      "loss": 0.9007,
      "step": 1980
    },
    {
      "epoch": 0.4264894984997857,
      "grad_norm": 0.024597998708486557,
      "learning_rate": 1.9431347335333622e-05,
      "loss": 1.0168,
      "step": 1990
    },
    {
      "epoch": 0.4286326618088298,
      "grad_norm": 46.23733901977539,
      "learning_rate": 1.9428489784254896e-05,
      "loss": 0.6427,
      "step": 2000
    },
    {
      "epoch": 0.430775825117874,
      "grad_norm": 50.842105865478516,
      "learning_rate": 1.942563223317617e-05,
      "loss": 1.0806,
      "step": 2010
    },
    {
      "epoch": 0.4329189884269181,
      "grad_norm": 0.3159264922142029,
      "learning_rate": 1.9422774682097444e-05,
      "loss": 0.6536,
      "step": 2020
    },
    {
      "epoch": 0.4350621517359623,
      "grad_norm": 0.1906583160161972,
      "learning_rate": 1.941991713101872e-05,
      "loss": 0.2119,
      "step": 2030
    },
    {
      "epoch": 0.4372053150450064,
      "grad_norm": 1.3309926986694336,
      "learning_rate": 1.941705957993999e-05,
      "loss": 0.1617,
      "step": 2040
    },
    {
      "epoch": 0.43934847835405055,
      "grad_norm": 0.028132902458310127,
      "learning_rate": 1.9414202028861265e-05,
      "loss": 0.1876,
      "step": 2050
    },
    {
      "epoch": 0.44149164166309474,
      "grad_norm": 0.09920861572027206,
      "learning_rate": 1.9411344477782543e-05,
      "loss": 0.496,
      "step": 2060
    },
    {
      "epoch": 0.44363480497213886,
      "grad_norm": 0.11499667167663574,
      "learning_rate": 1.9408486926703817e-05,
      "loss": 0.5637,
      "step": 2070
    },
    {
      "epoch": 0.44577796828118305,
      "grad_norm": 0.34014609456062317,
      "learning_rate": 1.940562937562509e-05,
      "loss": 0.3983,
      "step": 2080
    },
    {
      "epoch": 0.4479211315902272,
      "grad_norm": 0.2735397219657898,
      "learning_rate": 1.9402771824546364e-05,
      "loss": 0.3557,
      "step": 2090
    },
    {
      "epoch": 0.4500642948992713,
      "grad_norm": 32.29095458984375,
      "learning_rate": 1.939991427346764e-05,
      "loss": 0.9376,
      "step": 2100
    },
    {
      "epoch": 0.4522074582083155,
      "grad_norm": 11.714077949523926,
      "learning_rate": 1.9397056722388912e-05,
      "loss": 0.1173,
      "step": 2110
    },
    {
      "epoch": 0.4543506215173596,
      "grad_norm": 0.04530104249715805,
      "learning_rate": 1.939419917131019e-05,
      "loss": 0.8139,
      "step": 2120
    },
    {
      "epoch": 0.4564937848264038,
      "grad_norm": 1.6330645084381104,
      "learning_rate": 1.9391341620231464e-05,
      "loss": 1.1402,
      "step": 2130
    },
    {
      "epoch": 0.4586369481354479,
      "grad_norm": 0.06510211527347565,
      "learning_rate": 1.9388484069152737e-05,
      "loss": 0.1519,
      "step": 2140
    },
    {
      "epoch": 0.46078011144449205,
      "grad_norm": 35.96360778808594,
      "learning_rate": 1.938562651807401e-05,
      "loss": 0.6218,
      "step": 2150
    },
    {
      "epoch": 0.46292327475353623,
      "grad_norm": 0.2544761300086975,
      "learning_rate": 1.9382768966995285e-05,
      "loss": 0.3262,
      "step": 2160
    },
    {
      "epoch": 0.46506643806258036,
      "grad_norm": 0.05220474302768707,
      "learning_rate": 1.9379911415916563e-05,
      "loss": 0.4147,
      "step": 2170
    },
    {
      "epoch": 0.46720960137162454,
      "grad_norm": 34.597328186035156,
      "learning_rate": 1.9377053864837837e-05,
      "loss": 0.7064,
      "step": 2180
    },
    {
      "epoch": 0.46935276468066867,
      "grad_norm": 84.72537994384766,
      "learning_rate": 1.937419631375911e-05,
      "loss": 0.9362,
      "step": 2190
    },
    {
      "epoch": 0.4714959279897128,
      "grad_norm": 0.09185595065355301,
      "learning_rate": 1.9371338762680384e-05,
      "loss": 0.279,
      "step": 2200
    },
    {
      "epoch": 0.473639091298757,
      "grad_norm": 0.03113478422164917,
      "learning_rate": 1.936848121160166e-05,
      "loss": 0.2825,
      "step": 2210
    },
    {
      "epoch": 0.4757822546078011,
      "grad_norm": 0.29165229201316833,
      "learning_rate": 1.9365623660522936e-05,
      "loss": 0.4467,
      "step": 2220
    },
    {
      "epoch": 0.4779254179168453,
      "grad_norm": 34.709808349609375,
      "learning_rate": 1.936276610944421e-05,
      "loss": 0.9795,
      "step": 2230
    },
    {
      "epoch": 0.4800685812258894,
      "grad_norm": 68.96041870117188,
      "learning_rate": 1.9359908558365484e-05,
      "loss": 0.3994,
      "step": 2240
    },
    {
      "epoch": 0.48221174453493354,
      "grad_norm": 0.13246777653694153,
      "learning_rate": 1.9357051007286757e-05,
      "loss": 0.4383,
      "step": 2250
    },
    {
      "epoch": 0.4843549078439777,
      "grad_norm": 81.76934051513672,
      "learning_rate": 1.935419345620803e-05,
      "loss": 0.716,
      "step": 2260
    },
    {
      "epoch": 0.48649807115302185,
      "grad_norm": 0.14833173155784607,
      "learning_rate": 1.9351335905129305e-05,
      "loss": 0.3152,
      "step": 2270
    },
    {
      "epoch": 0.48864123446206603,
      "grad_norm": 19.637428283691406,
      "learning_rate": 1.934847835405058e-05,
      "loss": 1.0109,
      "step": 2280
    },
    {
      "epoch": 0.49078439777111016,
      "grad_norm": 63.77598190307617,
      "learning_rate": 1.9345620802971853e-05,
      "loss": 0.2701,
      "step": 2290
    },
    {
      "epoch": 0.4929275610801543,
      "grad_norm": 0.08013544231653214,
      "learning_rate": 1.9342763251893127e-05,
      "loss": 0.4312,
      "step": 2300
    },
    {
      "epoch": 0.49507072438919847,
      "grad_norm": 38.05827713012695,
      "learning_rate": 1.9339905700814404e-05,
      "loss": 0.4087,
      "step": 2310
    },
    {
      "epoch": 0.4972138876982426,
      "grad_norm": 48.2198486328125,
      "learning_rate": 1.933704814973568e-05,
      "loss": 0.3913,
      "step": 2320
    },
    {
      "epoch": 0.4993570510072868,
      "grad_norm": 2.785332679748535,
      "learning_rate": 1.9334190598656952e-05,
      "loss": 0.7489,
      "step": 2330
    },
    {
      "epoch": 0.5015002143163309,
      "grad_norm": 29.796478271484375,
      "learning_rate": 1.9331333047578226e-05,
      "loss": 0.3524,
      "step": 2340
    },
    {
      "epoch": 0.503643377625375,
      "grad_norm": 41.19815444946289,
      "learning_rate": 1.93284754964995e-05,
      "loss": 0.5275,
      "step": 2350
    },
    {
      "epoch": 0.5057865409344192,
      "grad_norm": 0.08234618604183197,
      "learning_rate": 1.9325617945420777e-05,
      "loss": 0.2354,
      "step": 2360
    },
    {
      "epoch": 0.5079297042434634,
      "grad_norm": 13.698305130004883,
      "learning_rate": 1.932276039434205e-05,
      "loss": 0.4472,
      "step": 2370
    },
    {
      "epoch": 0.5100728675525075,
      "grad_norm": 36.668426513671875,
      "learning_rate": 1.9319902843263325e-05,
      "loss": 0.1734,
      "step": 2380
    },
    {
      "epoch": 0.5122160308615517,
      "grad_norm": 1.7220427989959717,
      "learning_rate": 1.93170452921846e-05,
      "loss": 0.6479,
      "step": 2390
    },
    {
      "epoch": 0.5143591941705958,
      "grad_norm": 35.46559143066406,
      "learning_rate": 1.9314187741105873e-05,
      "loss": 0.8524,
      "step": 2400
    },
    {
      "epoch": 0.5165023574796399,
      "grad_norm": 0.6292113065719604,
      "learning_rate": 1.931133019002715e-05,
      "loss": 0.9564,
      "step": 2410
    },
    {
      "epoch": 0.5186455207886841,
      "grad_norm": 0.11697722226381302,
      "learning_rate": 1.9308472638948424e-05,
      "loss": 0.4493,
      "step": 2420
    },
    {
      "epoch": 0.5207886840977283,
      "grad_norm": 42.33612823486328,
      "learning_rate": 1.93056150878697e-05,
      "loss": 0.6041,
      "step": 2430
    },
    {
      "epoch": 0.5229318474067723,
      "grad_norm": 0.30848732590675354,
      "learning_rate": 1.9302757536790972e-05,
      "loss": 0.7895,
      "step": 2440
    },
    {
      "epoch": 0.5250750107158165,
      "grad_norm": 0.08130035549402237,
      "learning_rate": 1.9299899985712246e-05,
      "loss": 0.7015,
      "step": 2450
    },
    {
      "epoch": 0.5272181740248607,
      "grad_norm": 0.35868704319000244,
      "learning_rate": 1.929704243463352e-05,
      "loss": 1.0208,
      "step": 2460
    },
    {
      "epoch": 0.5293613373339049,
      "grad_norm": 0.4525476098060608,
      "learning_rate": 1.9294184883554794e-05,
      "loss": 0.6579,
      "step": 2470
    },
    {
      "epoch": 0.531504500642949,
      "grad_norm": 0.0341498963534832,
      "learning_rate": 1.9291327332476068e-05,
      "loss": 0.003,
      "step": 2480
    },
    {
      "epoch": 0.5336476639519931,
      "grad_norm": 29.843984603881836,
      "learning_rate": 1.9288469781397342e-05,
      "loss": 0.7456,
      "step": 2490
    },
    {
      "epoch": 0.5357908272610373,
      "grad_norm": 0.1636560559272766,
      "learning_rate": 1.928561223031862e-05,
      "loss": 0.2043,
      "step": 2500
    },
    {
      "epoch": 0.5379339905700814,
      "grad_norm": 46.6652717590332,
      "learning_rate": 1.9282754679239893e-05,
      "loss": 1.1728,
      "step": 2510
    },
    {
      "epoch": 0.5400771538791256,
      "grad_norm": 1.2091058492660522,
      "learning_rate": 1.9279897128161167e-05,
      "loss": 1.1626,
      "step": 2520
    },
    {
      "epoch": 0.5422203171881698,
      "grad_norm": 28.68756103515625,
      "learning_rate": 1.927703957708244e-05,
      "loss": 0.7022,
      "step": 2530
    },
    {
      "epoch": 0.5443634804972138,
      "grad_norm": 0.19010815024375916,
      "learning_rate": 1.9274182026003715e-05,
      "loss": 0.3531,
      "step": 2540
    },
    {
      "epoch": 0.546506643806258,
      "grad_norm": 24.55257225036621,
      "learning_rate": 1.9271324474924992e-05,
      "loss": 0.3284,
      "step": 2550
    },
    {
      "epoch": 0.5486498071153022,
      "grad_norm": 0.7402098774909973,
      "learning_rate": 1.9268466923846266e-05,
      "loss": 1.2097,
      "step": 2560
    },
    {
      "epoch": 0.5507929704243464,
      "grad_norm": 1.484851360321045,
      "learning_rate": 1.926560937276754e-05,
      "loss": 0.477,
      "step": 2570
    },
    {
      "epoch": 0.5529361337333905,
      "grad_norm": 0.19555902481079102,
      "learning_rate": 1.9262751821688814e-05,
      "loss": 0.8891,
      "step": 2580
    },
    {
      "epoch": 0.5550792970424346,
      "grad_norm": 0.25429877638816833,
      "learning_rate": 1.9259894270610088e-05,
      "loss": 0.7764,
      "step": 2590
    },
    {
      "epoch": 0.5572224603514788,
      "grad_norm": 0.21863126754760742,
      "learning_rate": 1.9257036719531362e-05,
      "loss": 0.7065,
      "step": 2600
    },
    {
      "epoch": 0.5593656236605229,
      "grad_norm": 0.6433386206626892,
      "learning_rate": 1.925417916845264e-05,
      "loss": 0.6777,
      "step": 2610
    },
    {
      "epoch": 0.5615087869695671,
      "grad_norm": 1.05797278881073,
      "learning_rate": 1.9251321617373913e-05,
      "loss": 0.7464,
      "step": 2620
    },
    {
      "epoch": 0.5636519502786113,
      "grad_norm": 21.590335845947266,
      "learning_rate": 1.9248464066295187e-05,
      "loss": 0.7934,
      "step": 2630
    },
    {
      "epoch": 0.5657951135876553,
      "grad_norm": 20.13408660888672,
      "learning_rate": 1.924560651521646e-05,
      "loss": 1.1033,
      "step": 2640
    },
    {
      "epoch": 0.5679382768966995,
      "grad_norm": 0.9584383368492126,
      "learning_rate": 1.9242748964137735e-05,
      "loss": 1.0112,
      "step": 2650
    },
    {
      "epoch": 0.5700814402057437,
      "grad_norm": 0.5256986021995544,
      "learning_rate": 1.9239891413059012e-05,
      "loss": 0.603,
      "step": 2660
    },
    {
      "epoch": 0.5722246035147879,
      "grad_norm": 0.22401031851768494,
      "learning_rate": 1.9237033861980286e-05,
      "loss": 0.3416,
      "step": 2670
    },
    {
      "epoch": 0.574367766823832,
      "grad_norm": 0.3789297938346863,
      "learning_rate": 1.923417631090156e-05,
      "loss": 0.3175,
      "step": 2680
    },
    {
      "epoch": 0.5765109301328761,
      "grad_norm": 0.1280851513147354,
      "learning_rate": 1.9231318759822834e-05,
      "loss": 0.6363,
      "step": 2690
    },
    {
      "epoch": 0.5786540934419203,
      "grad_norm": 0.173068568110466,
      "learning_rate": 1.9228461208744108e-05,
      "loss": 0.3121,
      "step": 2700
    },
    {
      "epoch": 0.5807972567509644,
      "grad_norm": 0.7113522887229919,
      "learning_rate": 1.9225603657665382e-05,
      "loss": 0.0064,
      "step": 2710
    },
    {
      "epoch": 0.5829404200600086,
      "grad_norm": 0.16477717459201813,
      "learning_rate": 1.9222746106586656e-05,
      "loss": 1.0719,
      "step": 2720
    },
    {
      "epoch": 0.5850835833690528,
      "grad_norm": 0.41989001631736755,
      "learning_rate": 1.921988855550793e-05,
      "loss": 1.2255,
      "step": 2730
    },
    {
      "epoch": 0.5872267466780968,
      "grad_norm": 18.141672134399414,
      "learning_rate": 1.9217031004429204e-05,
      "loss": 0.8044,
      "step": 2740
    },
    {
      "epoch": 0.589369909987141,
      "grad_norm": 0.9534821510314941,
      "learning_rate": 1.921417345335048e-05,
      "loss": 0.7223,
      "step": 2750
    },
    {
      "epoch": 0.5915130732961852,
      "grad_norm": 0.8085315823554993,
      "learning_rate": 1.9211315902271755e-05,
      "loss": 0.5513,
      "step": 2760
    },
    {
      "epoch": 0.5936562366052294,
      "grad_norm": 0.44149482250213623,
      "learning_rate": 1.920845835119303e-05,
      "loss": 0.5114,
      "step": 2770
    },
    {
      "epoch": 0.5957993999142734,
      "grad_norm": 18.898061752319336,
      "learning_rate": 1.9205600800114303e-05,
      "loss": 0.6579,
      "step": 2780
    },
    {
      "epoch": 0.5979425632233176,
      "grad_norm": 0.5475150346755981,
      "learning_rate": 1.9202743249035577e-05,
      "loss": 0.6605,
      "step": 2790
    },
    {
      "epoch": 0.6000857265323618,
      "grad_norm": 17.8432674407959,
      "learning_rate": 1.9199885697956854e-05,
      "loss": 0.5311,
      "step": 2800
    },
    {
      "epoch": 0.6022288898414059,
      "grad_norm": 0.16669531166553497,
      "learning_rate": 1.9197028146878128e-05,
      "loss": 0.1801,
      "step": 2810
    },
    {
      "epoch": 0.6043720531504501,
      "grad_norm": 0.2146846055984497,
      "learning_rate": 1.9194170595799402e-05,
      "loss": 0.758,
      "step": 2820
    },
    {
      "epoch": 0.6065152164594942,
      "grad_norm": 0.5160669684410095,
      "learning_rate": 1.9191313044720676e-05,
      "loss": 0.6897,
      "step": 2830
    },
    {
      "epoch": 0.6086583797685383,
      "grad_norm": 0.3143366277217865,
      "learning_rate": 1.918845549364195e-05,
      "loss": 0.4615,
      "step": 2840
    },
    {
      "epoch": 0.6108015430775825,
      "grad_norm": 0.35963675379753113,
      "learning_rate": 1.9185597942563227e-05,
      "loss": 0.6978,
      "step": 2850
    },
    {
      "epoch": 0.6129447063866267,
      "grad_norm": 18.073013305664062,
      "learning_rate": 1.91827403914845e-05,
      "loss": 1.3516,
      "step": 2860
    },
    {
      "epoch": 0.6150878696956709,
      "grad_norm": 34.43910217285156,
      "learning_rate": 1.9179882840405775e-05,
      "loss": 1.1117,
      "step": 2870
    },
    {
      "epoch": 0.6172310330047149,
      "grad_norm": 18.404722213745117,
      "learning_rate": 1.917702528932705e-05,
      "loss": 0.5168,
      "step": 2880
    },
    {
      "epoch": 0.6193741963137591,
      "grad_norm": 18.900480270385742,
      "learning_rate": 1.9174167738248323e-05,
      "loss": 0.7022,
      "step": 2890
    },
    {
      "epoch": 0.6215173596228033,
      "grad_norm": 0.3084920048713684,
      "learning_rate": 1.9171310187169597e-05,
      "loss": 0.5035,
      "step": 2900
    },
    {
      "epoch": 0.6236605229318474,
      "grad_norm": 19.50064468383789,
      "learning_rate": 1.916845263609087e-05,
      "loss": 0.5222,
      "step": 2910
    },
    {
      "epoch": 0.6258036862408916,
      "grad_norm": 0.2653012275695801,
      "learning_rate": 1.9165595085012145e-05,
      "loss": 0.3327,
      "step": 2920
    },
    {
      "epoch": 0.6279468495499357,
      "grad_norm": 0.32679441571235657,
      "learning_rate": 1.916273753393342e-05,
      "loss": 0.8306,
      "step": 2930
    },
    {
      "epoch": 0.6300900128589798,
      "grad_norm": 33.819461822509766,
      "learning_rate": 1.9159879982854696e-05,
      "loss": 0.834,
      "step": 2940
    },
    {
      "epoch": 0.632233176168024,
      "grad_norm": 0.33134031295776367,
      "learning_rate": 1.915702243177597e-05,
      "loss": 0.685,
      "step": 2950
    },
    {
      "epoch": 0.6343763394770682,
      "grad_norm": 17.515504837036133,
      "learning_rate": 1.9154164880697244e-05,
      "loss": 0.5158,
      "step": 2960
    },
    {
      "epoch": 0.6365195027861124,
      "grad_norm": 0.2987613379955292,
      "learning_rate": 1.9151307329618518e-05,
      "loss": 0.3516,
      "step": 2970
    },
    {
      "epoch": 0.6386626660951564,
      "grad_norm": 0.35637369751930237,
      "learning_rate": 1.914844977853979e-05,
      "loss": 1.0047,
      "step": 2980
    },
    {
      "epoch": 0.6408058294042006,
      "grad_norm": 0.45345956087112427,
      "learning_rate": 1.914559222746107e-05,
      "loss": 0.3241,
      "step": 2990
    },
    {
      "epoch": 0.6429489927132448,
      "grad_norm": 20.663837432861328,
      "learning_rate": 1.9142734676382343e-05,
      "loss": 1.1673,
      "step": 3000
    },
    {
      "epoch": 0.6450921560222889,
      "grad_norm": 0.7931038737297058,
      "learning_rate": 1.9139877125303617e-05,
      "loss": 0.7775,
      "step": 3010
    },
    {
      "epoch": 0.647235319331333,
      "grad_norm": 0.8102073073387146,
      "learning_rate": 1.913701957422489e-05,
      "loss": 0.9214,
      "step": 3020
    },
    {
      "epoch": 0.6493784826403772,
      "grad_norm": 17.994102478027344,
      "learning_rate": 1.9134162023146165e-05,
      "loss": 0.8344,
      "step": 3030
    },
    {
      "epoch": 0.6515216459494213,
      "grad_norm": 0.5405578017234802,
      "learning_rate": 1.913130447206744e-05,
      "loss": 0.2924,
      "step": 3040
    },
    {
      "epoch": 0.6536648092584655,
      "grad_norm": 15.687830924987793,
      "learning_rate": 1.9128446920988716e-05,
      "loss": 0.9152,
      "step": 3050
    },
    {
      "epoch": 0.6558079725675097,
      "grad_norm": 19.099313735961914,
      "learning_rate": 1.912558936990999e-05,
      "loss": 0.7086,
      "step": 3060
    },
    {
      "epoch": 0.6579511358765537,
      "grad_norm": 0.5539299845695496,
      "learning_rate": 1.9122731818831264e-05,
      "loss": 0.4625,
      "step": 3070
    },
    {
      "epoch": 0.6600942991855979,
      "grad_norm": 0.4292497932910919,
      "learning_rate": 1.9119874267752538e-05,
      "loss": 0.6243,
      "step": 3080
    },
    {
      "epoch": 0.6622374624946421,
      "grad_norm": 18.398590087890625,
      "learning_rate": 1.911701671667381e-05,
      "loss": 0.7826,
      "step": 3090
    },
    {
      "epoch": 0.6643806258036863,
      "grad_norm": 15.728188514709473,
      "learning_rate": 1.911415916559509e-05,
      "loss": 0.9625,
      "step": 3100
    },
    {
      "epoch": 0.6665237891127304,
      "grad_norm": 0.9924945831298828,
      "learning_rate": 1.9111301614516363e-05,
      "loss": 0.1464,
      "step": 3110
    },
    {
      "epoch": 0.6686669524217745,
      "grad_norm": 0.464039146900177,
      "learning_rate": 1.9108444063437633e-05,
      "loss": 0.1479,
      "step": 3120
    },
    {
      "epoch": 0.6708101157308187,
      "grad_norm": 17.84373664855957,
      "learning_rate": 1.910558651235891e-05,
      "loss": 0.9239,
      "step": 3130
    },
    {
      "epoch": 0.6729532790398628,
      "grad_norm": 0.245808944106102,
      "learning_rate": 1.9102728961280185e-05,
      "loss": 0.4175,
      "step": 3140
    },
    {
      "epoch": 0.675096442348907,
      "grad_norm": 0.31914275884628296,
      "learning_rate": 1.909987141020146e-05,
      "loss": 1.2238,
      "step": 3150
    },
    {
      "epoch": 0.6772396056579512,
      "grad_norm": 0.36673539876937866,
      "learning_rate": 1.9097013859122732e-05,
      "loss": 1.1732,
      "step": 3160
    },
    {
      "epoch": 0.6793827689669952,
      "grad_norm": 0.4942369759082794,
      "learning_rate": 1.9094156308044006e-05,
      "loss": 0.6109,
      "step": 3170
    },
    {
      "epoch": 0.6815259322760394,
      "grad_norm": 0.544219970703125,
      "learning_rate": 1.909129875696528e-05,
      "loss": 0.6024,
      "step": 3180
    },
    {
      "epoch": 0.6836690955850836,
      "grad_norm": 15.186234474182129,
      "learning_rate": 1.9088441205886558e-05,
      "loss": 0.6291,
      "step": 3190
    },
    {
      "epoch": 0.6858122588941278,
      "grad_norm": 15.809562683105469,
      "learning_rate": 1.908558365480783e-05,
      "loss": 0.7446,
      "step": 3200
    },
    {
      "epoch": 0.6879554222031719,
      "grad_norm": 15.54177188873291,
      "learning_rate": 1.9082726103729105e-05,
      "loss": 0.7591,
      "step": 3210
    },
    {
      "epoch": 0.690098585512216,
      "grad_norm": 0.6209981441497803,
      "learning_rate": 1.907986855265038e-05,
      "loss": 0.6035,
      "step": 3220
    },
    {
      "epoch": 0.6922417488212602,
      "grad_norm": 0.4576064646244049,
      "learning_rate": 1.9077011001571653e-05,
      "loss": 0.1647,
      "step": 3230
    },
    {
      "epoch": 0.6943849121303043,
      "grad_norm": 0.42924758791923523,
      "learning_rate": 1.907415345049293e-05,
      "loss": 0.8551,
      "step": 3240
    },
    {
      "epoch": 0.6965280754393485,
      "grad_norm": 0.3856777548789978,
      "learning_rate": 1.9071295899414205e-05,
      "loss": 0.1676,
      "step": 3250
    },
    {
      "epoch": 0.6986712387483927,
      "grad_norm": 16.252513885498047,
      "learning_rate": 1.906843834833548e-05,
      "loss": 0.8189,
      "step": 3260
    },
    {
      "epoch": 0.7008144020574367,
      "grad_norm": 0.3180178999900818,
      "learning_rate": 1.9065580797256752e-05,
      "loss": 0.3334,
      "step": 3270
    },
    {
      "epoch": 0.7029575653664809,
      "grad_norm": 0.5192029476165771,
      "learning_rate": 1.9062723246178026e-05,
      "loss": 0.9586,
      "step": 3280
    },
    {
      "epoch": 0.7051007286755251,
      "grad_norm": 0.5925605893135071,
      "learning_rate": 1.9059865695099304e-05,
      "loss": 0.592,
      "step": 3290
    },
    {
      "epoch": 0.7072438919845693,
      "grad_norm": 0.6837958097457886,
      "learning_rate": 1.9057008144020578e-05,
      "loss": 0.7516,
      "step": 3300
    },
    {
      "epoch": 0.7093870552936133,
      "grad_norm": 15.523847579956055,
      "learning_rate": 1.905415059294185e-05,
      "loss": 0.6092,
      "step": 3310
    },
    {
      "epoch": 0.7115302186026575,
      "grad_norm": 0.668694257736206,
      "learning_rate": 1.9051293041863125e-05,
      "loss": 1.036,
      "step": 3320
    },
    {
      "epoch": 0.7136733819117017,
      "grad_norm": 0.8556711673736572,
      "learning_rate": 1.90484354907844e-05,
      "loss": 0.88,
      "step": 3330
    },
    {
      "epoch": 0.7158165452207458,
      "grad_norm": 16.258255004882812,
      "learning_rate": 1.9045577939705673e-05,
      "loss": 0.291,
      "step": 3340
    },
    {
      "epoch": 0.71795970852979,
      "grad_norm": 17.88384437561035,
      "learning_rate": 1.9042720388626947e-05,
      "loss": 1.1076,
      "step": 3350
    },
    {
      "epoch": 0.7201028718388341,
      "grad_norm": 0.6020762324333191,
      "learning_rate": 1.903986283754822e-05,
      "loss": 0.3124,
      "step": 3360
    },
    {
      "epoch": 0.7222460351478782,
      "grad_norm": 31.888425827026367,
      "learning_rate": 1.9037005286469495e-05,
      "loss": 1.0453,
      "step": 3370
    },
    {
      "epoch": 0.7243891984569224,
      "grad_norm": 0.5916479229927063,
      "learning_rate": 1.9034147735390772e-05,
      "loss": 0.3098,
      "step": 3380
    },
    {
      "epoch": 0.7265323617659666,
      "grad_norm": 0.3776668906211853,
      "learning_rate": 1.9031290184312046e-05,
      "loss": 0.6512,
      "step": 3390
    },
    {
      "epoch": 0.7286755250750108,
      "grad_norm": 0.36388054490089417,
      "learning_rate": 1.902843263323332e-05,
      "loss": 0.8293,
      "step": 3400
    },
    {
      "epoch": 0.7308186883840548,
      "grad_norm": 17.502532958984375,
      "learning_rate": 1.9025575082154594e-05,
      "loss": 0.8711,
      "step": 3410
    },
    {
      "epoch": 0.732961851693099,
      "grad_norm": 31.527124404907227,
      "learning_rate": 1.9022717531075868e-05,
      "loss": 0.5584,
      "step": 3420
    },
    {
      "epoch": 0.7351050150021432,
      "grad_norm": 16.764402389526367,
      "learning_rate": 1.9019859979997145e-05,
      "loss": 0.8241,
      "step": 3430
    },
    {
      "epoch": 0.7372481783111873,
      "grad_norm": 0.580824613571167,
      "learning_rate": 1.901700242891842e-05,
      "loss": 0.7476,
      "step": 3440
    },
    {
      "epoch": 0.7393913416202315,
      "grad_norm": 0.5170252919197083,
      "learning_rate": 1.9014144877839693e-05,
      "loss": 0.3156,
      "step": 3450
    },
    {
      "epoch": 0.7415345049292756,
      "grad_norm": 0.5222381949424744,
      "learning_rate": 1.9011287326760967e-05,
      "loss": 1.1074,
      "step": 3460
    },
    {
      "epoch": 0.7436776682383197,
      "grad_norm": 0.7356435656547546,
      "learning_rate": 1.900842977568224e-05,
      "loss": 0.5885,
      "step": 3470
    },
    {
      "epoch": 0.7458208315473639,
      "grad_norm": 0.8333365321159363,
      "learning_rate": 1.900557222460352e-05,
      "loss": 0.8867,
      "step": 3480
    },
    {
      "epoch": 0.7479639948564081,
      "grad_norm": 31.520992279052734,
      "learning_rate": 1.9002714673524792e-05,
      "loss": 0.9397,
      "step": 3490
    },
    {
      "epoch": 0.7501071581654523,
      "grad_norm": 0.726772665977478,
      "learning_rate": 1.8999857122446066e-05,
      "loss": 0.2817,
      "step": 3500
    },
    {
      "epoch": 0.7522503214744963,
      "grad_norm": 0.32750263810157776,
      "learning_rate": 1.899699957136734e-05,
      "loss": 0.1681,
      "step": 3510
    },
    {
      "epoch": 0.7543934847835405,
      "grad_norm": 15.794947624206543,
      "learning_rate": 1.8994142020288614e-05,
      "loss": 0.7083,
      "step": 3520
    },
    {
      "epoch": 0.7565366480925847,
      "grad_norm": 0.3170892894268036,
      "learning_rate": 1.8991284469209888e-05,
      "loss": 0.7053,
      "step": 3530
    },
    {
      "epoch": 0.7586798114016288,
      "grad_norm": 0.25599703192710876,
      "learning_rate": 1.8988426918131165e-05,
      "loss": 0.6757,
      "step": 3540
    },
    {
      "epoch": 0.760822974710673,
      "grad_norm": 0.34634146094322205,
      "learning_rate": 1.8985569367052436e-05,
      "loss": 0.5085,
      "step": 3550
    },
    {
      "epoch": 0.7629661380197171,
      "grad_norm": 0.4450069069862366,
      "learning_rate": 1.898271181597371e-05,
      "loss": 0.6781,
      "step": 3560
    },
    {
      "epoch": 0.7651093013287612,
      "grad_norm": 18.157081604003906,
      "learning_rate": 1.8979854264894987e-05,
      "loss": 0.1849,
      "step": 3570
    },
    {
      "epoch": 0.7672524646378054,
      "grad_norm": 30.75884246826172,
      "learning_rate": 1.897699671381626e-05,
      "loss": 0.5225,
      "step": 3580
    },
    {
      "epoch": 0.7693956279468496,
      "grad_norm": 0.2647932469844818,
      "learning_rate": 1.8974139162737535e-05,
      "loss": 0.1725,
      "step": 3590
    },
    {
      "epoch": 0.7715387912558938,
      "grad_norm": 0.210061714053154,
      "learning_rate": 1.897128161165881e-05,
      "loss": 0.3622,
      "step": 3600
    },
    {
      "epoch": 0.7736819545649378,
      "grad_norm": 16.15433692932129,
      "learning_rate": 1.8968424060580083e-05,
      "loss": 0.71,
      "step": 3610
    },
    {
      "epoch": 0.775825117873982,
      "grad_norm": 0.31506332755088806,
      "learning_rate": 1.896556650950136e-05,
      "loss": 0.3525,
      "step": 3620
    },
    {
      "epoch": 0.7779682811830262,
      "grad_norm": 30.885324478149414,
      "learning_rate": 1.8962708958422634e-05,
      "loss": 0.8308,
      "step": 3630
    },
    {
      "epoch": 0.7801114444920703,
      "grad_norm": 31.084877014160156,
      "learning_rate": 1.8959851407343908e-05,
      "loss": 0.6486,
      "step": 3640
    },
    {
      "epoch": 0.7822546078011144,
      "grad_norm": 0.33867576718330383,
      "learning_rate": 1.8956993856265182e-05,
      "loss": 0.3396,
      "step": 3650
    },
    {
      "epoch": 0.7843977711101586,
      "grad_norm": 31.756132125854492,
      "learning_rate": 1.8954136305186456e-05,
      "loss": 1.1442,
      "step": 3660
    },
    {
      "epoch": 0.7865409344192027,
      "grad_norm": 16.325098037719727,
      "learning_rate": 1.895127875410773e-05,
      "loss": 0.8304,
      "step": 3670
    },
    {
      "epoch": 0.7886840977282469,
      "grad_norm": 0.4860697388648987,
      "learning_rate": 1.8948421203029007e-05,
      "loss": 0.1607,
      "step": 3680
    },
    {
      "epoch": 0.7908272610372911,
      "grad_norm": 0.36930274963378906,
      "learning_rate": 1.894556365195028e-05,
      "loss": 0.3144,
      "step": 3690
    },
    {
      "epoch": 0.7929704243463352,
      "grad_norm": 15.49400520324707,
      "learning_rate": 1.8942706100871555e-05,
      "loss": 0.3586,
      "step": 3700
    },
    {
      "epoch": 0.7951135876553793,
      "grad_norm": 0.29916396737098694,
      "learning_rate": 1.893984854979283e-05,
      "loss": 0.3413,
      "step": 3710
    },
    {
      "epoch": 0.7972567509644235,
      "grad_norm": 0.2628585994243622,
      "learning_rate": 1.8936990998714103e-05,
      "loss": 0.3468,
      "step": 3720
    },
    {
      "epoch": 0.7993999142734677,
      "grad_norm": 0.2685531973838806,
      "learning_rate": 1.893413344763538e-05,
      "loss": 0.689,
      "step": 3730
    },
    {
      "epoch": 0.8015430775825118,
      "grad_norm": 15.527459144592285,
      "learning_rate": 1.8931275896556654e-05,
      "loss": 0.3517,
      "step": 3740
    },
    {
      "epoch": 0.8036862408915559,
      "grad_norm": 0.3547232151031494,
      "learning_rate": 1.8928418345477928e-05,
      "loss": 0.883,
      "step": 3750
    },
    {
      "epoch": 0.8058294042006001,
      "grad_norm": 0.46614110469818115,
      "learning_rate": 1.8925560794399202e-05,
      "loss": 0.826,
      "step": 3760
    },
    {
      "epoch": 0.8079725675096442,
      "grad_norm": 30.561330795288086,
      "learning_rate": 1.8922703243320476e-05,
      "loss": 1.1864,
      "step": 3770
    },
    {
      "epoch": 0.8101157308186884,
      "grad_norm": 0.6216450333595276,
      "learning_rate": 1.891984569224175e-05,
      "loss": 0.4496,
      "step": 3780
    },
    {
      "epoch": 0.8122588941277326,
      "grad_norm": 0.6761218905448914,
      "learning_rate": 1.8916988141163024e-05,
      "loss": 1.0344,
      "step": 3790
    },
    {
      "epoch": 0.8144020574367766,
      "grad_norm": 0.6691073775291443,
      "learning_rate": 1.8914130590084298e-05,
      "loss": 0.4279,
      "step": 3800
    },
    {
      "epoch": 0.8165452207458208,
      "grad_norm": 0.5424839854240417,
      "learning_rate": 1.891127303900557e-05,
      "loss": 0.603,
      "step": 3810
    },
    {
      "epoch": 0.818688384054865,
      "grad_norm": 0.49773913621902466,
      "learning_rate": 1.890841548792685e-05,
      "loss": 0.4923,
      "step": 3820
    },
    {
      "epoch": 0.8208315473639092,
      "grad_norm": 14.689887046813965,
      "learning_rate": 1.8905557936848123e-05,
      "loss": 0.8745,
      "step": 3830
    },
    {
      "epoch": 0.8229747106729532,
      "grad_norm": 15.552255630493164,
      "learning_rate": 1.8902700385769397e-05,
      "loss": 0.5766,
      "step": 3840
    },
    {
      "epoch": 0.8251178739819974,
      "grad_norm": 0.7020803093910217,
      "learning_rate": 1.889984283469067e-05,
      "loss": 0.4295,
      "step": 3850
    },
    {
      "epoch": 0.8272610372910416,
      "grad_norm": 0.5409446358680725,
      "learning_rate": 1.8896985283611945e-05,
      "loss": 0.4594,
      "step": 3860
    },
    {
      "epoch": 0.8294042006000857,
      "grad_norm": 0.5057083368301392,
      "learning_rate": 1.8894127732533222e-05,
      "loss": 0.9538,
      "step": 3870
    },
    {
      "epoch": 0.8315473639091299,
      "grad_norm": 16.067611694335938,
      "learning_rate": 1.8891270181454496e-05,
      "loss": 0.3094,
      "step": 3880
    },
    {
      "epoch": 0.833690527218174,
      "grad_norm": 15.77580451965332,
      "learning_rate": 1.888841263037577e-05,
      "loss": 0.4553,
      "step": 3890
    },
    {
      "epoch": 0.8358336905272181,
      "grad_norm": 15.33351993560791,
      "learning_rate": 1.8885555079297044e-05,
      "loss": 1.0962,
      "step": 3900
    },
    {
      "epoch": 0.8379768538362623,
      "grad_norm": 0.7316340208053589,
      "learning_rate": 1.8882697528218318e-05,
      "loss": 0.4433,
      "step": 3910
    },
    {
      "epoch": 0.8401200171453065,
      "grad_norm": 16.33447265625,
      "learning_rate": 1.8879839977139595e-05,
      "loss": 0.4618,
      "step": 3920
    },
    {
      "epoch": 0.8422631804543507,
      "grad_norm": 0.4638259708881378,
      "learning_rate": 1.887698242606087e-05,
      "loss": 0.3224,
      "step": 3930
    },
    {
      "epoch": 0.8444063437633947,
      "grad_norm": 16.82600212097168,
      "learning_rate": 1.8874124874982143e-05,
      "loss": 0.7919,
      "step": 3940
    },
    {
      "epoch": 0.8465495070724389,
      "grad_norm": 0.5451375842094421,
      "learning_rate": 1.8871267323903417e-05,
      "loss": 0.7651,
      "step": 3950
    },
    {
      "epoch": 0.8486926703814831,
      "grad_norm": 0.46794775128364563,
      "learning_rate": 1.886840977282469e-05,
      "loss": 0.1604,
      "step": 3960
    },
    {
      "epoch": 0.8508358336905272,
      "grad_norm": 15.06682014465332,
      "learning_rate": 1.8865552221745968e-05,
      "loss": 0.333,
      "step": 3970
    },
    {
      "epoch": 0.8529789969995714,
      "grad_norm": 15.582606315612793,
      "learning_rate": 1.886269467066724e-05,
      "loss": 0.5052,
      "step": 3980
    },
    {
      "epoch": 0.8551221603086155,
      "grad_norm": 16.710460662841797,
      "learning_rate": 1.8859837119588512e-05,
      "loss": 0.8347,
      "step": 3990
    },
    {
      "epoch": 0.8572653236176596,
      "grad_norm": 0.3032894730567932,
      "learning_rate": 1.8856979568509786e-05,
      "loss": 0.4894,
      "step": 4000
    },
    {
      "epoch": 0.8594084869267038,
      "grad_norm": 15.99094295501709,
      "learning_rate": 1.8854122017431064e-05,
      "loss": 1.2368,
      "step": 4010
    },
    {
      "epoch": 0.861551650235748,
      "grad_norm": 0.8937053680419922,
      "learning_rate": 1.8851264466352338e-05,
      "loss": 0.7753,
      "step": 4020
    },
    {
      "epoch": 0.8636948135447922,
      "grad_norm": 0.6240004897117615,
      "learning_rate": 1.884840691527361e-05,
      "loss": 0.8823,
      "step": 4030
    },
    {
      "epoch": 0.8658379768538362,
      "grad_norm": 17.537921905517578,
      "learning_rate": 1.8845549364194885e-05,
      "loss": 0.1888,
      "step": 4040
    },
    {
      "epoch": 0.8679811401628804,
      "grad_norm": 0.47050532698631287,
      "learning_rate": 1.884269181311616e-05,
      "loss": 0.7594,
      "step": 4050
    },
    {
      "epoch": 0.8701243034719246,
      "grad_norm": 15.3953218460083,
      "learning_rate": 1.8839834262037437e-05,
      "loss": 0.3162,
      "step": 4060
    },
    {
      "epoch": 0.8722674667809687,
      "grad_norm": 16.953821182250977,
      "learning_rate": 1.883697671095871e-05,
      "loss": 0.6455,
      "step": 4070
    },
    {
      "epoch": 0.8744106300900129,
      "grad_norm": 15.13116455078125,
      "learning_rate": 1.8834119159879985e-05,
      "loss": 0.6277,
      "step": 4080
    },
    {
      "epoch": 0.876553793399057,
      "grad_norm": 14.898475646972656,
      "learning_rate": 1.883126160880126e-05,
      "loss": 1.1997,
      "step": 4090
    },
    {
      "epoch": 0.8786969567081011,
      "grad_norm": 0.957951009273529,
      "learning_rate": 1.8828404057722532e-05,
      "loss": 0.5568,
      "step": 4100
    },
    {
      "epoch": 0.8808401200171453,
      "grad_norm": 14.783308029174805,
      "learning_rate": 1.882554650664381e-05,
      "loss": 0.547,
      "step": 4110
    },
    {
      "epoch": 0.8829832833261895,
      "grad_norm": 0.5658339262008667,
      "learning_rate": 1.8822688955565084e-05,
      "loss": 0.2857,
      "step": 4120
    },
    {
      "epoch": 0.8851264466352337,
      "grad_norm": 30.072694778442383,
      "learning_rate": 1.8819831404486358e-05,
      "loss": 0.7415,
      "step": 4130
    },
    {
      "epoch": 0.8872696099442777,
      "grad_norm": 0.40351608395576477,
      "learning_rate": 1.881697385340763e-05,
      "loss": 0.669,
      "step": 4140
    },
    {
      "epoch": 0.8894127732533219,
      "grad_norm": 0.529215395450592,
      "learning_rate": 1.8814116302328905e-05,
      "loss": 0.6127,
      "step": 4150
    },
    {
      "epoch": 0.8915559365623661,
      "grad_norm": 19.29387855529785,
      "learning_rate": 1.881125875125018e-05,
      "loss": 0.489,
      "step": 4160
    },
    {
      "epoch": 0.8936990998714102,
      "grad_norm": 0.5527243614196777,
      "learning_rate": 1.8808401200171457e-05,
      "loss": 0.6105,
      "step": 4170
    },
    {
      "epoch": 0.8958422631804543,
      "grad_norm": 0.5827564597129822,
      "learning_rate": 1.880554364909273e-05,
      "loss": 0.6098,
      "step": 4180
    },
    {
      "epoch": 0.8979854264894985,
      "grad_norm": 0.49349990487098694,
      "learning_rate": 1.8802686098014e-05,
      "loss": 0.62,
      "step": 4190
    },
    {
      "epoch": 0.9001285897985426,
      "grad_norm": 0.577173113822937,
      "learning_rate": 1.879982854693528e-05,
      "loss": 0.3228,
      "step": 4200
    },
    {
      "epoch": 0.9022717531075868,
      "grad_norm": 0.32826539874076843,
      "learning_rate": 1.8796970995856552e-05,
      "loss": 0.4974,
      "step": 4210
    },
    {
      "epoch": 0.904414916416631,
      "grad_norm": 0.33160921931266785,
      "learning_rate": 1.8794113444777826e-05,
      "loss": 0.3346,
      "step": 4220
    },
    {
      "epoch": 0.9065580797256751,
      "grad_norm": 0.38863223791122437,
      "learning_rate": 1.87912558936991e-05,
      "loss": 0.8346,
      "step": 4230
    },
    {
      "epoch": 0.9087012430347192,
      "grad_norm": 0.5070211291313171,
      "learning_rate": 1.8788398342620374e-05,
      "loss": 0.932,
      "step": 4240
    },
    {
      "epoch": 0.9108444063437634,
      "grad_norm": 16.37457847595215,
      "learning_rate": 1.878554079154165e-05,
      "loss": 1.0059,
      "step": 4250
    },
    {
      "epoch": 0.9129875696528076,
      "grad_norm": 0.7655684947967529,
      "learning_rate": 1.8782683240462925e-05,
      "loss": 0.169,
      "step": 4260
    },
    {
      "epoch": 0.9151307329618517,
      "grad_norm": 0.49704569578170776,
      "learning_rate": 1.87798256893842e-05,
      "loss": 0.8703,
      "step": 4270
    },
    {
      "epoch": 0.9172738962708958,
      "grad_norm": 0.5082821846008301,
      "learning_rate": 1.8776968138305473e-05,
      "loss": 0.3113,
      "step": 4280
    },
    {
      "epoch": 0.91941705957994,
      "grad_norm": 15.412548065185547,
      "learning_rate": 1.8774110587226747e-05,
      "loss": 1.1103,
      "step": 4290
    },
    {
      "epoch": 0.9215602228889841,
      "grad_norm": 15.379013061523438,
      "learning_rate": 1.877125303614802e-05,
      "loss": 0.4554,
      "step": 4300
    },
    {
      "epoch": 0.9237033861980283,
      "grad_norm": 0.586713969707489,
      "learning_rate": 1.87683954850693e-05,
      "loss": 0.5951,
      "step": 4310
    },
    {
      "epoch": 0.9258465495070725,
      "grad_norm": 0.6215522289276123,
      "learning_rate": 1.8765537933990572e-05,
      "loss": 0.4464,
      "step": 4320
    },
    {
      "epoch": 0.9279897128161166,
      "grad_norm": 15.435920715332031,
      "learning_rate": 1.8762680382911846e-05,
      "loss": 0.3286,
      "step": 4330
    },
    {
      "epoch": 0.9301328761251607,
      "grad_norm": 0.3224979639053345,
      "learning_rate": 1.875982283183312e-05,
      "loss": 0.4866,
      "step": 4340
    },
    {
      "epoch": 0.9322760394342049,
      "grad_norm": 0.23731334507465363,
      "learning_rate": 1.8756965280754394e-05,
      "loss": 0.1791,
      "step": 4350
    },
    {
      "epoch": 0.9344192027432491,
      "grad_norm": 15.875256538391113,
      "learning_rate": 1.875410772967567e-05,
      "loss": 0.7253,
      "step": 4360
    },
    {
      "epoch": 0.9365623660522931,
      "grad_norm": 0.18785934150218964,
      "learning_rate": 1.8751250178596945e-05,
      "loss": 0.0042,
      "step": 4370
    },
    {
      "epoch": 0.9387055293613373,
      "grad_norm": 18.452795028686523,
      "learning_rate": 1.874839262751822e-05,
      "loss": 0.7467,
      "step": 4380
    },
    {
      "epoch": 0.9408486926703815,
      "grad_norm": 0.19822904467582703,
      "learning_rate": 1.8745535076439493e-05,
      "loss": 0.9143,
      "step": 4390
    },
    {
      "epoch": 0.9429918559794256,
      "grad_norm": 17.151247024536133,
      "learning_rate": 1.8742677525360767e-05,
      "loss": 0.8632,
      "step": 4400
    },
    {
      "epoch": 0.9451350192884698,
      "grad_norm": 0.3321950137615204,
      "learning_rate": 1.873981997428204e-05,
      "loss": 0.509,
      "step": 4410
    },
    {
      "epoch": 0.947278182597514,
      "grad_norm": 0.29278936982154846,
      "learning_rate": 1.8736962423203315e-05,
      "loss": 0.3464,
      "step": 4420
    },
    {
      "epoch": 0.9494213459065581,
      "grad_norm": 16.035497665405273,
      "learning_rate": 1.873410487212459e-05,
      "loss": 0.7125,
      "step": 4430
    },
    {
      "epoch": 0.9515645092156022,
      "grad_norm": 0.30392810702323914,
      "learning_rate": 1.8731247321045863e-05,
      "loss": 0.7153,
      "step": 4440
    },
    {
      "epoch": 0.9537076725246464,
      "grad_norm": 0.28050678968429565,
      "learning_rate": 1.872838976996714e-05,
      "loss": 0.3511,
      "step": 4450
    },
    {
      "epoch": 0.9558508358336906,
      "grad_norm": 17.01681137084961,
      "learning_rate": 1.8725532218888414e-05,
      "loss": 1.1598,
      "step": 4460
    },
    {
      "epoch": 0.9579939991427346,
      "grad_norm": 30.22393798828125,
      "learning_rate": 1.8722674667809688e-05,
      "loss": 0.6191,
      "step": 4470
    },
    {
      "epoch": 0.9601371624517788,
      "grad_norm": 0.3378126621246338,
      "learning_rate": 1.8719817116730962e-05,
      "loss": 0.0079,
      "step": 4480
    },
    {
      "epoch": 0.962280325760823,
      "grad_norm": 0.3147573173046112,
      "learning_rate": 1.8716959565652236e-05,
      "loss": 1.0887,
      "step": 4490
    },
    {
      "epoch": 0.9644234890698671,
      "grad_norm": 15.635735511779785,
      "learning_rate": 1.8714102014573513e-05,
      "loss": 0.4887,
      "step": 4500
    },
    {
      "epoch": 0.9665666523789113,
      "grad_norm": 15.406915664672852,
      "learning_rate": 1.8711244463494787e-05,
      "loss": 0.64,
      "step": 4510
    },
    {
      "epoch": 0.9687098156879554,
      "grad_norm": 0.4430706799030304,
      "learning_rate": 1.870838691241606e-05,
      "loss": 0.4873,
      "step": 4520
    },
    {
      "epoch": 0.9708529789969995,
      "grad_norm": 15.716716766357422,
      "learning_rate": 1.8705529361337335e-05,
      "loss": 0.8095,
      "step": 4530
    },
    {
      "epoch": 0.9729961423060437,
      "grad_norm": 15.168107032775879,
      "learning_rate": 1.870267181025861e-05,
      "loss": 0.3221,
      "step": 4540
    },
    {
      "epoch": 0.9751393056150879,
      "grad_norm": 0.3623160719871521,
      "learning_rate": 1.8699814259179886e-05,
      "loss": 0.1687,
      "step": 4550
    },
    {
      "epoch": 0.9772824689241321,
      "grad_norm": 0.24954265356063843,
      "learning_rate": 1.869695670810116e-05,
      "loss": 0.1706,
      "step": 4560
    },
    {
      "epoch": 0.9794256322331761,
      "grad_norm": 0.22723352909088135,
      "learning_rate": 1.8694099157022434e-05,
      "loss": 0.7116,
      "step": 4570
    },
    {
      "epoch": 0.9815687955422203,
      "grad_norm": 0.2689661681652069,
      "learning_rate": 1.8691241605943708e-05,
      "loss": 1.0358,
      "step": 4580
    },
    {
      "epoch": 0.9837119588512645,
      "grad_norm": 15.244123458862305,
      "learning_rate": 1.8688384054864982e-05,
      "loss": 0.8189,
      "step": 4590
    },
    {
      "epoch": 0.9858551221603086,
      "grad_norm": 30.497400283813477,
      "learning_rate": 1.8685526503786256e-05,
      "loss": 0.7892,
      "step": 4600
    },
    {
      "epoch": 0.9879982854693528,
      "grad_norm": 0.5379997491836548,
      "learning_rate": 1.8682668952707533e-05,
      "loss": 0.6378,
      "step": 4610
    },
    {
      "epoch": 0.9901414487783969,
      "grad_norm": 14.909135818481445,
      "learning_rate": 1.8679811401628804e-05,
      "loss": 0.3174,
      "step": 4620
    },
    {
      "epoch": 0.992284612087441,
      "grad_norm": 17.172229766845703,
      "learning_rate": 1.8676953850550078e-05,
      "loss": 0.3296,
      "step": 4630
    },
    {
      "epoch": 0.9944277753964852,
      "grad_norm": 0.3516238331794739,
      "learning_rate": 1.8674096299471355e-05,
      "loss": 0.509,
      "step": 4640
    },
    {
      "epoch": 0.9965709387055294,
      "grad_norm": 0.33817118406295776,
      "learning_rate": 1.867123874839263e-05,
      "loss": 0.3356,
      "step": 4650
    },
    {
      "epoch": 0.9987141020145736,
      "grad_norm": 0.3627559244632721,
      "learning_rate": 1.8668381197313903e-05,
      "loss": 0.6633,
      "step": 4660
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8736666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.6221495866775513,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 429.3499,
      "eval_samples_per_second": 6.987,
      "eval_steps_per_second": 2.329,
      "step": 4666
    },
    {
      "epoch": 1.0008572653236176,
      "grad_norm": 0.39693301916122437,
      "learning_rate": 1.8665523646235177e-05,
      "loss": 0.663,
      "step": 4670
    },
    {
      "epoch": 1.0030004286326617,
      "grad_norm": 0.5129254460334778,
      "learning_rate": 1.866266609515645e-05,
      "loss": 0.9643,
      "step": 4680
    },
    {
      "epoch": 1.005143591941706,
      "grad_norm": 0.6478223204612732,
      "learning_rate": 1.8659808544077728e-05,
      "loss": 0.9431,
      "step": 4690
    },
    {
      "epoch": 1.00728675525075,
      "grad_norm": 0.5591806769371033,
      "learning_rate": 1.8656950992999002e-05,
      "loss": 0.3008,
      "step": 4700
    },
    {
      "epoch": 1.0094299185597944,
      "grad_norm": 18.0612850189209,
      "learning_rate": 1.8654093441920276e-05,
      "loss": 0.612,
      "step": 4710
    },
    {
      "epoch": 1.0115730818688384,
      "grad_norm": 15.240530967712402,
      "learning_rate": 1.865123589084155e-05,
      "loss": 1.3362,
      "step": 4720
    },
    {
      "epoch": 1.0137162451778825,
      "grad_norm": 1.037161111831665,
      "learning_rate": 1.8648378339762824e-05,
      "loss": 0.6874,
      "step": 4730
    },
    {
      "epoch": 1.0158594084869268,
      "grad_norm": 29.818593978881836,
      "learning_rate": 1.8645520788684098e-05,
      "loss": 0.5653,
      "step": 4740
    },
    {
      "epoch": 1.0180025717959709,
      "grad_norm": 15.197068214416504,
      "learning_rate": 1.8642663237605375e-05,
      "loss": 0.4487,
      "step": 4750
    },
    {
      "epoch": 1.020145735105015,
      "grad_norm": 0.5097345113754272,
      "learning_rate": 1.863980568652665e-05,
      "loss": 1.0339,
      "step": 4760
    },
    {
      "epoch": 1.0222888984140592,
      "grad_norm": 0.564984917640686,
      "learning_rate": 1.8636948135447923e-05,
      "loss": 0.3131,
      "step": 4770
    },
    {
      "epoch": 1.0244320617231033,
      "grad_norm": 0.5048461556434631,
      "learning_rate": 1.8634090584369197e-05,
      "loss": 0.4762,
      "step": 4780
    },
    {
      "epoch": 1.0265752250321474,
      "grad_norm": 31.566370010375977,
      "learning_rate": 1.863123303329047e-05,
      "loss": 0.8204,
      "step": 4790
    },
    {
      "epoch": 1.0287183883411917,
      "grad_norm": 17.20240020751953,
      "learning_rate": 1.8628375482211748e-05,
      "loss": 0.4989,
      "step": 4800
    },
    {
      "epoch": 1.0308615516502357,
      "grad_norm": 0.5589869618415833,
      "learning_rate": 1.8625517931133022e-05,
      "loss": 0.9565,
      "step": 4810
    },
    {
      "epoch": 1.0330047149592798,
      "grad_norm": 0.6546781659126282,
      "learning_rate": 1.8622660380054296e-05,
      "loss": 0.6038,
      "step": 4820
    },
    {
      "epoch": 1.035147878268324,
      "grad_norm": 15.860616683959961,
      "learning_rate": 1.861980282897557e-05,
      "loss": 0.8792,
      "step": 4830
    },
    {
      "epoch": 1.0372910415773682,
      "grad_norm": 0.6491108536720276,
      "learning_rate": 1.8616945277896844e-05,
      "loss": 0.5808,
      "step": 4840
    },
    {
      "epoch": 1.0394342048864122,
      "grad_norm": 0.6637446284294128,
      "learning_rate": 1.8614087726818118e-05,
      "loss": 0.873,
      "step": 4850
    },
    {
      "epoch": 1.0415773681954565,
      "grad_norm": 18.507455825805664,
      "learning_rate": 1.861123017573939e-05,
      "loss": 0.5638,
      "step": 4860
    },
    {
      "epoch": 1.0437205315045006,
      "grad_norm": 15.937836647033691,
      "learning_rate": 1.8608372624660666e-05,
      "loss": 0.7139,
      "step": 4870
    },
    {
      "epoch": 1.0458636948135447,
      "grad_norm": 0.7756967544555664,
      "learning_rate": 1.860551507358194e-05,
      "loss": 0.6934,
      "step": 4880
    },
    {
      "epoch": 1.048006858122589,
      "grad_norm": 0.5131648182868958,
      "learning_rate": 1.8602657522503217e-05,
      "loss": 0.569,
      "step": 4890
    },
    {
      "epoch": 1.050150021431633,
      "grad_norm": 0.30841943621635437,
      "learning_rate": 1.859979997142449e-05,
      "loss": 0.1682,
      "step": 4900
    },
    {
      "epoch": 1.0522931847406773,
      "grad_norm": 0.25713735818862915,
      "learning_rate": 1.8596942420345765e-05,
      "loss": 0.6727,
      "step": 4910
    },
    {
      "epoch": 1.0544363480497214,
      "grad_norm": 0.2403799593448639,
      "learning_rate": 1.859408486926704e-05,
      "loss": 0.3283,
      "step": 4920
    },
    {
      "epoch": 1.0565795113587655,
      "grad_norm": 0.30970755219459534,
      "learning_rate": 1.8591227318188313e-05,
      "loss": 0.6908,
      "step": 4930
    },
    {
      "epoch": 1.0587226746678098,
      "grad_norm": 15.879939079284668,
      "learning_rate": 1.858836976710959e-05,
      "loss": 1.1287,
      "step": 4940
    },
    {
      "epoch": 1.0608658379768539,
      "grad_norm": 0.6457372903823853,
      "learning_rate": 1.8585512216030864e-05,
      "loss": 0.8992,
      "step": 4950
    },
    {
      "epoch": 1.063009001285898,
      "grad_norm": 0.38581666350364685,
      "learning_rate": 1.8582654664952138e-05,
      "loss": 0.2995,
      "step": 4960
    },
    {
      "epoch": 1.0651521645949422,
      "grad_norm": 34.10990905761719,
      "learning_rate": 1.857979711387341e-05,
      "loss": 1.2368,
      "step": 4970
    },
    {
      "epoch": 1.0672953279039863,
      "grad_norm": 0.8707447648048401,
      "learning_rate": 1.8576939562794686e-05,
      "loss": 0.8593,
      "step": 4980
    },
    {
      "epoch": 1.0694384912130304,
      "grad_norm": 18.064077377319336,
      "learning_rate": 1.8574082011715963e-05,
      "loss": 0.5531,
      "step": 4990
    },
    {
      "epoch": 1.0715816545220747,
      "grad_norm": 0.46328046917915344,
      "learning_rate": 1.8571224460637237e-05,
      "loss": 0.3314,
      "step": 5000
    },
    {
      "epoch": 1.0737248178311187,
      "grad_norm": 18.129425048828125,
      "learning_rate": 1.856836690955851e-05,
      "loss": 0.5272,
      "step": 5010
    },
    {
      "epoch": 1.0758679811401628,
      "grad_norm": 18.045814514160156,
      "learning_rate": 1.8565509358479785e-05,
      "loss": 0.7081,
      "step": 5020
    },
    {
      "epoch": 1.078011144449207,
      "grad_norm": 17.935659408569336,
      "learning_rate": 1.856265180740106e-05,
      "loss": 1.025,
      "step": 5030
    },
    {
      "epoch": 1.0801543077582512,
      "grad_norm": 0.6547850370407104,
      "learning_rate": 1.8559794256322336e-05,
      "loss": 0.4993,
      "step": 5040
    },
    {
      "epoch": 1.0822974710672952,
      "grad_norm": 0.37660637497901917,
      "learning_rate": 1.8556936705243606e-05,
      "loss": 0.4792,
      "step": 5050
    },
    {
      "epoch": 1.0844406343763395,
      "grad_norm": 19.69894027709961,
      "learning_rate": 1.855407915416488e-05,
      "loss": 0.1966,
      "step": 5060
    },
    {
      "epoch": 1.0865837976853836,
      "grad_norm": 0.2271074652671814,
      "learning_rate": 1.8551221603086154e-05,
      "loss": 0.8094,
      "step": 5070
    },
    {
      "epoch": 1.0887269609944277,
      "grad_norm": 18.218202590942383,
      "learning_rate": 1.854836405200743e-05,
      "loss": 0.5571,
      "step": 5080
    },
    {
      "epoch": 1.090870124303472,
      "grad_norm": 32.225502014160156,
      "learning_rate": 1.8545506500928706e-05,
      "loss": 0.9875,
      "step": 5090
    },
    {
      "epoch": 1.093013287612516,
      "grad_norm": 0.5119786858558655,
      "learning_rate": 1.854264894984998e-05,
      "loss": 0.6445,
      "step": 5100
    },
    {
      "epoch": 1.0951564509215603,
      "grad_norm": 0.6701372861862183,
      "learning_rate": 1.8539791398771253e-05,
      "loss": 0.4468,
      "step": 5110
    },
    {
      "epoch": 1.0972996142306044,
      "grad_norm": 0.42135658860206604,
      "learning_rate": 1.8536933847692527e-05,
      "loss": 0.4798,
      "step": 5120
    },
    {
      "epoch": 1.0994427775396485,
      "grad_norm": 30.533161163330078,
      "learning_rate": 1.8534076296613805e-05,
      "loss": 0.967,
      "step": 5130
    },
    {
      "epoch": 1.1015859408486928,
      "grad_norm": 17.122041702270508,
      "learning_rate": 1.853121874553508e-05,
      "loss": 0.9117,
      "step": 5140
    },
    {
      "epoch": 1.1037291041577368,
      "grad_norm": 15.106460571289062,
      "learning_rate": 1.8528361194456353e-05,
      "loss": 0.7121,
      "step": 5150
    },
    {
      "epoch": 1.105872267466781,
      "grad_norm": 16.6873722076416,
      "learning_rate": 1.8525503643377626e-05,
      "loss": 0.473,
      "step": 5160
    },
    {
      "epoch": 1.1080154307758252,
      "grad_norm": 0.3468436896800995,
      "learning_rate": 1.85226460922989e-05,
      "loss": 0.4853,
      "step": 5170
    },
    {
      "epoch": 1.1101585940848693,
      "grad_norm": 0.30196237564086914,
      "learning_rate": 1.8519788541220178e-05,
      "loss": 0.1709,
      "step": 5180
    },
    {
      "epoch": 1.1123017573939133,
      "grad_norm": 16.84092903137207,
      "learning_rate": 1.851693099014145e-05,
      "loss": 0.5234,
      "step": 5190
    },
    {
      "epoch": 1.1144449207029576,
      "grad_norm": 0.24782676994800568,
      "learning_rate": 1.8514073439062726e-05,
      "loss": 0.5391,
      "step": 5200
    },
    {
      "epoch": 1.1165880840120017,
      "grad_norm": 15.365310668945312,
      "learning_rate": 1.8511215887984e-05,
      "loss": 0.5284,
      "step": 5210
    },
    {
      "epoch": 1.1187312473210458,
      "grad_norm": 31.435749053955078,
      "learning_rate": 1.8508358336905273e-05,
      "loss": 1.218,
      "step": 5220
    },
    {
      "epoch": 1.12087441063009,
      "grad_norm": 0.5096717476844788,
      "learning_rate": 1.8505500785826547e-05,
      "loss": 0.8139,
      "step": 5230
    },
    {
      "epoch": 1.1230175739391342,
      "grad_norm": 0.5201197862625122,
      "learning_rate": 1.8502643234747825e-05,
      "loss": 0.3145,
      "step": 5240
    },
    {
      "epoch": 1.1251607372481782,
      "grad_norm": 0.5032008290290833,
      "learning_rate": 1.84997856836691e-05,
      "loss": 1.2209,
      "step": 5250
    },
    {
      "epoch": 1.1273039005572225,
      "grad_norm": 0.7130652070045471,
      "learning_rate": 1.849692813259037e-05,
      "loss": 0.9081,
      "step": 5260
    },
    {
      "epoch": 1.1294470638662666,
      "grad_norm": 15.977267265319824,
      "learning_rate": 1.8494070581511646e-05,
      "loss": 0.2967,
      "step": 5270
    },
    {
      "epoch": 1.1315902271753107,
      "grad_norm": 0.48385295271873474,
      "learning_rate": 1.849121303043292e-05,
      "loss": 0.4615,
      "step": 5280
    },
    {
      "epoch": 1.133733390484355,
      "grad_norm": 18.023984909057617,
      "learning_rate": 1.8488355479354194e-05,
      "loss": 0.4881,
      "step": 5290
    },
    {
      "epoch": 1.135876553793399,
      "grad_norm": 16.7421875,
      "learning_rate": 1.8485497928275468e-05,
      "loss": 0.8339,
      "step": 5300
    },
    {
      "epoch": 1.138019717102443,
      "grad_norm": 0.5284939408302307,
      "learning_rate": 1.8482640377196742e-05,
      "loss": 0.4843,
      "step": 5310
    },
    {
      "epoch": 1.1401628804114874,
      "grad_norm": 20.814416885375977,
      "learning_rate": 1.847978282611802e-05,
      "loss": 0.6901,
      "step": 5320
    },
    {
      "epoch": 1.1423060437205315,
      "grad_norm": 16.63301658630371,
      "learning_rate": 1.8476925275039293e-05,
      "loss": 0.9224,
      "step": 5330
    },
    {
      "epoch": 1.1444492070295755,
      "grad_norm": 18.562274932861328,
      "learning_rate": 1.8474067723960567e-05,
      "loss": 0.4751,
      "step": 5340
    },
    {
      "epoch": 1.1465923703386198,
      "grad_norm": 0.5805556178092957,
      "learning_rate": 1.847121017288184e-05,
      "loss": 0.4542,
      "step": 5350
    },
    {
      "epoch": 1.148735533647664,
      "grad_norm": 16.648279190063477,
      "learning_rate": 1.8468352621803115e-05,
      "loss": 0.8773,
      "step": 5360
    },
    {
      "epoch": 1.1508786969567082,
      "grad_norm": 1.0022791624069214,
      "learning_rate": 1.846549507072439e-05,
      "loss": 0.6418,
      "step": 5370
    },
    {
      "epoch": 1.1530218602657523,
      "grad_norm": 18.497509002685547,
      "learning_rate": 1.8462637519645666e-05,
      "loss": 0.5395,
      "step": 5380
    },
    {
      "epoch": 1.1551650235747963,
      "grad_norm": 0.634804368019104,
      "learning_rate": 1.845977996856694e-05,
      "loss": 0.7384,
      "step": 5390
    },
    {
      "epoch": 1.1573081868838406,
      "grad_norm": 32.066184997558594,
      "learning_rate": 1.8456922417488214e-05,
      "loss": 0.5128,
      "step": 5400
    },
    {
      "epoch": 1.1594513501928847,
      "grad_norm": 0.6488763093948364,
      "learning_rate": 1.8454064866409488e-05,
      "loss": 1.5114,
      "step": 5410
    },
    {
      "epoch": 1.1615945135019288,
      "grad_norm": 15.446784019470215,
      "learning_rate": 1.8451207315330762e-05,
      "loss": 0.6749,
      "step": 5420
    },
    {
      "epoch": 1.163737676810973,
      "grad_norm": 33.50446701049805,
      "learning_rate": 1.844834976425204e-05,
      "loss": 0.5247,
      "step": 5430
    },
    {
      "epoch": 1.1658808401200171,
      "grad_norm": 0.6061171293258667,
      "learning_rate": 1.8445492213173313e-05,
      "loss": 0.4147,
      "step": 5440
    },
    {
      "epoch": 1.1680240034290612,
      "grad_norm": 0.2197338193655014,
      "learning_rate": 1.8442634662094587e-05,
      "loss": 0.3761,
      "step": 5450
    },
    {
      "epoch": 1.1701671667381055,
      "grad_norm": 16.56955909729004,
      "learning_rate": 1.843977711101586e-05,
      "loss": 1.4073,
      "step": 5460
    },
    {
      "epoch": 1.1723103300471496,
      "grad_norm": 0.5600820779800415,
      "learning_rate": 1.8436919559937135e-05,
      "loss": 0.4689,
      "step": 5470
    },
    {
      "epoch": 1.1744534933561936,
      "grad_norm": 19.647342681884766,
      "learning_rate": 1.843406200885841e-05,
      "loss": 1.0689,
      "step": 5480
    },
    {
      "epoch": 1.176596656665238,
      "grad_norm": 17.402875900268555,
      "learning_rate": 1.8431204457779683e-05,
      "loss": 0.4212,
      "step": 5490
    },
    {
      "epoch": 1.178739819974282,
      "grad_norm": 0.7858536839485168,
      "learning_rate": 1.8428346906700957e-05,
      "loss": 0.3029,
      "step": 5500
    },
    {
      "epoch": 1.1808829832833263,
      "grad_norm": 15.798318862915039,
      "learning_rate": 1.842548935562223e-05,
      "loss": 0.3468,
      "step": 5510
    },
    {
      "epoch": 1.1830261465923704,
      "grad_norm": 0.2605836093425751,
      "learning_rate": 1.8422631804543508e-05,
      "loss": 0.8721,
      "step": 5520
    },
    {
      "epoch": 1.1851693099014144,
      "grad_norm": 16.03866958618164,
      "learning_rate": 1.8419774253464782e-05,
      "loss": 0.6853,
      "step": 5530
    },
    {
      "epoch": 1.1873124732104587,
      "grad_norm": 15.971959114074707,
      "learning_rate": 1.8416916702386056e-05,
      "loss": 1.0053,
      "step": 5540
    },
    {
      "epoch": 1.1894556365195028,
      "grad_norm": 15.160421371459961,
      "learning_rate": 1.841405915130733e-05,
      "loss": 0.4737,
      "step": 5550
    },
    {
      "epoch": 1.1915987998285469,
      "grad_norm": 15.994454383850098,
      "learning_rate": 1.8411201600228604e-05,
      "loss": 0.7652,
      "step": 5560
    },
    {
      "epoch": 1.1937419631375912,
      "grad_norm": 0.6653358340263367,
      "learning_rate": 1.840834404914988e-05,
      "loss": 0.4356,
      "step": 5570
    },
    {
      "epoch": 1.1958851264466352,
      "grad_norm": 0.6606608033180237,
      "learning_rate": 1.8405486498071155e-05,
      "loss": 0.2946,
      "step": 5580
    },
    {
      "epoch": 1.1980282897556793,
      "grad_norm": 0.5732982754707336,
      "learning_rate": 1.840262894699243e-05,
      "loss": 0.9102,
      "step": 5590
    },
    {
      "epoch": 1.2001714530647236,
      "grad_norm": 0.6083049178123474,
      "learning_rate": 1.8399771395913703e-05,
      "loss": 1.208,
      "step": 5600
    },
    {
      "epoch": 1.2023146163737677,
      "grad_norm": 15.411617279052734,
      "learning_rate": 1.8396913844834977e-05,
      "loss": 0.724,
      "step": 5610
    },
    {
      "epoch": 1.2044577796828118,
      "grad_norm": 0.6646303534507751,
      "learning_rate": 1.8394056293756254e-05,
      "loss": 0.7332,
      "step": 5620
    },
    {
      "epoch": 1.206600942991856,
      "grad_norm": 0.5661506652832031,
      "learning_rate": 1.8391198742677528e-05,
      "loss": 0.4436,
      "step": 5630
    },
    {
      "epoch": 1.2087441063009001,
      "grad_norm": 17.104360580444336,
      "learning_rate": 1.8388341191598802e-05,
      "loss": 0.3367,
      "step": 5640
    },
    {
      "epoch": 1.2108872696099442,
      "grad_norm": 0.32455796003341675,
      "learning_rate": 1.8385483640520076e-05,
      "loss": 0.6768,
      "step": 5650
    },
    {
      "epoch": 1.2130304329189885,
      "grad_norm": 17.19094467163086,
      "learning_rate": 1.838262608944135e-05,
      "loss": 0.3574,
      "step": 5660
    },
    {
      "epoch": 1.2151735962280326,
      "grad_norm": 0.2732830345630646,
      "learning_rate": 1.8379768538362627e-05,
      "loss": 0.5195,
      "step": 5670
    },
    {
      "epoch": 1.2173167595370766,
      "grad_norm": 0.2759300470352173,
      "learning_rate": 1.83769109872839e-05,
      "loss": 0.7018,
      "step": 5680
    },
    {
      "epoch": 1.219459922846121,
      "grad_norm": 0.4121303856372833,
      "learning_rate": 1.8374053436205172e-05,
      "loss": 0.5101,
      "step": 5690
    },
    {
      "epoch": 1.221603086155165,
      "grad_norm": 15.845955848693848,
      "learning_rate": 1.8371195885126446e-05,
      "loss": 0.491,
      "step": 5700
    },
    {
      "epoch": 1.223746249464209,
      "grad_norm": 0.5195204019546509,
      "learning_rate": 1.8368338334047723e-05,
      "loss": 0.8034,
      "step": 5710
    },
    {
      "epoch": 1.2258894127732534,
      "grad_norm": 16.80362319946289,
      "learning_rate": 1.8365480782968997e-05,
      "loss": 0.4706,
      "step": 5720
    },
    {
      "epoch": 1.2280325760822974,
      "grad_norm": 31.256237030029297,
      "learning_rate": 1.836262323189027e-05,
      "loss": 0.6384,
      "step": 5730
    },
    {
      "epoch": 1.2301757393913415,
      "grad_norm": 0.49997058510780334,
      "learning_rate": 1.8359765680811545e-05,
      "loss": 0.467,
      "step": 5740
    },
    {
      "epoch": 1.2323189027003858,
      "grad_norm": 16.009342193603516,
      "learning_rate": 1.835690812973282e-05,
      "loss": 0.7866,
      "step": 5750
    },
    {
      "epoch": 1.2344620660094299,
      "grad_norm": 16.094924926757812,
      "learning_rate": 1.8354050578654096e-05,
      "loss": 0.9687,
      "step": 5760
    },
    {
      "epoch": 1.2366052293184742,
      "grad_norm": 0.39074766635894775,
      "learning_rate": 1.835119302757537e-05,
      "loss": 0.3473,
      "step": 5770
    },
    {
      "epoch": 1.2387483926275182,
      "grad_norm": 16.20665168762207,
      "learning_rate": 1.8348335476496644e-05,
      "loss": 0.7558,
      "step": 5780
    },
    {
      "epoch": 1.2408915559365623,
      "grad_norm": 0.42750269174575806,
      "learning_rate": 1.8345477925417918e-05,
      "loss": 0.3143,
      "step": 5790
    },
    {
      "epoch": 1.2430347192456066,
      "grad_norm": 0.3696328103542328,
      "learning_rate": 1.8342620374339192e-05,
      "loss": 0.6476,
      "step": 5800
    },
    {
      "epoch": 1.2451778825546507,
      "grad_norm": 0.30248039960861206,
      "learning_rate": 1.833976282326047e-05,
      "loss": 0.1759,
      "step": 5810
    },
    {
      "epoch": 1.2473210458636947,
      "grad_norm": 0.2391376942396164,
      "learning_rate": 1.8336905272181743e-05,
      "loss": 0.5001,
      "step": 5820
    },
    {
      "epoch": 1.249464209172739,
      "grad_norm": 0.26338469982147217,
      "learning_rate": 1.8334047721103017e-05,
      "loss": 0.5159,
      "step": 5830
    },
    {
      "epoch": 1.251607372481783,
      "grad_norm": 15.672869682312012,
      "learning_rate": 1.833119017002429e-05,
      "loss": 0.8466,
      "step": 5840
    },
    {
      "epoch": 1.2537505357908272,
      "grad_norm": 15.702392578125,
      "learning_rate": 1.8328332618945565e-05,
      "loss": 0.9682,
      "step": 5850
    },
    {
      "epoch": 1.2558936990998715,
      "grad_norm": 0.32995298504829407,
      "learning_rate": 1.832547506786684e-05,
      "loss": 0.31,
      "step": 5860
    },
    {
      "epoch": 1.2580368624089155,
      "grad_norm": 15.012899398803711,
      "learning_rate": 1.8322617516788116e-05,
      "loss": 1.0506,
      "step": 5870
    },
    {
      "epoch": 1.2601800257179598,
      "grad_norm": 0.7485464811325073,
      "learning_rate": 1.831975996570939e-05,
      "loss": 0.7417,
      "step": 5880
    },
    {
      "epoch": 1.262323189027004,
      "grad_norm": 0.8991453051567078,
      "learning_rate": 1.8316902414630664e-05,
      "loss": 0.4175,
      "step": 5890
    },
    {
      "epoch": 1.264466352336048,
      "grad_norm": 0.6194312572479248,
      "learning_rate": 1.8314044863551938e-05,
      "loss": 0.2959,
      "step": 5900
    },
    {
      "epoch": 1.2666095156450923,
      "grad_norm": 0.4889979362487793,
      "learning_rate": 1.8311187312473212e-05,
      "loss": 0.4444,
      "step": 5910
    },
    {
      "epoch": 1.2687526789541363,
      "grad_norm": 0.44362467527389526,
      "learning_rate": 1.8308329761394486e-05,
      "loss": 0.3121,
      "step": 5920
    },
    {
      "epoch": 1.2708958422631804,
      "grad_norm": 0.43146756291389465,
      "learning_rate": 1.830547221031576e-05,
      "loss": 0.6604,
      "step": 5930
    },
    {
      "epoch": 1.2730390055722247,
      "grad_norm": 0.4299526512622833,
      "learning_rate": 1.8302614659237034e-05,
      "loss": 0.3318,
      "step": 5940
    },
    {
      "epoch": 1.2751821688812688,
      "grad_norm": 16.67586326599121,
      "learning_rate": 1.829975710815831e-05,
      "loss": 1.2757,
      "step": 5950
    },
    {
      "epoch": 1.2773253321903129,
      "grad_norm": 0.45860782265663147,
      "learning_rate": 1.8296899557079585e-05,
      "loss": 0.3225,
      "step": 5960
    },
    {
      "epoch": 1.2794684954993572,
      "grad_norm": 0.42158013582229614,
      "learning_rate": 1.829404200600086e-05,
      "loss": 0.7885,
      "step": 5970
    },
    {
      "epoch": 1.2816116588084012,
      "grad_norm": 0.43443652987480164,
      "learning_rate": 1.8291184454922133e-05,
      "loss": 0.6256,
      "step": 5980
    },
    {
      "epoch": 1.2837548221174453,
      "grad_norm": 0.4765426516532898,
      "learning_rate": 1.8288326903843407e-05,
      "loss": 0.9126,
      "step": 5990
    },
    {
      "epoch": 1.2858979854264896,
      "grad_norm": 16.03815269470215,
      "learning_rate": 1.828546935276468e-05,
      "loss": 0.7719,
      "step": 6000
    },
    {
      "epoch": 1.2880411487355337,
      "grad_norm": 15.759086608886719,
      "learning_rate": 1.8282611801685958e-05,
      "loss": 1.3388,
      "step": 6010
    },
    {
      "epoch": 1.2901843120445777,
      "grad_norm": 0.8063138723373413,
      "learning_rate": 1.8279754250607232e-05,
      "loss": 0.4164,
      "step": 6020
    },
    {
      "epoch": 1.292327475353622,
      "grad_norm": 0.5873730182647705,
      "learning_rate": 1.8276896699528506e-05,
      "loss": 0.4322,
      "step": 6030
    },
    {
      "epoch": 1.294470638662666,
      "grad_norm": 16.307415008544922,
      "learning_rate": 1.827403914844978e-05,
      "loss": 0.5953,
      "step": 6040
    },
    {
      "epoch": 1.2966138019717102,
      "grad_norm": 0.6869285106658936,
      "learning_rate": 1.8271181597371054e-05,
      "loss": 0.916,
      "step": 6050
    },
    {
      "epoch": 1.2987569652807545,
      "grad_norm": 0.7819902896881104,
      "learning_rate": 1.826832404629233e-05,
      "loss": 0.7348,
      "step": 6060
    },
    {
      "epoch": 1.3009001285897985,
      "grad_norm": 14.825066566467285,
      "learning_rate": 1.8265466495213605e-05,
      "loss": 0.4326,
      "step": 6070
    },
    {
      "epoch": 1.3030432918988426,
      "grad_norm": 14.963475227355957,
      "learning_rate": 1.826260894413488e-05,
      "loss": 0.3354,
      "step": 6080
    },
    {
      "epoch": 1.305186455207887,
      "grad_norm": 15.86044692993164,
      "learning_rate": 1.8259751393056153e-05,
      "loss": 0.9961,
      "step": 6090
    },
    {
      "epoch": 1.307329618516931,
      "grad_norm": 0.4283030033111572,
      "learning_rate": 1.8256893841977427e-05,
      "loss": 0.8025,
      "step": 6100
    },
    {
      "epoch": 1.309472781825975,
      "grad_norm": 0.41949477791786194,
      "learning_rate": 1.8254036290898704e-05,
      "loss": 0.1649,
      "step": 6110
    },
    {
      "epoch": 1.3116159451350193,
      "grad_norm": 0.40191707015037537,
      "learning_rate": 1.8251178739819974e-05,
      "loss": 0.9539,
      "step": 6120
    },
    {
      "epoch": 1.3137591084440634,
      "grad_norm": 0.3655376434326172,
      "learning_rate": 1.824832118874125e-05,
      "loss": 0.4946,
      "step": 6130
    },
    {
      "epoch": 1.3159022717531075,
      "grad_norm": 0.35943394899368286,
      "learning_rate": 1.8245463637662522e-05,
      "loss": 0.8237,
      "step": 6140
    },
    {
      "epoch": 1.3180454350621518,
      "grad_norm": 0.3902145326137543,
      "learning_rate": 1.82426060865838e-05,
      "loss": 0.169,
      "step": 6150
    },
    {
      "epoch": 1.3201885983711958,
      "grad_norm": 15.589881896972656,
      "learning_rate": 1.8239748535505073e-05,
      "loss": 0.4982,
      "step": 6160
    },
    {
      "epoch": 1.32233176168024,
      "grad_norm": 0.3129041790962219,
      "learning_rate": 1.8236890984426347e-05,
      "loss": 0.0073,
      "step": 6170
    },
    {
      "epoch": 1.3244749249892842,
      "grad_norm": 0.231546089053154,
      "learning_rate": 1.823403343334762e-05,
      "loss": 0.188,
      "step": 6180
    },
    {
      "epoch": 1.3266180882983283,
      "grad_norm": 0.34910982847213745,
      "learning_rate": 1.8231175882268895e-05,
      "loss": 0.8736,
      "step": 6190
    },
    {
      "epoch": 1.3287612516073724,
      "grad_norm": 15.935295104980469,
      "learning_rate": 1.8228318331190173e-05,
      "loss": 0.6944,
      "step": 6200
    },
    {
      "epoch": 1.3309044149164166,
      "grad_norm": 0.29048952460289,
      "learning_rate": 1.8225460780111447e-05,
      "loss": 0.3341,
      "step": 6210
    },
    {
      "epoch": 1.3330475782254607,
      "grad_norm": 0.3511234223842621,
      "learning_rate": 1.822260322903272e-05,
      "loss": 0.8479,
      "step": 6220
    },
    {
      "epoch": 1.335190741534505,
      "grad_norm": 0.37706321477890015,
      "learning_rate": 1.8219745677953994e-05,
      "loss": 0.8195,
      "step": 6230
    },
    {
      "epoch": 1.337333904843549,
      "grad_norm": 0.34168460965156555,
      "learning_rate": 1.8216888126875268e-05,
      "loss": 0.4943,
      "step": 6240
    },
    {
      "epoch": 1.3394770681525932,
      "grad_norm": 0.3713662028312683,
      "learning_rate": 1.8214030575796546e-05,
      "loss": 0.7987,
      "step": 6250
    },
    {
      "epoch": 1.3416202314616374,
      "grad_norm": 0.4996476173400879,
      "learning_rate": 1.821117302471782e-05,
      "loss": 0.6317,
      "step": 6260
    },
    {
      "epoch": 1.3437633947706815,
      "grad_norm": 0.5122207999229431,
      "learning_rate": 1.8208315473639093e-05,
      "loss": 0.1685,
      "step": 6270
    },
    {
      "epoch": 1.3459065580797258,
      "grad_norm": 0.3274134695529938,
      "learning_rate": 1.8205457922560367e-05,
      "loss": 0.491,
      "step": 6280
    },
    {
      "epoch": 1.3480497213887699,
      "grad_norm": 0.25383707880973816,
      "learning_rate": 1.820260037148164e-05,
      "loss": 0.1714,
      "step": 6290
    },
    {
      "epoch": 1.350192884697814,
      "grad_norm": 0.24858470261096954,
      "learning_rate": 1.8199742820402915e-05,
      "loss": 0.3776,
      "step": 6300
    },
    {
      "epoch": 1.3523360480068582,
      "grad_norm": 0.20369142293930054,
      "learning_rate": 1.8196885269324193e-05,
      "loss": 0.1787,
      "step": 6310
    },
    {
      "epoch": 1.3544792113159023,
      "grad_norm": 0.24790121614933014,
      "learning_rate": 1.8194027718245467e-05,
      "loss": 0.8974,
      "step": 6320
    },
    {
      "epoch": 1.3566223746249464,
      "grad_norm": 0.31655633449554443,
      "learning_rate": 1.819117016716674e-05,
      "loss": 0.3642,
      "step": 6330
    },
    {
      "epoch": 1.3587655379339907,
      "grad_norm": 0.29057279229164124,
      "learning_rate": 1.8188312616088014e-05,
      "loss": 0.8817,
      "step": 6340
    },
    {
      "epoch": 1.3609087012430348,
      "grad_norm": 15.121158599853516,
      "learning_rate": 1.8185455065009288e-05,
      "loss": 0.675,
      "step": 6350
    },
    {
      "epoch": 1.3630518645520788,
      "grad_norm": 0.44533005356788635,
      "learning_rate": 1.8182597513930562e-05,
      "loss": 0.818,
      "step": 6360
    },
    {
      "epoch": 1.3651950278611231,
      "grad_norm": 15.905738830566406,
      "learning_rate": 1.8179739962851836e-05,
      "loss": 0.6282,
      "step": 6370
    },
    {
      "epoch": 1.3673381911701672,
      "grad_norm": 0.48076435923576355,
      "learning_rate": 1.817688241177311e-05,
      "loss": 1.0579,
      "step": 6380
    },
    {
      "epoch": 1.3694813544792113,
      "grad_norm": 0.6621367335319519,
      "learning_rate": 1.8174024860694387e-05,
      "loss": 0.3095,
      "step": 6390
    },
    {
      "epoch": 1.3716245177882556,
      "grad_norm": 0.5110146999359131,
      "learning_rate": 1.817116730961566e-05,
      "loss": 0.4486,
      "step": 6400
    },
    {
      "epoch": 1.3737676810972996,
      "grad_norm": 0.4762279689311981,
      "learning_rate": 1.8168309758536935e-05,
      "loss": 0.9006,
      "step": 6410
    },
    {
      "epoch": 1.3759108444063437,
      "grad_norm": 16.625831604003906,
      "learning_rate": 1.816545220745821e-05,
      "loss": 0.4647,
      "step": 6420
    },
    {
      "epoch": 1.378054007715388,
      "grad_norm": 0.6051499247550964,
      "learning_rate": 1.8162594656379483e-05,
      "loss": 0.6362,
      "step": 6430
    },
    {
      "epoch": 1.380197171024432,
      "grad_norm": 14.904462814331055,
      "learning_rate": 1.815973710530076e-05,
      "loss": 0.4678,
      "step": 6440
    },
    {
      "epoch": 1.3823403343334761,
      "grad_norm": 0.48887473344802856,
      "learning_rate": 1.8156879554222034e-05,
      "loss": 0.4665,
      "step": 6450
    },
    {
      "epoch": 1.3844834976425204,
      "grad_norm": 0.46118229627609253,
      "learning_rate": 1.8154022003143308e-05,
      "loss": 0.7811,
      "step": 6460
    },
    {
      "epoch": 1.3866266609515645,
      "grad_norm": 15.70207691192627,
      "learning_rate": 1.8151164452064582e-05,
      "loss": 0.7764,
      "step": 6470
    },
    {
      "epoch": 1.3887698242606086,
      "grad_norm": 0.4558967351913452,
      "learning_rate": 1.8148306900985856e-05,
      "loss": 0.4741,
      "step": 6480
    },
    {
      "epoch": 1.3909129875696529,
      "grad_norm": 0.39894214272499084,
      "learning_rate": 1.814544934990713e-05,
      "loss": 0.01,
      "step": 6490
    },
    {
      "epoch": 1.393056150878697,
      "grad_norm": 16.677419662475586,
      "learning_rate": 1.8142591798828407e-05,
      "loss": 0.6578,
      "step": 6500
    },
    {
      "epoch": 1.395199314187741,
      "grad_norm": 0.37102261185646057,
      "learning_rate": 1.813973424774968e-05,
      "loss": 0.9801,
      "step": 6510
    },
    {
      "epoch": 1.3973424774967853,
      "grad_norm": 15.642964363098145,
      "learning_rate": 1.8136876696670955e-05,
      "loss": 0.788,
      "step": 6520
    },
    {
      "epoch": 1.3994856408058294,
      "grad_norm": 0.5607868432998657,
      "learning_rate": 1.813401914559223e-05,
      "loss": 0.3173,
      "step": 6530
    },
    {
      "epoch": 1.4016288041148735,
      "grad_norm": 0.4060143828392029,
      "learning_rate": 1.8131161594513503e-05,
      "loss": 0.171,
      "step": 6540
    },
    {
      "epoch": 1.4037719674239177,
      "grad_norm": 0.319153368473053,
      "learning_rate": 1.8128304043434777e-05,
      "loss": 0.4993,
      "step": 6550
    },
    {
      "epoch": 1.4059151307329618,
      "grad_norm": 0.27929508686065674,
      "learning_rate": 1.812544649235605e-05,
      "loss": 0.5151,
      "step": 6560
    },
    {
      "epoch": 1.4080582940420059,
      "grad_norm": 0.2523656189441681,
      "learning_rate": 1.8122588941277325e-05,
      "loss": 0.6956,
      "step": 6570
    },
    {
      "epoch": 1.4102014573510502,
      "grad_norm": 0.2727937400341034,
      "learning_rate": 1.8119731390198602e-05,
      "loss": 0.1755,
      "step": 6580
    },
    {
      "epoch": 1.4123446206600943,
      "grad_norm": 0.3005635440349579,
      "learning_rate": 1.8116873839119876e-05,
      "loss": 0.8553,
      "step": 6590
    },
    {
      "epoch": 1.4144877839691383,
      "grad_norm": 0.416594922542572,
      "learning_rate": 1.811401628804115e-05,
      "loss": 0.5046,
      "step": 6600
    },
    {
      "epoch": 1.4166309472781826,
      "grad_norm": 15.972086906433105,
      "learning_rate": 1.8111158736962424e-05,
      "loss": 0.8133,
      "step": 6610
    },
    {
      "epoch": 1.4187741105872267,
      "grad_norm": 0.4340132176876068,
      "learning_rate": 1.8108301185883698e-05,
      "loss": 0.6396,
      "step": 6620
    },
    {
      "epoch": 1.4209172738962708,
      "grad_norm": 31.304306030273438,
      "learning_rate": 1.8105443634804972e-05,
      "loss": 0.931,
      "step": 6630
    },
    {
      "epoch": 1.423060437205315,
      "grad_norm": 16.17920684814453,
      "learning_rate": 1.810258608372625e-05,
      "loss": 0.8806,
      "step": 6640
    },
    {
      "epoch": 1.4252036005143591,
      "grad_norm": 14.784770011901855,
      "learning_rate": 1.8099728532647523e-05,
      "loss": 0.4191,
      "step": 6650
    },
    {
      "epoch": 1.4273467638234034,
      "grad_norm": 0.5269368290901184,
      "learning_rate": 1.8096870981568797e-05,
      "loss": 0.1592,
      "step": 6660
    },
    {
      "epoch": 1.4294899271324475,
      "grad_norm": 0.4881483018398285,
      "learning_rate": 1.809401343049007e-05,
      "loss": 0.6066,
      "step": 6670
    },
    {
      "epoch": 1.4316330904414916,
      "grad_norm": 15.541069984436035,
      "learning_rate": 1.8091155879411345e-05,
      "loss": 0.3169,
      "step": 6680
    },
    {
      "epoch": 1.4337762537505359,
      "grad_norm": 0.3373526632785797,
      "learning_rate": 1.8088298328332622e-05,
      "loss": 0.008,
      "step": 6690
    },
    {
      "epoch": 1.43591941705958,
      "grad_norm": 15.672778129577637,
      "learning_rate": 1.8085440777253896e-05,
      "loss": 0.856,
      "step": 6700
    },
    {
      "epoch": 1.4380625803686242,
      "grad_norm": 0.32788124680519104,
      "learning_rate": 1.808258322617517e-05,
      "loss": 0.3325,
      "step": 6710
    },
    {
      "epoch": 1.4402057436776683,
      "grad_norm": 15.5863037109375,
      "learning_rate": 1.8079725675096444e-05,
      "loss": 0.3502,
      "step": 6720
    },
    {
      "epoch": 1.4423489069867124,
      "grad_norm": 0.38056907057762146,
      "learning_rate": 1.8076868124017718e-05,
      "loss": 0.6777,
      "step": 6730
    },
    {
      "epoch": 1.4444920702957567,
      "grad_norm": 0.3936500549316406,
      "learning_rate": 1.8074010572938995e-05,
      "loss": 0.5036,
      "step": 6740
    },
    {
      "epoch": 1.4466352336048007,
      "grad_norm": 0.43711239099502563,
      "learning_rate": 1.807115302186027e-05,
      "loss": 0.6458,
      "step": 6750
    },
    {
      "epoch": 1.4487783969138448,
      "grad_norm": 30.531946182250977,
      "learning_rate": 1.8068295470781543e-05,
      "loss": 0.7897,
      "step": 6760
    },
    {
      "epoch": 1.450921560222889,
      "grad_norm": 0.5704796314239502,
      "learning_rate": 1.8065437919702814e-05,
      "loss": 0.619,
      "step": 6770
    },
    {
      "epoch": 1.4530647235319332,
      "grad_norm": 0.6467806696891785,
      "learning_rate": 1.806258036862409e-05,
      "loss": 0.4459,
      "step": 6780
    },
    {
      "epoch": 1.4552078868409772,
      "grad_norm": 0.6337684392929077,
      "learning_rate": 1.8059722817545365e-05,
      "loss": 0.4576,
      "step": 6790
    },
    {
      "epoch": 1.4573510501500215,
      "grad_norm": 0.5388390421867371,
      "learning_rate": 1.805686526646664e-05,
      "loss": 0.6005,
      "step": 6800
    },
    {
      "epoch": 1.4594942134590656,
      "grad_norm": 0.5790397524833679,
      "learning_rate": 1.8054007715387913e-05,
      "loss": 0.9096,
      "step": 6810
    },
    {
      "epoch": 1.4616373767681097,
      "grad_norm": 0.540549635887146,
      "learning_rate": 1.8051150164309187e-05,
      "loss": 0.4572,
      "step": 6820
    },
    {
      "epoch": 1.463780540077154,
      "grad_norm": 0.5784009695053101,
      "learning_rate": 1.8048292613230464e-05,
      "loss": 0.4661,
      "step": 6830
    },
    {
      "epoch": 1.465923703386198,
      "grad_norm": 15.305289268493652,
      "learning_rate": 1.8045435062151738e-05,
      "loss": 1.176,
      "step": 6840
    },
    {
      "epoch": 1.4680668666952421,
      "grad_norm": 0.7830255627632141,
      "learning_rate": 1.8042577511073012e-05,
      "loss": 0.4309,
      "step": 6850
    },
    {
      "epoch": 1.4702100300042864,
      "grad_norm": 0.6328555345535278,
      "learning_rate": 1.8039719959994286e-05,
      "loss": 0.4515,
      "step": 6860
    },
    {
      "epoch": 1.4723531933133305,
      "grad_norm": 0.5852890610694885,
      "learning_rate": 1.803686240891556e-05,
      "loss": 0.4311,
      "step": 6870
    },
    {
      "epoch": 1.4744963566223745,
      "grad_norm": 14.7264404296875,
      "learning_rate": 1.8034004857836837e-05,
      "loss": 0.4689,
      "step": 6880
    },
    {
      "epoch": 1.4766395199314188,
      "grad_norm": 30.373960494995117,
      "learning_rate": 1.803114730675811e-05,
      "loss": 0.6333,
      "step": 6890
    },
    {
      "epoch": 1.478782683240463,
      "grad_norm": 0.3009743392467499,
      "learning_rate": 1.8028289755679385e-05,
      "loss": 0.6541,
      "step": 6900
    },
    {
      "epoch": 1.480925846549507,
      "grad_norm": 0.30693674087524414,
      "learning_rate": 1.802543220460066e-05,
      "loss": 0.1664,
      "step": 6910
    },
    {
      "epoch": 1.4830690098585513,
      "grad_norm": 0.30960503220558167,
      "learning_rate": 1.8022574653521933e-05,
      "loss": 0.8426,
      "step": 6920
    },
    {
      "epoch": 1.4852121731675954,
      "grad_norm": 0.358869731426239,
      "learning_rate": 1.8019717102443207e-05,
      "loss": 0.3329,
      "step": 6930
    },
    {
      "epoch": 1.4873553364766394,
      "grad_norm": 14.903160095214844,
      "learning_rate": 1.8016859551364484e-05,
      "loss": 0.8326,
      "step": 6940
    },
    {
      "epoch": 1.4894984997856837,
      "grad_norm": 15.09700870513916,
      "learning_rate": 1.8014002000285758e-05,
      "loss": 0.3355,
      "step": 6950
    },
    {
      "epoch": 1.4916416630947278,
      "grad_norm": 0.36393532156944275,
      "learning_rate": 1.8011144449207032e-05,
      "loss": 0.825,
      "step": 6960
    },
    {
      "epoch": 1.4937848264037719,
      "grad_norm": 0.3724695146083832,
      "learning_rate": 1.8008286898128306e-05,
      "loss": 0.1636,
      "step": 6970
    },
    {
      "epoch": 1.4959279897128162,
      "grad_norm": 15.242286682128906,
      "learning_rate": 1.800542934704958e-05,
      "loss": 0.6572,
      "step": 6980
    },
    {
      "epoch": 1.4980711530218602,
      "grad_norm": 0.47949492931365967,
      "learning_rate": 1.8002571795970854e-05,
      "loss": 1.4353,
      "step": 6990
    },
    {
      "epoch": 1.5002143163309043,
      "grad_norm": 0.6773558855056763,
      "learning_rate": 1.7999714244892128e-05,
      "loss": 0.738,
      "step": 7000
    },
    {
      "epoch": 1.5023574796399486,
      "grad_norm": 15.174327850341797,
      "learning_rate": 1.79968566938134e-05,
      "loss": 0.832,
      "step": 7010
    },
    {
      "epoch": 1.5045006429489927,
      "grad_norm": 0.8296085000038147,
      "learning_rate": 1.799399914273468e-05,
      "loss": 0.1496,
      "step": 7020
    },
    {
      "epoch": 1.5066438062580367,
      "grad_norm": 15.180862426757812,
      "learning_rate": 1.7991141591655953e-05,
      "loss": 0.8572,
      "step": 7030
    },
    {
      "epoch": 1.508786969567081,
      "grad_norm": 0.905724287033081,
      "learning_rate": 1.7988284040577227e-05,
      "loss": 0.8293,
      "step": 7040
    },
    {
      "epoch": 1.5109301328761253,
      "grad_norm": 29.103092193603516,
      "learning_rate": 1.79854264894985e-05,
      "loss": 1.0339,
      "step": 7050
    },
    {
      "epoch": 1.5130732961851692,
      "grad_norm": 14.32369327545166,
      "learning_rate": 1.7982568938419774e-05,
      "loss": 0.9104,
      "step": 7060
    },
    {
      "epoch": 1.5152164594942135,
      "grad_norm": 14.972249984741211,
      "learning_rate": 1.797971138734105e-05,
      "loss": 0.5015,
      "step": 7070
    },
    {
      "epoch": 1.5173596228032578,
      "grad_norm": 0.7143323421478271,
      "learning_rate": 1.7976853836262326e-05,
      "loss": 0.7821,
      "step": 7080
    },
    {
      "epoch": 1.5195027861123016,
      "grad_norm": 14.635100364685059,
      "learning_rate": 1.79739962851836e-05,
      "loss": 0.8845,
      "step": 7090
    },
    {
      "epoch": 1.521645949421346,
      "grad_norm": 0.8965330123901367,
      "learning_rate": 1.7971138734104874e-05,
      "loss": 0.6459,
      "step": 7100
    },
    {
      "epoch": 1.5237891127303902,
      "grad_norm": 0.9741208553314209,
      "learning_rate": 1.7968281183026147e-05,
      "loss": 0.7853,
      "step": 7110
    },
    {
      "epoch": 1.525932276039434,
      "grad_norm": 0.7330318689346313,
      "learning_rate": 1.796542363194742e-05,
      "loss": 0.4127,
      "step": 7120
    },
    {
      "epoch": 1.5280754393484783,
      "grad_norm": 0.5836857557296753,
      "learning_rate": 1.79625660808687e-05,
      "loss": 0.5966,
      "step": 7130
    },
    {
      "epoch": 1.5302186026575226,
      "grad_norm": 0.5038506984710693,
      "learning_rate": 1.7959708529789973e-05,
      "loss": 0.4408,
      "step": 7140
    },
    {
      "epoch": 1.5323617659665667,
      "grad_norm": 0.3556772768497467,
      "learning_rate": 1.7956850978711247e-05,
      "loss": 0.1635,
      "step": 7150
    },
    {
      "epoch": 1.5345049292756108,
      "grad_norm": 0.3185029923915863,
      "learning_rate": 1.795399342763252e-05,
      "loss": 0.3244,
      "step": 7160
    },
    {
      "epoch": 1.536648092584655,
      "grad_norm": 0.1867625117301941,
      "learning_rate": 1.7951135876553794e-05,
      "loss": 0.179,
      "step": 7170
    },
    {
      "epoch": 1.5387912558936991,
      "grad_norm": 0.17577248811721802,
      "learning_rate": 1.7948278325475072e-05,
      "loss": 0.569,
      "step": 7180
    },
    {
      "epoch": 1.5409344192027432,
      "grad_norm": 0.2439918965101242,
      "learning_rate": 1.7945420774396346e-05,
      "loss": 1.0331,
      "step": 7190
    },
    {
      "epoch": 1.5430775825117875,
      "grad_norm": 0.2803245484828949,
      "learning_rate": 1.7942563223317616e-05,
      "loss": 0.5143,
      "step": 7200
    },
    {
      "epoch": 1.5452207458208316,
      "grad_norm": 0.4702194929122925,
      "learning_rate": 1.793970567223889e-05,
      "loss": 0.6793,
      "step": 7210
    },
    {
      "epoch": 1.5473639091298756,
      "grad_norm": 0.5591081976890564,
      "learning_rate": 1.7936848121160167e-05,
      "loss": 0.4708,
      "step": 7220
    },
    {
      "epoch": 1.54950707243892,
      "grad_norm": 0.45051851868629456,
      "learning_rate": 1.793399057008144e-05,
      "loss": 0.4696,
      "step": 7230
    },
    {
      "epoch": 1.551650235747964,
      "grad_norm": 0.3364163339138031,
      "learning_rate": 1.7931133019002715e-05,
      "loss": 0.6358,
      "step": 7240
    },
    {
      "epoch": 1.553793399057008,
      "grad_norm": 0.44303297996520996,
      "learning_rate": 1.792827546792399e-05,
      "loss": 1.1256,
      "step": 7250
    },
    {
      "epoch": 1.5559365623660524,
      "grad_norm": 0.343362033367157,
      "learning_rate": 1.7925417916845263e-05,
      "loss": 0.3307,
      "step": 7260
    },
    {
      "epoch": 1.5580797256750964,
      "grad_norm": 0.3914550840854645,
      "learning_rate": 1.792256036576654e-05,
      "loss": 0.3546,
      "step": 7270
    },
    {
      "epoch": 1.5602228889841405,
      "grad_norm": 15.488643646240234,
      "learning_rate": 1.7919702814687814e-05,
      "loss": 0.4998,
      "step": 7280
    },
    {
      "epoch": 1.5623660522931848,
      "grad_norm": 0.29767176508903503,
      "learning_rate": 1.791684526360909e-05,
      "loss": 0.336,
      "step": 7290
    },
    {
      "epoch": 1.5645092156022289,
      "grad_norm": 0.28157246112823486,
      "learning_rate": 1.7913987712530362e-05,
      "loss": 0.3391,
      "step": 7300
    },
    {
      "epoch": 1.566652378911273,
      "grad_norm": 0.3287455141544342,
      "learning_rate": 1.7911130161451636e-05,
      "loss": 1.0103,
      "step": 7310
    },
    {
      "epoch": 1.5687955422203173,
      "grad_norm": 16.607166290283203,
      "learning_rate": 1.7908272610372914e-05,
      "loss": 0.4842,
      "step": 7320
    },
    {
      "epoch": 1.5709387055293613,
      "grad_norm": 0.4803384244441986,
      "learning_rate": 1.7905415059294187e-05,
      "loss": 0.7943,
      "step": 7330
    },
    {
      "epoch": 1.5730818688384054,
      "grad_norm": 0.5648708939552307,
      "learning_rate": 1.790255750821546e-05,
      "loss": 0.3338,
      "step": 7340
    },
    {
      "epoch": 1.5752250321474497,
      "grad_norm": 29.28932762145996,
      "learning_rate": 1.7899699957136735e-05,
      "loss": 0.8001,
      "step": 7350
    },
    {
      "epoch": 1.5773681954564938,
      "grad_norm": 0.5017217993736267,
      "learning_rate": 1.789684240605801e-05,
      "loss": 1.2592,
      "step": 7360
    },
    {
      "epoch": 1.5795113587655378,
      "grad_norm": 0.4779583215713501,
      "learning_rate": 1.7893984854979287e-05,
      "loss": 0.1624,
      "step": 7370
    },
    {
      "epoch": 1.5816545220745821,
      "grad_norm": 0.36251336336135864,
      "learning_rate": 1.789112730390056e-05,
      "loss": 0.46,
      "step": 7380
    },
    {
      "epoch": 1.5837976853836262,
      "grad_norm": 0.4576554298400879,
      "learning_rate": 1.7888269752821834e-05,
      "loss": 0.6409,
      "step": 7390
    },
    {
      "epoch": 1.5859408486926703,
      "grad_norm": 14.989091873168945,
      "learning_rate": 1.788541220174311e-05,
      "loss": 0.6537,
      "step": 7400
    },
    {
      "epoch": 1.5880840120017146,
      "grad_norm": 29.494020462036133,
      "learning_rate": 1.7882554650664382e-05,
      "loss": 1.2088,
      "step": 7410
    },
    {
      "epoch": 1.5902271753107586,
      "grad_norm": 0.6952443718910217,
      "learning_rate": 1.7879697099585656e-05,
      "loss": 0.7202,
      "step": 7420
    },
    {
      "epoch": 1.5923703386198027,
      "grad_norm": 0.6394216418266296,
      "learning_rate": 1.787683954850693e-05,
      "loss": 0.4356,
      "step": 7430
    },
    {
      "epoch": 1.594513501928847,
      "grad_norm": 0.7105109691619873,
      "learning_rate": 1.7873981997428204e-05,
      "loss": 0.8823,
      "step": 7440
    },
    {
      "epoch": 1.5966566652378913,
      "grad_norm": 0.7053595185279846,
      "learning_rate": 1.7871124446349478e-05,
      "loss": 0.4389,
      "step": 7450
    },
    {
      "epoch": 1.5987998285469351,
      "grad_norm": 0.5807654857635498,
      "learning_rate": 1.7868266895270755e-05,
      "loss": 0.7414,
      "step": 7460
    },
    {
      "epoch": 1.6009429918559794,
      "grad_norm": 0.5132867693901062,
      "learning_rate": 1.786540934419203e-05,
      "loss": 0.4641,
      "step": 7470
    },
    {
      "epoch": 1.6030861551650237,
      "grad_norm": 0.491403192281723,
      "learning_rate": 1.7862551793113303e-05,
      "loss": 0.6065,
      "step": 7480
    },
    {
      "epoch": 1.6052293184740676,
      "grad_norm": 15.924878120422363,
      "learning_rate": 1.7859694242034577e-05,
      "loss": 0.616,
      "step": 7490
    },
    {
      "epoch": 1.6073724817831119,
      "grad_norm": 0.5522511601448059,
      "learning_rate": 1.785683669095585e-05,
      "loss": 0.4649,
      "step": 7500
    },
    {
      "epoch": 1.6095156450921562,
      "grad_norm": 14.852587699890137,
      "learning_rate": 1.785397913987713e-05,
      "loss": 0.6102,
      "step": 7510
    },
    {
      "epoch": 1.6116588084012,
      "grad_norm": 29.730451583862305,
      "learning_rate": 1.7851121588798402e-05,
      "loss": 0.4648,
      "step": 7520
    },
    {
      "epoch": 1.6138019717102443,
      "grad_norm": 14.936938285827637,
      "learning_rate": 1.7848264037719676e-05,
      "loss": 0.4636,
      "step": 7530
    },
    {
      "epoch": 1.6159451350192886,
      "grad_norm": 29.65781593322754,
      "learning_rate": 1.784540648664095e-05,
      "loss": 0.7879,
      "step": 7540
    },
    {
      "epoch": 1.6180882983283325,
      "grad_norm": 0.4772883951663971,
      "learning_rate": 1.7842548935562224e-05,
      "loss": 0.4701,
      "step": 7550
    },
    {
      "epoch": 1.6202314616373767,
      "grad_norm": 0.41523364186286926,
      "learning_rate": 1.7839691384483498e-05,
      "loss": 0.4815,
      "step": 7560
    },
    {
      "epoch": 1.622374624946421,
      "grad_norm": 0.42208170890808105,
      "learning_rate": 1.7836833833404775e-05,
      "loss": 0.4683,
      "step": 7570
    },
    {
      "epoch": 1.6245177882554651,
      "grad_norm": 15.182202339172363,
      "learning_rate": 1.783397628232605e-05,
      "loss": 0.943,
      "step": 7580
    },
    {
      "epoch": 1.6266609515645092,
      "grad_norm": 0.5276702046394348,
      "learning_rate": 1.7831118731247323e-05,
      "loss": 0.4708,
      "step": 7590
    },
    {
      "epoch": 1.6288041148735535,
      "grad_norm": 29.704145431518555,
      "learning_rate": 1.7828261180168597e-05,
      "loss": 0.6197,
      "step": 7600
    },
    {
      "epoch": 1.6309472781825975,
      "grad_norm": 0.39695799350738525,
      "learning_rate": 1.782540362908987e-05,
      "loss": 0.321,
      "step": 7610
    },
    {
      "epoch": 1.6330904414916416,
      "grad_norm": 0.36541277170181274,
      "learning_rate": 1.782254607801115e-05,
      "loss": 0.4697,
      "step": 7620
    },
    {
      "epoch": 1.635233604800686,
      "grad_norm": 15.245000839233398,
      "learning_rate": 1.781968852693242e-05,
      "loss": 0.487,
      "step": 7630
    },
    {
      "epoch": 1.63737676810973,
      "grad_norm": 0.4247564375400543,
      "learning_rate": 1.7816830975853693e-05,
      "loss": 0.6417,
      "step": 7640
    },
    {
      "epoch": 1.639519931418774,
      "grad_norm": 14.951476097106934,
      "learning_rate": 1.781397342477497e-05,
      "loss": 0.9568,
      "step": 7650
    },
    {
      "epoch": 1.6416630947278184,
      "grad_norm": 14.905744552612305,
      "learning_rate": 1.7811115873696244e-05,
      "loss": 0.9413,
      "step": 7660
    },
    {
      "epoch": 1.6438062580368624,
      "grad_norm": 14.844386100769043,
      "learning_rate": 1.7808258322617518e-05,
      "loss": 0.8721,
      "step": 7670
    },
    {
      "epoch": 1.6459494213459065,
      "grad_norm": 0.7834233641624451,
      "learning_rate": 1.7805400771538792e-05,
      "loss": 0.2883,
      "step": 7680
    },
    {
      "epoch": 1.6480925846549508,
      "grad_norm": 0.6259122490882874,
      "learning_rate": 1.7802543220460066e-05,
      "loss": 0.4428,
      "step": 7690
    },
    {
      "epoch": 1.6502357479639949,
      "grad_norm": 14.675957679748535,
      "learning_rate": 1.779968566938134e-05,
      "loss": 0.4605,
      "step": 7700
    },
    {
      "epoch": 1.652378911273039,
      "grad_norm": 0.551095187664032,
      "learning_rate": 1.7796828118302617e-05,
      "loss": 0.7572,
      "step": 7710
    },
    {
      "epoch": 1.6545220745820832,
      "grad_norm": 0.5450117588043213,
      "learning_rate": 1.779397056722389e-05,
      "loss": 0.6035,
      "step": 7720
    },
    {
      "epoch": 1.6566652378911273,
      "grad_norm": 0.5727967023849487,
      "learning_rate": 1.7791113016145165e-05,
      "loss": 0.7214,
      "step": 7730
    },
    {
      "epoch": 1.6588084012001714,
      "grad_norm": 16.212730407714844,
      "learning_rate": 1.778825546506644e-05,
      "loss": 0.3038,
      "step": 7740
    },
    {
      "epoch": 1.6609515645092157,
      "grad_norm": 0.3506193161010742,
      "learning_rate": 1.7785397913987713e-05,
      "loss": 0.6546,
      "step": 7750
    },
    {
      "epoch": 1.6630947278182597,
      "grad_norm": 16.198720932006836,
      "learning_rate": 1.778254036290899e-05,
      "loss": 0.6265,
      "step": 7760
    },
    {
      "epoch": 1.6652378911273038,
      "grad_norm": 0.6125158667564392,
      "learning_rate": 1.7779682811830264e-05,
      "loss": 0.6098,
      "step": 7770
    },
    {
      "epoch": 1.667381054436348,
      "grad_norm": 15.250506401062012,
      "learning_rate": 1.7776825260751538e-05,
      "loss": 0.7433,
      "step": 7780
    },
    {
      "epoch": 1.6695242177453922,
      "grad_norm": 0.7470229864120483,
      "learning_rate": 1.7773967709672812e-05,
      "loss": 0.7162,
      "step": 7790
    },
    {
      "epoch": 1.6716673810544362,
      "grad_norm": 0.6897099018096924,
      "learning_rate": 1.7771110158594086e-05,
      "loss": 0.2868,
      "step": 7800
    },
    {
      "epoch": 1.6738105443634805,
      "grad_norm": 0.5200850963592529,
      "learning_rate": 1.7768252607515363e-05,
      "loss": 0.7447,
      "step": 7810
    },
    {
      "epoch": 1.6759537076725246,
      "grad_norm": 0.4924495220184326,
      "learning_rate": 1.7765395056436637e-05,
      "loss": 0.4687,
      "step": 7820
    },
    {
      "epoch": 1.6780968709815687,
      "grad_norm": 0.40992045402526855,
      "learning_rate": 1.776253750535791e-05,
      "loss": 0.4658,
      "step": 7830
    },
    {
      "epoch": 1.680240034290613,
      "grad_norm": 0.33914944529533386,
      "learning_rate": 1.775967995427918e-05,
      "loss": 0.0083,
      "step": 7840
    },
    {
      "epoch": 1.682383197599657,
      "grad_norm": 0.2968120574951172,
      "learning_rate": 1.775682240320046e-05,
      "loss": 0.514,
      "step": 7850
    },
    {
      "epoch": 1.6845263609087011,
      "grad_norm": 0.32693007588386536,
      "learning_rate": 1.7753964852121733e-05,
      "loss": 0.9974,
      "step": 7860
    },
    {
      "epoch": 1.6866695242177454,
      "grad_norm": 0.4149627685546875,
      "learning_rate": 1.7751107301043007e-05,
      "loss": 0.8133,
      "step": 7870
    },
    {
      "epoch": 1.6888126875267897,
      "grad_norm": 0.41131067276000977,
      "learning_rate": 1.774824974996428e-05,
      "loss": 0.4639,
      "step": 7880
    },
    {
      "epoch": 1.6909558508358336,
      "grad_norm": 0.404238760471344,
      "learning_rate": 1.7745392198885555e-05,
      "loss": 0.6291,
      "step": 7890
    },
    {
      "epoch": 1.6930990141448778,
      "grad_norm": 15.567281723022461,
      "learning_rate": 1.7742534647806832e-05,
      "loss": 0.3305,
      "step": 7900
    },
    {
      "epoch": 1.6952421774539221,
      "grad_norm": 0.413442462682724,
      "learning_rate": 1.7739677096728106e-05,
      "loss": 0.4852,
      "step": 7910
    },
    {
      "epoch": 1.697385340762966,
      "grad_norm": 0.25747838616371155,
      "learning_rate": 1.773681954564938e-05,
      "loss": 0.178,
      "step": 7920
    },
    {
      "epoch": 1.6995285040720103,
      "grad_norm": 15.206215858459473,
      "learning_rate": 1.7733961994570654e-05,
      "loss": 0.5396,
      "step": 7930
    },
    {
      "epoch": 1.7016716673810546,
      "grad_norm": 15.011079788208008,
      "learning_rate": 1.7731104443491928e-05,
      "loss": 0.6824,
      "step": 7940
    },
    {
      "epoch": 1.7038148306900984,
      "grad_norm": 16.372909545898438,
      "learning_rate": 1.7728246892413205e-05,
      "loss": 0.1693,
      "step": 7950
    },
    {
      "epoch": 1.7059579939991427,
      "grad_norm": 15.705330848693848,
      "learning_rate": 1.772538934133448e-05,
      "loss": 0.9944,
      "step": 7960
    },
    {
      "epoch": 1.708101157308187,
      "grad_norm": 0.38995361328125,
      "learning_rate": 1.7722531790255753e-05,
      "loss": 0.4892,
      "step": 7970
    },
    {
      "epoch": 1.710244320617231,
      "grad_norm": 0.5377277135848999,
      "learning_rate": 1.7719674239177027e-05,
      "loss": 0.6328,
      "step": 7980
    },
    {
      "epoch": 1.7123874839262752,
      "grad_norm": 0.4203226864337921,
      "learning_rate": 1.77168166880983e-05,
      "loss": 0.0104,
      "step": 7990
    },
    {
      "epoch": 1.7145306472353194,
      "grad_norm": 15.524290084838867,
      "learning_rate": 1.7713959137019578e-05,
      "loss": 0.6468,
      "step": 8000
    },
    {
      "epoch": 1.7166738105443635,
      "grad_norm": 0.4011126160621643,
      "learning_rate": 1.7711101585940852e-05,
      "loss": 0.9836,
      "step": 8010
    },
    {
      "epoch": 1.7188169738534076,
      "grad_norm": 0.4410516023635864,
      "learning_rate": 1.7708244034862126e-05,
      "loss": 0.4861,
      "step": 8020
    },
    {
      "epoch": 1.7209601371624519,
      "grad_norm": 0.29230621457099915,
      "learning_rate": 1.77053864837834e-05,
      "loss": 0.0087,
      "step": 8030
    },
    {
      "epoch": 1.723103300471496,
      "grad_norm": 46.06523132324219,
      "learning_rate": 1.7702528932704674e-05,
      "loss": 1.3448,
      "step": 8040
    },
    {
      "epoch": 1.72524646378054,
      "grad_norm": 0.40437984466552734,
      "learning_rate": 1.7699671381625948e-05,
      "loss": 1.1509,
      "step": 8050
    },
    {
      "epoch": 1.7273896270895843,
      "grad_norm": 14.648015022277832,
      "learning_rate": 1.769681383054722e-05,
      "loss": 0.4783,
      "step": 8060
    },
    {
      "epoch": 1.7295327903986284,
      "grad_norm": 0.4711858928203583,
      "learning_rate": 1.7693956279468495e-05,
      "loss": 0.6244,
      "step": 8070
    },
    {
      "epoch": 1.7316759537076725,
      "grad_norm": 0.44286879897117615,
      "learning_rate": 1.769109872838977e-05,
      "loss": 0.7805,
      "step": 8080
    },
    {
      "epoch": 1.7338191170167168,
      "grad_norm": 0.537659227848053,
      "learning_rate": 1.7688241177311047e-05,
      "loss": 1.0647,
      "step": 8090
    },
    {
      "epoch": 1.7359622803257608,
      "grad_norm": 0.5732101798057556,
      "learning_rate": 1.768538362623232e-05,
      "loss": 0.7527,
      "step": 8100
    },
    {
      "epoch": 1.738105443634805,
      "grad_norm": 15.378396034240723,
      "learning_rate": 1.7682526075153595e-05,
      "loss": 0.5952,
      "step": 8110
    },
    {
      "epoch": 1.7402486069438492,
      "grad_norm": 0.5560799241065979,
      "learning_rate": 1.767966852407487e-05,
      "loss": 0.3068,
      "step": 8120
    },
    {
      "epoch": 1.7423917702528933,
      "grad_norm": 14.762441635131836,
      "learning_rate": 1.7676810972996142e-05,
      "loss": 0.6269,
      "step": 8130
    },
    {
      "epoch": 1.7445349335619373,
      "grad_norm": 14.950118064880371,
      "learning_rate": 1.767395342191742e-05,
      "loss": 0.6348,
      "step": 8140
    },
    {
      "epoch": 1.7466780968709816,
      "grad_norm": 15.270151138305664,
      "learning_rate": 1.7671095870838694e-05,
      "loss": 0.4673,
      "step": 8150
    },
    {
      "epoch": 1.7488212601800257,
      "grad_norm": 14.977802276611328,
      "learning_rate": 1.7668238319759968e-05,
      "loss": 1.0659,
      "step": 8160
    },
    {
      "epoch": 1.7509644234890698,
      "grad_norm": 0.6196393966674805,
      "learning_rate": 1.766538076868124e-05,
      "loss": 0.1556,
      "step": 8170
    },
    {
      "epoch": 1.753107586798114,
      "grad_norm": 0.4820650815963745,
      "learning_rate": 1.7662523217602515e-05,
      "loss": 0.1562,
      "step": 8180
    },
    {
      "epoch": 1.7552507501071581,
      "grad_norm": 0.3701944649219513,
      "learning_rate": 1.765966566652379e-05,
      "loss": 0.3257,
      "step": 8190
    },
    {
      "epoch": 1.7573939134162022,
      "grad_norm": 0.35525190830230713,
      "learning_rate": 1.7656808115445067e-05,
      "loss": 0.6679,
      "step": 8200
    },
    {
      "epoch": 1.7595370767252465,
      "grad_norm": 0.33372431993484497,
      "learning_rate": 1.765395056436634e-05,
      "loss": 0.8345,
      "step": 8210
    },
    {
      "epoch": 1.7616802400342906,
      "grad_norm": 16.343412399291992,
      "learning_rate": 1.7651093013287615e-05,
      "loss": 0.3328,
      "step": 8220
    },
    {
      "epoch": 1.7638234033433347,
      "grad_norm": 14.877974510192871,
      "learning_rate": 1.764823546220889e-05,
      "loss": 1.1371,
      "step": 8230
    },
    {
      "epoch": 1.765966566652379,
      "grad_norm": 0.46270814538002014,
      "learning_rate": 1.7645377911130162e-05,
      "loss": 0.6309,
      "step": 8240
    },
    {
      "epoch": 1.768109729961423,
      "grad_norm": 0.5227504372596741,
      "learning_rate": 1.764252036005144e-05,
      "loss": 0.4648,
      "step": 8250
    },
    {
      "epoch": 1.770252893270467,
      "grad_norm": 14.692070960998535,
      "learning_rate": 1.7639662808972714e-05,
      "loss": 0.4716,
      "step": 8260
    },
    {
      "epoch": 1.7723960565795114,
      "grad_norm": 0.4618082344532013,
      "learning_rate": 1.7636805257893984e-05,
      "loss": 0.4682,
      "step": 8270
    },
    {
      "epoch": 1.7745392198885555,
      "grad_norm": 0.46557772159576416,
      "learning_rate": 1.763394770681526e-05,
      "loss": 0.4675,
      "step": 8280
    },
    {
      "epoch": 1.7766823831975995,
      "grad_norm": 0.3866390287876129,
      "learning_rate": 1.7631090155736535e-05,
      "loss": 0.164,
      "step": 8290
    },
    {
      "epoch": 1.7788255465066438,
      "grad_norm": 0.3510986566543579,
      "learning_rate": 1.762823260465781e-05,
      "loss": 0.1719,
      "step": 8300
    },
    {
      "epoch": 1.7809687098156881,
      "grad_norm": 0.3094005584716797,
      "learning_rate": 1.7625375053579083e-05,
      "loss": 0.8344,
      "step": 8310
    },
    {
      "epoch": 1.783111873124732,
      "grad_norm": 0.3002806305885315,
      "learning_rate": 1.7622517502500357e-05,
      "loss": 0.8341,
      "step": 8320
    },
    {
      "epoch": 1.7852550364337763,
      "grad_norm": 0.29913008213043213,
      "learning_rate": 1.761965995142163e-05,
      "loss": 0.5052,
      "step": 8330
    },
    {
      "epoch": 1.7873981997428205,
      "grad_norm": 29.65328598022461,
      "learning_rate": 1.761680240034291e-05,
      "loss": 0.9795,
      "step": 8340
    },
    {
      "epoch": 1.7895413630518644,
      "grad_norm": 0.40911754965782166,
      "learning_rate": 1.7613944849264182e-05,
      "loss": 0.478,
      "step": 8350
    },
    {
      "epoch": 1.7916845263609087,
      "grad_norm": 0.6025404334068298,
      "learning_rate": 1.7611087298185456e-05,
      "loss": 0.6206,
      "step": 8360
    },
    {
      "epoch": 1.793827689669953,
      "grad_norm": 0.4146447777748108,
      "learning_rate": 1.760822974710673e-05,
      "loss": 0.4688,
      "step": 8370
    },
    {
      "epoch": 1.7959708529789968,
      "grad_norm": 0.38179969787597656,
      "learning_rate": 1.7605372196028004e-05,
      "loss": 0.4793,
      "step": 8380
    },
    {
      "epoch": 1.7981140162880411,
      "grad_norm": 0.3904199004173279,
      "learning_rate": 1.760251464494928e-05,
      "loss": 0.9647,
      "step": 8390
    },
    {
      "epoch": 1.8002571795970854,
      "grad_norm": 14.826074600219727,
      "learning_rate": 1.7599657093870555e-05,
      "loss": 1.2777,
      "step": 8400
    },
    {
      "epoch": 1.8024003429061295,
      "grad_norm": 0.48147207498550415,
      "learning_rate": 1.759679954279183e-05,
      "loss": 0.1618,
      "step": 8410
    },
    {
      "epoch": 1.8045435062151736,
      "grad_norm": 14.718733787536621,
      "learning_rate": 1.7593941991713103e-05,
      "loss": 1.0579,
      "step": 8420
    },
    {
      "epoch": 1.8066866695242179,
      "grad_norm": 0.4772450029850006,
      "learning_rate": 1.7591084440634377e-05,
      "loss": 0.0113,
      "step": 8430
    },
    {
      "epoch": 1.808829832833262,
      "grad_norm": 0.3629071116447449,
      "learning_rate": 1.7588226889555654e-05,
      "loss": 0.64,
      "step": 8440
    },
    {
      "epoch": 1.810972996142306,
      "grad_norm": 31.878271102905273,
      "learning_rate": 1.758536933847693e-05,
      "loss": 0.9865,
      "step": 8450
    },
    {
      "epoch": 1.8131161594513503,
      "grad_norm": 14.845821380615234,
      "learning_rate": 1.7582511787398202e-05,
      "loss": 0.8147,
      "step": 8460
    },
    {
      "epoch": 1.8152593227603944,
      "grad_norm": 0.46596068143844604,
      "learning_rate": 1.7579654236319476e-05,
      "loss": 0.959,
      "step": 8470
    },
    {
      "epoch": 1.8174024860694384,
      "grad_norm": 14.631492614746094,
      "learning_rate": 1.757679668524075e-05,
      "loss": 0.6254,
      "step": 8480
    },
    {
      "epoch": 1.8195456493784827,
      "grad_norm": 14.647809982299805,
      "learning_rate": 1.7573939134162024e-05,
      "loss": 0.6129,
      "step": 8490
    },
    {
      "epoch": 1.8216888126875268,
      "grad_norm": 0.5960673093795776,
      "learning_rate": 1.7571081583083298e-05,
      "loss": 0.5937,
      "step": 8500
    },
    {
      "epoch": 1.8238319759965709,
      "grad_norm": 0.6828972101211548,
      "learning_rate": 1.7568224032004572e-05,
      "loss": 0.868,
      "step": 8510
    },
    {
      "epoch": 1.8259751393056152,
      "grad_norm": 0.5643708109855652,
      "learning_rate": 1.7565366480925846e-05,
      "loss": 0.1627,
      "step": 8520
    },
    {
      "epoch": 1.8281183026146592,
      "grad_norm": 15.07699966430664,
      "learning_rate": 1.7562508929847123e-05,
      "loss": 0.6267,
      "step": 8530
    },
    {
      "epoch": 1.8302614659237033,
      "grad_norm": 14.838422775268555,
      "learning_rate": 1.7559651378768397e-05,
      "loss": 0.4887,
      "step": 8540
    },
    {
      "epoch": 1.8324046292327476,
      "grad_norm": 0.45453840494155884,
      "learning_rate": 1.755679382768967e-05,
      "loss": 1.1097,
      "step": 8550
    },
    {
      "epoch": 1.8345477925417917,
      "grad_norm": 14.650635719299316,
      "learning_rate": 1.7553936276610945e-05,
      "loss": 1.3833,
      "step": 8560
    },
    {
      "epoch": 1.8366909558508357,
      "grad_norm": 14.632487297058105,
      "learning_rate": 1.755107872553222e-05,
      "loss": 0.4462,
      "step": 8570
    },
    {
      "epoch": 1.83883411915988,
      "grad_norm": 14.640880584716797,
      "learning_rate": 1.7548221174453496e-05,
      "loss": 0.458,
      "step": 8580
    },
    {
      "epoch": 1.8409772824689241,
      "grad_norm": 0.5174399614334106,
      "learning_rate": 1.754536362337477e-05,
      "loss": 0.1609,
      "step": 8590
    },
    {
      "epoch": 1.8431204457779682,
      "grad_norm": 14.709816932678223,
      "learning_rate": 1.7542506072296044e-05,
      "loss": 0.9297,
      "step": 8600
    },
    {
      "epoch": 1.8452636090870125,
      "grad_norm": 0.573486328125,
      "learning_rate": 1.7539648521217318e-05,
      "loss": 0.6089,
      "step": 8610
    },
    {
      "epoch": 1.8474067723960566,
      "grad_norm": 0.4955589473247528,
      "learning_rate": 1.7536790970138592e-05,
      "loss": 0.3122,
      "step": 8620
    },
    {
      "epoch": 1.8495499357051006,
      "grad_norm": 15.260154724121094,
      "learning_rate": 1.7533933419059866e-05,
      "loss": 0.9282,
      "step": 8630
    },
    {
      "epoch": 1.851693099014145,
      "grad_norm": 15.249765396118164,
      "learning_rate": 1.7531075867981143e-05,
      "loss": 0.1692,
      "step": 8640
    },
    {
      "epoch": 1.853836262323189,
      "grad_norm": 14.83582592010498,
      "learning_rate": 1.7528218316902417e-05,
      "loss": 0.3237,
      "step": 8650
    },
    {
      "epoch": 1.855979425632233,
      "grad_norm": 0.36164772510528564,
      "learning_rate": 1.752536076582369e-05,
      "loss": 0.9924,
      "step": 8660
    },
    {
      "epoch": 1.8581225889412774,
      "grad_norm": 0.3271482586860657,
      "learning_rate": 1.7522503214744965e-05,
      "loss": 0.3275,
      "step": 8670
    },
    {
      "epoch": 1.8602657522503214,
      "grad_norm": 0.2889866232872009,
      "learning_rate": 1.751964566366624e-05,
      "loss": 0.1747,
      "step": 8680
    },
    {
      "epoch": 1.8624089155593655,
      "grad_norm": 0.27166810631752014,
      "learning_rate": 1.7516788112587516e-05,
      "loss": 0.3394,
      "step": 8690
    },
    {
      "epoch": 1.8645520788684098,
      "grad_norm": 14.875713348388672,
      "learning_rate": 1.7513930561508787e-05,
      "loss": 0.519,
      "step": 8700
    },
    {
      "epoch": 1.866695242177454,
      "grad_norm": 14.964813232421875,
      "learning_rate": 1.751107301043006e-05,
      "loss": 1.0193,
      "step": 8710
    },
    {
      "epoch": 1.868838405486498,
      "grad_norm": 0.39826682209968567,
      "learning_rate": 1.7508215459351338e-05,
      "loss": 0.6533,
      "step": 8720
    },
    {
      "epoch": 1.8709815687955422,
      "grad_norm": 0.4649738371372223,
      "learning_rate": 1.7505357908272612e-05,
      "loss": 0.3191,
      "step": 8730
    },
    {
      "epoch": 1.8731247321045865,
      "grad_norm": 14.914368629455566,
      "learning_rate": 1.7502500357193886e-05,
      "loss": 0.485,
      "step": 8740
    },
    {
      "epoch": 1.8752678954136304,
      "grad_norm": 15.215265274047852,
      "learning_rate": 1.749964280611516e-05,
      "loss": 0.6429,
      "step": 8750
    },
    {
      "epoch": 1.8774110587226747,
      "grad_norm": 0.4126446843147278,
      "learning_rate": 1.7496785255036434e-05,
      "loss": 0.335,
      "step": 8760
    },
    {
      "epoch": 1.879554222031719,
      "grad_norm": 0.4714741110801697,
      "learning_rate": 1.7493927703957708e-05,
      "loss": 0.946,
      "step": 8770
    },
    {
      "epoch": 1.8816973853407628,
      "grad_norm": 0.39614659547805786,
      "learning_rate": 1.7491070152878985e-05,
      "loss": 0.3182,
      "step": 8780
    },
    {
      "epoch": 1.883840548649807,
      "grad_norm": 0.40454843640327454,
      "learning_rate": 1.748821260180026e-05,
      "loss": 0.805,
      "step": 8790
    },
    {
      "epoch": 1.8859837119588514,
      "grad_norm": 0.36871322989463806,
      "learning_rate": 1.7485355050721533e-05,
      "loss": 0.4816,
      "step": 8800
    },
    {
      "epoch": 1.8881268752678955,
      "grad_norm": 0.36511164903640747,
      "learning_rate": 1.7482497499642807e-05,
      "loss": 0.1834,
      "step": 8810
    },
    {
      "epoch": 1.8902700385769395,
      "grad_norm": 15.10610580444336,
      "learning_rate": 1.747963994856408e-05,
      "loss": 1.1508,
      "step": 8820
    },
    {
      "epoch": 1.8924132018859838,
      "grad_norm": 0.4639316201210022,
      "learning_rate": 1.7476782397485358e-05,
      "loss": 0.1637,
      "step": 8830
    },
    {
      "epoch": 1.894556365195028,
      "grad_norm": 0.48155465722084045,
      "learning_rate": 1.7473924846406632e-05,
      "loss": 0.7603,
      "step": 8840
    },
    {
      "epoch": 1.896699528504072,
      "grad_norm": 0.5448974967002869,
      "learning_rate": 1.7471067295327906e-05,
      "loss": 0.4573,
      "step": 8850
    },
    {
      "epoch": 1.8988426918131163,
      "grad_norm": 0.4741624593734741,
      "learning_rate": 1.746820974424918e-05,
      "loss": 0.3093,
      "step": 8860
    },
    {
      "epoch": 1.9009858551221603,
      "grad_norm": 0.5122076272964478,
      "learning_rate": 1.7465352193170454e-05,
      "loss": 0.7739,
      "step": 8870
    },
    {
      "epoch": 1.9031290184312044,
      "grad_norm": 0.5088636875152588,
      "learning_rate": 1.746249464209173e-05,
      "loss": 0.7575,
      "step": 8880
    },
    {
      "epoch": 1.9052721817402487,
      "grad_norm": 14.746241569519043,
      "learning_rate": 1.7459637091013005e-05,
      "loss": 0.3173,
      "step": 8890
    },
    {
      "epoch": 1.9074153450492928,
      "grad_norm": 0.4818785786628723,
      "learning_rate": 1.745677953993428e-05,
      "loss": 0.7779,
      "step": 8900
    },
    {
      "epoch": 1.9095585083583368,
      "grad_norm": 0.5393946170806885,
      "learning_rate": 1.745392198885555e-05,
      "loss": 0.9194,
      "step": 8910
    },
    {
      "epoch": 1.9117016716673811,
      "grad_norm": 0.5904912948608398,
      "learning_rate": 1.7451064437776827e-05,
      "loss": 0.6022,
      "step": 8920
    },
    {
      "epoch": 1.9138448349764252,
      "grad_norm": 14.685126304626465,
      "learning_rate": 1.74482068866981e-05,
      "loss": 0.4473,
      "step": 8930
    },
    {
      "epoch": 1.9159879982854693,
      "grad_norm": 0.5211689472198486,
      "learning_rate": 1.7445349335619375e-05,
      "loss": 0.3073,
      "step": 8940
    },
    {
      "epoch": 1.9181311615945136,
      "grad_norm": 0.5255922079086304,
      "learning_rate": 1.744249178454065e-05,
      "loss": 0.4581,
      "step": 8950
    },
    {
      "epoch": 1.9202743249035577,
      "grad_norm": 0.318042516708374,
      "learning_rate": 1.7439634233461922e-05,
      "loss": 0.3345,
      "step": 8960
    },
    {
      "epoch": 1.9224174882126017,
      "grad_norm": 0.26704317331314087,
      "learning_rate": 1.74367766823832e-05,
      "loss": 0.5105,
      "step": 8970
    },
    {
      "epoch": 1.924560651521646,
      "grad_norm": 0.2768119275569916,
      "learning_rate": 1.7433919131304474e-05,
      "loss": 0.8586,
      "step": 8980
    },
    {
      "epoch": 1.92670381483069,
      "grad_norm": 29.828535079956055,
      "learning_rate": 1.7431061580225748e-05,
      "loss": 0.8453,
      "step": 8990
    },
    {
      "epoch": 1.9288469781397342,
      "grad_norm": 15.099980354309082,
      "learning_rate": 1.742820402914702e-05,
      "loss": 0.6513,
      "step": 9000
    },
    {
      "epoch": 1.9309901414487785,
      "grad_norm": 14.791423797607422,
      "learning_rate": 1.7425346478068296e-05,
      "loss": 0.9868,
      "step": 9010
    },
    {
      "epoch": 1.9331333047578225,
      "grad_norm": 0.4845902919769287,
      "learning_rate": 1.7422488926989573e-05,
      "loss": 0.7902,
      "step": 9020
    },
    {
      "epoch": 1.9352764680668666,
      "grad_norm": 0.43267762660980225,
      "learning_rate": 1.7419631375910847e-05,
      "loss": 0.0108,
      "step": 9030
    },
    {
      "epoch": 1.9374196313759109,
      "grad_norm": 0.43490588665008545,
      "learning_rate": 1.741677382483212e-05,
      "loss": 0.799,
      "step": 9040
    },
    {
      "epoch": 1.939562794684955,
      "grad_norm": 0.3940899670124054,
      "learning_rate": 1.7413916273753395e-05,
      "loss": 0.3299,
      "step": 9050
    },
    {
      "epoch": 1.941705957993999,
      "grad_norm": 0.42167869210243225,
      "learning_rate": 1.741105872267467e-05,
      "loss": 0.4822,
      "step": 9060
    },
    {
      "epoch": 1.9438491213030433,
      "grad_norm": 14.95739459991455,
      "learning_rate": 1.7408201171595946e-05,
      "loss": 0.6471,
      "step": 9070
    },
    {
      "epoch": 1.9459922846120874,
      "grad_norm": 15.692193984985352,
      "learning_rate": 1.740534362051722e-05,
      "loss": 0.6486,
      "step": 9080
    },
    {
      "epoch": 1.9481354479211315,
      "grad_norm": 0.44995737075805664,
      "learning_rate": 1.7402486069438494e-05,
      "loss": 0.6293,
      "step": 9090
    },
    {
      "epoch": 1.9502786112301758,
      "grad_norm": 0.49140384793281555,
      "learning_rate": 1.7399628518359768e-05,
      "loss": 0.3119,
      "step": 9100
    },
    {
      "epoch": 1.9524217745392198,
      "grad_norm": 0.44763827323913574,
      "learning_rate": 1.739677096728104e-05,
      "loss": 0.3161,
      "step": 9110
    },
    {
      "epoch": 1.954564937848264,
      "grad_norm": 15.599955558776855,
      "learning_rate": 1.7393913416202316e-05,
      "loss": 0.3221,
      "step": 9120
    },
    {
      "epoch": 1.9567081011573082,
      "grad_norm": 0.23337632417678833,
      "learning_rate": 1.739105586512359e-05,
      "loss": 0.3367,
      "step": 9130
    },
    {
      "epoch": 1.9588512644663525,
      "grad_norm": 0.3661227226257324,
      "learning_rate": 1.7388198314044863e-05,
      "loss": 0.8731,
      "step": 9140
    },
    {
      "epoch": 1.9609944277753963,
      "grad_norm": 0.2813880145549774,
      "learning_rate": 1.7385340762966137e-05,
      "loss": 0.3403,
      "step": 9150
    },
    {
      "epoch": 1.9631375910844406,
      "grad_norm": 0.3027017116546631,
      "learning_rate": 1.7382483211887415e-05,
      "loss": 0.512,
      "step": 9160
    },
    {
      "epoch": 1.965280754393485,
      "grad_norm": 16.146928787231445,
      "learning_rate": 1.737962566080869e-05,
      "loss": 0.536,
      "step": 9170
    },
    {
      "epoch": 1.9674239177025288,
      "grad_norm": 15.209136962890625,
      "learning_rate": 1.7376768109729962e-05,
      "loss": 0.8604,
      "step": 9180
    },
    {
      "epoch": 1.969567081011573,
      "grad_norm": 0.3866942226886749,
      "learning_rate": 1.7373910558651236e-05,
      "loss": 0.8325,
      "step": 9190
    },
    {
      "epoch": 1.9717102443206174,
      "grad_norm": 0.36619675159454346,
      "learning_rate": 1.737105300757251e-05,
      "loss": 0.1671,
      "step": 9200
    },
    {
      "epoch": 1.9738534076296612,
      "grad_norm": 0.31994912028312683,
      "learning_rate": 1.7368195456493788e-05,
      "loss": 0.5033,
      "step": 9210
    },
    {
      "epoch": 1.9759965709387055,
      "grad_norm": 0.3562352955341339,
      "learning_rate": 1.736533790541506e-05,
      "loss": 0.3327,
      "step": 9220
    },
    {
      "epoch": 1.9781397342477498,
      "grad_norm": 15.532602310180664,
      "learning_rate": 1.7362480354336335e-05,
      "loss": 0.6631,
      "step": 9230
    },
    {
      "epoch": 1.9802828975567939,
      "grad_norm": 14.810184478759766,
      "learning_rate": 1.735962280325761e-05,
      "loss": 0.643,
      "step": 9240
    },
    {
      "epoch": 1.982426060865838,
      "grad_norm": 0.38262760639190674,
      "learning_rate": 1.7356765252178883e-05,
      "loss": 0.1667,
      "step": 9250
    },
    {
      "epoch": 1.9845692241748822,
      "grad_norm": 0.30371391773223877,
      "learning_rate": 1.7353907701100157e-05,
      "loss": 0.1707,
      "step": 9260
    },
    {
      "epoch": 1.9867123874839263,
      "grad_norm": 15.076539039611816,
      "learning_rate": 1.7351050150021435e-05,
      "loss": 0.8576,
      "step": 9270
    },
    {
      "epoch": 1.9888555507929704,
      "grad_norm": 0.2706078290939331,
      "learning_rate": 1.734819259894271e-05,
      "loss": 0.0059,
      "step": 9280
    },
    {
      "epoch": 1.9909987141020147,
      "grad_norm": 15.669220924377441,
      "learning_rate": 1.7345335047863982e-05,
      "loss": 0.7004,
      "step": 9290
    },
    {
      "epoch": 1.9931418774110587,
      "grad_norm": 0.3081175982952118,
      "learning_rate": 1.7342477496785256e-05,
      "loss": 1.0472,
      "step": 9300
    },
    {
      "epoch": 1.9952850407201028,
      "grad_norm": 0.4984901249408722,
      "learning_rate": 1.733961994570653e-05,
      "loss": 0.8018,
      "step": 9310
    },
    {
      "epoch": 1.9974282040291471,
      "grad_norm": 29.89714241027832,
      "learning_rate": 1.7336762394627808e-05,
      "loss": 0.4781,
      "step": 9320
    },
    {
      "epoch": 1.9995713673381912,
      "grad_norm": 14.92232894897461,
      "learning_rate": 1.733390484354908e-05,
      "loss": 0.3307,
      "step": 9330
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8736666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.6186227202415466,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 427.5058,
      "eval_samples_per_second": 7.017,
      "eval_steps_per_second": 2.339,
      "step": 9332
    },
    {
      "epoch": 2.0017145306472353,
      "grad_norm": 30.07917594909668,
      "learning_rate": 1.7331047292470352e-05,
      "loss": 0.9721,
      "step": 9340
    },
    {
      "epoch": 2.0038576939562796,
      "grad_norm": 29.688858032226562,
      "learning_rate": 1.732818974139163e-05,
      "loss": 0.6398,
      "step": 9350
    },
    {
      "epoch": 2.0060008572653234,
      "grad_norm": 0.42247793078422546,
      "learning_rate": 1.7325332190312903e-05,
      "loss": 0.4881,
      "step": 9360
    },
    {
      "epoch": 2.0081440205743677,
      "grad_norm": 0.4037569463253021,
      "learning_rate": 1.7322474639234177e-05,
      "loss": 0.4716,
      "step": 9370
    },
    {
      "epoch": 2.010287183883412,
      "grad_norm": 0.49910250306129456,
      "learning_rate": 1.731961708815545e-05,
      "loss": 0.6107,
      "step": 9380
    },
    {
      "epoch": 2.0124303471924563,
      "grad_norm": 0.43858033418655396,
      "learning_rate": 1.7316759537076725e-05,
      "loss": 0.167,
      "step": 9390
    },
    {
      "epoch": 2.0145735105015,
      "grad_norm": 0.3424507677555084,
      "learning_rate": 1.7313901985998e-05,
      "loss": 0.1671,
      "step": 9400
    },
    {
      "epoch": 2.0167166738105444,
      "grad_norm": 0.32769420742988586,
      "learning_rate": 1.7311044434919276e-05,
      "loss": 0.9931,
      "step": 9410
    },
    {
      "epoch": 2.0188598371195887,
      "grad_norm": 0.373990535736084,
      "learning_rate": 1.730818688384055e-05,
      "loss": 0.4903,
      "step": 9420
    },
    {
      "epoch": 2.0210030004286326,
      "grad_norm": 15.529179573059082,
      "learning_rate": 1.7305329332761824e-05,
      "loss": 0.4838,
      "step": 9430
    },
    {
      "epoch": 2.023146163737677,
      "grad_norm": 15.001739501953125,
      "learning_rate": 1.7302471781683098e-05,
      "loss": 1.1006,
      "step": 9440
    },
    {
      "epoch": 2.025289327046721,
      "grad_norm": 0.44944363832473755,
      "learning_rate": 1.7299614230604372e-05,
      "loss": 0.1663,
      "step": 9450
    },
    {
      "epoch": 2.027432490355765,
      "grad_norm": 14.734511375427246,
      "learning_rate": 1.729675667952565e-05,
      "loss": 0.3236,
      "step": 9460
    },
    {
      "epoch": 2.0295756536648093,
      "grad_norm": 15.043434143066406,
      "learning_rate": 1.7293899128446923e-05,
      "loss": 0.6642,
      "step": 9470
    },
    {
      "epoch": 2.0317188169738536,
      "grad_norm": 15.027536392211914,
      "learning_rate": 1.7291041577368197e-05,
      "loss": 1.6111,
      "step": 9480
    },
    {
      "epoch": 2.0338619802828974,
      "grad_norm": 0.5316810607910156,
      "learning_rate": 1.728818402628947e-05,
      "loss": 0.6124,
      "step": 9490
    },
    {
      "epoch": 2.0360051435919417,
      "grad_norm": 14.629661560058594,
      "learning_rate": 1.7285326475210745e-05,
      "loss": 0.3056,
      "step": 9500
    },
    {
      "epoch": 2.038148306900986,
      "grad_norm": 14.50090503692627,
      "learning_rate": 1.7282468924132022e-05,
      "loss": 1.4722,
      "step": 9510
    },
    {
      "epoch": 2.04029147021003,
      "grad_norm": 0.6711584329605103,
      "learning_rate": 1.7279611373053296e-05,
      "loss": 0.4375,
      "step": 9520
    },
    {
      "epoch": 2.042434633519074,
      "grad_norm": 0.6413007974624634,
      "learning_rate": 1.727675382197457e-05,
      "loss": 0.7054,
      "step": 9530
    },
    {
      "epoch": 2.0445777968281185,
      "grad_norm": 0.7093530297279358,
      "learning_rate": 1.7273896270895844e-05,
      "loss": 0.8503,
      "step": 9540
    },
    {
      "epoch": 2.0467209601371623,
      "grad_norm": 15.16413688659668,
      "learning_rate": 1.7271038719817118e-05,
      "loss": 0.5622,
      "step": 9550
    },
    {
      "epoch": 2.0488641234462066,
      "grad_norm": 0.7490565776824951,
      "learning_rate": 1.7268181168738392e-05,
      "loss": 0.4285,
      "step": 9560
    },
    {
      "epoch": 2.051007286755251,
      "grad_norm": 0.6043475866317749,
      "learning_rate": 1.7265323617659666e-05,
      "loss": 0.152,
      "step": 9570
    },
    {
      "epoch": 2.0531504500642948,
      "grad_norm": 0.49447596073150635,
      "learning_rate": 1.726246606658094e-05,
      "loss": 0.7704,
      "step": 9580
    },
    {
      "epoch": 2.055293613373339,
      "grad_norm": 0.4675344228744507,
      "learning_rate": 1.7259608515502214e-05,
      "loss": 0.6219,
      "step": 9590
    },
    {
      "epoch": 2.0574367766823833,
      "grad_norm": 0.4145316779613495,
      "learning_rate": 1.725675096442349e-05,
      "loss": 0.4721,
      "step": 9600
    },
    {
      "epoch": 2.059579939991427,
      "grad_norm": 15.038368225097656,
      "learning_rate": 1.7253893413344765e-05,
      "loss": 0.1797,
      "step": 9610
    },
    {
      "epoch": 2.0617231033004715,
      "grad_norm": 0.28722673654556274,
      "learning_rate": 1.725103586226604e-05,
      "loss": 0.8664,
      "step": 9620
    },
    {
      "epoch": 2.0638662666095158,
      "grad_norm": 29.765085220336914,
      "learning_rate": 1.7248178311187313e-05,
      "loss": 1.1535,
      "step": 9630
    },
    {
      "epoch": 2.0660094299185596,
      "grad_norm": 0.44381392002105713,
      "learning_rate": 1.7245320760108587e-05,
      "loss": 1.0983,
      "step": 9640
    },
    {
      "epoch": 2.068152593227604,
      "grad_norm": 0.4850004017353058,
      "learning_rate": 1.7242463209029864e-05,
      "loss": 0.011,
      "step": 9650
    },
    {
      "epoch": 2.070295756536648,
      "grad_norm": 0.3743644952774048,
      "learning_rate": 1.7239605657951138e-05,
      "loss": 0.4769,
      "step": 9660
    },
    {
      "epoch": 2.072438919845692,
      "grad_norm": 0.40128815174102783,
      "learning_rate": 1.7236748106872412e-05,
      "loss": 0.7898,
      "step": 9670
    },
    {
      "epoch": 2.0745820831547364,
      "grad_norm": 14.675457954406738,
      "learning_rate": 1.7233890555793686e-05,
      "loss": 0.1661,
      "step": 9680
    },
    {
      "epoch": 2.0767252464637806,
      "grad_norm": 0.4261748790740967,
      "learning_rate": 1.723103300471496e-05,
      "loss": 0.7894,
      "step": 9690
    },
    {
      "epoch": 2.0788684097728245,
      "grad_norm": 0.49440866708755493,
      "learning_rate": 1.7228175453636237e-05,
      "loss": 0.7751,
      "step": 9700
    },
    {
      "epoch": 2.081011573081869,
      "grad_norm": 15.080199241638184,
      "learning_rate": 1.722531790255751e-05,
      "loss": 0.6089,
      "step": 9710
    },
    {
      "epoch": 2.083154736390913,
      "grad_norm": 0.500835120677948,
      "learning_rate": 1.7222460351478785e-05,
      "loss": 0.1582,
      "step": 9720
    },
    {
      "epoch": 2.085297899699957,
      "grad_norm": 0.44609588384628296,
      "learning_rate": 1.721960280040006e-05,
      "loss": 0.9431,
      "step": 9730
    },
    {
      "epoch": 2.0874410630090012,
      "grad_norm": 0.5052411556243896,
      "learning_rate": 1.7216745249321333e-05,
      "loss": 0.777,
      "step": 9740
    },
    {
      "epoch": 2.0895842263180455,
      "grad_norm": 0.5228021144866943,
      "learning_rate": 1.7213887698242607e-05,
      "loss": 0.9265,
      "step": 9750
    },
    {
      "epoch": 2.0917273896270894,
      "grad_norm": 14.486588478088379,
      "learning_rate": 1.7211030147163884e-05,
      "loss": 1.0449,
      "step": 9760
    },
    {
      "epoch": 2.0938705529361337,
      "grad_norm": 0.6722996830940247,
      "learning_rate": 1.7208172596085155e-05,
      "loss": 0.5758,
      "step": 9770
    },
    {
      "epoch": 2.096013716245178,
      "grad_norm": 0.7208183407783508,
      "learning_rate": 1.720531504500643e-05,
      "loss": 0.5684,
      "step": 9780
    },
    {
      "epoch": 2.098156879554222,
      "grad_norm": 0.6322365403175354,
      "learning_rate": 1.7202457493927706e-05,
      "loss": 0.156,
      "step": 9790
    },
    {
      "epoch": 2.100300042863266,
      "grad_norm": 0.4290541708469391,
      "learning_rate": 1.719959994284898e-05,
      "loss": 0.1636,
      "step": 9800
    },
    {
      "epoch": 2.1024432061723104,
      "grad_norm": 0.4501364827156067,
      "learning_rate": 1.7196742391770254e-05,
      "loss": 0.9423,
      "step": 9810
    },
    {
      "epoch": 2.1045863694813547,
      "grad_norm": 0.4070143401622772,
      "learning_rate": 1.7193884840691528e-05,
      "loss": 0.1644,
      "step": 9820
    },
    {
      "epoch": 2.1067295327903985,
      "grad_norm": 0.3580043911933899,
      "learning_rate": 1.71910272896128e-05,
      "loss": 0.4854,
      "step": 9830
    },
    {
      "epoch": 2.108872696099443,
      "grad_norm": 14.732425689697266,
      "learning_rate": 1.718816973853408e-05,
      "loss": 0.6534,
      "step": 9840
    },
    {
      "epoch": 2.111015859408487,
      "grad_norm": 15.168862342834473,
      "learning_rate": 1.7185312187455353e-05,
      "loss": 0.8198,
      "step": 9850
    },
    {
      "epoch": 2.113159022717531,
      "grad_norm": 0.39104321599006653,
      "learning_rate": 1.7182454636376627e-05,
      "loss": 0.3264,
      "step": 9860
    },
    {
      "epoch": 2.1153021860265753,
      "grad_norm": 0.3518851399421692,
      "learning_rate": 1.71795970852979e-05,
      "loss": 0.1673,
      "step": 9870
    },
    {
      "epoch": 2.1174453493356196,
      "grad_norm": 14.929759979248047,
      "learning_rate": 1.7176739534219175e-05,
      "loss": 0.8202,
      "step": 9880
    },
    {
      "epoch": 2.1195885126446634,
      "grad_norm": 0.36963266134262085,
      "learning_rate": 1.717388198314045e-05,
      "loss": 0.1676,
      "step": 9890
    },
    {
      "epoch": 2.1217316759537077,
      "grad_norm": 0.4259832799434662,
      "learning_rate": 1.7171024432061726e-05,
      "loss": 1.1285,
      "step": 9900
    },
    {
      "epoch": 2.123874839262752,
      "grad_norm": 29.59259796142578,
      "learning_rate": 1.7168166880983e-05,
      "loss": 0.9349,
      "step": 9910
    },
    {
      "epoch": 2.126018002571796,
      "grad_norm": 0.5140203237533569,
      "learning_rate": 1.7165309329904274e-05,
      "loss": 0.4674,
      "step": 9920
    },
    {
      "epoch": 2.12816116588084,
      "grad_norm": 0.5094291567802429,
      "learning_rate": 1.7162451778825548e-05,
      "loss": 0.4615,
      "step": 9930
    },
    {
      "epoch": 2.1303043291898844,
      "grad_norm": 0.4372062683105469,
      "learning_rate": 1.715959422774682e-05,
      "loss": 0.3135,
      "step": 9940
    },
    {
      "epoch": 2.1324474924989283,
      "grad_norm": 14.711752891540527,
      "learning_rate": 1.71567366766681e-05,
      "loss": 0.6313,
      "step": 9950
    },
    {
      "epoch": 2.1345906558079726,
      "grad_norm": 14.650426864624023,
      "learning_rate": 1.7153879125589373e-05,
      "loss": 0.3204,
      "step": 9960
    },
    {
      "epoch": 2.136733819117017,
      "grad_norm": 15.203989028930664,
      "learning_rate": 1.7151021574510647e-05,
      "loss": 0.8013,
      "step": 9970
    },
    {
      "epoch": 2.1388769824260607,
      "grad_norm": 0.390585720539093,
      "learning_rate": 1.714816402343192e-05,
      "loss": 0.6412,
      "step": 9980
    },
    {
      "epoch": 2.141020145735105,
      "grad_norm": 0.4582727253437042,
      "learning_rate": 1.7145306472353195e-05,
      "loss": 0.4712,
      "step": 9990
    },
    {
      "epoch": 2.1431633090441493,
      "grad_norm": 0.5195616483688354,
      "learning_rate": 1.714244892127447e-05,
      "loss": 0.771,
      "step": 10000
    },
    {
      "epoch": 2.145306472353193,
      "grad_norm": 0.46394264698028564,
      "learning_rate": 1.7139591370195743e-05,
      "loss": 0.4554,
      "step": 10010
    },
    {
      "epoch": 2.1474496356622375,
      "grad_norm": 14.8599271774292,
      "learning_rate": 1.7136733819117016e-05,
      "loss": 0.6215,
      "step": 10020
    },
    {
      "epoch": 2.1495927989712817,
      "grad_norm": 0.43722784519195557,
      "learning_rate": 1.713387626803829e-05,
      "loss": 0.6357,
      "step": 10030
    },
    {
      "epoch": 2.1517359622803256,
      "grad_norm": 0.4311148524284363,
      "learning_rate": 1.7131018716959568e-05,
      "loss": 0.3178,
      "step": 10040
    },
    {
      "epoch": 2.15387912558937,
      "grad_norm": 14.990021705627441,
      "learning_rate": 1.712816116588084e-05,
      "loss": 0.9422,
      "step": 10050
    },
    {
      "epoch": 2.156022288898414,
      "grad_norm": 15.010313034057617,
      "learning_rate": 1.7125303614802116e-05,
      "loss": 0.7742,
      "step": 10060
    },
    {
      "epoch": 2.158165452207458,
      "grad_norm": 0.6270743608474731,
      "learning_rate": 1.712244606372339e-05,
      "loss": 0.7457,
      "step": 10070
    },
    {
      "epoch": 2.1603086155165023,
      "grad_norm": 0.7051724195480347,
      "learning_rate": 1.7119588512644663e-05,
      "loss": 0.5789,
      "step": 10080
    },
    {
      "epoch": 2.1624517788255466,
      "grad_norm": 14.769968032836914,
      "learning_rate": 1.711673096156594e-05,
      "loss": 0.3014,
      "step": 10090
    },
    {
      "epoch": 2.1645949421345905,
      "grad_norm": 29.903778076171875,
      "learning_rate": 1.7113873410487215e-05,
      "loss": 0.9031,
      "step": 10100
    },
    {
      "epoch": 2.1667381054436348,
      "grad_norm": 14.960624694824219,
      "learning_rate": 1.711101585940849e-05,
      "loss": 0.4553,
      "step": 10110
    },
    {
      "epoch": 2.168881268752679,
      "grad_norm": 0.5667457580566406,
      "learning_rate": 1.7108158308329763e-05,
      "loss": 0.6007,
      "step": 10120
    },
    {
      "epoch": 2.171024432061723,
      "grad_norm": 14.618934631347656,
      "learning_rate": 1.7105300757251036e-05,
      "loss": 0.7513,
      "step": 10130
    },
    {
      "epoch": 2.173167595370767,
      "grad_norm": 0.4548826515674591,
      "learning_rate": 1.7102443206172314e-05,
      "loss": 0.1597,
      "step": 10140
    },
    {
      "epoch": 2.1753107586798115,
      "grad_norm": 0.3580849766731262,
      "learning_rate": 1.7099585655093588e-05,
      "loss": 0.1704,
      "step": 10150
    },
    {
      "epoch": 2.1774539219888553,
      "grad_norm": 0.36087945103645325,
      "learning_rate": 1.709672810401486e-05,
      "loss": 0.8191,
      "step": 10160
    },
    {
      "epoch": 2.1795970852978996,
      "grad_norm": 0.3998197913169861,
      "learning_rate": 1.7093870552936136e-05,
      "loss": 0.6439,
      "step": 10170
    },
    {
      "epoch": 2.181740248606944,
      "grad_norm": 0.3993193209171295,
      "learning_rate": 1.709101300185741e-05,
      "loss": 0.3245,
      "step": 10180
    },
    {
      "epoch": 2.1838834119159882,
      "grad_norm": 0.4214526414871216,
      "learning_rate": 1.7088155450778683e-05,
      "loss": 0.9528,
      "step": 10190
    },
    {
      "epoch": 2.186026575225032,
      "grad_norm": 0.4342522621154785,
      "learning_rate": 1.7085297899699957e-05,
      "loss": 0.3208,
      "step": 10200
    },
    {
      "epoch": 2.1881697385340764,
      "grad_norm": 0.43535465002059937,
      "learning_rate": 1.708244034862123e-05,
      "loss": 0.637,
      "step": 10210
    },
    {
      "epoch": 2.1903129018431207,
      "grad_norm": 0.4362090826034546,
      "learning_rate": 1.7079582797542505e-05,
      "loss": 0.4753,
      "step": 10220
    },
    {
      "epoch": 2.1924560651521645,
      "grad_norm": 0.479412317276001,
      "learning_rate": 1.7076725246463783e-05,
      "loss": 0.9361,
      "step": 10230
    },
    {
      "epoch": 2.194599228461209,
      "grad_norm": 0.49536535143852234,
      "learning_rate": 1.7073867695385056e-05,
      "loss": 0.4567,
      "step": 10240
    },
    {
      "epoch": 2.196742391770253,
      "grad_norm": 14.725295066833496,
      "learning_rate": 1.707101014430633e-05,
      "loss": 0.4686,
      "step": 10250
    },
    {
      "epoch": 2.198885555079297,
      "grad_norm": 0.46568986773490906,
      "learning_rate": 1.7068152593227604e-05,
      "loss": 0.6274,
      "step": 10260
    },
    {
      "epoch": 2.2010287183883412,
      "grad_norm": 0.5047627091407776,
      "learning_rate": 1.7065295042148878e-05,
      "loss": 0.7697,
      "step": 10270
    },
    {
      "epoch": 2.2031718816973855,
      "grad_norm": 0.48305028676986694,
      "learning_rate": 1.7062437491070156e-05,
      "loss": 0.3091,
      "step": 10280
    },
    {
      "epoch": 2.2053150450064294,
      "grad_norm": 0.47432443499565125,
      "learning_rate": 1.705957993999143e-05,
      "loss": 0.6126,
      "step": 10290
    },
    {
      "epoch": 2.2074582083154737,
      "grad_norm": 0.4742204546928406,
      "learning_rate": 1.7056722388912703e-05,
      "loss": 0.619,
      "step": 10300
    },
    {
      "epoch": 2.209601371624518,
      "grad_norm": 14.771809577941895,
      "learning_rate": 1.7053864837833977e-05,
      "loss": 0.6119,
      "step": 10310
    },
    {
      "epoch": 2.211744534933562,
      "grad_norm": 0.47398456931114197,
      "learning_rate": 1.705100728675525e-05,
      "loss": 0.1575,
      "step": 10320
    },
    {
      "epoch": 2.213887698242606,
      "grad_norm": 0.3856954276561737,
      "learning_rate": 1.7048149735676525e-05,
      "loss": 0.1645,
      "step": 10330
    },
    {
      "epoch": 2.2160308615516504,
      "grad_norm": 14.772636413574219,
      "learning_rate": 1.7045292184597803e-05,
      "loss": 0.6412,
      "step": 10340
    },
    {
      "epoch": 2.2181740248606943,
      "grad_norm": 0.41947630047798157,
      "learning_rate": 1.7042434633519076e-05,
      "loss": 0.4832,
      "step": 10350
    },
    {
      "epoch": 2.2203171881697386,
      "grad_norm": 0.4329093098640442,
      "learning_rate": 1.703957708244035e-05,
      "loss": 0.6328,
      "step": 10360
    },
    {
      "epoch": 2.222460351478783,
      "grad_norm": 0.4717959463596344,
      "learning_rate": 1.7036719531361624e-05,
      "loss": 0.4761,
      "step": 10370
    },
    {
      "epoch": 2.2246035147878267,
      "grad_norm": 0.4343801736831665,
      "learning_rate": 1.7033861980282898e-05,
      "loss": 0.32,
      "step": 10380
    },
    {
      "epoch": 2.226746678096871,
      "grad_norm": 0.39141136407852173,
      "learning_rate": 1.7031004429204176e-05,
      "loss": 0.1658,
      "step": 10390
    },
    {
      "epoch": 2.2288898414059153,
      "grad_norm": 0.26564183831214905,
      "learning_rate": 1.702814687812545e-05,
      "loss": 0.6848,
      "step": 10400
    },
    {
      "epoch": 2.231033004714959,
      "grad_norm": 14.808981895446777,
      "learning_rate": 1.7025289327046723e-05,
      "loss": 0.8359,
      "step": 10410
    },
    {
      "epoch": 2.2331761680240034,
      "grad_norm": 0.3100592792034149,
      "learning_rate": 1.7022431775967997e-05,
      "loss": 0.1727,
      "step": 10420
    },
    {
      "epoch": 2.2353193313330477,
      "grad_norm": 15.547508239746094,
      "learning_rate": 1.701957422488927e-05,
      "loss": 0.8369,
      "step": 10430
    },
    {
      "epoch": 2.2374624946420916,
      "grad_norm": 0.40537431836128235,
      "learning_rate": 1.7016716673810545e-05,
      "loss": 0.9768,
      "step": 10440
    },
    {
      "epoch": 2.239605657951136,
      "grad_norm": 0.45052438974380493,
      "learning_rate": 1.701385912273182e-05,
      "loss": 0.1636,
      "step": 10450
    },
    {
      "epoch": 2.24174882126018,
      "grad_norm": 0.482364684343338,
      "learning_rate": 1.7011001571653093e-05,
      "loss": 0.623,
      "step": 10460
    },
    {
      "epoch": 2.243891984569224,
      "grad_norm": 0.434647798538208,
      "learning_rate": 1.7008144020574367e-05,
      "loss": 0.4702,
      "step": 10470
    },
    {
      "epoch": 2.2460351478782683,
      "grad_norm": 0.46107324957847595,
      "learning_rate": 1.7005286469495644e-05,
      "loss": 0.4719,
      "step": 10480
    },
    {
      "epoch": 2.2481783111873126,
      "grad_norm": 14.63244915008545,
      "learning_rate": 1.7002428918416918e-05,
      "loss": 1.0734,
      "step": 10490
    },
    {
      "epoch": 2.2503214744963564,
      "grad_norm": 0.617422878742218,
      "learning_rate": 1.6999571367338192e-05,
      "loss": 0.6085,
      "step": 10500
    },
    {
      "epoch": 2.2524646378054007,
      "grad_norm": 0.701621949672699,
      "learning_rate": 1.6996713816259466e-05,
      "loss": 0.7295,
      "step": 10510
    },
    {
      "epoch": 2.254607801114445,
      "grad_norm": 14.905743598937988,
      "learning_rate": 1.699385626518074e-05,
      "loss": 0.8462,
      "step": 10520
    },
    {
      "epoch": 2.256750964423489,
      "grad_norm": 0.6510520577430725,
      "learning_rate": 1.6990998714102017e-05,
      "loss": 0.2905,
      "step": 10530
    },
    {
      "epoch": 2.258894127732533,
      "grad_norm": 0.6820962429046631,
      "learning_rate": 1.698814116302329e-05,
      "loss": 0.461,
      "step": 10540
    },
    {
      "epoch": 2.2610372910415775,
      "grad_norm": 0.5410662293434143,
      "learning_rate": 1.6985283611944565e-05,
      "loss": 0.4534,
      "step": 10550
    },
    {
      "epoch": 2.2631804543506213,
      "grad_norm": 0.4552328288555145,
      "learning_rate": 1.698242606086584e-05,
      "loss": 0.7549,
      "step": 10560
    },
    {
      "epoch": 2.2653236176596656,
      "grad_norm": 15.194652557373047,
      "learning_rate": 1.6979568509787113e-05,
      "loss": 0.7747,
      "step": 10570
    },
    {
      "epoch": 2.26746678096871,
      "grad_norm": 0.5590389966964722,
      "learning_rate": 1.697671095870839e-05,
      "loss": 0.5971,
      "step": 10580
    },
    {
      "epoch": 2.2696099442777538,
      "grad_norm": 0.4211697280406952,
      "learning_rate": 1.6973853407629664e-05,
      "loss": 0.4487,
      "step": 10590
    },
    {
      "epoch": 2.271753107586798,
      "grad_norm": 0.32786282896995544,
      "learning_rate": 1.6970995856550938e-05,
      "loss": 0.3285,
      "step": 10600
    },
    {
      "epoch": 2.2738962708958423,
      "grad_norm": 0.299024373292923,
      "learning_rate": 1.6968138305472212e-05,
      "loss": 0.3429,
      "step": 10610
    },
    {
      "epoch": 2.276039434204886,
      "grad_norm": 16.460447311401367,
      "learning_rate": 1.6965280754393486e-05,
      "loss": 0.3506,
      "step": 10620
    },
    {
      "epoch": 2.2781825975139305,
      "grad_norm": 0.22129663825035095,
      "learning_rate": 1.696242320331476e-05,
      "loss": 0.3562,
      "step": 10630
    },
    {
      "epoch": 2.280325760822975,
      "grad_norm": 0.2611422836780548,
      "learning_rate": 1.6959565652236034e-05,
      "loss": 0.5237,
      "step": 10640
    },
    {
      "epoch": 2.2824689241320186,
      "grad_norm": 0.21439893543720245,
      "learning_rate": 1.6956708101157308e-05,
      "loss": 0.0055,
      "step": 10650
    },
    {
      "epoch": 2.284612087441063,
      "grad_norm": 14.979022979736328,
      "learning_rate": 1.6953850550078582e-05,
      "loss": 0.8923,
      "step": 10660
    },
    {
      "epoch": 2.286755250750107,
      "grad_norm": 0.23524396121501923,
      "learning_rate": 1.695099299899986e-05,
      "loss": 0.0053,
      "step": 10670
    },
    {
      "epoch": 2.288898414059151,
      "grad_norm": 0.22039425373077393,
      "learning_rate": 1.6948135447921133e-05,
      "loss": 0.3592,
      "step": 10680
    },
    {
      "epoch": 2.2910415773681954,
      "grad_norm": 14.843989372253418,
      "learning_rate": 1.6945277896842407e-05,
      "loss": 1.053,
      "step": 10690
    },
    {
      "epoch": 2.2931847406772397,
      "grad_norm": 0.3251294791698456,
      "learning_rate": 1.694242034576368e-05,
      "loss": 0.7037,
      "step": 10700
    },
    {
      "epoch": 2.295327903986284,
      "grad_norm": 44.71157455444336,
      "learning_rate": 1.6939562794684955e-05,
      "loss": 1.782,
      "step": 10710
    },
    {
      "epoch": 2.297471067295328,
      "grad_norm": 0.5533833503723145,
      "learning_rate": 1.6936705243606232e-05,
      "loss": 0.6442,
      "step": 10720
    },
    {
      "epoch": 2.299614230604372,
      "grad_norm": 14.657266616821289,
      "learning_rate": 1.6933847692527506e-05,
      "loss": 0.3189,
      "step": 10730
    },
    {
      "epoch": 2.3017573939134164,
      "grad_norm": 14.65463638305664,
      "learning_rate": 1.693099014144878e-05,
      "loss": 0.6391,
      "step": 10740
    },
    {
      "epoch": 2.3039005572224602,
      "grad_norm": 0.38797277212142944,
      "learning_rate": 1.6928132590370054e-05,
      "loss": 0.6377,
      "step": 10750
    },
    {
      "epoch": 2.3060437205315045,
      "grad_norm": 29.88909912109375,
      "learning_rate": 1.6925275039291328e-05,
      "loss": 1.0912,
      "step": 10760
    },
    {
      "epoch": 2.308186883840549,
      "grad_norm": 15.553895950317383,
      "learning_rate": 1.6922417488212605e-05,
      "loss": 0.619,
      "step": 10770
    },
    {
      "epoch": 2.3103300471495927,
      "grad_norm": 0.6291047930717468,
      "learning_rate": 1.691955993713388e-05,
      "loss": 0.5982,
      "step": 10780
    },
    {
      "epoch": 2.312473210458637,
      "grad_norm": 14.53845500946045,
      "learning_rate": 1.6916702386055153e-05,
      "loss": 0.4517,
      "step": 10790
    },
    {
      "epoch": 2.3146163737676813,
      "grad_norm": 14.895476341247559,
      "learning_rate": 1.6913844834976427e-05,
      "loss": 0.7414,
      "step": 10800
    },
    {
      "epoch": 2.316759537076725,
      "grad_norm": 14.495509147644043,
      "learning_rate": 1.69109872838977e-05,
      "loss": 0.7313,
      "step": 10810
    },
    {
      "epoch": 2.3189027003857694,
      "grad_norm": 0.6947680115699768,
      "learning_rate": 1.6908129732818975e-05,
      "loss": 0.572,
      "step": 10820
    },
    {
      "epoch": 2.3210458636948137,
      "grad_norm": 14.660072326660156,
      "learning_rate": 1.6905272181740252e-05,
      "loss": 0.3136,
      "step": 10830
    },
    {
      "epoch": 2.3231890270038575,
      "grad_norm": 0.5004312992095947,
      "learning_rate": 1.6902414630661526e-05,
      "loss": 0.4637,
      "step": 10840
    },
    {
      "epoch": 2.325332190312902,
      "grad_norm": 14.913427352905273,
      "learning_rate": 1.6899557079582797e-05,
      "loss": 0.3187,
      "step": 10850
    },
    {
      "epoch": 2.327475353621946,
      "grad_norm": 0.39473769068717957,
      "learning_rate": 1.6896699528504074e-05,
      "loss": 0.4804,
      "step": 10860
    },
    {
      "epoch": 2.32961851693099,
      "grad_norm": 0.46698907017707825,
      "learning_rate": 1.6893841977425348e-05,
      "loss": 1.0969,
      "step": 10870
    },
    {
      "epoch": 2.3317616802400343,
      "grad_norm": 0.5243343710899353,
      "learning_rate": 1.6890984426346622e-05,
      "loss": 0.4606,
      "step": 10880
    },
    {
      "epoch": 2.3339048435490786,
      "grad_norm": 0.4814993739128113,
      "learning_rate": 1.6888126875267896e-05,
      "loss": 0.3131,
      "step": 10890
    },
    {
      "epoch": 2.3360480068581224,
      "grad_norm": 0.36497506499290466,
      "learning_rate": 1.688526932418917e-05,
      "loss": 0.0093,
      "step": 10900
    },
    {
      "epoch": 2.3381911701671667,
      "grad_norm": 14.769511222839355,
      "learning_rate": 1.6882411773110447e-05,
      "loss": 0.6487,
      "step": 10910
    },
    {
      "epoch": 2.340334333476211,
      "grad_norm": 0.37738698720932007,
      "learning_rate": 1.687955422203172e-05,
      "loss": 0.9715,
      "step": 10920
    },
    {
      "epoch": 2.342477496785255,
      "grad_norm": 0.39618831872940063,
      "learning_rate": 1.6876696670952995e-05,
      "loss": 0.7995,
      "step": 10930
    },
    {
      "epoch": 2.344620660094299,
      "grad_norm": 0.39176708459854126,
      "learning_rate": 1.687383911987427e-05,
      "loss": 0.4776,
      "step": 10940
    },
    {
      "epoch": 2.3467638234033434,
      "grad_norm": 29.83347511291504,
      "learning_rate": 1.6870981568795543e-05,
      "loss": 1.1053,
      "step": 10950
    },
    {
      "epoch": 2.3489069867123873,
      "grad_norm": 0.48191937804222107,
      "learning_rate": 1.6868124017716817e-05,
      "loss": 0.6319,
      "step": 10960
    },
    {
      "epoch": 2.3510501500214316,
      "grad_norm": 0.5647438764572144,
      "learning_rate": 1.6865266466638094e-05,
      "loss": 0.467,
      "step": 10970
    },
    {
      "epoch": 2.353193313330476,
      "grad_norm": 0.6253038644790649,
      "learning_rate": 1.6862408915559368e-05,
      "loss": 0.7485,
      "step": 10980
    },
    {
      "epoch": 2.35533647663952,
      "grad_norm": 15.677783012390137,
      "learning_rate": 1.6859551364480642e-05,
      "loss": 0.4483,
      "step": 10990
    },
    {
      "epoch": 2.357479639948564,
      "grad_norm": 0.6178061366081238,
      "learning_rate": 1.6856693813401916e-05,
      "loss": 0.7425,
      "step": 11000
    },
    {
      "epoch": 2.3596228032576083,
      "grad_norm": 14.640499114990234,
      "learning_rate": 1.685383626232319e-05,
      "loss": 0.5883,
      "step": 11010
    },
    {
      "epoch": 2.3617659665666526,
      "grad_norm": 14.611875534057617,
      "learning_rate": 1.6850978711244467e-05,
      "loss": 0.4538,
      "step": 11020
    },
    {
      "epoch": 2.3639091298756965,
      "grad_norm": 0.5697776675224304,
      "learning_rate": 1.684812116016574e-05,
      "loss": 0.9209,
      "step": 11030
    },
    {
      "epoch": 2.3660522931847408,
      "grad_norm": 14.520844459533691,
      "learning_rate": 1.6845263609087015e-05,
      "loss": 1.0242,
      "step": 11040
    },
    {
      "epoch": 2.368195456493785,
      "grad_norm": 0.769422709941864,
      "learning_rate": 1.684240605800829e-05,
      "loss": 0.5669,
      "step": 11050
    },
    {
      "epoch": 2.370338619802829,
      "grad_norm": 0.7297554016113281,
      "learning_rate": 1.6839548506929563e-05,
      "loss": 0.4229,
      "step": 11060
    },
    {
      "epoch": 2.372481783111873,
      "grad_norm": 0.6143782734870911,
      "learning_rate": 1.6836690955850837e-05,
      "loss": 0.2935,
      "step": 11070
    },
    {
      "epoch": 2.3746249464209175,
      "grad_norm": 14.83877182006836,
      "learning_rate": 1.683383340477211e-05,
      "loss": 0.729,
      "step": 11080
    },
    {
      "epoch": 2.3767681097299613,
      "grad_norm": 14.469942092895508,
      "learning_rate": 1.6830975853693384e-05,
      "loss": 0.7368,
      "step": 11090
    },
    {
      "epoch": 2.3789112730390056,
      "grad_norm": 0.6178737878799438,
      "learning_rate": 1.682811830261466e-05,
      "loss": 0.5879,
      "step": 11100
    },
    {
      "epoch": 2.38105443634805,
      "grad_norm": 29.459590911865234,
      "learning_rate": 1.6825260751535936e-05,
      "loss": 0.7429,
      "step": 11110
    },
    {
      "epoch": 2.3831975996570938,
      "grad_norm": 14.757206916809082,
      "learning_rate": 1.682240320045721e-05,
      "loss": 0.7409,
      "step": 11120
    },
    {
      "epoch": 2.385340762966138,
      "grad_norm": 14.569082260131836,
      "learning_rate": 1.6819545649378484e-05,
      "loss": 0.445,
      "step": 11130
    },
    {
      "epoch": 2.3874839262751824,
      "grad_norm": 0.6042527556419373,
      "learning_rate": 1.6816688098299757e-05,
      "loss": 0.4432,
      "step": 11140
    },
    {
      "epoch": 2.389627089584226,
      "grad_norm": 0.6000532507896423,
      "learning_rate": 1.681383054722103e-05,
      "loss": 0.5902,
      "step": 11150
    },
    {
      "epoch": 2.3917702528932705,
      "grad_norm": 0.619554340839386,
      "learning_rate": 1.681097299614231e-05,
      "loss": 0.8674,
      "step": 11160
    },
    {
      "epoch": 2.393913416202315,
      "grad_norm": 14.523086547851562,
      "learning_rate": 1.6808115445063583e-05,
      "loss": 0.7239,
      "step": 11170
    },
    {
      "epoch": 2.3960565795113586,
      "grad_norm": 0.7505048513412476,
      "learning_rate": 1.6805257893984857e-05,
      "loss": 0.8557,
      "step": 11180
    },
    {
      "epoch": 2.398199742820403,
      "grad_norm": 14.564488410949707,
      "learning_rate": 1.680240034290613e-05,
      "loss": 0.2951,
      "step": 11190
    },
    {
      "epoch": 2.4003429061294472,
      "grad_norm": 0.5872166752815247,
      "learning_rate": 1.6799542791827404e-05,
      "loss": 0.7494,
      "step": 11200
    },
    {
      "epoch": 2.402486069438491,
      "grad_norm": 0.4981025457382202,
      "learning_rate": 1.6796685240748682e-05,
      "loss": 0.5828,
      "step": 11210
    },
    {
      "epoch": 2.4046292327475354,
      "grad_norm": 0.7378205060958862,
      "learning_rate": 1.6793827689669956e-05,
      "loss": 1.0123,
      "step": 11220
    },
    {
      "epoch": 2.4067723960565797,
      "grad_norm": 0.7807645201683044,
      "learning_rate": 1.679097013859123e-05,
      "loss": 0.4238,
      "step": 11230
    },
    {
      "epoch": 2.4089155593656235,
      "grad_norm": 14.437814712524414,
      "learning_rate": 1.6788112587512503e-05,
      "loss": 0.7098,
      "step": 11240
    },
    {
      "epoch": 2.411058722674668,
      "grad_norm": 0.663765549659729,
      "learning_rate": 1.6785255036433777e-05,
      "loss": 0.1532,
      "step": 11250
    },
    {
      "epoch": 2.413201885983712,
      "grad_norm": 14.48723030090332,
      "learning_rate": 1.6782397485355055e-05,
      "loss": 0.5871,
      "step": 11260
    },
    {
      "epoch": 2.415345049292756,
      "grad_norm": 0.5076256990432739,
      "learning_rate": 1.677953993427633e-05,
      "loss": 0.1598,
      "step": 11270
    },
    {
      "epoch": 2.4174882126018002,
      "grad_norm": 0.39498746395111084,
      "learning_rate": 1.67766823831976e-05,
      "loss": 0.6227,
      "step": 11280
    },
    {
      "epoch": 2.4196313759108445,
      "grad_norm": 0.3874543607234955,
      "learning_rate": 1.6773824832118873e-05,
      "loss": 0.4717,
      "step": 11290
    },
    {
      "epoch": 2.4217745392198884,
      "grad_norm": 0.44676414132118225,
      "learning_rate": 1.677096728104015e-05,
      "loss": 0.8031,
      "step": 11300
    },
    {
      "epoch": 2.4239177025289327,
      "grad_norm": 0.47486183047294617,
      "learning_rate": 1.6768109729961424e-05,
      "loss": 0.9389,
      "step": 11310
    },
    {
      "epoch": 2.426060865837977,
      "grad_norm": 0.4498187303543091,
      "learning_rate": 1.6765252178882698e-05,
      "loss": 0.7682,
      "step": 11320
    },
    {
      "epoch": 2.428204029147021,
      "grad_norm": 15.017950057983398,
      "learning_rate": 1.6762394627803972e-05,
      "loss": 0.1588,
      "step": 11330
    },
    {
      "epoch": 2.430347192456065,
      "grad_norm": 14.734227180480957,
      "learning_rate": 1.6759537076725246e-05,
      "loss": 0.4722,
      "step": 11340
    },
    {
      "epoch": 2.4324903557651094,
      "grad_norm": 0.45572471618652344,
      "learning_rate": 1.6756679525646523e-05,
      "loss": 0.6243,
      "step": 11350
    },
    {
      "epoch": 2.4346335190741533,
      "grad_norm": 0.5056543350219727,
      "learning_rate": 1.6753821974567797e-05,
      "loss": 0.4601,
      "step": 11360
    },
    {
      "epoch": 2.4367766823831976,
      "grad_norm": 0.45686760544776917,
      "learning_rate": 1.675096442348907e-05,
      "loss": 0.3109,
      "step": 11370
    },
    {
      "epoch": 2.438919845692242,
      "grad_norm": 31.870454788208008,
      "learning_rate": 1.6748106872410345e-05,
      "loss": 0.9629,
      "step": 11380
    },
    {
      "epoch": 2.4410630090012857,
      "grad_norm": 14.640010833740234,
      "learning_rate": 1.674524932133162e-05,
      "loss": 0.4845,
      "step": 11390
    },
    {
      "epoch": 2.44320617231033,
      "grad_norm": 0.36151522397994995,
      "learning_rate": 1.6742391770252897e-05,
      "loss": 0.8218,
      "step": 11400
    },
    {
      "epoch": 2.4453493356193743,
      "grad_norm": 0.41299760341644287,
      "learning_rate": 1.673953421917417e-05,
      "loss": 0.4885,
      "step": 11410
    },
    {
      "epoch": 2.447492498928418,
      "grad_norm": 0.3682083189487457,
      "learning_rate": 1.6736676668095444e-05,
      "loss": 0.3235,
      "step": 11420
    },
    {
      "epoch": 2.4496356622374624,
      "grad_norm": 15.054451942443848,
      "learning_rate": 1.6733819117016718e-05,
      "loss": 0.4854,
      "step": 11430
    },
    {
      "epoch": 2.4517788255465067,
      "grad_norm": 0.31730401515960693,
      "learning_rate": 1.6730961565937992e-05,
      "loss": 0.3486,
      "step": 11440
    },
    {
      "epoch": 2.4539219888555506,
      "grad_norm": 15.293440818786621,
      "learning_rate": 1.6728104014859266e-05,
      "loss": 0.4965,
      "step": 11450
    },
    {
      "epoch": 2.456065152164595,
      "grad_norm": 14.763832092285156,
      "learning_rate": 1.6725246463780543e-05,
      "loss": 0.9874,
      "step": 11460
    },
    {
      "epoch": 2.458208315473639,
      "grad_norm": 14.666629791259766,
      "learning_rate": 1.6722388912701817e-05,
      "loss": 0.4824,
      "step": 11470
    },
    {
      "epoch": 2.460351478782683,
      "grad_norm": 0.2837251126766205,
      "learning_rate": 1.671953136162309e-05,
      "loss": 0.3224,
      "step": 11480
    },
    {
      "epoch": 2.4624946420917273,
      "grad_norm": 0.3655175566673279,
      "learning_rate": 1.6716673810544365e-05,
      "loss": 0.5015,
      "step": 11490
    },
    {
      "epoch": 2.4646378054007716,
      "grad_norm": 0.3711259663105011,
      "learning_rate": 1.671381625946564e-05,
      "loss": 0.4914,
      "step": 11500
    },
    {
      "epoch": 2.4667809687098154,
      "grad_norm": 0.349675714969635,
      "learning_rate": 1.6710958708386913e-05,
      "loss": 0.3274,
      "step": 11510
    },
    {
      "epoch": 2.4689241320188597,
      "grad_norm": 0.39432457089424133,
      "learning_rate": 1.6708101157308187e-05,
      "loss": 0.9738,
      "step": 11520
    },
    {
      "epoch": 2.471067295327904,
      "grad_norm": 0.40640565752983093,
      "learning_rate": 1.670524360622946e-05,
      "loss": 0.4836,
      "step": 11530
    },
    {
      "epoch": 2.4732104586369483,
      "grad_norm": 0.38408154249191284,
      "learning_rate": 1.6702386055150738e-05,
      "loss": 0.3214,
      "step": 11540
    },
    {
      "epoch": 2.475353621945992,
      "grad_norm": 0.4078867733478546,
      "learning_rate": 1.6699528504072012e-05,
      "loss": 0.6409,
      "step": 11550
    },
    {
      "epoch": 2.4774967852550365,
      "grad_norm": 0.30341607332229614,
      "learning_rate": 1.6696670952993286e-05,
      "loss": 0.1711,
      "step": 11560
    },
    {
      "epoch": 2.4796399485640808,
      "grad_norm": 0.28081271052360535,
      "learning_rate": 1.669381340191456e-05,
      "loss": 0.3429,
      "step": 11570
    },
    {
      "epoch": 2.4817831118731246,
      "grad_norm": 0.34775981307029724,
      "learning_rate": 1.6690955850835834e-05,
      "loss": 1.3641,
      "step": 11580
    },
    {
      "epoch": 2.483926275182169,
      "grad_norm": 14.714375495910645,
      "learning_rate": 1.6688098299757108e-05,
      "loss": 0.7968,
      "step": 11590
    },
    {
      "epoch": 2.486069438491213,
      "grad_norm": 0.4617200493812561,
      "learning_rate": 1.6685240748678385e-05,
      "loss": 0.6377,
      "step": 11600
    },
    {
      "epoch": 2.488212601800257,
      "grad_norm": 0.43237829208374023,
      "learning_rate": 1.668238319759966e-05,
      "loss": 0.1632,
      "step": 11610
    },
    {
      "epoch": 2.4903557651093013,
      "grad_norm": 0.4068787395954132,
      "learning_rate": 1.6679525646520933e-05,
      "loss": 0.7907,
      "step": 11620
    },
    {
      "epoch": 2.4924989284183456,
      "grad_norm": 14.693117141723633,
      "learning_rate": 1.6676668095442207e-05,
      "loss": 0.6221,
      "step": 11630
    },
    {
      "epoch": 2.4946420917273895,
      "grad_norm": 0.5915199518203735,
      "learning_rate": 1.667381054436348e-05,
      "loss": 0.4538,
      "step": 11640
    },
    {
      "epoch": 2.496785255036434,
      "grad_norm": 0.6219478845596313,
      "learning_rate": 1.6670952993284758e-05,
      "loss": 0.4445,
      "step": 11650
    },
    {
      "epoch": 2.498928418345478,
      "grad_norm": 14.652342796325684,
      "learning_rate": 1.6668095442206032e-05,
      "loss": 0.3109,
      "step": 11660
    },
    {
      "epoch": 2.501071581654522,
      "grad_norm": 14.687466621398926,
      "learning_rate": 1.6665237891127306e-05,
      "loss": 0.6235,
      "step": 11670
    },
    {
      "epoch": 2.503214744963566,
      "grad_norm": 0.508969247341156,
      "learning_rate": 1.666238034004858e-05,
      "loss": 0.9214,
      "step": 11680
    },
    {
      "epoch": 2.5053579082726105,
      "grad_norm": 0.5209168791770935,
      "learning_rate": 1.6659522788969854e-05,
      "loss": 0.4604,
      "step": 11690
    },
    {
      "epoch": 2.5075010715816544,
      "grad_norm": 29.5603084564209,
      "learning_rate": 1.665666523789113e-05,
      "loss": 0.7605,
      "step": 11700
    },
    {
      "epoch": 2.5096442348906987,
      "grad_norm": 14.754392623901367,
      "learning_rate": 1.6653807686812402e-05,
      "loss": 0.6086,
      "step": 11710
    },
    {
      "epoch": 2.511787398199743,
      "grad_norm": 0.49356403946876526,
      "learning_rate": 1.6650950135733676e-05,
      "loss": 0.1604,
      "step": 11720
    },
    {
      "epoch": 2.5139305615087872,
      "grad_norm": 0.4284224212169647,
      "learning_rate": 1.664809258465495e-05,
      "loss": 0.6244,
      "step": 11730
    },
    {
      "epoch": 2.516073724817831,
      "grad_norm": 14.585738182067871,
      "learning_rate": 1.6645235033576227e-05,
      "loss": 0.4672,
      "step": 11740
    },
    {
      "epoch": 2.5182168881268754,
      "grad_norm": 15.901724815368652,
      "learning_rate": 1.66423774824975e-05,
      "loss": 0.4735,
      "step": 11750
    },
    {
      "epoch": 2.5203600514359197,
      "grad_norm": 0.5087321400642395,
      "learning_rate": 1.6639519931418775e-05,
      "loss": 0.7688,
      "step": 11760
    },
    {
      "epoch": 2.5225032147449635,
      "grad_norm": 14.525686264038086,
      "learning_rate": 1.663666238034005e-05,
      "loss": 1.072,
      "step": 11770
    },
    {
      "epoch": 2.524646378054008,
      "grad_norm": 0.5570108294487,
      "learning_rate": 1.6633804829261323e-05,
      "loss": 0.1655,
      "step": 11780
    },
    {
      "epoch": 2.526789541363052,
      "grad_norm": 0.49362537264823914,
      "learning_rate": 1.66309472781826e-05,
      "loss": 0.3115,
      "step": 11790
    },
    {
      "epoch": 2.528932704672096,
      "grad_norm": 14.664368629455566,
      "learning_rate": 1.6628089727103874e-05,
      "loss": 0.1657,
      "step": 11800
    },
    {
      "epoch": 2.5310758679811403,
      "grad_norm": 0.39480772614479065,
      "learning_rate": 1.6625232176025148e-05,
      "loss": 0.9601,
      "step": 11810
    },
    {
      "epoch": 2.5332190312901846,
      "grad_norm": 0.4508158266544342,
      "learning_rate": 1.6622374624946422e-05,
      "loss": 0.9445,
      "step": 11820
    },
    {
      "epoch": 2.5353621945992284,
      "grad_norm": 14.58447265625,
      "learning_rate": 1.6619517073867696e-05,
      "loss": 0.4665,
      "step": 11830
    },
    {
      "epoch": 2.5375053579082727,
      "grad_norm": 15.112174034118652,
      "learning_rate": 1.6616659522788973e-05,
      "loss": 0.6202,
      "step": 11840
    },
    {
      "epoch": 2.539648521217317,
      "grad_norm": 14.604220390319824,
      "learning_rate": 1.6613801971710247e-05,
      "loss": 0.4693,
      "step": 11850
    },
    {
      "epoch": 2.541791684526361,
      "grad_norm": 14.594090461730957,
      "learning_rate": 1.661094442063152e-05,
      "loss": 0.767,
      "step": 11860
    },
    {
      "epoch": 2.543934847835405,
      "grad_norm": 0.4904593229293823,
      "learning_rate": 1.6608086869552795e-05,
      "loss": 0.4635,
      "step": 11870
    },
    {
      "epoch": 2.5460780111444494,
      "grad_norm": 0.5192161798477173,
      "learning_rate": 1.660522931847407e-05,
      "loss": 0.613,
      "step": 11880
    },
    {
      "epoch": 2.5482211744534933,
      "grad_norm": 0.5328896045684814,
      "learning_rate": 1.6602371767395343e-05,
      "loss": 0.4561,
      "step": 11890
    },
    {
      "epoch": 2.5503643377625376,
      "grad_norm": 0.5070648193359375,
      "learning_rate": 1.659951421631662e-05,
      "loss": 0.1587,
      "step": 11900
    },
    {
      "epoch": 2.552507501071582,
      "grad_norm": 0.4482298195362091,
      "learning_rate": 1.6596656665237894e-05,
      "loss": 0.614,
      "step": 11910
    },
    {
      "epoch": 2.5546506643806257,
      "grad_norm": 14.927559852600098,
      "learning_rate": 1.6593799114159164e-05,
      "loss": 0.781,
      "step": 11920
    },
    {
      "epoch": 2.55679382768967,
      "grad_norm": 0.45953240990638733,
      "learning_rate": 1.6590941563080442e-05,
      "loss": 0.4685,
      "step": 11930
    },
    {
      "epoch": 2.5589369909987143,
      "grad_norm": 0.47260451316833496,
      "learning_rate": 1.6588084012001716e-05,
      "loss": 0.6134,
      "step": 11940
    },
    {
      "epoch": 2.561080154307758,
      "grad_norm": 14.987680435180664,
      "learning_rate": 1.658522646092299e-05,
      "loss": 0.7726,
      "step": 11950
    },
    {
      "epoch": 2.5632233176168024,
      "grad_norm": 0.48789408802986145,
      "learning_rate": 1.6582368909844264e-05,
      "loss": 0.4629,
      "step": 11960
    },
    {
      "epoch": 2.5653664809258467,
      "grad_norm": 0.47527018189430237,
      "learning_rate": 1.6579511358765538e-05,
      "loss": 0.1624,
      "step": 11970
    },
    {
      "epoch": 2.5675096442348906,
      "grad_norm": 0.44209128618240356,
      "learning_rate": 1.6576653807686815e-05,
      "loss": 0.3193,
      "step": 11980
    },
    {
      "epoch": 2.569652807543935,
      "grad_norm": 0.3890415132045746,
      "learning_rate": 1.657379625660809e-05,
      "loss": 0.3277,
      "step": 11990
    },
    {
      "epoch": 2.571795970852979,
      "grad_norm": 0.362907737493515,
      "learning_rate": 1.6570938705529363e-05,
      "loss": 0.0083,
      "step": 12000
    },
    {
      "epoch": 2.573939134162023,
      "grad_norm": 0.3370411992073059,
      "learning_rate": 1.6568081154450637e-05,
      "loss": 1.1562,
      "step": 12010
    },
    {
      "epoch": 2.5760822974710673,
      "grad_norm": 0.377153605222702,
      "learning_rate": 1.656522360337191e-05,
      "loss": 0.4914,
      "step": 12020
    },
    {
      "epoch": 2.5782254607801116,
      "grad_norm": 29.45035171508789,
      "learning_rate": 1.6562366052293184e-05,
      "loss": 1.5856,
      "step": 12030
    },
    {
      "epoch": 2.5803686240891555,
      "grad_norm": 0.5475157499313354,
      "learning_rate": 1.6559508501214462e-05,
      "loss": 1.0627,
      "step": 12040
    },
    {
      "epoch": 2.5825117873981998,
      "grad_norm": 0.6232032775878906,
      "learning_rate": 1.6556650950135736e-05,
      "loss": 0.6047,
      "step": 12050
    },
    {
      "epoch": 2.584654950707244,
      "grad_norm": 0.5902038216590881,
      "learning_rate": 1.655379339905701e-05,
      "loss": 0.2999,
      "step": 12060
    },
    {
      "epoch": 2.586798114016288,
      "grad_norm": 15.066446304321289,
      "learning_rate": 1.6550935847978284e-05,
      "loss": 0.3063,
      "step": 12070
    },
    {
      "epoch": 2.588941277325332,
      "grad_norm": 0.5214199423789978,
      "learning_rate": 1.6548078296899558e-05,
      "loss": 1.0535,
      "step": 12080
    },
    {
      "epoch": 2.5910844406343765,
      "grad_norm": 0.37979164719581604,
      "learning_rate": 1.6545220745820835e-05,
      "loss": 0.1683,
      "step": 12090
    },
    {
      "epoch": 2.5932276039434203,
      "grad_norm": 0.3489111661911011,
      "learning_rate": 1.654236319474211e-05,
      "loss": 0.3322,
      "step": 12100
    },
    {
      "epoch": 2.5953707672524646,
      "grad_norm": 0.3081277012825012,
      "learning_rate": 1.6539505643663383e-05,
      "loss": 0.8273,
      "step": 12110
    },
    {
      "epoch": 2.597513930561509,
      "grad_norm": 0.33162921667099,
      "learning_rate": 1.6536648092584657e-05,
      "loss": 1.1291,
      "step": 12120
    },
    {
      "epoch": 2.5996570938705528,
      "grad_norm": 0.5509173274040222,
      "learning_rate": 1.653379054150593e-05,
      "loss": 1.4138,
      "step": 12130
    },
    {
      "epoch": 2.601800257179597,
      "grad_norm": 0.6463185548782349,
      "learning_rate": 1.6530932990427204e-05,
      "loss": 0.7418,
      "step": 12140
    },
    {
      "epoch": 2.6039434204886414,
      "grad_norm": 0.7688195109367371,
      "learning_rate": 1.652807543934848e-05,
      "loss": 0.5663,
      "step": 12150
    },
    {
      "epoch": 2.606086583797685,
      "grad_norm": 0.7149349451065063,
      "learning_rate": 1.6525217888269752e-05,
      "loss": 0.8457,
      "step": 12160
    },
    {
      "epoch": 2.6082297471067295,
      "grad_norm": 14.3738431930542,
      "learning_rate": 1.6522360337191026e-05,
      "loss": 0.5777,
      "step": 12170
    },
    {
      "epoch": 2.610372910415774,
      "grad_norm": 15.036450386047363,
      "learning_rate": 1.6519502786112304e-05,
      "loss": 0.2984,
      "step": 12180
    },
    {
      "epoch": 2.6125160737248176,
      "grad_norm": 0.5192086100578308,
      "learning_rate": 1.6516645235033577e-05,
      "loss": 0.7549,
      "step": 12190
    },
    {
      "epoch": 2.614659237033862,
      "grad_norm": 0.5410780906677246,
      "learning_rate": 1.651378768395485e-05,
      "loss": 0.4575,
      "step": 12200
    },
    {
      "epoch": 2.6168024003429062,
      "grad_norm": 0.43903785943984985,
      "learning_rate": 1.6510930132876125e-05,
      "loss": 0.3144,
      "step": 12210
    },
    {
      "epoch": 2.61894556365195,
      "grad_norm": 0.4549647867679596,
      "learning_rate": 1.65080725817974e-05,
      "loss": 0.4759,
      "step": 12220
    },
    {
      "epoch": 2.6210887269609944,
      "grad_norm": 0.3665798008441925,
      "learning_rate": 1.6505215030718677e-05,
      "loss": 0.1673,
      "step": 12230
    },
    {
      "epoch": 2.6232318902700387,
      "grad_norm": 0.3792969286441803,
      "learning_rate": 1.650235747963995e-05,
      "loss": 0.4932,
      "step": 12240
    },
    {
      "epoch": 2.6253750535790825,
      "grad_norm": 14.676292419433594,
      "learning_rate": 1.6499499928561224e-05,
      "loss": 1.2861,
      "step": 12250
    },
    {
      "epoch": 2.627518216888127,
      "grad_norm": 0.5337225794792175,
      "learning_rate": 1.64966423774825e-05,
      "loss": 0.923,
      "step": 12260
    },
    {
      "epoch": 2.629661380197171,
      "grad_norm": 0.6212235689163208,
      "learning_rate": 1.6493784826403772e-05,
      "loss": 0.7372,
      "step": 12270
    },
    {
      "epoch": 2.631804543506215,
      "grad_norm": 0.7528835535049438,
      "learning_rate": 1.649092727532505e-05,
      "loss": 0.5724,
      "step": 12280
    },
    {
      "epoch": 2.6339477068152592,
      "grad_norm": 14.509029388427734,
      "learning_rate": 1.6488069724246324e-05,
      "loss": 0.6975,
      "step": 12290
    },
    {
      "epoch": 2.6360908701243035,
      "grad_norm": 0.8203142285346985,
      "learning_rate": 1.6485212173167597e-05,
      "loss": 0.9636,
      "step": 12300
    },
    {
      "epoch": 2.6382340334333474,
      "grad_norm": 0.9131968021392822,
      "learning_rate": 1.648235462208887e-05,
      "loss": 0.677,
      "step": 12310
    },
    {
      "epoch": 2.6403771967423917,
      "grad_norm": 0.7816184759140015,
      "learning_rate": 1.6479497071010145e-05,
      "loss": 0.2814,
      "step": 12320
    },
    {
      "epoch": 2.642520360051436,
      "grad_norm": 14.453152656555176,
      "learning_rate": 1.6476639519931423e-05,
      "loss": 0.8255,
      "step": 12330
    },
    {
      "epoch": 2.64466352336048,
      "grad_norm": 0.6830412745475769,
      "learning_rate": 1.6473781968852697e-05,
      "loss": 0.5625,
      "step": 12340
    },
    {
      "epoch": 2.646806686669524,
      "grad_norm": 0.7738933563232422,
      "learning_rate": 1.6470924417773967e-05,
      "loss": 0.5724,
      "step": 12350
    },
    {
      "epoch": 2.6489498499785684,
      "grad_norm": 14.453102111816406,
      "learning_rate": 1.646806686669524e-05,
      "loss": 0.4341,
      "step": 12360
    },
    {
      "epoch": 2.6510930132876123,
      "grad_norm": 0.6783629655838013,
      "learning_rate": 1.646520931561652e-05,
      "loss": 1.0167,
      "step": 12370
    },
    {
      "epoch": 2.6532361765966566,
      "grad_norm": 14.681123733520508,
      "learning_rate": 1.6462351764537792e-05,
      "loss": 0.3021,
      "step": 12380
    },
    {
      "epoch": 2.655379339905701,
      "grad_norm": 0.5328580737113953,
      "learning_rate": 1.6459494213459066e-05,
      "loss": 0.5927,
      "step": 12390
    },
    {
      "epoch": 2.6575225032147447,
      "grad_norm": 0.6005557179450989,
      "learning_rate": 1.645663666238034e-05,
      "loss": 0.8898,
      "step": 12400
    },
    {
      "epoch": 2.659665666523789,
      "grad_norm": 0.6295058727264404,
      "learning_rate": 1.6453779111301614e-05,
      "loss": 0.4442,
      "step": 12410
    },
    {
      "epoch": 2.6618088298328333,
      "grad_norm": 29.511873245239258,
      "learning_rate": 1.645092156022289e-05,
      "loss": 0.4479,
      "step": 12420
    },
    {
      "epoch": 2.663951993141877,
      "grad_norm": 0.567111611366272,
      "learning_rate": 1.6448064009144165e-05,
      "loss": 1.0442,
      "step": 12430
    },
    {
      "epoch": 2.6660951564509214,
      "grad_norm": 0.5793830156326294,
      "learning_rate": 1.644520645806544e-05,
      "loss": 0.5911,
      "step": 12440
    },
    {
      "epoch": 2.6682383197599657,
      "grad_norm": 0.6037693023681641,
      "learning_rate": 1.6442348906986713e-05,
      "loss": 0.5882,
      "step": 12450
    },
    {
      "epoch": 2.67038148306901,
      "grad_norm": 0.5369989275932312,
      "learning_rate": 1.6439491355907987e-05,
      "loss": 0.4443,
      "step": 12460
    },
    {
      "epoch": 2.672524646378054,
      "grad_norm": 14.634456634521484,
      "learning_rate": 1.6436633804829264e-05,
      "loss": 0.4633,
      "step": 12470
    },
    {
      "epoch": 2.674667809687098,
      "grad_norm": 0.4234856963157654,
      "learning_rate": 1.643377625375054e-05,
      "loss": 0.4718,
      "step": 12480
    },
    {
      "epoch": 2.6768109729961425,
      "grad_norm": 14.67575740814209,
      "learning_rate": 1.6430918702671812e-05,
      "loss": 0.3295,
      "step": 12490
    },
    {
      "epoch": 2.6789541363051863,
      "grad_norm": 14.810346603393555,
      "learning_rate": 1.6428061151593086e-05,
      "loss": 0.8034,
      "step": 12500
    },
    {
      "epoch": 2.6810972996142306,
      "grad_norm": 14.696106910705566,
      "learning_rate": 1.642520360051436e-05,
      "loss": 0.4888,
      "step": 12510
    },
    {
      "epoch": 2.683240462923275,
      "grad_norm": 0.4041179418563843,
      "learning_rate": 1.6422346049435634e-05,
      "loss": 0.4873,
      "step": 12520
    },
    {
      "epoch": 2.6853836262323187,
      "grad_norm": 0.532084047794342,
      "learning_rate": 1.641948849835691e-05,
      "loss": 0.9211,
      "step": 12530
    },
    {
      "epoch": 2.687526789541363,
      "grad_norm": 0.5127865672111511,
      "learning_rate": 1.6416630947278185e-05,
      "loss": 0.4571,
      "step": 12540
    },
    {
      "epoch": 2.6896699528504073,
      "grad_norm": 0.4423689842224121,
      "learning_rate": 1.641377339619946e-05,
      "loss": 0.1592,
      "step": 12550
    },
    {
      "epoch": 2.6918131161594516,
      "grad_norm": 30.147705078125,
      "learning_rate": 1.6410915845120733e-05,
      "loss": 0.4852,
      "step": 12560
    },
    {
      "epoch": 2.6939562794684955,
      "grad_norm": 14.789042472839355,
      "learning_rate": 1.6408058294042007e-05,
      "loss": 0.6421,
      "step": 12570
    },
    {
      "epoch": 2.6960994427775398,
      "grad_norm": 0.4337696433067322,
      "learning_rate": 1.640520074296328e-05,
      "loss": 0.955,
      "step": 12580
    },
    {
      "epoch": 2.698242606086584,
      "grad_norm": 15.008687973022461,
      "learning_rate": 1.6402343191884555e-05,
      "loss": 0.4752,
      "step": 12590
    },
    {
      "epoch": 2.700385769395628,
      "grad_norm": 29.668474197387695,
      "learning_rate": 1.639948564080583e-05,
      "loss": 1.2225,
      "step": 12600
    },
    {
      "epoch": 2.702528932704672,
      "grad_norm": 0.5935567617416382,
      "learning_rate": 1.6396628089727106e-05,
      "loss": 0.7492,
      "step": 12610
    },
    {
      "epoch": 2.7046720960137165,
      "grad_norm": 14.456911087036133,
      "learning_rate": 1.639377053864838e-05,
      "loss": 0.5866,
      "step": 12620
    },
    {
      "epoch": 2.7068152593227603,
      "grad_norm": 0.5773200392723083,
      "learning_rate": 1.6390912987569654e-05,
      "loss": 0.1577,
      "step": 12630
    },
    {
      "epoch": 2.7089584226318046,
      "grad_norm": 0.4328716993331909,
      "learning_rate": 1.6388055436490928e-05,
      "loss": 0.3206,
      "step": 12640
    },
    {
      "epoch": 2.711101585940849,
      "grad_norm": 0.43332698941230774,
      "learning_rate": 1.6385197885412202e-05,
      "loss": 0.6334,
      "step": 12650
    },
    {
      "epoch": 2.713244749249893,
      "grad_norm": 0.42611029744148254,
      "learning_rate": 1.6382340334333476e-05,
      "loss": 0.3171,
      "step": 12660
    },
    {
      "epoch": 2.715387912558937,
      "grad_norm": 0.38139641284942627,
      "learning_rate": 1.6379482783254753e-05,
      "loss": 0.477,
      "step": 12670
    },
    {
      "epoch": 2.7175310758679814,
      "grad_norm": 14.662875175476074,
      "learning_rate": 1.6376625232176027e-05,
      "loss": 0.7926,
      "step": 12680
    },
    {
      "epoch": 2.719674239177025,
      "grad_norm": 0.45839422941207886,
      "learning_rate": 1.63737676810973e-05,
      "loss": 0.7846,
      "step": 12690
    },
    {
      "epoch": 2.7218174024860695,
      "grad_norm": 14.715587615966797,
      "learning_rate": 1.6370910130018575e-05,
      "loss": 0.9107,
      "step": 12700
    },
    {
      "epoch": 2.723960565795114,
      "grad_norm": 0.6572192311286926,
      "learning_rate": 1.636805257893985e-05,
      "loss": 0.5894,
      "step": 12710
    },
    {
      "epoch": 2.7261037291041577,
      "grad_norm": 29.220863342285156,
      "learning_rate": 1.6365195027861126e-05,
      "loss": 0.9982,
      "step": 12720
    },
    {
      "epoch": 2.728246892413202,
      "grad_norm": 0.7935431599617004,
      "learning_rate": 1.63623374767824e-05,
      "loss": 0.4204,
      "step": 12730
    },
    {
      "epoch": 2.7303900557222462,
      "grad_norm": 0.7136359214782715,
      "learning_rate": 1.6359479925703674e-05,
      "loss": 0.5616,
      "step": 12740
    },
    {
      "epoch": 2.73253321903129,
      "grad_norm": 14.289549827575684,
      "learning_rate": 1.6356622374624948e-05,
      "loss": 0.8252,
      "step": 12750
    },
    {
      "epoch": 2.7346763823403344,
      "grad_norm": 0.7364599108695984,
      "learning_rate": 1.6353764823546222e-05,
      "loss": 0.5427,
      "step": 12760
    },
    {
      "epoch": 2.7368195456493787,
      "grad_norm": 0.6820641756057739,
      "learning_rate": 1.63509072724675e-05,
      "loss": 0.2812,
      "step": 12770
    },
    {
      "epoch": 2.7389627089584225,
      "grad_norm": 0.619655966758728,
      "learning_rate": 1.634804972138877e-05,
      "loss": 0.874,
      "step": 12780
    },
    {
      "epoch": 2.741105872267467,
      "grad_norm": 0.5568414330482483,
      "learning_rate": 1.6345192170310044e-05,
      "loss": 0.5855,
      "step": 12790
    },
    {
      "epoch": 2.743249035576511,
      "grad_norm": 0.5280658602714539,
      "learning_rate": 1.6342334619231318e-05,
      "loss": 0.316,
      "step": 12800
    },
    {
      "epoch": 2.745392198885555,
      "grad_norm": 14.694458961486816,
      "learning_rate": 1.6339477068152595e-05,
      "loss": 0.7594,
      "step": 12810
    },
    {
      "epoch": 2.7475353621945993,
      "grad_norm": 14.729278564453125,
      "learning_rate": 1.633661951707387e-05,
      "loss": 0.3209,
      "step": 12820
    },
    {
      "epoch": 2.7496785255036436,
      "grad_norm": 0.37818267941474915,
      "learning_rate": 1.6333761965995143e-05,
      "loss": 0.1681,
      "step": 12830
    },
    {
      "epoch": 2.7518216888126874,
      "grad_norm": 14.715638160705566,
      "learning_rate": 1.6330904414916417e-05,
      "loss": 0.9716,
      "step": 12840
    },
    {
      "epoch": 2.7539648521217317,
      "grad_norm": 0.4277735650539398,
      "learning_rate": 1.632804686383769e-05,
      "loss": 0.7917,
      "step": 12850
    },
    {
      "epoch": 2.756108015430776,
      "grad_norm": 0.4863429665565491,
      "learning_rate": 1.6325189312758968e-05,
      "loss": 0.774,
      "step": 12860
    },
    {
      "epoch": 2.75825117873982,
      "grad_norm": 0.5037294626235962,
      "learning_rate": 1.6322331761680242e-05,
      "loss": 0.7662,
      "step": 12870
    },
    {
      "epoch": 2.760394342048864,
      "grad_norm": 0.6555799841880798,
      "learning_rate": 1.6319474210601516e-05,
      "loss": 1.1926,
      "step": 12880
    },
    {
      "epoch": 2.7625375053579084,
      "grad_norm": 0.6350488662719727,
      "learning_rate": 1.631661665952279e-05,
      "loss": 0.1562,
      "step": 12890
    },
    {
      "epoch": 2.7646806686669523,
      "grad_norm": 0.5235721468925476,
      "learning_rate": 1.6313759108444064e-05,
      "loss": 0.4433,
      "step": 12900
    },
    {
      "epoch": 2.7668238319759966,
      "grad_norm": 0.35999223589897156,
      "learning_rate": 1.631090155736534e-05,
      "loss": 0.0096,
      "step": 12910
    },
    {
      "epoch": 2.768966995285041,
      "grad_norm": 0.29815706610679626,
      "learning_rate": 1.6308044006286615e-05,
      "loss": 0.8232,
      "step": 12920
    },
    {
      "epoch": 2.7711101585940847,
      "grad_norm": 14.801420211791992,
      "learning_rate": 1.630518645520789e-05,
      "loss": 0.6613,
      "step": 12930
    },
    {
      "epoch": 2.773253321903129,
      "grad_norm": 15.386134147644043,
      "learning_rate": 1.6302328904129163e-05,
      "loss": 0.4911,
      "step": 12940
    },
    {
      "epoch": 2.7753964852121733,
      "grad_norm": 0.36251214146614075,
      "learning_rate": 1.6299471353050437e-05,
      "loss": 0.3249,
      "step": 12950
    },
    {
      "epoch": 2.777539648521217,
      "grad_norm": 0.37587040662765503,
      "learning_rate": 1.6296613801971714e-05,
      "loss": 0.8152,
      "step": 12960
    },
    {
      "epoch": 2.7796828118302614,
      "grad_norm": 0.3639920651912689,
      "learning_rate": 1.6293756250892988e-05,
      "loss": 0.1662,
      "step": 12970
    },
    {
      "epoch": 2.7818259751393057,
      "grad_norm": 15.03816032409668,
      "learning_rate": 1.6290898699814262e-05,
      "loss": 0.8102,
      "step": 12980
    },
    {
      "epoch": 2.7839691384483496,
      "grad_norm": 0.5561173558235168,
      "learning_rate": 1.6288041148735532e-05,
      "loss": 0.9292,
      "step": 12990
    },
    {
      "epoch": 2.786112301757394,
      "grad_norm": 0.6936416625976562,
      "learning_rate": 1.628518359765681e-05,
      "loss": 0.8732,
      "step": 13000
    },
    {
      "epoch": 2.788255465066438,
      "grad_norm": 0.563797652721405,
      "learning_rate": 1.6282326046578084e-05,
      "loss": 0.5669,
      "step": 13010
    },
    {
      "epoch": 2.790398628375482,
      "grad_norm": 15.247526168823242,
      "learning_rate": 1.6279468495499358e-05,
      "loss": 0.453,
      "step": 13020
    },
    {
      "epoch": 2.7925417916845263,
      "grad_norm": 15.025315284729004,
      "learning_rate": 1.627661094442063e-05,
      "loss": 0.5937,
      "step": 13030
    },
    {
      "epoch": 2.7946849549935706,
      "grad_norm": 0.6408072710037231,
      "learning_rate": 1.6273753393341905e-05,
      "loss": 0.8771,
      "step": 13040
    },
    {
      "epoch": 2.7968281183026145,
      "grad_norm": 0.5596199035644531,
      "learning_rate": 1.6270895842263183e-05,
      "loss": 0.4374,
      "step": 13050
    },
    {
      "epoch": 2.7989712816116588,
      "grad_norm": 0.5808319449424744,
      "learning_rate": 1.6268038291184457e-05,
      "loss": 0.4482,
      "step": 13060
    },
    {
      "epoch": 2.801114444920703,
      "grad_norm": 0.5134269595146179,
      "learning_rate": 1.626518074010573e-05,
      "loss": 0.7518,
      "step": 13070
    },
    {
      "epoch": 2.803257608229747,
      "grad_norm": 0.5465497374534607,
      "learning_rate": 1.6262323189027005e-05,
      "loss": 0.3001,
      "step": 13080
    },
    {
      "epoch": 2.805400771538791,
      "grad_norm": 0.49295440316200256,
      "learning_rate": 1.625946563794828e-05,
      "loss": 0.4544,
      "step": 13090
    },
    {
      "epoch": 2.8075439348478355,
      "grad_norm": 0.5723537802696228,
      "learning_rate": 1.6256608086869556e-05,
      "loss": 1.0786,
      "step": 13100
    },
    {
      "epoch": 2.8096870981568793,
      "grad_norm": 0.4605371356010437,
      "learning_rate": 1.625375053579083e-05,
      "loss": 0.3046,
      "step": 13110
    },
    {
      "epoch": 2.8118302614659236,
      "grad_norm": 0.46994006633758545,
      "learning_rate": 1.6250892984712104e-05,
      "loss": 0.6089,
      "step": 13120
    },
    {
      "epoch": 2.813973424774968,
      "grad_norm": 0.4278135895729065,
      "learning_rate": 1.6248035433633378e-05,
      "loss": 1.0949,
      "step": 13130
    },
    {
      "epoch": 2.8161165880840118,
      "grad_norm": 0.6148150563240051,
      "learning_rate": 1.624517788255465e-05,
      "loss": 0.919,
      "step": 13140
    },
    {
      "epoch": 2.818259751393056,
      "grad_norm": 29.782812118530273,
      "learning_rate": 1.6242320331475925e-05,
      "loss": 0.5976,
      "step": 13150
    },
    {
      "epoch": 2.8204029147021004,
      "grad_norm": 14.485200881958008,
      "learning_rate": 1.6239462780397203e-05,
      "loss": 0.8954,
      "step": 13160
    },
    {
      "epoch": 2.822546078011144,
      "grad_norm": 0.655539333820343,
      "learning_rate": 1.6236605229318477e-05,
      "loss": 0.7547,
      "step": 13170
    },
    {
      "epoch": 2.8246892413201885,
      "grad_norm": 14.933818817138672,
      "learning_rate": 1.623374767823975e-05,
      "loss": 0.7254,
      "step": 13180
    },
    {
      "epoch": 2.826832404629233,
      "grad_norm": 0.6750649809837341,
      "learning_rate": 1.6230890127161025e-05,
      "loss": 0.2931,
      "step": 13190
    },
    {
      "epoch": 2.8289755679382766,
      "grad_norm": 0.6333702802658081,
      "learning_rate": 1.62280325760823e-05,
      "loss": 0.7131,
      "step": 13200
    },
    {
      "epoch": 2.831118731247321,
      "grad_norm": 0.5928114056587219,
      "learning_rate": 1.6225175025003572e-05,
      "loss": 0.5919,
      "step": 13210
    },
    {
      "epoch": 2.8332618945563652,
      "grad_norm": 0.5023283362388611,
      "learning_rate": 1.6222317473924846e-05,
      "loss": 0.5943,
      "step": 13220
    },
    {
      "epoch": 2.835405057865409,
      "grad_norm": 0.47121167182922363,
      "learning_rate": 1.621945992284612e-05,
      "loss": 0.1635,
      "step": 13230
    },
    {
      "epoch": 2.8375482211744534,
      "grad_norm": 0.4018798768520355,
      "learning_rate": 1.6216602371767398e-05,
      "loss": 0.6345,
      "step": 13240
    },
    {
      "epoch": 2.8396913844834977,
      "grad_norm": 0.41598349809646606,
      "learning_rate": 1.621374482068867e-05,
      "loss": 0.6428,
      "step": 13250
    },
    {
      "epoch": 2.8418345477925415,
      "grad_norm": 0.419211745262146,
      "learning_rate": 1.6210887269609945e-05,
      "loss": 0.4862,
      "step": 13260
    },
    {
      "epoch": 2.843977711101586,
      "grad_norm": 0.46911728382110596,
      "learning_rate": 1.620802971853122e-05,
      "loss": 0.9407,
      "step": 13270
    },
    {
      "epoch": 2.84612087441063,
      "grad_norm": 0.6128149628639221,
      "learning_rate": 1.6205172167452493e-05,
      "loss": 1.0548,
      "step": 13280
    },
    {
      "epoch": 2.8482640377196744,
      "grad_norm": 0.6685038208961487,
      "learning_rate": 1.6202314616373767e-05,
      "loss": 0.4395,
      "step": 13290
    },
    {
      "epoch": 2.8504072010287183,
      "grad_norm": 14.686407089233398,
      "learning_rate": 1.6199457065295045e-05,
      "loss": 0.2942,
      "step": 13300
    },
    {
      "epoch": 2.8525503643377625,
      "grad_norm": 0.5143837332725525,
      "learning_rate": 1.619659951421632e-05,
      "loss": 0.5909,
      "step": 13310
    },
    {
      "epoch": 2.854693527646807,
      "grad_norm": 0.4075525999069214,
      "learning_rate": 1.6193741963137592e-05,
      "loss": 0.1653,
      "step": 13320
    },
    {
      "epoch": 2.8568366909558507,
      "grad_norm": 14.937796592712402,
      "learning_rate": 1.6190884412058866e-05,
      "loss": 0.493,
      "step": 13330
    },
    {
      "epoch": 2.858979854264895,
      "grad_norm": 14.774752616882324,
      "learning_rate": 1.618802686098014e-05,
      "loss": 0.6532,
      "step": 13340
    },
    {
      "epoch": 2.8611230175739393,
      "grad_norm": 0.38359493017196655,
      "learning_rate": 1.6185169309901418e-05,
      "loss": 0.4842,
      "step": 13350
    },
    {
      "epoch": 2.863266180882983,
      "grad_norm": 0.3790193498134613,
      "learning_rate": 1.618231175882269e-05,
      "loss": 0.6504,
      "step": 13360
    },
    {
      "epoch": 2.8654093441920274,
      "grad_norm": 0.4057411551475525,
      "learning_rate": 1.6179454207743965e-05,
      "loss": 0.4898,
      "step": 13370
    },
    {
      "epoch": 2.8675525075010717,
      "grad_norm": 14.713507652282715,
      "learning_rate": 1.617659665666524e-05,
      "loss": 0.4863,
      "step": 13380
    },
    {
      "epoch": 2.869695670810116,
      "grad_norm": 0.37381190061569214,
      "learning_rate": 1.6173739105586513e-05,
      "loss": 0.6475,
      "step": 13390
    },
    {
      "epoch": 2.87183883411916,
      "grad_norm": 0.41469287872314453,
      "learning_rate": 1.617088155450779e-05,
      "loss": 0.9561,
      "step": 13400
    },
    {
      "epoch": 2.873981997428204,
      "grad_norm": 0.4922780990600586,
      "learning_rate": 1.6168024003429065e-05,
      "loss": 1.0953,
      "step": 13410
    },
    {
      "epoch": 2.8761251607372484,
      "grad_norm": 0.506598949432373,
      "learning_rate": 1.6165166452350335e-05,
      "loss": 0.1613,
      "step": 13420
    },
    {
      "epoch": 2.8782683240462923,
      "grad_norm": 0.41572725772857666,
      "learning_rate": 1.616230890127161e-05,
      "loss": 0.1618,
      "step": 13430
    },
    {
      "epoch": 2.8804114873553366,
      "grad_norm": 0.3712266683578491,
      "learning_rate": 1.6159451350192886e-05,
      "loss": 0.4803,
      "step": 13440
    },
    {
      "epoch": 2.882554650664381,
      "grad_norm": 14.696379661560059,
      "learning_rate": 1.615659379911416e-05,
      "loss": 0.4855,
      "step": 13450
    },
    {
      "epoch": 2.8846978139734247,
      "grad_norm": 0.43230289220809937,
      "learning_rate": 1.6153736248035434e-05,
      "loss": 0.808,
      "step": 13460
    },
    {
      "epoch": 2.886840977282469,
      "grad_norm": 0.44457873702049255,
      "learning_rate": 1.6150878696956708e-05,
      "loss": 0.318,
      "step": 13470
    },
    {
      "epoch": 2.8889841405915133,
      "grad_norm": 14.732800483703613,
      "learning_rate": 1.6148021145877982e-05,
      "loss": 1.0997,
      "step": 13480
    },
    {
      "epoch": 2.891127303900557,
      "grad_norm": 0.5162996053695679,
      "learning_rate": 1.614516359479926e-05,
      "loss": 1.2274,
      "step": 13490
    },
    {
      "epoch": 2.8932704672096015,
      "grad_norm": 0.5550850629806519,
      "learning_rate": 1.6142306043720533e-05,
      "loss": 0.7445,
      "step": 13500
    },
    {
      "epoch": 2.8954136305186458,
      "grad_norm": 0.6434548497200012,
      "learning_rate": 1.6139448492641807e-05,
      "loss": 0.7316,
      "step": 13510
    },
    {
      "epoch": 2.8975567938276896,
      "grad_norm": 14.56886100769043,
      "learning_rate": 1.613659094156308e-05,
      "loss": 0.4539,
      "step": 13520
    },
    {
      "epoch": 2.899699957136734,
      "grad_norm": 0.4768484830856323,
      "learning_rate": 1.6133733390484355e-05,
      "loss": 0.6197,
      "step": 13530
    },
    {
      "epoch": 2.901843120445778,
      "grad_norm": 0.41941097378730774,
      "learning_rate": 1.6130875839405632e-05,
      "loss": 0.4642,
      "step": 13540
    },
    {
      "epoch": 2.903986283754822,
      "grad_norm": 14.738920211791992,
      "learning_rate": 1.6128018288326906e-05,
      "loss": 0.3306,
      "step": 13550
    },
    {
      "epoch": 2.9061294470638663,
      "grad_norm": 0.34120050072669983,
      "learning_rate": 1.612516073724818e-05,
      "loss": 0.9846,
      "step": 13560
    },
    {
      "epoch": 2.9082726103729106,
      "grad_norm": 14.72375774383545,
      "learning_rate": 1.6122303186169454e-05,
      "loss": 0.4898,
      "step": 13570
    },
    {
      "epoch": 2.9104157736819545,
      "grad_norm": 14.687445640563965,
      "learning_rate": 1.6119445635090728e-05,
      "loss": 0.3291,
      "step": 13580
    },
    {
      "epoch": 2.9125589369909988,
      "grad_norm": 0.32994547486305237,
      "learning_rate": 1.6116588084012005e-05,
      "loss": 0.3274,
      "step": 13590
    },
    {
      "epoch": 2.914702100300043,
      "grad_norm": 16.223939895629883,
      "learning_rate": 1.611373053293328e-05,
      "loss": 0.653,
      "step": 13600
    },
    {
      "epoch": 2.916845263609087,
      "grad_norm": 14.730320930480957,
      "learning_rate": 1.6110872981854553e-05,
      "loss": 0.4866,
      "step": 13610
    },
    {
      "epoch": 2.918988426918131,
      "grad_norm": 0.40823203325271606,
      "learning_rate": 1.6108015430775827e-05,
      "loss": 0.6492,
      "step": 13620
    },
    {
      "epoch": 2.9211315902271755,
      "grad_norm": 14.662015914916992,
      "learning_rate": 1.61051578796971e-05,
      "loss": 0.7915,
      "step": 13630
    },
    {
      "epoch": 2.9232747535362194,
      "grad_norm": 0.4258248507976532,
      "learning_rate": 1.6102300328618375e-05,
      "loss": 0.3195,
      "step": 13640
    },
    {
      "epoch": 2.9254179168452636,
      "grad_norm": 0.4120965898036957,
      "learning_rate": 1.609944277753965e-05,
      "loss": 0.476,
      "step": 13650
    },
    {
      "epoch": 2.927561080154308,
      "grad_norm": 0.4025125503540039,
      "learning_rate": 1.6096585226460923e-05,
      "loss": 0.6445,
      "step": 13660
    },
    {
      "epoch": 2.929704243463352,
      "grad_norm": 0.3902691602706909,
      "learning_rate": 1.6093727675382197e-05,
      "loss": 0.1672,
      "step": 13670
    },
    {
      "epoch": 2.931847406772396,
      "grad_norm": 0.3519936800003052,
      "learning_rate": 1.6090870124303474e-05,
      "loss": 0.8267,
      "step": 13680
    },
    {
      "epoch": 2.9339905700814404,
      "grad_norm": 14.662919998168945,
      "learning_rate": 1.6088012573224748e-05,
      "loss": 1.5561,
      "step": 13690
    },
    {
      "epoch": 2.9361337333904842,
      "grad_norm": 0.5930610299110413,
      "learning_rate": 1.6085155022146022e-05,
      "loss": 0.4633,
      "step": 13700
    },
    {
      "epoch": 2.9382768966995285,
      "grad_norm": 0.5923587083816528,
      "learning_rate": 1.6082297471067296e-05,
      "loss": 0.2997,
      "step": 13710
    },
    {
      "epoch": 2.940420060008573,
      "grad_norm": 0.5428489446640015,
      "learning_rate": 1.607943991998857e-05,
      "loss": 0.4477,
      "step": 13720
    },
    {
      "epoch": 2.9425632233176167,
      "grad_norm": 0.5331541895866394,
      "learning_rate": 1.6076582368909847e-05,
      "loss": 0.8952,
      "step": 13730
    },
    {
      "epoch": 2.944706386626661,
      "grad_norm": 14.644085884094238,
      "learning_rate": 1.607372481783112e-05,
      "loss": 0.6029,
      "step": 13740
    },
    {
      "epoch": 2.9468495499357052,
      "grad_norm": 0.525641143321991,
      "learning_rate": 1.6070867266752395e-05,
      "loss": 0.6031,
      "step": 13750
    },
    {
      "epoch": 2.948992713244749,
      "grad_norm": 0.5263434052467346,
      "learning_rate": 1.606800971567367e-05,
      "loss": 0.6182,
      "step": 13760
    },
    {
      "epoch": 2.9511358765537934,
      "grad_norm": 44.776885986328125,
      "learning_rate": 1.6065152164594943e-05,
      "loss": 1.0799,
      "step": 13770
    },
    {
      "epoch": 2.9532790398628377,
      "grad_norm": 14.7578763961792,
      "learning_rate": 1.6062294613516217e-05,
      "loss": 0.6097,
      "step": 13780
    },
    {
      "epoch": 2.9554222031718815,
      "grad_norm": 0.47157493233680725,
      "learning_rate": 1.6059437062437494e-05,
      "loss": 0.0108,
      "step": 13790
    },
    {
      "epoch": 2.957565366480926,
      "grad_norm": 15.210217475891113,
      "learning_rate": 1.6056579511358768e-05,
      "loss": 0.4889,
      "step": 13800
    },
    {
      "epoch": 2.95970852978997,
      "grad_norm": 0.3469042181968689,
      "learning_rate": 1.6053721960280042e-05,
      "loss": 0.4981,
      "step": 13810
    },
    {
      "epoch": 2.961851693099014,
      "grad_norm": 0.321667343378067,
      "learning_rate": 1.6050864409201316e-05,
      "loss": 0.1735,
      "step": 13820
    },
    {
      "epoch": 2.9639948564080583,
      "grad_norm": 0.304569274187088,
      "learning_rate": 1.604800685812259e-05,
      "loss": 0.5057,
      "step": 13830
    },
    {
      "epoch": 2.9661380197171026,
      "grad_norm": 0.3030241131782532,
      "learning_rate": 1.6045149307043867e-05,
      "loss": 0.5063,
      "step": 13840
    },
    {
      "epoch": 2.9682811830261464,
      "grad_norm": 15.00192642211914,
      "learning_rate": 1.6042291755965138e-05,
      "loss": 0.5031,
      "step": 13850
    },
    {
      "epoch": 2.9704243463351907,
      "grad_norm": 0.35357344150543213,
      "learning_rate": 1.603943420488641e-05,
      "loss": 0.6557,
      "step": 13860
    },
    {
      "epoch": 2.972567509644235,
      "grad_norm": 0.40110158920288086,
      "learning_rate": 1.603657665380769e-05,
      "loss": 0.3256,
      "step": 13870
    },
    {
      "epoch": 2.974710672953279,
      "grad_norm": 0.3463103771209717,
      "learning_rate": 1.6033719102728963e-05,
      "loss": 0.4853,
      "step": 13880
    },
    {
      "epoch": 2.976853836262323,
      "grad_norm": 0.346362829208374,
      "learning_rate": 1.6030861551650237e-05,
      "loss": 0.5038,
      "step": 13890
    },
    {
      "epoch": 2.9789969995713674,
      "grad_norm": 14.760698318481445,
      "learning_rate": 1.602800400057151e-05,
      "loss": 0.5026,
      "step": 13900
    },
    {
      "epoch": 2.9811401628804113,
      "grad_norm": 0.3297070860862732,
      "learning_rate": 1.6025146449492785e-05,
      "loss": 0.1676,
      "step": 13910
    },
    {
      "epoch": 2.9832833261894556,
      "grad_norm": 0.29686763882637024,
      "learning_rate": 1.602228889841406e-05,
      "loss": 0.8409,
      "step": 13920
    },
    {
      "epoch": 2.9854264894985,
      "grad_norm": 14.98590087890625,
      "learning_rate": 1.6019431347335336e-05,
      "loss": 0.8172,
      "step": 13930
    },
    {
      "epoch": 2.9875696528075437,
      "grad_norm": 0.43755975365638733,
      "learning_rate": 1.601657379625661e-05,
      "loss": 0.7995,
      "step": 13940
    },
    {
      "epoch": 2.989712816116588,
      "grad_norm": 0.5142638087272644,
      "learning_rate": 1.6013716245177884e-05,
      "loss": 0.6155,
      "step": 13950
    },
    {
      "epoch": 2.9918559794256323,
      "grad_norm": 14.663951873779297,
      "learning_rate": 1.6010858694099158e-05,
      "loss": 1.218,
      "step": 13960
    },
    {
      "epoch": 2.993999142734676,
      "grad_norm": 0.6067065000534058,
      "learning_rate": 1.600800114302043e-05,
      "loss": 0.59,
      "step": 13970
    },
    {
      "epoch": 2.9961423060437204,
      "grad_norm": 29.58512306213379,
      "learning_rate": 1.600514359194171e-05,
      "loss": 0.989,
      "step": 13980
    },
    {
      "epoch": 2.9982854693527647,
      "grad_norm": 0.6723664999008179,
      "learning_rate": 1.6002286040862983e-05,
      "loss": 0.5845,
      "step": 13990
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.8736666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.5812892913818359,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 428.1706,
      "eval_samples_per_second": 7.007,
      "eval_steps_per_second": 2.336,
      "step": 13998
    },
    {
      "epoch": 3.000428632661809,
      "grad_norm": 14.56609058380127,
      "learning_rate": 1.5999428489784257e-05,
      "loss": 0.4578,
      "step": 14000
    },
    {
      "epoch": 3.002571795970853,
      "grad_norm": 0.44515007734298706,
      "learning_rate": 1.599657093870553e-05,
      "loss": 0.4704,
      "step": 14010
    },
    {
      "epoch": 3.004714959279897,
      "grad_norm": 29.728031158447266,
      "learning_rate": 1.5993713387626805e-05,
      "loss": 0.6343,
      "step": 14020
    },
    {
      "epoch": 3.0068581225889415,
      "grad_norm": 15.872457504272461,
      "learning_rate": 1.5990855836548082e-05,
      "loss": 0.4936,
      "step": 14030
    },
    {
      "epoch": 3.0090012858979853,
      "grad_norm": 0.36851686239242554,
      "learning_rate": 1.5987998285469356e-05,
      "loss": 0.7908,
      "step": 14040
    },
    {
      "epoch": 3.0111444492070296,
      "grad_norm": 15.067193031311035,
      "learning_rate": 1.598514073439063e-05,
      "loss": 0.649,
      "step": 14050
    },
    {
      "epoch": 3.013287612516074,
      "grad_norm": 0.39801087975502014,
      "learning_rate": 1.5982283183311904e-05,
      "loss": 0.4859,
      "step": 14060
    },
    {
      "epoch": 3.0154307758251178,
      "grad_norm": 0.5046412348747253,
      "learning_rate": 1.5979425632233178e-05,
      "loss": 0.4769,
      "step": 14070
    },
    {
      "epoch": 3.017573939134162,
      "grad_norm": 0.5807194113731384,
      "learning_rate": 1.597656808115445e-05,
      "loss": 0.7501,
      "step": 14080
    },
    {
      "epoch": 3.0197171024432063,
      "grad_norm": 0.5512368083000183,
      "learning_rate": 1.5973710530075726e-05,
      "loss": 0.158,
      "step": 14090
    },
    {
      "epoch": 3.02186026575225,
      "grad_norm": 0.46241745352745056,
      "learning_rate": 1.5970852978997e-05,
      "loss": 0.3251,
      "step": 14100
    },
    {
      "epoch": 3.0240034290612945,
      "grad_norm": 29.60295867919922,
      "learning_rate": 1.5967995427918273e-05,
      "loss": 1.1155,
      "step": 14110
    },
    {
      "epoch": 3.026146592370339,
      "grad_norm": 14.65479850769043,
      "learning_rate": 1.596513787683955e-05,
      "loss": 0.9177,
      "step": 14120
    },
    {
      "epoch": 3.0282897556793826,
      "grad_norm": 0.5846084356307983,
      "learning_rate": 1.5962280325760825e-05,
      "loss": 0.7477,
      "step": 14130
    },
    {
      "epoch": 3.030432918988427,
      "grad_norm": 14.43506908416748,
      "learning_rate": 1.59594227746821e-05,
      "loss": 0.8593,
      "step": 14140
    },
    {
      "epoch": 3.032576082297471,
      "grad_norm": 15.27979850769043,
      "learning_rate": 1.5956565223603372e-05,
      "loss": 0.4274,
      "step": 14150
    },
    {
      "epoch": 3.034719245606515,
      "grad_norm": 0.6443917155265808,
      "learning_rate": 1.5953707672524646e-05,
      "loss": 0.7268,
      "step": 14160
    },
    {
      "epoch": 3.0368624089155594,
      "grad_norm": 0.6573412418365479,
      "learning_rate": 1.5950850121445924e-05,
      "loss": 0.5872,
      "step": 14170
    },
    {
      "epoch": 3.0390055722246037,
      "grad_norm": 29.323562622070312,
      "learning_rate": 1.5947992570367198e-05,
      "loss": 0.9978,
      "step": 14180
    },
    {
      "epoch": 3.0411487355336475,
      "grad_norm": 0.7186468839645386,
      "learning_rate": 1.594513501928847e-05,
      "loss": 0.7222,
      "step": 14190
    },
    {
      "epoch": 3.043291898842692,
      "grad_norm": 0.7318240404129028,
      "learning_rate": 1.5942277468209746e-05,
      "loss": 0.5657,
      "step": 14200
    },
    {
      "epoch": 3.045435062151736,
      "grad_norm": 0.5731766819953918,
      "learning_rate": 1.593941991713102e-05,
      "loss": 0.4367,
      "step": 14210
    },
    {
      "epoch": 3.04757822546078,
      "grad_norm": 15.36801815032959,
      "learning_rate": 1.5936562366052293e-05,
      "loss": 0.7347,
      "step": 14220
    },
    {
      "epoch": 3.0497213887698242,
      "grad_norm": 0.5794695019721985,
      "learning_rate": 1.593370481497357e-05,
      "loss": 0.5863,
      "step": 14230
    },
    {
      "epoch": 3.0518645520788685,
      "grad_norm": 15.025426864624023,
      "learning_rate": 1.5930847263894845e-05,
      "loss": 0.1643,
      "step": 14240
    },
    {
      "epoch": 3.0540077153879124,
      "grad_norm": 0.4666331112384796,
      "learning_rate": 1.592798971281612e-05,
      "loss": 0.7703,
      "step": 14250
    },
    {
      "epoch": 3.0561508786969567,
      "grad_norm": 0.4416208267211914,
      "learning_rate": 1.5925132161737392e-05,
      "loss": 0.7754,
      "step": 14260
    },
    {
      "epoch": 3.058294042006001,
      "grad_norm": 0.49882376194000244,
      "learning_rate": 1.5922274610658666e-05,
      "loss": 0.4706,
      "step": 14270
    },
    {
      "epoch": 3.060437205315045,
      "grad_norm": 14.607734680175781,
      "learning_rate": 1.591941705957994e-05,
      "loss": 1.0498,
      "step": 14280
    },
    {
      "epoch": 3.062580368624089,
      "grad_norm": 0.5047459602355957,
      "learning_rate": 1.5916559508501214e-05,
      "loss": 0.156,
      "step": 14290
    },
    {
      "epoch": 3.0647235319331334,
      "grad_norm": 0.4639275074005127,
      "learning_rate": 1.5913701957422488e-05,
      "loss": 0.9179,
      "step": 14300
    },
    {
      "epoch": 3.0668666952421773,
      "grad_norm": 0.5175226926803589,
      "learning_rate": 1.5910844406343765e-05,
      "loss": 0.4645,
      "step": 14310
    },
    {
      "epoch": 3.0690098585512215,
      "grad_norm": 15.648069381713867,
      "learning_rate": 1.590798685526504e-05,
      "loss": 0.9026,
      "step": 14320
    },
    {
      "epoch": 3.071153021860266,
      "grad_norm": 14.577486038208008,
      "learning_rate": 1.5905129304186313e-05,
      "loss": 0.4485,
      "step": 14330
    },
    {
      "epoch": 3.0732961851693097,
      "grad_norm": 15.08803939819336,
      "learning_rate": 1.5902271753107587e-05,
      "loss": 0.3069,
      "step": 14340
    },
    {
      "epoch": 3.075439348478354,
      "grad_norm": 0.5064159035682678,
      "learning_rate": 1.589941420202886e-05,
      "loss": 1.0485,
      "step": 14350
    },
    {
      "epoch": 3.0775825117873983,
      "grad_norm": 15.054120063781738,
      "learning_rate": 1.5896556650950135e-05,
      "loss": 0.6019,
      "step": 14360
    },
    {
      "epoch": 3.0797256750964426,
      "grad_norm": 29.348167419433594,
      "learning_rate": 1.5893699099871412e-05,
      "loss": 1.0016,
      "step": 14370
    },
    {
      "epoch": 3.0818688384054864,
      "grad_norm": 0.7502221465110779,
      "learning_rate": 1.5890841548792686e-05,
      "loss": 0.4268,
      "step": 14380
    },
    {
      "epoch": 3.0840120017145307,
      "grad_norm": 14.597850799560547,
      "learning_rate": 1.588798399771396e-05,
      "loss": 0.2941,
      "step": 14390
    },
    {
      "epoch": 3.086155165023575,
      "grad_norm": 14.519759178161621,
      "learning_rate": 1.5885126446635234e-05,
      "loss": 0.7307,
      "step": 14400
    },
    {
      "epoch": 3.088298328332619,
      "grad_norm": 0.5503296256065369,
      "learning_rate": 1.5882268895556508e-05,
      "loss": 0.1574,
      "step": 14410
    },
    {
      "epoch": 3.090441491641663,
      "grad_norm": 14.742683410644531,
      "learning_rate": 1.5879411344477785e-05,
      "loss": 0.9039,
      "step": 14420
    },
    {
      "epoch": 3.0925846549507074,
      "grad_norm": 15.071491241455078,
      "learning_rate": 1.587655379339906e-05,
      "loss": 0.4561,
      "step": 14430
    },
    {
      "epoch": 3.0947278182597513,
      "grad_norm": 0.47260648012161255,
      "learning_rate": 1.5873696242320333e-05,
      "loss": 0.1611,
      "step": 14440
    },
    {
      "epoch": 3.0968709815687956,
      "grad_norm": 0.4000261723995209,
      "learning_rate": 1.5870838691241607e-05,
      "loss": 0.3206,
      "step": 14450
    },
    {
      "epoch": 3.09901414487784,
      "grad_norm": 14.993179321289062,
      "learning_rate": 1.586798114016288e-05,
      "loss": 0.9842,
      "step": 14460
    },
    {
      "epoch": 3.1011573081868837,
      "grad_norm": 0.5024294257164001,
      "learning_rate": 1.586512358908416e-05,
      "loss": 0.6228,
      "step": 14470
    },
    {
      "epoch": 3.103300471495928,
      "grad_norm": 0.5654188394546509,
      "learning_rate": 1.5862266038005432e-05,
      "loss": 0.3114,
      "step": 14480
    },
    {
      "epoch": 3.1054436348049723,
      "grad_norm": 0.5140573382377625,
      "learning_rate": 1.5859408486926706e-05,
      "loss": 0.4567,
      "step": 14490
    },
    {
      "epoch": 3.107586798114016,
      "grad_norm": 0.473766028881073,
      "learning_rate": 1.5856550935847977e-05,
      "loss": 0.4551,
      "step": 14500
    },
    {
      "epoch": 3.1097299614230605,
      "grad_norm": 15.041109085083008,
      "learning_rate": 1.5853693384769254e-05,
      "loss": 0.3175,
      "step": 14510
    },
    {
      "epoch": 3.1118731247321048,
      "grad_norm": 15.221464157104492,
      "learning_rate": 1.5850835833690528e-05,
      "loss": 0.7882,
      "step": 14520
    },
    {
      "epoch": 3.1140162880411486,
      "grad_norm": 14.754039764404297,
      "learning_rate": 1.5847978282611802e-05,
      "loss": 0.6398,
      "step": 14530
    },
    {
      "epoch": 3.116159451350193,
      "grad_norm": 0.47735875844955444,
      "learning_rate": 1.5845120731533076e-05,
      "loss": 0.4716,
      "step": 14540
    },
    {
      "epoch": 3.118302614659237,
      "grad_norm": 0.4906747341156006,
      "learning_rate": 1.584226318045435e-05,
      "loss": 1.2126,
      "step": 14550
    },
    {
      "epoch": 3.120445777968281,
      "grad_norm": 29.79414939880371,
      "learning_rate": 1.5839405629375627e-05,
      "loss": 0.7449,
      "step": 14560
    },
    {
      "epoch": 3.1225889412773253,
      "grad_norm": 0.541357696056366,
      "learning_rate": 1.58365480782969e-05,
      "loss": 0.4501,
      "step": 14570
    },
    {
      "epoch": 3.1247321045863696,
      "grad_norm": 0.50734543800354,
      "learning_rate": 1.5833690527218175e-05,
      "loss": 0.3052,
      "step": 14580
    },
    {
      "epoch": 3.1268752678954135,
      "grad_norm": 0.41291865706443787,
      "learning_rate": 1.583083297613945e-05,
      "loss": 0.3186,
      "step": 14590
    },
    {
      "epoch": 3.1290184312044578,
      "grad_norm": 29.961063385009766,
      "learning_rate": 1.5827975425060723e-05,
      "loss": 0.7984,
      "step": 14600
    },
    {
      "epoch": 3.131161594513502,
      "grad_norm": 0.3331853151321411,
      "learning_rate": 1.5825117873982e-05,
      "loss": 0.5042,
      "step": 14610
    },
    {
      "epoch": 3.133304757822546,
      "grad_norm": 14.905261993408203,
      "learning_rate": 1.5822260322903274e-05,
      "loss": 0.9709,
      "step": 14620
    },
    {
      "epoch": 3.13544792113159,
      "grad_norm": 0.4519560933113098,
      "learning_rate": 1.5819402771824548e-05,
      "loss": 0.4698,
      "step": 14630
    },
    {
      "epoch": 3.1375910844406345,
      "grad_norm": 0.442341148853302,
      "learning_rate": 1.5816545220745822e-05,
      "loss": 0.3179,
      "step": 14640
    },
    {
      "epoch": 3.1397342477496784,
      "grad_norm": 14.650875091552734,
      "learning_rate": 1.5813687669667096e-05,
      "loss": 0.9317,
      "step": 14650
    },
    {
      "epoch": 3.1418774110587226,
      "grad_norm": 14.696366310119629,
      "learning_rate": 1.5810830118588373e-05,
      "loss": 0.3223,
      "step": 14660
    },
    {
      "epoch": 3.144020574367767,
      "grad_norm": 15.658092498779297,
      "learning_rate": 1.5807972567509647e-05,
      "loss": 0.7913,
      "step": 14670
    },
    {
      "epoch": 3.146163737676811,
      "grad_norm": 0.5116154551506042,
      "learning_rate": 1.580511501643092e-05,
      "loss": 0.4724,
      "step": 14680
    },
    {
      "epoch": 3.148306900985855,
      "grad_norm": 0.4020639657974243,
      "learning_rate": 1.5802257465352195e-05,
      "loss": 0.1645,
      "step": 14690
    },
    {
      "epoch": 3.1504500642948994,
      "grad_norm": 14.919608116149902,
      "learning_rate": 1.579939991427347e-05,
      "loss": 0.6853,
      "step": 14700
    },
    {
      "epoch": 3.1525932276039432,
      "grad_norm": 0.32460248470306396,
      "learning_rate": 1.5796542363194743e-05,
      "loss": 0.6652,
      "step": 14710
    },
    {
      "epoch": 3.1547363909129875,
      "grad_norm": 14.766828536987305,
      "learning_rate": 1.5793684812116017e-05,
      "loss": 0.3337,
      "step": 14720
    },
    {
      "epoch": 3.156879554222032,
      "grad_norm": 0.3086468577384949,
      "learning_rate": 1.579082726103729e-05,
      "loss": 0.67,
      "step": 14730
    },
    {
      "epoch": 3.1590227175310757,
      "grad_norm": 0.33538222312927246,
      "learning_rate": 1.5787969709958565e-05,
      "loss": 0.8631,
      "step": 14740
    },
    {
      "epoch": 3.16116588084012,
      "grad_norm": 0.3740931451320648,
      "learning_rate": 1.5785112158879842e-05,
      "loss": 0.4891,
      "step": 14750
    },
    {
      "epoch": 3.1633090441491643,
      "grad_norm": 0.34231188893318176,
      "learning_rate": 1.5782254607801116e-05,
      "loss": 0.3398,
      "step": 14760
    },
    {
      "epoch": 3.165452207458208,
      "grad_norm": 0.3322213888168335,
      "learning_rate": 1.577939705672239e-05,
      "loss": 0.5082,
      "step": 14770
    },
    {
      "epoch": 3.1675953707672524,
      "grad_norm": 0.33477845788002014,
      "learning_rate": 1.5776539505643664e-05,
      "loss": 0.5024,
      "step": 14780
    },
    {
      "epoch": 3.1697385340762967,
      "grad_norm": 0.3303543031215668,
      "learning_rate": 1.5773681954564938e-05,
      "loss": 0.8355,
      "step": 14790
    },
    {
      "epoch": 3.1718816973853405,
      "grad_norm": 14.679548263549805,
      "learning_rate": 1.5770824403486215e-05,
      "loss": 1.1432,
      "step": 14800
    },
    {
      "epoch": 3.174024860694385,
      "grad_norm": 14.97593879699707,
      "learning_rate": 1.576796685240749e-05,
      "loss": 0.4842,
      "step": 14810
    },
    {
      "epoch": 3.176168024003429,
      "grad_norm": 0.4016622006893158,
      "learning_rate": 1.5765109301328763e-05,
      "loss": 0.6408,
      "step": 14820
    },
    {
      "epoch": 3.1783111873124734,
      "grad_norm": 14.890289306640625,
      "learning_rate": 1.5762251750250037e-05,
      "loss": 0.4711,
      "step": 14830
    },
    {
      "epoch": 3.1804543506215173,
      "grad_norm": 0.42525795102119446,
      "learning_rate": 1.575939419917131e-05,
      "loss": 0.4791,
      "step": 14840
    },
    {
      "epoch": 3.1825975139305616,
      "grad_norm": 15.271672248840332,
      "learning_rate": 1.5756536648092585e-05,
      "loss": 0.9628,
      "step": 14850
    },
    {
      "epoch": 3.184740677239606,
      "grad_norm": 0.5220919251441956,
      "learning_rate": 1.5753679097013862e-05,
      "loss": 1.381,
      "step": 14860
    },
    {
      "epoch": 3.1868838405486497,
      "grad_norm": 14.495641708374023,
      "learning_rate": 1.5750821545935136e-05,
      "loss": 0.4457,
      "step": 14870
    },
    {
      "epoch": 3.189027003857694,
      "grad_norm": 0.7338224053382874,
      "learning_rate": 1.574796399485641e-05,
      "loss": 0.4396,
      "step": 14880
    },
    {
      "epoch": 3.1911701671667383,
      "grad_norm": 14.872636795043945,
      "learning_rate": 1.5745106443777684e-05,
      "loss": 0.7255,
      "step": 14890
    },
    {
      "epoch": 3.193313330475782,
      "grad_norm": 0.6555531024932861,
      "learning_rate": 1.5742248892698958e-05,
      "loss": 1.0063,
      "step": 14900
    },
    {
      "epoch": 3.1954564937848264,
      "grad_norm": 15.387621879577637,
      "learning_rate": 1.5739391341620235e-05,
      "loss": 0.3074,
      "step": 14910
    },
    {
      "epoch": 3.1975996570938707,
      "grad_norm": 0.5902043581008911,
      "learning_rate": 1.573653379054151e-05,
      "loss": 0.4475,
      "step": 14920
    },
    {
      "epoch": 3.1997428204029146,
      "grad_norm": 0.5629281997680664,
      "learning_rate": 1.573367623946278e-05,
      "loss": 0.4497,
      "step": 14930
    },
    {
      "epoch": 3.201885983711959,
      "grad_norm": 0.4879128336906433,
      "learning_rate": 1.5730818688384057e-05,
      "loss": 0.3094,
      "step": 14940
    },
    {
      "epoch": 3.204029147021003,
      "grad_norm": 0.49892646074295044,
      "learning_rate": 1.572796113730533e-05,
      "loss": 0.6101,
      "step": 14950
    },
    {
      "epoch": 3.206172310330047,
      "grad_norm": 0.38331881165504456,
      "learning_rate": 1.5725103586226605e-05,
      "loss": 0.3202,
      "step": 14960
    },
    {
      "epoch": 3.2083154736390913,
      "grad_norm": 14.843953132629395,
      "learning_rate": 1.572224603514788e-05,
      "loss": 0.4952,
      "step": 14970
    },
    {
      "epoch": 3.2104586369481356,
      "grad_norm": 14.664734840393066,
      "learning_rate": 1.5719388484069153e-05,
      "loss": 1.3088,
      "step": 14980
    },
    {
      "epoch": 3.2126018002571795,
      "grad_norm": 0.4208545982837677,
      "learning_rate": 1.5716530932990426e-05,
      "loss": 0.3206,
      "step": 14990
    },
    {
      "epoch": 3.2147449635662237,
      "grad_norm": 14.831483840942383,
      "learning_rate": 1.5713673381911704e-05,
      "loss": 0.7916,
      "step": 15000
    },
    {
      "epoch": 3.216888126875268,
      "grad_norm": 0.49137163162231445,
      "learning_rate": 1.5710815830832978e-05,
      "loss": 0.468,
      "step": 15010
    },
    {
      "epoch": 3.219031290184312,
      "grad_norm": 0.46067723631858826,
      "learning_rate": 1.570795827975425e-05,
      "loss": 0.1627,
      "step": 15020
    },
    {
      "epoch": 3.221174453493356,
      "grad_norm": 0.395921915769577,
      "learning_rate": 1.5705100728675526e-05,
      "loss": 0.325,
      "step": 15030
    },
    {
      "epoch": 3.2233176168024005,
      "grad_norm": 14.7974214553833,
      "learning_rate": 1.57022431775968e-05,
      "loss": 1.2741,
      "step": 15040
    },
    {
      "epoch": 3.2254607801114443,
      "grad_norm": 0.4764846861362457,
      "learning_rate": 1.5699385626518077e-05,
      "loss": 0.4748,
      "step": 15050
    },
    {
      "epoch": 3.2276039434204886,
      "grad_norm": 0.45059290528297424,
      "learning_rate": 1.569652807543935e-05,
      "loss": 0.4595,
      "step": 15060
    },
    {
      "epoch": 3.229747106729533,
      "grad_norm": 14.900956153869629,
      "learning_rate": 1.5693670524360625e-05,
      "loss": 0.5983,
      "step": 15070
    },
    {
      "epoch": 3.2318902700385768,
      "grad_norm": 14.580296516418457,
      "learning_rate": 1.56908129732819e-05,
      "loss": 0.6002,
      "step": 15080
    },
    {
      "epoch": 3.234033433347621,
      "grad_norm": 0.6893566250801086,
      "learning_rate": 1.5687955422203173e-05,
      "loss": 0.8698,
      "step": 15090
    },
    {
      "epoch": 3.2361765966566653,
      "grad_norm": 15.817022323608398,
      "learning_rate": 1.568509787112445e-05,
      "loss": 0.724,
      "step": 15100
    },
    {
      "epoch": 3.238319759965709,
      "grad_norm": 0.8184433579444885,
      "learning_rate": 1.5682240320045724e-05,
      "loss": 0.5919,
      "step": 15110
    },
    {
      "epoch": 3.2404629232747535,
      "grad_norm": 0.5668817758560181,
      "learning_rate": 1.5679382768966998e-05,
      "loss": 0.1513,
      "step": 15120
    },
    {
      "epoch": 3.242606086583798,
      "grad_norm": 0.4313105642795563,
      "learning_rate": 1.567652521788827e-05,
      "loss": 0.7829,
      "step": 15130
    },
    {
      "epoch": 3.2447492498928416,
      "grad_norm": 0.35750341415405273,
      "learning_rate": 1.5673667666809546e-05,
      "loss": 0.6518,
      "step": 15140
    },
    {
      "epoch": 3.246892413201886,
      "grad_norm": 0.301809161901474,
      "learning_rate": 1.567081011573082e-05,
      "loss": 0.1716,
      "step": 15150
    },
    {
      "epoch": 3.2490355765109302,
      "grad_norm": 0.3159802556037903,
      "learning_rate": 1.5667952564652093e-05,
      "loss": 0.3378,
      "step": 15160
    },
    {
      "epoch": 3.2511787398199745,
      "grad_norm": 14.87564754486084,
      "learning_rate": 1.5665095013573367e-05,
      "loss": 0.3429,
      "step": 15170
    },
    {
      "epoch": 3.2533219031290184,
      "grad_norm": 14.988435745239258,
      "learning_rate": 1.566223746249464e-05,
      "loss": 1.1831,
      "step": 15180
    },
    {
      "epoch": 3.2554650664380627,
      "grad_norm": 0.3293652832508087,
      "learning_rate": 1.565937991141592e-05,
      "loss": 0.3387,
      "step": 15190
    },
    {
      "epoch": 3.257608229747107,
      "grad_norm": 0.31352555751800537,
      "learning_rate": 1.5656522360337193e-05,
      "loss": 0.6617,
      "step": 15200
    },
    {
      "epoch": 3.259751393056151,
      "grad_norm": 0.35284680128097534,
      "learning_rate": 1.5653664809258466e-05,
      "loss": 0.3338,
      "step": 15210
    },
    {
      "epoch": 3.261894556365195,
      "grad_norm": 14.848278045654297,
      "learning_rate": 1.565080725817974e-05,
      "loss": 1.1509,
      "step": 15220
    },
    {
      "epoch": 3.2640377196742394,
      "grad_norm": 0.38950005173683167,
      "learning_rate": 1.5647949707101014e-05,
      "loss": 0.4879,
      "step": 15230
    },
    {
      "epoch": 3.2661808829832832,
      "grad_norm": 0.40514716506004333,
      "learning_rate": 1.564509215602229e-05,
      "loss": 0.6653,
      "step": 15240
    },
    {
      "epoch": 3.2683240462923275,
      "grad_norm": 0.3668033480644226,
      "learning_rate": 1.5642234604943566e-05,
      "loss": 0.0087,
      "step": 15250
    },
    {
      "epoch": 3.270467209601372,
      "grad_norm": 30.059160232543945,
      "learning_rate": 1.563937705386484e-05,
      "loss": 0.8189,
      "step": 15260
    },
    {
      "epoch": 3.2726103729104157,
      "grad_norm": 0.3439328968524933,
      "learning_rate": 1.5636519502786113e-05,
      "loss": 0.6637,
      "step": 15270
    },
    {
      "epoch": 3.27475353621946,
      "grad_norm": 0.32522520422935486,
      "learning_rate": 1.5633661951707387e-05,
      "loss": 0.3267,
      "step": 15280
    },
    {
      "epoch": 3.2768966995285043,
      "grad_norm": 0.3289448320865631,
      "learning_rate": 1.5630804400628665e-05,
      "loss": 0.7991,
      "step": 15290
    },
    {
      "epoch": 3.279039862837548,
      "grad_norm": 14.722947120666504,
      "learning_rate": 1.562794684954994e-05,
      "loss": 0.4766,
      "step": 15300
    },
    {
      "epoch": 3.2811830261465924,
      "grad_norm": 0.3958187699317932,
      "learning_rate": 1.5625089298471213e-05,
      "loss": 0.3247,
      "step": 15310
    },
    {
      "epoch": 3.2833261894556367,
      "grad_norm": 15.693222045898438,
      "learning_rate": 1.5622231747392486e-05,
      "loss": 0.8215,
      "step": 15320
    },
    {
      "epoch": 3.2854693527646806,
      "grad_norm": 29.81221580505371,
      "learning_rate": 1.561937419631376e-05,
      "loss": 0.9548,
      "step": 15330
    },
    {
      "epoch": 3.287612516073725,
      "grad_norm": 0.5398578643798828,
      "learning_rate": 1.5616516645235034e-05,
      "loss": 0.47,
      "step": 15340
    },
    {
      "epoch": 3.289755679382769,
      "grad_norm": 29.52557373046875,
      "learning_rate": 1.561365909415631e-05,
      "loss": 1.0525,
      "step": 15350
    },
    {
      "epoch": 3.291898842691813,
      "grad_norm": 29.51909637451172,
      "learning_rate": 1.5610801543077582e-05,
      "loss": 1.163,
      "step": 15360
    },
    {
      "epoch": 3.2940420060008573,
      "grad_norm": 0.6860796213150024,
      "learning_rate": 1.5607943991998856e-05,
      "loss": 0.2951,
      "step": 15370
    },
    {
      "epoch": 3.2961851693099016,
      "grad_norm": 14.500847816467285,
      "learning_rate": 1.5605086440920133e-05,
      "loss": 1.139,
      "step": 15380
    },
    {
      "epoch": 3.2983283326189454,
      "grad_norm": 0.7524436116218567,
      "learning_rate": 1.5602228889841407e-05,
      "loss": 0.7161,
      "step": 15390
    },
    {
      "epoch": 3.3004714959279897,
      "grad_norm": 0.9579776525497437,
      "learning_rate": 1.559937133876268e-05,
      "loss": 0.8216,
      "step": 15400
    },
    {
      "epoch": 3.302614659237034,
      "grad_norm": 0.9098203778266907,
      "learning_rate": 1.5596513787683955e-05,
      "loss": 0.6823,
      "step": 15410
    },
    {
      "epoch": 3.304757822546078,
      "grad_norm": 1.031319260597229,
      "learning_rate": 1.559365623660523e-05,
      "loss": 0.926,
      "step": 15420
    },
    {
      "epoch": 3.306900985855122,
      "grad_norm": 0.9369280934333801,
      "learning_rate": 1.5590798685526506e-05,
      "loss": 0.2809,
      "step": 15430
    },
    {
      "epoch": 3.3090441491641664,
      "grad_norm": 0.8034947514533997,
      "learning_rate": 1.558794113444778e-05,
      "loss": 0.8092,
      "step": 15440
    },
    {
      "epoch": 3.3111873124732103,
      "grad_norm": 14.301173210144043,
      "learning_rate": 1.5585083583369054e-05,
      "loss": 0.6876,
      "step": 15450
    },
    {
      "epoch": 3.3133304757822546,
      "grad_norm": 0.7021748423576355,
      "learning_rate": 1.5582226032290328e-05,
      "loss": 0.5572,
      "step": 15460
    },
    {
      "epoch": 3.315473639091299,
      "grad_norm": 0.6081012487411499,
      "learning_rate": 1.5579368481211602e-05,
      "loss": 0.5784,
      "step": 15470
    },
    {
      "epoch": 3.3176168024003427,
      "grad_norm": 0.6326202750205994,
      "learning_rate": 1.5576510930132876e-05,
      "loss": 0.5852,
      "step": 15480
    },
    {
      "epoch": 3.319759965709387,
      "grad_norm": 0.6323580145835876,
      "learning_rate": 1.5573653379054153e-05,
      "loss": 0.5809,
      "step": 15490
    },
    {
      "epoch": 3.3219031290184313,
      "grad_norm": 14.455182075500488,
      "learning_rate": 1.5570795827975427e-05,
      "loss": 1.0075,
      "step": 15500
    },
    {
      "epoch": 3.324046292327475,
      "grad_norm": 0.690826952457428,
      "learning_rate": 1.55679382768967e-05,
      "loss": 0.297,
      "step": 15510
    },
    {
      "epoch": 3.3261894556365195,
      "grad_norm": 14.412525177001953,
      "learning_rate": 1.5565080725817975e-05,
      "loss": 1.1355,
      "step": 15520
    },
    {
      "epoch": 3.3283326189455638,
      "grad_norm": 0.7990928888320923,
      "learning_rate": 1.556222317473925e-05,
      "loss": 0.4239,
      "step": 15530
    },
    {
      "epoch": 3.3304757822546076,
      "grad_norm": 14.291589736938477,
      "learning_rate": 1.5559365623660526e-05,
      "loss": 1.2352,
      "step": 15540
    },
    {
      "epoch": 3.332618945563652,
      "grad_norm": 0.8405022621154785,
      "learning_rate": 1.55565080725818e-05,
      "loss": 0.2851,
      "step": 15550
    },
    {
      "epoch": 3.334762108872696,
      "grad_norm": 0.6998637318611145,
      "learning_rate": 1.5553650521503074e-05,
      "loss": 0.5715,
      "step": 15560
    },
    {
      "epoch": 3.33690527218174,
      "grad_norm": 0.7160356640815735,
      "learning_rate": 1.5550792970424348e-05,
      "loss": 0.9893,
      "step": 15570
    },
    {
      "epoch": 3.3390484354907843,
      "grad_norm": 14.369097709655762,
      "learning_rate": 1.5547935419345622e-05,
      "loss": 0.5647,
      "step": 15580
    },
    {
      "epoch": 3.3411915987998286,
      "grad_norm": 14.440569877624512,
      "learning_rate": 1.5545077868266896e-05,
      "loss": 0.573,
      "step": 15590
    },
    {
      "epoch": 3.3433347621088725,
      "grad_norm": 0.5437867045402527,
      "learning_rate": 1.554222031718817e-05,
      "loss": 0.0129,
      "step": 15600
    },
    {
      "epoch": 3.3454779254179168,
      "grad_norm": 0.4722121059894562,
      "learning_rate": 1.5539362766109444e-05,
      "loss": 0.6145,
      "step": 15610
    },
    {
      "epoch": 3.347621088726961,
      "grad_norm": 0.4410026967525482,
      "learning_rate": 1.5536505215030718e-05,
      "loss": 0.1615,
      "step": 15620
    },
    {
      "epoch": 3.349764252036005,
      "grad_norm": 0.412675142288208,
      "learning_rate": 1.5533647663951995e-05,
      "loss": 0.4768,
      "step": 15630
    },
    {
      "epoch": 3.351907415345049,
      "grad_norm": 0.41798681020736694,
      "learning_rate": 1.553079011287327e-05,
      "loss": 0.6378,
      "step": 15640
    },
    {
      "epoch": 3.3540505786540935,
      "grad_norm": 0.40599408745765686,
      "learning_rate": 1.5527932561794543e-05,
      "loss": 0.1647,
      "step": 15650
    },
    {
      "epoch": 3.3561937419631374,
      "grad_norm": 0.3710549473762512,
      "learning_rate": 1.5525075010715817e-05,
      "loss": 0.327,
      "step": 15660
    },
    {
      "epoch": 3.3583369052721816,
      "grad_norm": 14.806018829345703,
      "learning_rate": 1.552221745963709e-05,
      "loss": 0.6509,
      "step": 15670
    },
    {
      "epoch": 3.360480068581226,
      "grad_norm": 0.36582836508750916,
      "learning_rate": 1.5519359908558368e-05,
      "loss": 0.333,
      "step": 15680
    },
    {
      "epoch": 3.36262323189027,
      "grad_norm": 0.35650959610939026,
      "learning_rate": 1.5516502357479642e-05,
      "loss": 0.1664,
      "step": 15690
    },
    {
      "epoch": 3.364766395199314,
      "grad_norm": 0.2992384731769562,
      "learning_rate": 1.5513644806400916e-05,
      "loss": 0.3386,
      "step": 15700
    },
    {
      "epoch": 3.3669095585083584,
      "grad_norm": 0.2929329574108124,
      "learning_rate": 1.551078725532219e-05,
      "loss": 0.8428,
      "step": 15710
    },
    {
      "epoch": 3.3690527218174027,
      "grad_norm": 14.711490631103516,
      "learning_rate": 1.5507929704243464e-05,
      "loss": 0.9881,
      "step": 15720
    },
    {
      "epoch": 3.3711958851264465,
      "grad_norm": 0.41540759801864624,
      "learning_rate": 1.550507215316474e-05,
      "loss": 0.4873,
      "step": 15730
    },
    {
      "epoch": 3.373339048435491,
      "grad_norm": 0.43897706270217896,
      "learning_rate": 1.5502214602086015e-05,
      "loss": 0.7863,
      "step": 15740
    },
    {
      "epoch": 3.375482211744535,
      "grad_norm": 0.4660596251487732,
      "learning_rate": 1.549935705100729e-05,
      "loss": 0.4694,
      "step": 15750
    },
    {
      "epoch": 3.377625375053579,
      "grad_norm": 14.65650463104248,
      "learning_rate": 1.5496499499928563e-05,
      "loss": 0.7842,
      "step": 15760
    },
    {
      "epoch": 3.3797685383626233,
      "grad_norm": 0.4965467154979706,
      "learning_rate": 1.5493641948849837e-05,
      "loss": 0.3145,
      "step": 15770
    },
    {
      "epoch": 3.3819117016716675,
      "grad_norm": 14.663686752319336,
      "learning_rate": 1.549078439777111e-05,
      "loss": 0.166,
      "step": 15780
    },
    {
      "epoch": 3.3840548649807114,
      "grad_norm": 0.37421709299087524,
      "learning_rate": 1.5487926846692385e-05,
      "loss": 0.1668,
      "step": 15790
    },
    {
      "epoch": 3.3861980282897557,
      "grad_norm": 0.3738282322883606,
      "learning_rate": 1.548506929561366e-05,
      "loss": 0.9842,
      "step": 15800
    },
    {
      "epoch": 3.3883411915988,
      "grad_norm": 0.4669957756996155,
      "learning_rate": 1.5482211744534933e-05,
      "loss": 1.2564,
      "step": 15810
    },
    {
      "epoch": 3.390484354907844,
      "grad_norm": 14.726048469543457,
      "learning_rate": 1.547935419345621e-05,
      "loss": 0.4461,
      "step": 15820
    },
    {
      "epoch": 3.392627518216888,
      "grad_norm": 0.5734215974807739,
      "learning_rate": 1.5476496642377484e-05,
      "loss": 0.1563,
      "step": 15830
    },
    {
      "epoch": 3.3947706815259324,
      "grad_norm": 14.545650482177734,
      "learning_rate": 1.5473639091298758e-05,
      "loss": 0.745,
      "step": 15840
    },
    {
      "epoch": 3.3969138448349763,
      "grad_norm": 0.5284873843193054,
      "learning_rate": 1.5470781540220032e-05,
      "loss": 0.3068,
      "step": 15850
    },
    {
      "epoch": 3.3990570081440206,
      "grad_norm": 15.03903579711914,
      "learning_rate": 1.5467923989141306e-05,
      "loss": 0.6079,
      "step": 15860
    },
    {
      "epoch": 3.401200171453065,
      "grad_norm": 0.441104531288147,
      "learning_rate": 1.5465066438062583e-05,
      "loss": 0.0107,
      "step": 15870
    },
    {
      "epoch": 3.4033433347621087,
      "grad_norm": 0.37904834747314453,
      "learning_rate": 1.5462208886983857e-05,
      "loss": 0.6331,
      "step": 15880
    },
    {
      "epoch": 3.405486498071153,
      "grad_norm": 0.40235763788223267,
      "learning_rate": 1.545935133590513e-05,
      "loss": 0.6442,
      "step": 15890
    },
    {
      "epoch": 3.4076296613801973,
      "grad_norm": 29.902698516845703,
      "learning_rate": 1.5456493784826405e-05,
      "loss": 0.7897,
      "step": 15900
    },
    {
      "epoch": 3.409772824689241,
      "grad_norm": 15.054726600646973,
      "learning_rate": 1.545363623374768e-05,
      "loss": 0.6371,
      "step": 15910
    },
    {
      "epoch": 3.4119159879982854,
      "grad_norm": 0.46865808963775635,
      "learning_rate": 1.5450778682668953e-05,
      "loss": 0.6114,
      "step": 15920
    },
    {
      "epoch": 3.4140591513073297,
      "grad_norm": 0.4869787395000458,
      "learning_rate": 1.544792113159023e-05,
      "loss": 0.4655,
      "step": 15930
    },
    {
      "epoch": 3.4162023146163736,
      "grad_norm": 14.791306495666504,
      "learning_rate": 1.5445063580511504e-05,
      "loss": 0.6153,
      "step": 15940
    },
    {
      "epoch": 3.418345477925418,
      "grad_norm": 0.5344299077987671,
      "learning_rate": 1.5442206029432778e-05,
      "loss": 0.7767,
      "step": 15950
    },
    {
      "epoch": 3.420488641234462,
      "grad_norm": 0.5404255390167236,
      "learning_rate": 1.5439348478354052e-05,
      "loss": 1.0563,
      "step": 15960
    },
    {
      "epoch": 3.4226318045435065,
      "grad_norm": 0.46975621581077576,
      "learning_rate": 1.5436490927275326e-05,
      "loss": 0.1603,
      "step": 15970
    },
    {
      "epoch": 3.4247749678525503,
      "grad_norm": 0.4404856562614441,
      "learning_rate": 1.5433633376196603e-05,
      "loss": 0.6271,
      "step": 15980
    },
    {
      "epoch": 3.4269181311615946,
      "grad_norm": 14.904631614685059,
      "learning_rate": 1.5430775825117877e-05,
      "loss": 1.1384,
      "step": 15990
    },
    {
      "epoch": 3.429061294470639,
      "grad_norm": 0.5133549571037292,
      "learning_rate": 1.5427918274039147e-05,
      "loss": 1.0955,
      "step": 16000
    },
    {
      "epoch": 3.4312044577796827,
      "grad_norm": 0.545111894607544,
      "learning_rate": 1.5425060722960425e-05,
      "loss": 0.4585,
      "step": 16010
    },
    {
      "epoch": 3.433347621088727,
      "grad_norm": 14.525612831115723,
      "learning_rate": 1.54222031718817e-05,
      "loss": 0.8894,
      "step": 16020
    },
    {
      "epoch": 3.4354907843977713,
      "grad_norm": 14.535158157348633,
      "learning_rate": 1.5419345620802973e-05,
      "loss": 0.3046,
      "step": 16030
    },
    {
      "epoch": 3.437633947706815,
      "grad_norm": 0.5278614163398743,
      "learning_rate": 1.5416488069724247e-05,
      "loss": 0.8844,
      "step": 16040
    },
    {
      "epoch": 3.4397771110158595,
      "grad_norm": 0.6435720920562744,
      "learning_rate": 1.541363051864552e-05,
      "loss": 0.588,
      "step": 16050
    },
    {
      "epoch": 3.4419202743249038,
      "grad_norm": 0.630513608455658,
      "learning_rate": 1.5410772967566794e-05,
      "loss": 0.5761,
      "step": 16060
    },
    {
      "epoch": 3.4440634376339476,
      "grad_norm": 0.6295143365859985,
      "learning_rate": 1.5407915416488072e-05,
      "loss": 0.7241,
      "step": 16070
    },
    {
      "epoch": 3.446206600942992,
      "grad_norm": 14.567438125610352,
      "learning_rate": 1.5405057865409346e-05,
      "loss": 1.0135,
      "step": 16080
    },
    {
      "epoch": 3.448349764252036,
      "grad_norm": 14.473539352416992,
      "learning_rate": 1.540220031433062e-05,
      "loss": 0.7295,
      "step": 16090
    },
    {
      "epoch": 3.45049292756108,
      "grad_norm": 0.6819483637809753,
      "learning_rate": 1.5399342763251894e-05,
      "loss": 0.7264,
      "step": 16100
    },
    {
      "epoch": 3.4526360908701244,
      "grad_norm": 0.6052443981170654,
      "learning_rate": 1.5396485212173167e-05,
      "loss": 0.4393,
      "step": 16110
    },
    {
      "epoch": 3.4547792541791686,
      "grad_norm": 0.5359182357788086,
      "learning_rate": 1.5393627661094445e-05,
      "loss": 0.5976,
      "step": 16120
    },
    {
      "epoch": 3.4569224174882125,
      "grad_norm": 0.5056331753730774,
      "learning_rate": 1.539077011001572e-05,
      "loss": 0.4558,
      "step": 16130
    },
    {
      "epoch": 3.459065580797257,
      "grad_norm": 0.5559750199317932,
      "learning_rate": 1.5387912558936993e-05,
      "loss": 0.9158,
      "step": 16140
    },
    {
      "epoch": 3.461208744106301,
      "grad_norm": 14.673741340637207,
      "learning_rate": 1.5385055007858267e-05,
      "loss": 0.8897,
      "step": 16150
    },
    {
      "epoch": 3.463351907415345,
      "grad_norm": 0.540274977684021,
      "learning_rate": 1.538219745677954e-05,
      "loss": 0.1551,
      "step": 16160
    },
    {
      "epoch": 3.4654950707243892,
      "grad_norm": 0.5125916004180908,
      "learning_rate": 1.5379339905700818e-05,
      "loss": 0.6053,
      "step": 16170
    },
    {
      "epoch": 3.4676382340334335,
      "grad_norm": 0.5322217345237732,
      "learning_rate": 1.5376482354622092e-05,
      "loss": 0.7721,
      "step": 16180
    },
    {
      "epoch": 3.4697813973424774,
      "grad_norm": 14.701091766357422,
      "learning_rate": 1.5373624803543366e-05,
      "loss": 0.6151,
      "step": 16190
    },
    {
      "epoch": 3.4719245606515217,
      "grad_norm": 0.4799099564552307,
      "learning_rate": 1.537076725246464e-05,
      "loss": 0.7781,
      "step": 16200
    },
    {
      "epoch": 3.474067723960566,
      "grad_norm": 0.6019014120101929,
      "learning_rate": 1.5367909701385914e-05,
      "loss": 0.6029,
      "step": 16210
    },
    {
      "epoch": 3.47621088726961,
      "grad_norm": 0.6046006083488464,
      "learning_rate": 1.5365052150307187e-05,
      "loss": 0.9009,
      "step": 16220
    },
    {
      "epoch": 3.478354050578654,
      "grad_norm": 0.548670768737793,
      "learning_rate": 1.536219459922846e-05,
      "loss": 0.4583,
      "step": 16230
    },
    {
      "epoch": 3.4804972138876984,
      "grad_norm": 29.369348526000977,
      "learning_rate": 1.5359337048149735e-05,
      "loss": 0.4588,
      "step": 16240
    },
    {
      "epoch": 3.4826403771967422,
      "grad_norm": 29.89725685119629,
      "learning_rate": 1.535647949707101e-05,
      "loss": 0.6164,
      "step": 16250
    },
    {
      "epoch": 3.4847835405057865,
      "grad_norm": 0.42386943101882935,
      "learning_rate": 1.5353621945992287e-05,
      "loss": 0.1657,
      "step": 16260
    },
    {
      "epoch": 3.486926703814831,
      "grad_norm": 0.3702516257762909,
      "learning_rate": 1.535076439491356e-05,
      "loss": 0.3256,
      "step": 16270
    },
    {
      "epoch": 3.4890698671238747,
      "grad_norm": 0.3415338397026062,
      "learning_rate": 1.5347906843834834e-05,
      "loss": 0.495,
      "step": 16280
    },
    {
      "epoch": 3.491213030432919,
      "grad_norm": 14.725193977355957,
      "learning_rate": 1.534504929275611e-05,
      "loss": 0.6579,
      "step": 16290
    },
    {
      "epoch": 3.4933561937419633,
      "grad_norm": 14.686793327331543,
      "learning_rate": 1.5342191741677382e-05,
      "loss": 0.8071,
      "step": 16300
    },
    {
      "epoch": 3.495499357051007,
      "grad_norm": 14.726613998413086,
      "learning_rate": 1.533933419059866e-05,
      "loss": 0.4785,
      "step": 16310
    },
    {
      "epoch": 3.4976425203600514,
      "grad_norm": 0.4185697138309479,
      "learning_rate": 1.5336476639519933e-05,
      "loss": 0.3202,
      "step": 16320
    },
    {
      "epoch": 3.4997856836690957,
      "grad_norm": 0.4251890182495117,
      "learning_rate": 1.5333619088441207e-05,
      "loss": 0.3237,
      "step": 16330
    },
    {
      "epoch": 3.5019288469781396,
      "grad_norm": 0.370290607213974,
      "learning_rate": 1.533076153736248e-05,
      "loss": 0.4899,
      "step": 16340
    },
    {
      "epoch": 3.504072010287184,
      "grad_norm": 14.826559066772461,
      "learning_rate": 1.5327903986283755e-05,
      "loss": 0.6524,
      "step": 16350
    },
    {
      "epoch": 3.506215173596228,
      "grad_norm": 0.37690606713294983,
      "learning_rate": 1.5325046435205033e-05,
      "loss": 0.4884,
      "step": 16360
    },
    {
      "epoch": 3.508358336905272,
      "grad_norm": 14.94876766204834,
      "learning_rate": 1.5322188884126307e-05,
      "loss": 0.4995,
      "step": 16370
    },
    {
      "epoch": 3.5105015002143163,
      "grad_norm": 14.701217651367188,
      "learning_rate": 1.531933133304758e-05,
      "loss": 0.3278,
      "step": 16380
    },
    {
      "epoch": 3.5126446635233606,
      "grad_norm": 14.911558151245117,
      "learning_rate": 1.5316473781968854e-05,
      "loss": 0.4993,
      "step": 16390
    },
    {
      "epoch": 3.5147878268324044,
      "grad_norm": 14.755101203918457,
      "learning_rate": 1.531361623089013e-05,
      "loss": 0.8337,
      "step": 16400
    },
    {
      "epoch": 3.5169309901414487,
      "grad_norm": 0.35805293917655945,
      "learning_rate": 1.5310758679811402e-05,
      "loss": 0.3375,
      "step": 16410
    },
    {
      "epoch": 3.519074153450493,
      "grad_norm": 30.320751190185547,
      "learning_rate": 1.530790112873268e-05,
      "loss": 0.9763,
      "step": 16420
    },
    {
      "epoch": 3.521217316759537,
      "grad_norm": 0.5076680779457092,
      "learning_rate": 1.530504357765395e-05,
      "loss": 0.7939,
      "step": 16430
    },
    {
      "epoch": 3.523360480068581,
      "grad_norm": 14.558797836303711,
      "learning_rate": 1.5302186026575224e-05,
      "loss": 0.7597,
      "step": 16440
    },
    {
      "epoch": 3.5255036433776255,
      "grad_norm": 29.649192810058594,
      "learning_rate": 1.52993284754965e-05,
      "loss": 1.1857,
      "step": 16450
    },
    {
      "epoch": 3.5276468066866693,
      "grad_norm": 0.634706437587738,
      "learning_rate": 1.5296470924417775e-05,
      "loss": 0.5889,
      "step": 16460
    },
    {
      "epoch": 3.5297899699957136,
      "grad_norm": 0.6865851283073425,
      "learning_rate": 1.529361337333905e-05,
      "loss": 0.2927,
      "step": 16470
    },
    {
      "epoch": 3.531933133304758,
      "grad_norm": 14.455921173095703,
      "learning_rate": 1.5290755822260323e-05,
      "loss": 0.7219,
      "step": 16480
    },
    {
      "epoch": 3.5340762966138017,
      "grad_norm": 14.511609077453613,
      "learning_rate": 1.5287898271181597e-05,
      "loss": 0.5831,
      "step": 16490
    },
    {
      "epoch": 3.536219459922846,
      "grad_norm": 0.6558049321174622,
      "learning_rate": 1.5285040720102874e-05,
      "loss": 0.4419,
      "step": 16500
    },
    {
      "epoch": 3.5383626232318903,
      "grad_norm": 0.527731716632843,
      "learning_rate": 1.5282183169024148e-05,
      "loss": 0.1578,
      "step": 16510
    },
    {
      "epoch": 3.540505786540934,
      "grad_norm": 0.4638310372829437,
      "learning_rate": 1.5279325617945422e-05,
      "loss": 0.311,
      "step": 16520
    },
    {
      "epoch": 3.5426489498499785,
      "grad_norm": 0.404852032661438,
      "learning_rate": 1.5276468066866696e-05,
      "loss": 0.4758,
      "step": 16530
    },
    {
      "epoch": 3.5447921131590228,
      "grad_norm": 0.42236387729644775,
      "learning_rate": 1.527361051578797e-05,
      "loss": 0.7945,
      "step": 16540
    },
    {
      "epoch": 3.5469352764680666,
      "grad_norm": 14.677356719970703,
      "learning_rate": 1.5270752964709244e-05,
      "loss": 0.7897,
      "step": 16550
    },
    {
      "epoch": 3.549078439777111,
      "grad_norm": 0.465244859457016,
      "learning_rate": 1.526789541363052e-05,
      "loss": 0.7892,
      "step": 16560
    },
    {
      "epoch": 3.551221603086155,
      "grad_norm": 14.654472351074219,
      "learning_rate": 1.5265037862551795e-05,
      "loss": 0.6219,
      "step": 16570
    },
    {
      "epoch": 3.553364766395199,
      "grad_norm": 0.5156700015068054,
      "learning_rate": 1.526218031147307e-05,
      "loss": 0.9326,
      "step": 16580
    },
    {
      "epoch": 3.5555079297042433,
      "grad_norm": 0.5015915036201477,
      "learning_rate": 1.5259322760394343e-05,
      "loss": 0.6048,
      "step": 16590
    },
    {
      "epoch": 3.5576510930132876,
      "grad_norm": 14.498247146606445,
      "learning_rate": 1.5256465209315619e-05,
      "loss": 0.8875,
      "step": 16600
    },
    {
      "epoch": 3.5597942563223315,
      "grad_norm": 0.5699117183685303,
      "learning_rate": 1.5253607658236893e-05,
      "loss": 0.7361,
      "step": 16610
    },
    {
      "epoch": 3.561937419631376,
      "grad_norm": 0.575724720954895,
      "learning_rate": 1.5250750107158168e-05,
      "loss": 0.5882,
      "step": 16620
    },
    {
      "epoch": 3.56408058294042,
      "grad_norm": 0.536608874797821,
      "learning_rate": 1.5247892556079442e-05,
      "loss": 0.1612,
      "step": 16630
    },
    {
      "epoch": 3.5662237462494644,
      "grad_norm": 14.526944160461426,
      "learning_rate": 1.5245035005000714e-05,
      "loss": 1.332,
      "step": 16640
    },
    {
      "epoch": 3.568366909558508,
      "grad_norm": 0.6019639372825623,
      "learning_rate": 1.5242177453921988e-05,
      "loss": 0.446,
      "step": 16650
    },
    {
      "epoch": 3.5705100728675525,
      "grad_norm": 0.5444756150245667,
      "learning_rate": 1.5239319902843264e-05,
      "loss": 0.4489,
      "step": 16660
    },
    {
      "epoch": 3.572653236176597,
      "grad_norm": 0.5538817644119263,
      "learning_rate": 1.5236462351764538e-05,
      "loss": 0.4508,
      "step": 16670
    },
    {
      "epoch": 3.5747963994856407,
      "grad_norm": 0.5326846241950989,
      "learning_rate": 1.5233604800685814e-05,
      "loss": 0.7484,
      "step": 16680
    },
    {
      "epoch": 3.576939562794685,
      "grad_norm": 0.45815807580947876,
      "learning_rate": 1.5230747249607087e-05,
      "loss": 0.1616,
      "step": 16690
    },
    {
      "epoch": 3.5790827261037292,
      "grad_norm": 15.013374328613281,
      "learning_rate": 1.5227889698528361e-05,
      "loss": 0.7786,
      "step": 16700
    },
    {
      "epoch": 3.581225889412773,
      "grad_norm": 0.5016583204269409,
      "learning_rate": 1.5225032147449637e-05,
      "loss": 0.3221,
      "step": 16710
    },
    {
      "epoch": 3.5833690527218174,
      "grad_norm": 0.4015488922595978,
      "learning_rate": 1.5222174596370911e-05,
      "loss": 0.1633,
      "step": 16720
    },
    {
      "epoch": 3.5855122160308617,
      "grad_norm": 0.35652825236320496,
      "learning_rate": 1.5219317045292185e-05,
      "loss": 0.3296,
      "step": 16730
    },
    {
      "epoch": 3.587655379339906,
      "grad_norm": 0.37128010392189026,
      "learning_rate": 1.521645949421346e-05,
      "loss": 0.4873,
      "step": 16740
    },
    {
      "epoch": 3.58979854264895,
      "grad_norm": 0.37728482484817505,
      "learning_rate": 1.5213601943134734e-05,
      "loss": 0.6553,
      "step": 16750
    },
    {
      "epoch": 3.591941705957994,
      "grad_norm": 14.911856651306152,
      "learning_rate": 1.521074439205601e-05,
      "loss": 0.485,
      "step": 16760
    },
    {
      "epoch": 3.5940848692670384,
      "grad_norm": 0.3777354955673218,
      "learning_rate": 1.5207886840977284e-05,
      "loss": 0.4913,
      "step": 16770
    },
    {
      "epoch": 3.5962280325760823,
      "grad_norm": 29.92331886291504,
      "learning_rate": 1.5205029289898558e-05,
      "loss": 0.8096,
      "step": 16780
    },
    {
      "epoch": 3.5983711958851265,
      "grad_norm": 14.943364143371582,
      "learning_rate": 1.5202171738819834e-05,
      "loss": 0.8085,
      "step": 16790
    },
    {
      "epoch": 3.600514359194171,
      "grad_norm": 0.41412439942359924,
      "learning_rate": 1.5199314187741107e-05,
      "loss": 0.0091,
      "step": 16800
    },
    {
      "epoch": 3.6026575225032147,
      "grad_norm": 0.3431105315685272,
      "learning_rate": 1.5196456636662383e-05,
      "loss": 0.3358,
      "step": 16810
    },
    {
      "epoch": 3.604800685812259,
      "grad_norm": 0.32977330684661865,
      "learning_rate": 1.5193599085583657e-05,
      "loss": 0.8311,
      "step": 16820
    },
    {
      "epoch": 3.6069438491213033,
      "grad_norm": 14.69241714477539,
      "learning_rate": 1.5190741534504931e-05,
      "loss": 1.2952,
      "step": 16830
    },
    {
      "epoch": 3.609087012430347,
      "grad_norm": 0.4808514416217804,
      "learning_rate": 1.5187883983426207e-05,
      "loss": 0.9317,
      "step": 16840
    },
    {
      "epoch": 3.6112301757393914,
      "grad_norm": 0.5195025205612183,
      "learning_rate": 1.518502643234748e-05,
      "loss": 0.4664,
      "step": 16850
    },
    {
      "epoch": 3.6133733390484357,
      "grad_norm": 0.5358026027679443,
      "learning_rate": 1.5182168881268753e-05,
      "loss": 0.4571,
      "step": 16860
    },
    {
      "epoch": 3.6155165023574796,
      "grad_norm": 0.5160301327705383,
      "learning_rate": 1.5179311330190027e-05,
      "loss": 0.308,
      "step": 16870
    },
    {
      "epoch": 3.617659665666524,
      "grad_norm": 0.5097143054008484,
      "learning_rate": 1.5176453779111302e-05,
      "loss": 0.7689,
      "step": 16880
    },
    {
      "epoch": 3.619802828975568,
      "grad_norm": 14.907563209533691,
      "learning_rate": 1.5173596228032576e-05,
      "loss": 0.4621,
      "step": 16890
    },
    {
      "epoch": 3.621945992284612,
      "grad_norm": 0.5084816217422485,
      "learning_rate": 1.5170738676953852e-05,
      "loss": 0.6074,
      "step": 16900
    },
    {
      "epoch": 3.6240891555936563,
      "grad_norm": 14.581207275390625,
      "learning_rate": 1.5167881125875126e-05,
      "loss": 0.4607,
      "step": 16910
    },
    {
      "epoch": 3.6262323189027006,
      "grad_norm": 0.5051174759864807,
      "learning_rate": 1.51650235747964e-05,
      "loss": 0.4634,
      "step": 16920
    },
    {
      "epoch": 3.6283754822117444,
      "grad_norm": 14.642789840698242,
      "learning_rate": 1.5162166023717675e-05,
      "loss": 1.0746,
      "step": 16930
    },
    {
      "epoch": 3.6305186455207887,
      "grad_norm": 14.96120548248291,
      "learning_rate": 1.515930847263895e-05,
      "loss": 0.461,
      "step": 16940
    },
    {
      "epoch": 3.632661808829833,
      "grad_norm": 0.46083155274391174,
      "learning_rate": 1.5156450921560225e-05,
      "loss": 0.1642,
      "step": 16950
    },
    {
      "epoch": 3.634804972138877,
      "grad_norm": 14.773658752441406,
      "learning_rate": 1.5153593370481499e-05,
      "loss": 0.3287,
      "step": 16960
    },
    {
      "epoch": 3.636948135447921,
      "grad_norm": 14.677400588989258,
      "learning_rate": 1.5150735819402773e-05,
      "loss": 1.2702,
      "step": 16970
    },
    {
      "epoch": 3.6390912987569655,
      "grad_norm": 15.072930335998535,
      "learning_rate": 1.5147878268324048e-05,
      "loss": 1.0767,
      "step": 16980
    },
    {
      "epoch": 3.6412344620660093,
      "grad_norm": 0.5421894192695618,
      "learning_rate": 1.5145020717245322e-05,
      "loss": 0.1581,
      "step": 16990
    },
    {
      "epoch": 3.6433776253750536,
      "grad_norm": 0.489666610956192,
      "learning_rate": 1.5142163166166596e-05,
      "loss": 0.3095,
      "step": 17000
    },
    {
      "epoch": 3.645520788684098,
      "grad_norm": 0.43350541591644287,
      "learning_rate": 1.5139305615087872e-05,
      "loss": 0.3178,
      "step": 17010
    },
    {
      "epoch": 3.6476639519931418,
      "grad_norm": 0.4004337787628174,
      "learning_rate": 1.5136448064009146e-05,
      "loss": 0.3239,
      "step": 17020
    },
    {
      "epoch": 3.649807115302186,
      "grad_norm": 0.37725356221199036,
      "learning_rate": 1.5133590512930421e-05,
      "loss": 0.8066,
      "step": 17030
    },
    {
      "epoch": 3.6519502786112303,
      "grad_norm": 0.36323073506355286,
      "learning_rate": 1.5130732961851695e-05,
      "loss": 0.328,
      "step": 17040
    },
    {
      "epoch": 3.654093441920274,
      "grad_norm": 0.350090354681015,
      "learning_rate": 1.512787541077297e-05,
      "loss": 0.3283,
      "step": 17050
    },
    {
      "epoch": 3.6562366052293185,
      "grad_norm": 0.3673318922519684,
      "learning_rate": 1.5125017859694245e-05,
      "loss": 0.8183,
      "step": 17060
    },
    {
      "epoch": 3.6583797685383628,
      "grad_norm": 0.39061954617500305,
      "learning_rate": 1.5122160308615517e-05,
      "loss": 0.4905,
      "step": 17070
    },
    {
      "epoch": 3.6605229318474066,
      "grad_norm": 0.39445579051971436,
      "learning_rate": 1.5119302757536791e-05,
      "loss": 0.8026,
      "step": 17080
    },
    {
      "epoch": 3.662666095156451,
      "grad_norm": 14.938913345336914,
      "learning_rate": 1.5116445206458067e-05,
      "loss": 0.3262,
      "step": 17090
    },
    {
      "epoch": 3.664809258465495,
      "grad_norm": 30.02483558654785,
      "learning_rate": 1.511358765537934e-05,
      "loss": 1.1027,
      "step": 17100
    },
    {
      "epoch": 3.666952421774539,
      "grad_norm": 0.45843714475631714,
      "learning_rate": 1.5110730104300614e-05,
      "loss": 0.1654,
      "step": 17110
    },
    {
      "epoch": 3.6690955850835834,
      "grad_norm": 14.670032501220703,
      "learning_rate": 1.510787255322189e-05,
      "loss": 0.4731,
      "step": 17120
    },
    {
      "epoch": 3.6712387483926276,
      "grad_norm": 14.612024307250977,
      "learning_rate": 1.5105015002143164e-05,
      "loss": 0.7811,
      "step": 17130
    },
    {
      "epoch": 3.6733819117016715,
      "grad_norm": 14.57772445678711,
      "learning_rate": 1.5102157451064438e-05,
      "loss": 0.7685,
      "step": 17140
    },
    {
      "epoch": 3.675525075010716,
      "grad_norm": 14.89255428314209,
      "learning_rate": 1.5099299899985714e-05,
      "loss": 0.7671,
      "step": 17150
    },
    {
      "epoch": 3.67766823831976,
      "grad_norm": 0.5030301213264465,
      "learning_rate": 1.5096442348906988e-05,
      "loss": 0.4528,
      "step": 17160
    },
    {
      "epoch": 3.679811401628804,
      "grad_norm": 0.5861024260520935,
      "learning_rate": 1.5093584797828263e-05,
      "loss": 0.744,
      "step": 17170
    },
    {
      "epoch": 3.6819545649378482,
      "grad_norm": 0.5633596181869507,
      "learning_rate": 1.5090727246749537e-05,
      "loss": 0.4472,
      "step": 17180
    },
    {
      "epoch": 3.6840977282468925,
      "grad_norm": 14.518108367919922,
      "learning_rate": 1.5087869695670811e-05,
      "loss": 0.5963,
      "step": 17190
    },
    {
      "epoch": 3.6862408915559364,
      "grad_norm": 14.464676856994629,
      "learning_rate": 1.5085012144592087e-05,
      "loss": 1.1732,
      "step": 17200
    },
    {
      "epoch": 3.6883840548649807,
      "grad_norm": 0.6714260578155518,
      "learning_rate": 1.508215459351336e-05,
      "loss": 0.435,
      "step": 17210
    },
    {
      "epoch": 3.690527218174025,
      "grad_norm": 0.6288807392120361,
      "learning_rate": 1.5079297042434634e-05,
      "loss": 0.4519,
      "step": 17220
    },
    {
      "epoch": 3.692670381483069,
      "grad_norm": 0.5905371308326721,
      "learning_rate": 1.507643949135591e-05,
      "loss": 0.5881,
      "step": 17230
    },
    {
      "epoch": 3.694813544792113,
      "grad_norm": 0.47877398133277893,
      "learning_rate": 1.5073581940277184e-05,
      "loss": 0.5947,
      "step": 17240
    },
    {
      "epoch": 3.6969567081011574,
      "grad_norm": 29.6676082611084,
      "learning_rate": 1.507072438919846e-05,
      "loss": 0.7694,
      "step": 17250
    },
    {
      "epoch": 3.6990998714102012,
      "grad_norm": 0.43993622064590454,
      "learning_rate": 1.5067866838119734e-05,
      "loss": 0.3176,
      "step": 17260
    },
    {
      "epoch": 3.7012430347192455,
      "grad_norm": 14.68990707397461,
      "learning_rate": 1.5065009287041008e-05,
      "loss": 0.4742,
      "step": 17270
    },
    {
      "epoch": 3.70338619802829,
      "grad_norm": 0.44413450360298157,
      "learning_rate": 1.5062151735962283e-05,
      "loss": 0.8129,
      "step": 17280
    },
    {
      "epoch": 3.7055293613373337,
      "grad_norm": 0.4495091736316681,
      "learning_rate": 1.5059294184883555e-05,
      "loss": 0.3193,
      "step": 17290
    },
    {
      "epoch": 3.707672524646378,
      "grad_norm": 14.656729698181152,
      "learning_rate": 1.505643663380483e-05,
      "loss": 0.3174,
      "step": 17300
    },
    {
      "epoch": 3.7098156879554223,
      "grad_norm": 0.3703935146331787,
      "learning_rate": 1.5053579082726105e-05,
      "loss": 0.4774,
      "step": 17310
    },
    {
      "epoch": 3.711958851264466,
      "grad_norm": 0.3966010808944702,
      "learning_rate": 1.5050721531647379e-05,
      "loss": 0.4822,
      "step": 17320
    },
    {
      "epoch": 3.7141020145735104,
      "grad_norm": 0.3808141350746155,
      "learning_rate": 1.5047863980568653e-05,
      "loss": 0.8128,
      "step": 17330
    },
    {
      "epoch": 3.7162451778825547,
      "grad_norm": 14.774852752685547,
      "learning_rate": 1.5045006429489928e-05,
      "loss": 0.4819,
      "step": 17340
    },
    {
      "epoch": 3.7183883411915986,
      "grad_norm": 0.38093534111976624,
      "learning_rate": 1.5042148878411202e-05,
      "loss": 0.3219,
      "step": 17350
    },
    {
      "epoch": 3.720531504500643,
      "grad_norm": 0.40887048840522766,
      "learning_rate": 1.5039291327332476e-05,
      "loss": 1.2706,
      "step": 17360
    },
    {
      "epoch": 3.722674667809687,
      "grad_norm": 0.4356526732444763,
      "learning_rate": 1.5036433776253752e-05,
      "loss": 0.4754,
      "step": 17370
    },
    {
      "epoch": 3.724817831118731,
      "grad_norm": 0.4619251489639282,
      "learning_rate": 1.5033576225175026e-05,
      "loss": 0.7804,
      "step": 17380
    },
    {
      "epoch": 3.7269609944277753,
      "grad_norm": 0.5054184794425964,
      "learning_rate": 1.5030718674096301e-05,
      "loss": 0.3131,
      "step": 17390
    },
    {
      "epoch": 3.7291041577368196,
      "grad_norm": 14.765064239501953,
      "learning_rate": 1.5027861123017575e-05,
      "loss": 0.6169,
      "step": 17400
    },
    {
      "epoch": 3.7312473210458634,
      "grad_norm": 14.61949634552002,
      "learning_rate": 1.502500357193885e-05,
      "loss": 0.4669,
      "step": 17410
    },
    {
      "epoch": 3.7333904843549077,
      "grad_norm": 14.672460556030273,
      "learning_rate": 1.5022146020860125e-05,
      "loss": 0.7721,
      "step": 17420
    },
    {
      "epoch": 3.735533647663952,
      "grad_norm": 0.5455838441848755,
      "learning_rate": 1.5019288469781399e-05,
      "loss": 0.6092,
      "step": 17430
    },
    {
      "epoch": 3.737676810972996,
      "grad_norm": 0.5370780229568481,
      "learning_rate": 1.5016430918702673e-05,
      "loss": 0.3055,
      "step": 17440
    },
    {
      "epoch": 3.73981997428204,
      "grad_norm": 0.5002577900886536,
      "learning_rate": 1.5013573367623948e-05,
      "loss": 0.7525,
      "step": 17450
    },
    {
      "epoch": 3.7419631375910845,
      "grad_norm": 0.5280993580818176,
      "learning_rate": 1.5010715816545222e-05,
      "loss": 0.7544,
      "step": 17460
    },
    {
      "epoch": 3.7441063009001287,
      "grad_norm": 14.50691032409668,
      "learning_rate": 1.5007858265466498e-05,
      "loss": 1.0518,
      "step": 17470
    },
    {
      "epoch": 3.7462494642091726,
      "grad_norm": 0.579286515712738,
      "learning_rate": 1.5005000714387772e-05,
      "loss": 0.3028,
      "step": 17480
    },
    {
      "epoch": 3.748392627518217,
      "grad_norm": 0.5191131830215454,
      "learning_rate": 1.5002143163309046e-05,
      "loss": 0.3142,
      "step": 17490
    },
    {
      "epoch": 3.750535790827261,
      "grad_norm": 0.5118924379348755,
      "learning_rate": 1.4999285612230318e-05,
      "loss": 1.0691,
      "step": 17500
    },
    {
      "epoch": 3.752678954136305,
      "grad_norm": 0.4592973589897156,
      "learning_rate": 1.4996428061151594e-05,
      "loss": 0.3107,
      "step": 17510
    },
    {
      "epoch": 3.7548221174453493,
      "grad_norm": 0.4394761025905609,
      "learning_rate": 1.4993570510072868e-05,
      "loss": 0.0103,
      "step": 17520
    },
    {
      "epoch": 3.7569652807543936,
      "grad_norm": 14.69378662109375,
      "learning_rate": 1.4990712958994143e-05,
      "loss": 0.4842,
      "step": 17530
    },
    {
      "epoch": 3.7591084440634375,
      "grad_norm": 0.38164234161376953,
      "learning_rate": 1.4987855407915417e-05,
      "loss": 0.8018,
      "step": 17540
    },
    {
      "epoch": 3.7612516073724818,
      "grad_norm": 0.3668719232082367,
      "learning_rate": 1.4984997856836691e-05,
      "loss": 0.3241,
      "step": 17550
    },
    {
      "epoch": 3.763394770681526,
      "grad_norm": 14.706904411315918,
      "learning_rate": 1.4982140305757967e-05,
      "loss": 0.488,
      "step": 17560
    },
    {
      "epoch": 3.7655379339905704,
      "grad_norm": 15.086797714233398,
      "learning_rate": 1.497928275467924e-05,
      "loss": 0.8181,
      "step": 17570
    },
    {
      "epoch": 3.767681097299614,
      "grad_norm": 14.827183723449707,
      "learning_rate": 1.4976425203600515e-05,
      "loss": 0.6399,
      "step": 17580
    },
    {
      "epoch": 3.7698242606086585,
      "grad_norm": 0.5247494578361511,
      "learning_rate": 1.497356765252179e-05,
      "loss": 0.7733,
      "step": 17590
    },
    {
      "epoch": 3.771967423917703,
      "grad_norm": 30.197246551513672,
      "learning_rate": 1.4970710101443064e-05,
      "loss": 0.7606,
      "step": 17600
    },
    {
      "epoch": 3.7741105872267466,
      "grad_norm": 0.5811519026756287,
      "learning_rate": 1.496785255036434e-05,
      "loss": 0.5959,
      "step": 17610
    },
    {
      "epoch": 3.776253750535791,
      "grad_norm": 15.025062561035156,
      "learning_rate": 1.4964994999285614e-05,
      "loss": 0.3093,
      "step": 17620
    },
    {
      "epoch": 3.7783969138448352,
      "grad_norm": 0.5311627388000488,
      "learning_rate": 1.4962137448206888e-05,
      "loss": 0.6043,
      "step": 17630
    },
    {
      "epoch": 3.780540077153879,
      "grad_norm": 14.914719581604004,
      "learning_rate": 1.4959279897128163e-05,
      "loss": 1.0446,
      "step": 17640
    },
    {
      "epoch": 3.7826832404629234,
      "grad_norm": 0.5739005208015442,
      "learning_rate": 1.4956422346049437e-05,
      "loss": 0.8845,
      "step": 17650
    },
    {
      "epoch": 3.7848264037719677,
      "grad_norm": 29.616086959838867,
      "learning_rate": 1.4953564794970713e-05,
      "loss": 0.5925,
      "step": 17660
    },
    {
      "epoch": 3.7869695670810115,
      "grad_norm": 14.572786331176758,
      "learning_rate": 1.4950707243891987e-05,
      "loss": 0.452,
      "step": 17670
    },
    {
      "epoch": 3.789112730390056,
      "grad_norm": 14.51848316192627,
      "learning_rate": 1.494784969281326e-05,
      "loss": 1.0408,
      "step": 17680
    },
    {
      "epoch": 3.7912558936991,
      "grad_norm": 0.5900289416313171,
      "learning_rate": 1.4944992141734536e-05,
      "loss": 0.4494,
      "step": 17690
    },
    {
      "epoch": 3.793399057008144,
      "grad_norm": 14.535907745361328,
      "learning_rate": 1.494213459065581e-05,
      "loss": 0.5993,
      "step": 17700
    },
    {
      "epoch": 3.7955422203171882,
      "grad_norm": 0.6037614345550537,
      "learning_rate": 1.4939277039577084e-05,
      "loss": 1.0318,
      "step": 17710
    },
    {
      "epoch": 3.7976853836262325,
      "grad_norm": 0.6057909727096558,
      "learning_rate": 1.4936419488498356e-05,
      "loss": 0.4442,
      "step": 17720
    },
    {
      "epoch": 3.7998285469352764,
      "grad_norm": 0.543375551700592,
      "learning_rate": 1.4933561937419632e-05,
      "loss": 0.3021,
      "step": 17730
    },
    {
      "epoch": 3.8019717102443207,
      "grad_norm": 0.47886401414871216,
      "learning_rate": 1.4930704386340906e-05,
      "loss": 0.448,
      "step": 17740
    },
    {
      "epoch": 3.804114873553365,
      "grad_norm": 0.4438716173171997,
      "learning_rate": 1.4927846835262181e-05,
      "loss": 0.315,
      "step": 17750
    },
    {
      "epoch": 3.806258036862409,
      "grad_norm": 0.42327237129211426,
      "learning_rate": 1.4924989284183455e-05,
      "loss": 0.3175,
      "step": 17760
    },
    {
      "epoch": 3.808401200171453,
      "grad_norm": 0.4159643352031708,
      "learning_rate": 1.492213173310473e-05,
      "loss": 0.4829,
      "step": 17770
    },
    {
      "epoch": 3.8105443634804974,
      "grad_norm": 0.35201653838157654,
      "learning_rate": 1.4919274182026005e-05,
      "loss": 0.3234,
      "step": 17780
    },
    {
      "epoch": 3.8126875267895413,
      "grad_norm": 15.726950645446777,
      "learning_rate": 1.4916416630947279e-05,
      "loss": 0.6682,
      "step": 17790
    },
    {
      "epoch": 3.8148306900985856,
      "grad_norm": 14.952605247497559,
      "learning_rate": 1.4913559079868554e-05,
      "loss": 0.4905,
      "step": 17800
    },
    {
      "epoch": 3.81697385340763,
      "grad_norm": 0.38111600279808044,
      "learning_rate": 1.4910701528789828e-05,
      "loss": 0.492,
      "step": 17810
    },
    {
      "epoch": 3.8191170167166737,
      "grad_norm": 15.48954963684082,
      "learning_rate": 1.4907843977711102e-05,
      "loss": 0.649,
      "step": 17820
    },
    {
      "epoch": 3.821260180025718,
      "grad_norm": 16.178482055664062,
      "learning_rate": 1.4904986426632378e-05,
      "loss": 0.4888,
      "step": 17830
    },
    {
      "epoch": 3.8234033433347623,
      "grad_norm": 0.4275936186313629,
      "learning_rate": 1.4902128875553652e-05,
      "loss": 0.1638,
      "step": 17840
    },
    {
      "epoch": 3.825546506643806,
      "grad_norm": 14.710400581359863,
      "learning_rate": 1.4899271324474926e-05,
      "loss": 0.3245,
      "step": 17850
    },
    {
      "epoch": 3.8276896699528504,
      "grad_norm": 0.37920665740966797,
      "learning_rate": 1.4896413773396201e-05,
      "loss": 0.655,
      "step": 17860
    },
    {
      "epoch": 3.8298328332618947,
      "grad_norm": 0.3431054651737213,
      "learning_rate": 1.4893556222317475e-05,
      "loss": 0.3276,
      "step": 17870
    },
    {
      "epoch": 3.8319759965709386,
      "grad_norm": 0.36349958181381226,
      "learning_rate": 1.4890698671238751e-05,
      "loss": 0.3279,
      "step": 17880
    },
    {
      "epoch": 3.834119159879983,
      "grad_norm": 0.29829901456832886,
      "learning_rate": 1.4887841120160025e-05,
      "loss": 0.4939,
      "step": 17890
    },
    {
      "epoch": 3.836262323189027,
      "grad_norm": 0.2903256416320801,
      "learning_rate": 1.4884983569081299e-05,
      "loss": 0.5146,
      "step": 17900
    },
    {
      "epoch": 3.838405486498071,
      "grad_norm": 0.2939165234565735,
      "learning_rate": 1.4882126018002574e-05,
      "loss": 1.0107,
      "step": 17910
    },
    {
      "epoch": 3.8405486498071153,
      "grad_norm": 29.723480224609375,
      "learning_rate": 1.4879268466923848e-05,
      "loss": 0.9897,
      "step": 17920
    },
    {
      "epoch": 3.8426918131161596,
      "grad_norm": 0.3999119699001312,
      "learning_rate": 1.487641091584512e-05,
      "loss": 0.4855,
      "step": 17930
    },
    {
      "epoch": 3.8448349764252034,
      "grad_norm": 0.3867100477218628,
      "learning_rate": 1.4873553364766396e-05,
      "loss": 0.1708,
      "step": 17940
    },
    {
      "epoch": 3.8469781397342477,
      "grad_norm": 0.2918800711631775,
      "learning_rate": 1.487069581368767e-05,
      "loss": 0.3341,
      "step": 17950
    },
    {
      "epoch": 3.849121303043292,
      "grad_norm": 0.29774364829063416,
      "learning_rate": 1.4867838262608944e-05,
      "loss": 0.6787,
      "step": 17960
    },
    {
      "epoch": 3.851264466352336,
      "grad_norm": 0.3375741243362427,
      "learning_rate": 1.486498071153022e-05,
      "loss": 1.0164,
      "step": 17970
    },
    {
      "epoch": 3.85340762966138,
      "grad_norm": 0.3563976585865021,
      "learning_rate": 1.4862123160451494e-05,
      "loss": 0.1697,
      "step": 17980
    },
    {
      "epoch": 3.8555507929704245,
      "grad_norm": 0.3574104309082031,
      "learning_rate": 1.4859265609372768e-05,
      "loss": 0.3354,
      "step": 17990
    },
    {
      "epoch": 3.8576939562794683,
      "grad_norm": 0.33242467045783997,
      "learning_rate": 1.4856408058294043e-05,
      "loss": 0.1733,
      "step": 18000
    },
    {
      "epoch": 3.8598371195885126,
      "grad_norm": 14.770233154296875,
      "learning_rate": 1.4853550507215317e-05,
      "loss": 0.6701,
      "step": 18010
    },
    {
      "epoch": 3.861980282897557,
      "grad_norm": 0.33391067385673523,
      "learning_rate": 1.4850692956136593e-05,
      "loss": 0.837,
      "step": 18020
    },
    {
      "epoch": 3.8641234462066008,
      "grad_norm": 0.3519139587879181,
      "learning_rate": 1.4847835405057867e-05,
      "loss": 0.498,
      "step": 18030
    },
    {
      "epoch": 3.866266609515645,
      "grad_norm": 14.826370239257812,
      "learning_rate": 1.484497785397914e-05,
      "loss": 0.6556,
      "step": 18040
    },
    {
      "epoch": 3.8684097728246893,
      "grad_norm": 0.366362601518631,
      "learning_rate": 1.4842120302900416e-05,
      "loss": 0.1697,
      "step": 18050
    },
    {
      "epoch": 3.870552936133733,
      "grad_norm": 0.37783369421958923,
      "learning_rate": 1.483926275182169e-05,
      "loss": 0.8085,
      "step": 18060
    },
    {
      "epoch": 3.8726960994427775,
      "grad_norm": 29.748939514160156,
      "learning_rate": 1.4836405200742964e-05,
      "loss": 1.102,
      "step": 18070
    },
    {
      "epoch": 3.8748392627518218,
      "grad_norm": 14.789105415344238,
      "learning_rate": 1.483354764966424e-05,
      "loss": 0.6235,
      "step": 18080
    },
    {
      "epoch": 3.8769824260608656,
      "grad_norm": 0.617293119430542,
      "learning_rate": 1.4830690098585514e-05,
      "loss": 0.7519,
      "step": 18090
    },
    {
      "epoch": 3.87912558936991,
      "grad_norm": 0.6209398508071899,
      "learning_rate": 1.482783254750679e-05,
      "loss": 0.1566,
      "step": 18100
    },
    {
      "epoch": 3.881268752678954,
      "grad_norm": 14.618515014648438,
      "learning_rate": 1.4824974996428063e-05,
      "loss": 0.5937,
      "step": 18110
    },
    {
      "epoch": 3.883411915987998,
      "grad_norm": 0.5603220462799072,
      "learning_rate": 1.4822117445349337e-05,
      "loss": 1.0467,
      "step": 18120
    },
    {
      "epoch": 3.8855550792970424,
      "grad_norm": 0.5738746523857117,
      "learning_rate": 1.4819259894270613e-05,
      "loss": 0.5939,
      "step": 18130
    },
    {
      "epoch": 3.8876982426060867,
      "grad_norm": 29.38156509399414,
      "learning_rate": 1.4816402343191887e-05,
      "loss": 1.1694,
      "step": 18140
    },
    {
      "epoch": 3.8898414059151305,
      "grad_norm": 14.432989120483398,
      "learning_rate": 1.4813544792113159e-05,
      "loss": 0.5821,
      "step": 18150
    },
    {
      "epoch": 3.891984569224175,
      "grad_norm": 0.6039011478424072,
      "learning_rate": 1.4810687241034435e-05,
      "loss": 0.2971,
      "step": 18160
    },
    {
      "epoch": 3.894127732533219,
      "grad_norm": 0.49532249569892883,
      "learning_rate": 1.4807829689955708e-05,
      "loss": 0.154,
      "step": 18170
    },
    {
      "epoch": 3.896270895842263,
      "grad_norm": 0.3875884413719177,
      "learning_rate": 1.4804972138876982e-05,
      "loss": 0.4703,
      "step": 18180
    },
    {
      "epoch": 3.8984140591513072,
      "grad_norm": 0.3577831983566284,
      "learning_rate": 1.4802114587798258e-05,
      "loss": 0.3262,
      "step": 18190
    },
    {
      "epoch": 3.9005572224603515,
      "grad_norm": 14.763854026794434,
      "learning_rate": 1.4799257036719532e-05,
      "loss": 0.4968,
      "step": 18200
    },
    {
      "epoch": 3.9027003857693954,
      "grad_norm": 29.78203010559082,
      "learning_rate": 1.4796399485640806e-05,
      "loss": 0.6615,
      "step": 18210
    },
    {
      "epoch": 3.9048435490784397,
      "grad_norm": 0.36819279193878174,
      "learning_rate": 1.4793541934562082e-05,
      "loss": 0.8283,
      "step": 18220
    },
    {
      "epoch": 3.906986712387484,
      "grad_norm": 0.40775614976882935,
      "learning_rate": 1.4790684383483355e-05,
      "loss": 1.1189,
      "step": 18230
    },
    {
      "epoch": 3.909129875696528,
      "grad_norm": 0.4659050703048706,
      "learning_rate": 1.4787826832404631e-05,
      "loss": 0.9291,
      "step": 18240
    },
    {
      "epoch": 3.911273039005572,
      "grad_norm": 0.5691627264022827,
      "learning_rate": 1.4784969281325905e-05,
      "loss": 0.6186,
      "step": 18250
    },
    {
      "epoch": 3.9134162023146164,
      "grad_norm": 0.6239548921585083,
      "learning_rate": 1.4782111730247179e-05,
      "loss": 0.8918,
      "step": 18260
    },
    {
      "epoch": 3.9155593656236602,
      "grad_norm": 14.388470649719238,
      "learning_rate": 1.4779254179168455e-05,
      "loss": 0.7145,
      "step": 18270
    },
    {
      "epoch": 3.9177025289327045,
      "grad_norm": 0.7501351833343506,
      "learning_rate": 1.4776396628089728e-05,
      "loss": 0.7008,
      "step": 18280
    },
    {
      "epoch": 3.919845692241749,
      "grad_norm": 14.274739265441895,
      "learning_rate": 1.4773539077011002e-05,
      "loss": 0.694,
      "step": 18290
    },
    {
      "epoch": 3.921988855550793,
      "grad_norm": 0.7956295609474182,
      "learning_rate": 1.4770681525932278e-05,
      "loss": 0.55,
      "step": 18300
    },
    {
      "epoch": 3.924132018859837,
      "grad_norm": 0.7388196587562561,
      "learning_rate": 1.4767823974853552e-05,
      "loss": 0.2885,
      "step": 18310
    },
    {
      "epoch": 3.9262751821688813,
      "grad_norm": 29.401288986206055,
      "learning_rate": 1.4764966423774828e-05,
      "loss": 0.5757,
      "step": 18320
    },
    {
      "epoch": 3.9284183454779256,
      "grad_norm": 14.405023574829102,
      "learning_rate": 1.4762108872696101e-05,
      "loss": 0.9974,
      "step": 18330
    },
    {
      "epoch": 3.9305615087869694,
      "grad_norm": 0.7980234026908875,
      "learning_rate": 1.4759251321617375e-05,
      "loss": 0.6964,
      "step": 18340
    },
    {
      "epoch": 3.9327046720960137,
      "grad_norm": 0.830264151096344,
      "learning_rate": 1.4756393770538651e-05,
      "loss": 0.4187,
      "step": 18350
    },
    {
      "epoch": 3.934847835405058,
      "grad_norm": 14.49704360961914,
      "learning_rate": 1.4753536219459923e-05,
      "loss": 0.8401,
      "step": 18360
    },
    {
      "epoch": 3.936990998714102,
      "grad_norm": 0.6565833687782288,
      "learning_rate": 1.4750678668381197e-05,
      "loss": 0.0155,
      "step": 18370
    },
    {
      "epoch": 3.939134162023146,
      "grad_norm": 0.531554102897644,
      "learning_rate": 1.4747821117302473e-05,
      "loss": 0.3062,
      "step": 18380
    },
    {
      "epoch": 3.9412773253321904,
      "grad_norm": 0.4859061539173126,
      "learning_rate": 1.4744963566223747e-05,
      "loss": 0.6106,
      "step": 18390
    },
    {
      "epoch": 3.9434204886412347,
      "grad_norm": 0.48000502586364746,
      "learning_rate": 1.474210601514502e-05,
      "loss": 0.6171,
      "step": 18400
    },
    {
      "epoch": 3.9455636519502786,
      "grad_norm": 14.57306957244873,
      "learning_rate": 1.4739248464066296e-05,
      "loss": 0.4634,
      "step": 18410
    },
    {
      "epoch": 3.947706815259323,
      "grad_norm": 0.5177602767944336,
      "learning_rate": 1.473639091298757e-05,
      "loss": 0.4655,
      "step": 18420
    },
    {
      "epoch": 3.949849978568367,
      "grad_norm": 0.4675728976726532,
      "learning_rate": 1.4733533361908844e-05,
      "loss": 0.7606,
      "step": 18430
    },
    {
      "epoch": 3.951993141877411,
      "grad_norm": 0.5132210850715637,
      "learning_rate": 1.473067581083012e-05,
      "loss": 0.6299,
      "step": 18440
    },
    {
      "epoch": 3.9541363051864553,
      "grad_norm": 0.5539768934249878,
      "learning_rate": 1.4727818259751394e-05,
      "loss": 1.0409,
      "step": 18450
    },
    {
      "epoch": 3.9562794684954996,
      "grad_norm": 14.528682708740234,
      "learning_rate": 1.472496070867267e-05,
      "loss": 0.5948,
      "step": 18460
    },
    {
      "epoch": 3.9584226318045435,
      "grad_norm": 0.5636974573135376,
      "learning_rate": 1.4722103157593943e-05,
      "loss": 0.0129,
      "step": 18470
    },
    {
      "epoch": 3.9605657951135878,
      "grad_norm": 0.4919518828392029,
      "learning_rate": 1.4719245606515217e-05,
      "loss": 0.6148,
      "step": 18480
    },
    {
      "epoch": 3.962708958422632,
      "grad_norm": 0.4628373384475708,
      "learning_rate": 1.4716388055436493e-05,
      "loss": 0.3153,
      "step": 18490
    },
    {
      "epoch": 3.964852121731676,
      "grad_norm": 0.4570581912994385,
      "learning_rate": 1.4713530504357767e-05,
      "loss": 0.9347,
      "step": 18500
    },
    {
      "epoch": 3.96699528504072,
      "grad_norm": 14.576834678649902,
      "learning_rate": 1.4710672953279042e-05,
      "loss": 0.7658,
      "step": 18510
    },
    {
      "epoch": 3.9691384483497645,
      "grad_norm": 0.4941052198410034,
      "learning_rate": 1.4707815402200316e-05,
      "loss": 0.0112,
      "step": 18520
    },
    {
      "epoch": 3.9712816116588083,
      "grad_norm": 0.42532017827033997,
      "learning_rate": 1.470495785112159e-05,
      "loss": 0.4649,
      "step": 18530
    },
    {
      "epoch": 3.9734247749678526,
      "grad_norm": 0.41730043292045593,
      "learning_rate": 1.4702100300042866e-05,
      "loss": 0.4851,
      "step": 18540
    },
    {
      "epoch": 3.975567938276897,
      "grad_norm": 0.416681706905365,
      "learning_rate": 1.469924274896414e-05,
      "loss": 0.6418,
      "step": 18550
    },
    {
      "epoch": 3.9777111015859408,
      "grad_norm": 0.4249226450920105,
      "learning_rate": 1.4696385197885414e-05,
      "loss": 0.4748,
      "step": 18560
    },
    {
      "epoch": 3.979854264894985,
      "grad_norm": 14.75870418548584,
      "learning_rate": 1.469352764680669e-05,
      "loss": 0.3239,
      "step": 18570
    },
    {
      "epoch": 3.9819974282040294,
      "grad_norm": 0.3855708837509155,
      "learning_rate": 1.4690670095727962e-05,
      "loss": 0.3243,
      "step": 18580
    },
    {
      "epoch": 3.984140591513073,
      "grad_norm": 0.3534889221191406,
      "learning_rate": 1.4687812544649235e-05,
      "loss": 0.6581,
      "step": 18590
    },
    {
      "epoch": 3.9862837548221175,
      "grad_norm": 0.39153918623924255,
      "learning_rate": 1.4684954993570511e-05,
      "loss": 0.8116,
      "step": 18600
    },
    {
      "epoch": 3.988426918131162,
      "grad_norm": 0.4369700253009796,
      "learning_rate": 1.4682097442491785e-05,
      "loss": 0.477,
      "step": 18610
    },
    {
      "epoch": 3.9905700814402056,
      "grad_norm": 14.84390640258789,
      "learning_rate": 1.4679239891413059e-05,
      "loss": 0.4733,
      "step": 18620
    },
    {
      "epoch": 3.99271324474925,
      "grad_norm": 14.716116905212402,
      "learning_rate": 1.4676382340334335e-05,
      "loss": 1.0982,
      "step": 18630
    },
    {
      "epoch": 3.9948564080582942,
      "grad_norm": 0.47693514823913574,
      "learning_rate": 1.4673524789255609e-05,
      "loss": 0.6298,
      "step": 18640
    },
    {
      "epoch": 3.996999571367338,
      "grad_norm": 0.48573100566864014,
      "learning_rate": 1.4670667238176884e-05,
      "loss": 0.4638,
      "step": 18650
    },
    {
      "epoch": 3.9991427346763824,
      "grad_norm": 14.609889030456543,
      "learning_rate": 1.4667809687098158e-05,
      "loss": 1.0711,
      "step": 18660
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.8736666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.5764217972755432,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 516.9718,
      "eval_samples_per_second": 5.803,
      "eval_steps_per_second": 1.934,
      "step": 18664
    },
    {
      "epoch": 4.001285897985427,
      "grad_norm": 14.584925651550293,
      "learning_rate": 1.4664952136019432e-05,
      "loss": 0.7586,
      "step": 18670
    },
    {
      "epoch": 4.0034290612944705,
      "grad_norm": 0.5401526689529419,
      "learning_rate": 1.4662094584940708e-05,
      "loss": 0.8987,
      "step": 18680
    },
    {
      "epoch": 4.005572224603514,
      "grad_norm": 0.5211775898933411,
      "learning_rate": 1.4659237033861982e-05,
      "loss": 0.3076,
      "step": 18690
    },
    {
      "epoch": 4.007715387912559,
      "grad_norm": 0.4891270399093628,
      "learning_rate": 1.4656379482783255e-05,
      "loss": 0.4591,
      "step": 18700
    },
    {
      "epoch": 4.009858551221603,
      "grad_norm": 0.48483502864837646,
      "learning_rate": 1.4653521931704531e-05,
      "loss": 0.4695,
      "step": 18710
    },
    {
      "epoch": 4.012001714530647,
      "grad_norm": 14.636655807495117,
      "learning_rate": 1.4650664380625805e-05,
      "loss": 0.463,
      "step": 18720
    },
    {
      "epoch": 4.0141448778396915,
      "grad_norm": 0.43616729974746704,
      "learning_rate": 1.464780682954708e-05,
      "loss": 0.4722,
      "step": 18730
    },
    {
      "epoch": 4.016288041148735,
      "grad_norm": 29.723608016967773,
      "learning_rate": 1.4644949278468355e-05,
      "loss": 1.2387,
      "step": 18740
    },
    {
      "epoch": 4.01843120445778,
      "grad_norm": 0.48711490631103516,
      "learning_rate": 1.4642091727389629e-05,
      "loss": 0.4711,
      "step": 18750
    },
    {
      "epoch": 4.020574367766824,
      "grad_norm": 14.586386680603027,
      "learning_rate": 1.4639234176310904e-05,
      "loss": 0.911,
      "step": 18760
    },
    {
      "epoch": 4.022717531075868,
      "grad_norm": 0.6273118853569031,
      "learning_rate": 1.4636376625232178e-05,
      "loss": 0.7413,
      "step": 18770
    },
    {
      "epoch": 4.024860694384913,
      "grad_norm": 0.6870942711830139,
      "learning_rate": 1.4633519074153452e-05,
      "loss": 0.8592,
      "step": 18780
    },
    {
      "epoch": 4.027003857693956,
      "grad_norm": 0.728100061416626,
      "learning_rate": 1.4630661523074726e-05,
      "loss": 0.5688,
      "step": 18790
    },
    {
      "epoch": 4.029147021003,
      "grad_norm": 14.383561134338379,
      "learning_rate": 1.4627803971996e-05,
      "loss": 0.4308,
      "step": 18800
    },
    {
      "epoch": 4.031290184312045,
      "grad_norm": 0.6290528774261475,
      "learning_rate": 1.4624946420917274e-05,
      "loss": 0.294,
      "step": 18810
    },
    {
      "epoch": 4.033433347621089,
      "grad_norm": 0.5813580751419067,
      "learning_rate": 1.462208886983855e-05,
      "loss": 0.5845,
      "step": 18820
    },
    {
      "epoch": 4.035576510930133,
      "grad_norm": 0.47211194038391113,
      "learning_rate": 1.4619231318759823e-05,
      "loss": 0.1651,
      "step": 18830
    },
    {
      "epoch": 4.037719674239177,
      "grad_norm": 0.48320984840393066,
      "learning_rate": 1.4616373767681097e-05,
      "loss": 0.9296,
      "step": 18840
    },
    {
      "epoch": 4.039862837548221,
      "grad_norm": 0.5209179520606995,
      "learning_rate": 1.4613516216602373e-05,
      "loss": 0.4601,
      "step": 18850
    },
    {
      "epoch": 4.042006000857265,
      "grad_norm": 14.646072387695312,
      "learning_rate": 1.4610658665523647e-05,
      "loss": 1.0437,
      "step": 18860
    },
    {
      "epoch": 4.04414916416631,
      "grad_norm": 0.637151837348938,
      "learning_rate": 1.4607801114444922e-05,
      "loss": 1.0244,
      "step": 18870
    },
    {
      "epoch": 4.046292327475354,
      "grad_norm": 0.6526799201965332,
      "learning_rate": 1.4604943563366196e-05,
      "loss": 0.2933,
      "step": 18880
    },
    {
      "epoch": 4.048435490784398,
      "grad_norm": 0.586270272731781,
      "learning_rate": 1.460208601228747e-05,
      "loss": 0.4473,
      "step": 18890
    },
    {
      "epoch": 4.050578654093442,
      "grad_norm": 14.609760284423828,
      "learning_rate": 1.4599228461208746e-05,
      "loss": 0.7514,
      "step": 18900
    },
    {
      "epoch": 4.052721817402486,
      "grad_norm": 0.48506060242652893,
      "learning_rate": 1.459637091013002e-05,
      "loss": 0.0117,
      "step": 18910
    },
    {
      "epoch": 4.05486498071153,
      "grad_norm": 0.4529983699321747,
      "learning_rate": 1.4593513359051294e-05,
      "loss": 0.466,
      "step": 18920
    },
    {
      "epoch": 4.057008144020575,
      "grad_norm": 14.69118881225586,
      "learning_rate": 1.459065580797257e-05,
      "loss": 0.9443,
      "step": 18930
    },
    {
      "epoch": 4.059151307329619,
      "grad_norm": 0.42962533235549927,
      "learning_rate": 1.4587798256893843e-05,
      "loss": 0.3179,
      "step": 18940
    },
    {
      "epoch": 4.0612944706386624,
      "grad_norm": 0.4395163357257843,
      "learning_rate": 1.4584940705815119e-05,
      "loss": 0.7828,
      "step": 18950
    },
    {
      "epoch": 4.063437633947707,
      "grad_norm": 0.4521287977695465,
      "learning_rate": 1.4582083154736393e-05,
      "loss": 0.47,
      "step": 18960
    },
    {
      "epoch": 4.065580797256751,
      "grad_norm": 29.705751419067383,
      "learning_rate": 1.4579225603657667e-05,
      "loss": 0.9412,
      "step": 18970
    },
    {
      "epoch": 4.067723960565795,
      "grad_norm": 14.958887100219727,
      "learning_rate": 1.4576368052578942e-05,
      "loss": 0.9279,
      "step": 18980
    },
    {
      "epoch": 4.06986712387484,
      "grad_norm": 14.61385726928711,
      "learning_rate": 1.4573510501500216e-05,
      "loss": 0.6096,
      "step": 18990
    },
    {
      "epoch": 4.0720102871838835,
      "grad_norm": 0.5569698214530945,
      "learning_rate": 1.457065295042149e-05,
      "loss": 0.6066,
      "step": 19000
    },
    {
      "epoch": 4.074153450492927,
      "grad_norm": 0.5430896282196045,
      "learning_rate": 1.4567795399342764e-05,
      "loss": 0.7456,
      "step": 19010
    },
    {
      "epoch": 4.076296613801972,
      "grad_norm": 14.85798168182373,
      "learning_rate": 1.4564937848264038e-05,
      "loss": 0.5954,
      "step": 19020
    },
    {
      "epoch": 4.078439777111016,
      "grad_norm": 14.534381866455078,
      "learning_rate": 1.4562080297185312e-05,
      "loss": 0.584,
      "step": 19030
    },
    {
      "epoch": 4.08058294042006,
      "grad_norm": 0.566234827041626,
      "learning_rate": 1.4559222746106588e-05,
      "loss": 0.1564,
      "step": 19040
    },
    {
      "epoch": 4.0827261037291045,
      "grad_norm": 0.5909769535064697,
      "learning_rate": 1.4556365195027862e-05,
      "loss": 0.8865,
      "step": 19050
    },
    {
      "epoch": 4.084869267038148,
      "grad_norm": 0.5574560761451721,
      "learning_rate": 1.4553507643949136e-05,
      "loss": 0.1573,
      "step": 19060
    },
    {
      "epoch": 4.087012430347192,
      "grad_norm": 0.5138894319534302,
      "learning_rate": 1.4550650092870411e-05,
      "loss": 0.3059,
      "step": 19070
    },
    {
      "epoch": 4.089155593656237,
      "grad_norm": 0.4290197193622589,
      "learning_rate": 1.4547792541791685e-05,
      "loss": 0.3188,
      "step": 19080
    },
    {
      "epoch": 4.091298756965281,
      "grad_norm": 14.708918571472168,
      "learning_rate": 1.454493499071296e-05,
      "loss": 0.6343,
      "step": 19090
    },
    {
      "epoch": 4.093441920274325,
      "grad_norm": 14.915009498596191,
      "learning_rate": 1.4542077439634235e-05,
      "loss": 1.1048,
      "step": 19100
    },
    {
      "epoch": 4.095585083583369,
      "grad_norm": 0.4528238773345947,
      "learning_rate": 1.4539219888555509e-05,
      "loss": 0.4724,
      "step": 19110
    },
    {
      "epoch": 4.097728246892413,
      "grad_norm": 14.649641036987305,
      "learning_rate": 1.4536362337476784e-05,
      "loss": 0.3178,
      "step": 19120
    },
    {
      "epoch": 4.099871410201457,
      "grad_norm": 14.61286449432373,
      "learning_rate": 1.4533504786398058e-05,
      "loss": 0.9409,
      "step": 19130
    },
    {
      "epoch": 4.102014573510502,
      "grad_norm": 14.599713325500488,
      "learning_rate": 1.4530647235319332e-05,
      "loss": 0.4627,
      "step": 19140
    },
    {
      "epoch": 4.104157736819546,
      "grad_norm": 0.481330931186676,
      "learning_rate": 1.4527789684240608e-05,
      "loss": 0.4634,
      "step": 19150
    },
    {
      "epoch": 4.1063009001285895,
      "grad_norm": 0.36914026737213135,
      "learning_rate": 1.4524932133161882e-05,
      "loss": 0.164,
      "step": 19160
    },
    {
      "epoch": 4.108444063437634,
      "grad_norm": 0.3378230035305023,
      "learning_rate": 1.4522074582083157e-05,
      "loss": 0.5004,
      "step": 19170
    },
    {
      "epoch": 4.110587226746678,
      "grad_norm": 0.32891377806663513,
      "learning_rate": 1.4519217031004431e-05,
      "loss": 0.1705,
      "step": 19180
    },
    {
      "epoch": 4.112730390055722,
      "grad_norm": 0.302724152803421,
      "learning_rate": 1.4516359479925705e-05,
      "loss": 0.338,
      "step": 19190
    },
    {
      "epoch": 4.114873553364767,
      "grad_norm": 0.2810373306274414,
      "learning_rate": 1.451350192884698e-05,
      "loss": 0.6855,
      "step": 19200
    },
    {
      "epoch": 4.1170167166738105,
      "grad_norm": 0.30008241534233093,
      "learning_rate": 1.4510644377768255e-05,
      "loss": 1.0186,
      "step": 19210
    },
    {
      "epoch": 4.119159879982854,
      "grad_norm": 14.803494453430176,
      "learning_rate": 1.4507786826689527e-05,
      "loss": 0.5179,
      "step": 19220
    },
    {
      "epoch": 4.121303043291899,
      "grad_norm": 0.26196789741516113,
      "learning_rate": 1.4504929275610802e-05,
      "loss": 0.5013,
      "step": 19230
    },
    {
      "epoch": 4.123446206600943,
      "grad_norm": 0.35243701934814453,
      "learning_rate": 1.4502071724532076e-05,
      "loss": 1.0022,
      "step": 19240
    },
    {
      "epoch": 4.125589369909987,
      "grad_norm": 0.44575151801109314,
      "learning_rate": 1.449921417345335e-05,
      "loss": 0.8149,
      "step": 19250
    },
    {
      "epoch": 4.1277325332190316,
      "grad_norm": 0.4774368107318878,
      "learning_rate": 1.4496356622374626e-05,
      "loss": 0.473,
      "step": 19260
    },
    {
      "epoch": 4.129875696528075,
      "grad_norm": 0.43774446845054626,
      "learning_rate": 1.44934990712959e-05,
      "loss": 0.3132,
      "step": 19270
    },
    {
      "epoch": 4.132018859837119,
      "grad_norm": 0.4845392405986786,
      "learning_rate": 1.4490641520217174e-05,
      "loss": 1.3883,
      "step": 19280
    },
    {
      "epoch": 4.134162023146164,
      "grad_norm": 14.635467529296875,
      "learning_rate": 1.448778396913845e-05,
      "loss": 0.7564,
      "step": 19290
    },
    {
      "epoch": 4.136305186455208,
      "grad_norm": 0.5918552279472351,
      "learning_rate": 1.4484926418059723e-05,
      "loss": 0.4467,
      "step": 19300
    },
    {
      "epoch": 4.138448349764252,
      "grad_norm": 0.5590094327926636,
      "learning_rate": 1.4482068866980999e-05,
      "loss": 0.4566,
      "step": 19310
    },
    {
      "epoch": 4.140591513073296,
      "grad_norm": 0.5436198711395264,
      "learning_rate": 1.4479211315902273e-05,
      "loss": 0.7464,
      "step": 19320
    },
    {
      "epoch": 4.14273467638234,
      "grad_norm": 14.667487144470215,
      "learning_rate": 1.4476353764823547e-05,
      "loss": 0.7514,
      "step": 19330
    },
    {
      "epoch": 4.144877839691384,
      "grad_norm": 0.5692768692970276,
      "learning_rate": 1.4473496213744822e-05,
      "loss": 0.4497,
      "step": 19340
    },
    {
      "epoch": 4.147021003000429,
      "grad_norm": 14.407718658447266,
      "learning_rate": 1.4470638662666096e-05,
      "loss": 0.7336,
      "step": 19350
    },
    {
      "epoch": 4.149164166309473,
      "grad_norm": 0.562703013420105,
      "learning_rate": 1.4467781111587372e-05,
      "loss": 0.1546,
      "step": 19360
    },
    {
      "epoch": 4.151307329618517,
      "grad_norm": 14.595157623291016,
      "learning_rate": 1.4464923560508646e-05,
      "loss": 0.4532,
      "step": 19370
    },
    {
      "epoch": 4.153450492927561,
      "grad_norm": 14.62133502960205,
      "learning_rate": 1.446206600942992e-05,
      "loss": 0.7677,
      "step": 19380
    },
    {
      "epoch": 4.155593656236605,
      "grad_norm": 0.47710174322128296,
      "learning_rate": 1.4459208458351195e-05,
      "loss": 0.4719,
      "step": 19390
    },
    {
      "epoch": 4.157736819545649,
      "grad_norm": 0.4437520205974579,
      "learning_rate": 1.445635090727247e-05,
      "loss": 0.7815,
      "step": 19400
    },
    {
      "epoch": 4.159879982854694,
      "grad_norm": 14.670501708984375,
      "learning_rate": 1.4453493356193743e-05,
      "loss": 0.3164,
      "step": 19410
    },
    {
      "epoch": 4.162023146163738,
      "grad_norm": 0.42140376567840576,
      "learning_rate": 1.4450635805115019e-05,
      "loss": 0.6261,
      "step": 19420
    },
    {
      "epoch": 4.164166309472781,
      "grad_norm": 0.38263750076293945,
      "learning_rate": 1.4447778254036291e-05,
      "loss": 0.3251,
      "step": 19430
    },
    {
      "epoch": 4.166309472781826,
      "grad_norm": 0.3495247960090637,
      "learning_rate": 1.4444920702957565e-05,
      "loss": 0.3253,
      "step": 19440
    },
    {
      "epoch": 4.16845263609087,
      "grad_norm": 0.3483675420284271,
      "learning_rate": 1.444206315187884e-05,
      "loss": 0.4984,
      "step": 19450
    },
    {
      "epoch": 4.170595799399914,
      "grad_norm": 14.788516998291016,
      "learning_rate": 1.4439205600800115e-05,
      "loss": 0.3336,
      "step": 19460
    },
    {
      "epoch": 4.172738962708959,
      "grad_norm": 0.3300511837005615,
      "learning_rate": 1.4436348049721389e-05,
      "loss": 0.4999,
      "step": 19470
    },
    {
      "epoch": 4.1748821260180025,
      "grad_norm": 0.3576350212097168,
      "learning_rate": 1.4433490498642664e-05,
      "loss": 0.4966,
      "step": 19480
    },
    {
      "epoch": 4.177025289327046,
      "grad_norm": 14.722943305969238,
      "learning_rate": 1.4430632947563938e-05,
      "loss": 0.6553,
      "step": 19490
    },
    {
      "epoch": 4.179168452636091,
      "grad_norm": 14.750041961669922,
      "learning_rate": 1.4427775396485214e-05,
      "loss": 0.4952,
      "step": 19500
    },
    {
      "epoch": 4.181311615945135,
      "grad_norm": 0.34350961446762085,
      "learning_rate": 1.4424917845406488e-05,
      "loss": 0.4927,
      "step": 19510
    },
    {
      "epoch": 4.183454779254179,
      "grad_norm": 0.3634934723377228,
      "learning_rate": 1.4422060294327762e-05,
      "loss": 0.4913,
      "step": 19520
    },
    {
      "epoch": 4.1855979425632235,
      "grad_norm": 0.3877238929271698,
      "learning_rate": 1.4419202743249037e-05,
      "loss": 0.6496,
      "step": 19530
    },
    {
      "epoch": 4.187741105872267,
      "grad_norm": 0.39653709530830383,
      "learning_rate": 1.4416345192170311e-05,
      "loss": 0.8032,
      "step": 19540
    },
    {
      "epoch": 4.189884269181311,
      "grad_norm": 0.41992491483688354,
      "learning_rate": 1.4413487641091585e-05,
      "loss": 0.7906,
      "step": 19550
    },
    {
      "epoch": 4.192027432490356,
      "grad_norm": 14.624439239501953,
      "learning_rate": 1.441063009001286e-05,
      "loss": 0.6258,
      "step": 19560
    },
    {
      "epoch": 4.1941705957994,
      "grad_norm": 14.559309005737305,
      "learning_rate": 1.4407772538934135e-05,
      "loss": 1.0673,
      "step": 19570
    },
    {
      "epoch": 4.196313759108444,
      "grad_norm": 0.6227012276649475,
      "learning_rate": 1.440491498785541e-05,
      "loss": 0.737,
      "step": 19580
    },
    {
      "epoch": 4.198456922417488,
      "grad_norm": 0.6787766218185425,
      "learning_rate": 1.4402057436776684e-05,
      "loss": 0.8641,
      "step": 19590
    },
    {
      "epoch": 4.200600085726532,
      "grad_norm": 0.6451395750045776,
      "learning_rate": 1.4399199885697958e-05,
      "loss": 0.4353,
      "step": 19600
    },
    {
      "epoch": 4.202743249035576,
      "grad_norm": 0.6325010657310486,
      "learning_rate": 1.4396342334619234e-05,
      "loss": 0.5799,
      "step": 19610
    },
    {
      "epoch": 4.204886412344621,
      "grad_norm": 0.6123883128166199,
      "learning_rate": 1.4393484783540508e-05,
      "loss": 0.5862,
      "step": 19620
    },
    {
      "epoch": 4.207029575653665,
      "grad_norm": 0.6010149717330933,
      "learning_rate": 1.4390627232461782e-05,
      "loss": 0.444,
      "step": 19630
    },
    {
      "epoch": 4.209172738962709,
      "grad_norm": 29.61737060546875,
      "learning_rate": 1.4387769681383057e-05,
      "loss": 0.4543,
      "step": 19640
    },
    {
      "epoch": 4.211315902271753,
      "grad_norm": 0.45504337549209595,
      "learning_rate": 1.438491213030433e-05,
      "loss": 0.4642,
      "step": 19650
    },
    {
      "epoch": 4.213459065580797,
      "grad_norm": 0.48655959963798523,
      "learning_rate": 1.4382054579225603e-05,
      "loss": 0.4672,
      "step": 19660
    },
    {
      "epoch": 4.215602228889842,
      "grad_norm": 0.4521558880805969,
      "learning_rate": 1.4379197028146879e-05,
      "loss": 0.3151,
      "step": 19670
    },
    {
      "epoch": 4.217745392198886,
      "grad_norm": 14.650947570800781,
      "learning_rate": 1.4376339477068153e-05,
      "loss": 0.6312,
      "step": 19680
    },
    {
      "epoch": 4.2198885555079295,
      "grad_norm": 0.4399995505809784,
      "learning_rate": 1.4373481925989427e-05,
      "loss": 0.4738,
      "step": 19690
    },
    {
      "epoch": 4.222031718816974,
      "grad_norm": 0.40291738510131836,
      "learning_rate": 1.4370624374910703e-05,
      "loss": 0.1641,
      "step": 19700
    },
    {
      "epoch": 4.224174882126018,
      "grad_norm": 29.690967559814453,
      "learning_rate": 1.4367766823831976e-05,
      "loss": 1.2702,
      "step": 19710
    },
    {
      "epoch": 4.226318045435062,
      "grad_norm": 0.46047839522361755,
      "learning_rate": 1.4364909272753252e-05,
      "loss": 0.4738,
      "step": 19720
    },
    {
      "epoch": 4.228461208744107,
      "grad_norm": 29.702852249145508,
      "learning_rate": 1.4362051721674526e-05,
      "loss": 0.4735,
      "step": 19730
    },
    {
      "epoch": 4.2306043720531505,
      "grad_norm": 14.73536491394043,
      "learning_rate": 1.43591941705958e-05,
      "loss": 0.778,
      "step": 19740
    },
    {
      "epoch": 4.232747535362194,
      "grad_norm": 0.45820382237434387,
      "learning_rate": 1.4356336619517076e-05,
      "loss": 0.4774,
      "step": 19750
    },
    {
      "epoch": 4.234890698671239,
      "grad_norm": 0.4461694359779358,
      "learning_rate": 1.435347906843835e-05,
      "loss": 0.3173,
      "step": 19760
    },
    {
      "epoch": 4.237033861980283,
      "grad_norm": 0.4310136139392853,
      "learning_rate": 1.4350621517359623e-05,
      "loss": 0.9355,
      "step": 19770
    },
    {
      "epoch": 4.239177025289327,
      "grad_norm": 0.4146972596645355,
      "learning_rate": 1.4347763966280899e-05,
      "loss": 0.7823,
      "step": 19780
    },
    {
      "epoch": 4.241320188598372,
      "grad_norm": 14.885167121887207,
      "learning_rate": 1.4344906415202173e-05,
      "loss": 0.6254,
      "step": 19790
    },
    {
      "epoch": 4.243463351907415,
      "grad_norm": 0.5048443675041199,
      "learning_rate": 1.4342048864123449e-05,
      "loss": 0.6186,
      "step": 19800
    },
    {
      "epoch": 4.245606515216459,
      "grad_norm": 14.609542846679688,
      "learning_rate": 1.4339191313044723e-05,
      "loss": 0.4644,
      "step": 19810
    },
    {
      "epoch": 4.247749678525504,
      "grad_norm": 0.4739305377006531,
      "learning_rate": 1.4336333761965996e-05,
      "loss": 0.4624,
      "step": 19820
    },
    {
      "epoch": 4.249892841834548,
      "grad_norm": 0.4912508428096771,
      "learning_rate": 1.4333476210887272e-05,
      "loss": 0.6123,
      "step": 19830
    },
    {
      "epoch": 4.252036005143592,
      "grad_norm": 0.4363795220851898,
      "learning_rate": 1.4330618659808546e-05,
      "loss": 0.1633,
      "step": 19840
    },
    {
      "epoch": 4.254179168452636,
      "grad_norm": 14.641608238220215,
      "learning_rate": 1.432776110872982e-05,
      "loss": 0.7781,
      "step": 19850
    },
    {
      "epoch": 4.25632233176168,
      "grad_norm": 14.60235595703125,
      "learning_rate": 1.4324903557651094e-05,
      "loss": 0.9274,
      "step": 19860
    },
    {
      "epoch": 4.258465495070724,
      "grad_norm": 14.648290634155273,
      "learning_rate": 1.4322046006572368e-05,
      "loss": 0.6274,
      "step": 19870
    },
    {
      "epoch": 4.260608658379769,
      "grad_norm": 0.4741125702857971,
      "learning_rate": 1.4319188455493642e-05,
      "loss": 0.4713,
      "step": 19880
    },
    {
      "epoch": 4.262751821688813,
      "grad_norm": 0.4984118938446045,
      "learning_rate": 1.4316330904414917e-05,
      "loss": 0.7698,
      "step": 19890
    },
    {
      "epoch": 4.264894984997857,
      "grad_norm": 0.4957610070705414,
      "learning_rate": 1.4313473353336191e-05,
      "loss": 0.309,
      "step": 19900
    },
    {
      "epoch": 4.267038148306901,
      "grad_norm": 0.46159622073173523,
      "learning_rate": 1.4310615802257465e-05,
      "loss": 0.9228,
      "step": 19910
    },
    {
      "epoch": 4.269181311615945,
      "grad_norm": 0.4546078145503998,
      "learning_rate": 1.430775825117874e-05,
      "loss": 0.1634,
      "step": 19920
    },
    {
      "epoch": 4.271324474924989,
      "grad_norm": 0.44218480587005615,
      "learning_rate": 1.4304900700100015e-05,
      "loss": 0.7798,
      "step": 19930
    },
    {
      "epoch": 4.273467638234034,
      "grad_norm": 0.4415680766105652,
      "learning_rate": 1.430204314902129e-05,
      "loss": 0.9363,
      "step": 19940
    },
    {
      "epoch": 4.275610801543078,
      "grad_norm": 0.5502667427062988,
      "learning_rate": 1.4299185597942564e-05,
      "loss": 0.9185,
      "step": 19950
    },
    {
      "epoch": 4.2777539648521214,
      "grad_norm": 0.626259982585907,
      "learning_rate": 1.4296328046863838e-05,
      "loss": 1.0342,
      "step": 19960
    },
    {
      "epoch": 4.279897128161166,
      "grad_norm": 0.5699563026428223,
      "learning_rate": 1.4293470495785114e-05,
      "loss": 0.1558,
      "step": 19970
    },
    {
      "epoch": 4.28204029147021,
      "grad_norm": 29.574283599853516,
      "learning_rate": 1.4290612944706388e-05,
      "loss": 0.7538,
      "step": 19980
    },
    {
      "epoch": 4.284183454779254,
      "grad_norm": 0.4664512872695923,
      "learning_rate": 1.4287755393627662e-05,
      "loss": 0.314,
      "step": 19990
    },
    {
      "epoch": 4.286326618088299,
      "grad_norm": 14.647916793823242,
      "learning_rate": 1.4284897842548937e-05,
      "loss": 0.9281,
      "step": 20000
    },
    {
      "epoch": 4.2884697813973425,
      "grad_norm": 0.4390031695365906,
      "learning_rate": 1.4282040291470211e-05,
      "loss": 0.4698,
      "step": 20010
    },
    {
      "epoch": 4.290612944706386,
      "grad_norm": 0.4726410210132599,
      "learning_rate": 1.4279182740391487e-05,
      "loss": 0.6244,
      "step": 20020
    },
    {
      "epoch": 4.292756108015431,
      "grad_norm": 14.718626022338867,
      "learning_rate": 1.427632518931276e-05,
      "loss": 0.6225,
      "step": 20030
    },
    {
      "epoch": 4.294899271324475,
      "grad_norm": 0.4957525134086609,
      "learning_rate": 1.4273467638234035e-05,
      "loss": 0.6138,
      "step": 20040
    },
    {
      "epoch": 4.297042434633519,
      "grad_norm": 0.537865400314331,
      "learning_rate": 1.427061008715531e-05,
      "loss": 1.0488,
      "step": 20050
    },
    {
      "epoch": 4.2991855979425635,
      "grad_norm": 0.5600255131721497,
      "learning_rate": 1.4267752536076584e-05,
      "loss": 0.5991,
      "step": 20060
    },
    {
      "epoch": 4.301328761251607,
      "grad_norm": 0.5638468265533447,
      "learning_rate": 1.426489498499786e-05,
      "loss": 0.7414,
      "step": 20070
    },
    {
      "epoch": 4.303471924560651,
      "grad_norm": 0.5288602709770203,
      "learning_rate": 1.4262037433919132e-05,
      "loss": 0.158,
      "step": 20080
    },
    {
      "epoch": 4.305615087869696,
      "grad_norm": 0.5027535557746887,
      "learning_rate": 1.4259179882840406e-05,
      "loss": 1.5147,
      "step": 20090
    },
    {
      "epoch": 4.30775825117874,
      "grad_norm": 0.4943386912345886,
      "learning_rate": 1.425632233176168e-05,
      "loss": 0.4617,
      "step": 20100
    },
    {
      "epoch": 4.309901414487784,
      "grad_norm": 0.5073238015174866,
      "learning_rate": 1.4253464780682956e-05,
      "loss": 0.4593,
      "step": 20110
    },
    {
      "epoch": 4.312044577796828,
      "grad_norm": 0.4942820072174072,
      "learning_rate": 1.425060722960423e-05,
      "loss": 0.6106,
      "step": 20120
    },
    {
      "epoch": 4.314187741105872,
      "grad_norm": 0.4789370000362396,
      "learning_rate": 1.4247749678525503e-05,
      "loss": 0.6192,
      "step": 20130
    },
    {
      "epoch": 4.316330904414916,
      "grad_norm": 0.5166818499565125,
      "learning_rate": 1.4244892127446779e-05,
      "loss": 0.9165,
      "step": 20140
    },
    {
      "epoch": 4.318474067723961,
      "grad_norm": 0.5532554388046265,
      "learning_rate": 1.4242034576368053e-05,
      "loss": 0.4516,
      "step": 20150
    },
    {
      "epoch": 4.320617231033005,
      "grad_norm": 0.5065302848815918,
      "learning_rate": 1.4239177025289329e-05,
      "loss": 0.452,
      "step": 20160
    },
    {
      "epoch": 4.3227603943420485,
      "grad_norm": 14.603984832763672,
      "learning_rate": 1.4236319474210603e-05,
      "loss": 0.4675,
      "step": 20170
    },
    {
      "epoch": 4.324903557651093,
      "grad_norm": 0.4648967683315277,
      "learning_rate": 1.4233461923131876e-05,
      "loss": 0.6206,
      "step": 20180
    },
    {
      "epoch": 4.327046720960137,
      "grad_norm": 0.46962279081344604,
      "learning_rate": 1.4230604372053152e-05,
      "loss": 0.4701,
      "step": 20190
    },
    {
      "epoch": 4.329189884269181,
      "grad_norm": 0.47197970747947693,
      "learning_rate": 1.4227746820974426e-05,
      "loss": 0.6163,
      "step": 20200
    },
    {
      "epoch": 4.331333047578226,
      "grad_norm": 15.463671684265137,
      "learning_rate": 1.4224889269895702e-05,
      "loss": 0.7726,
      "step": 20210
    },
    {
      "epoch": 4.3334762108872695,
      "grad_norm": 0.42553186416625977,
      "learning_rate": 1.4222031718816976e-05,
      "loss": 0.6248,
      "step": 20220
    },
    {
      "epoch": 4.335619374196313,
      "grad_norm": 0.4297829866409302,
      "learning_rate": 1.421917416773825e-05,
      "loss": 0.9453,
      "step": 20230
    },
    {
      "epoch": 4.337762537505358,
      "grad_norm": 0.42457225918769836,
      "learning_rate": 1.4216316616659525e-05,
      "loss": 0.4734,
      "step": 20240
    },
    {
      "epoch": 4.339905700814402,
      "grad_norm": 0.4077315926551819,
      "learning_rate": 1.4213459065580799e-05,
      "loss": 0.4774,
      "step": 20250
    },
    {
      "epoch": 4.342048864123446,
      "grad_norm": 0.362979531288147,
      "learning_rate": 1.4210601514502073e-05,
      "loss": 0.1681,
      "step": 20260
    },
    {
      "epoch": 4.3441920274324906,
      "grad_norm": 0.32081398367881775,
      "learning_rate": 1.4207743963423349e-05,
      "loss": 0.6665,
      "step": 20270
    },
    {
      "epoch": 4.346335190741534,
      "grad_norm": 15.000672340393066,
      "learning_rate": 1.4204886412344623e-05,
      "loss": 0.4922,
      "step": 20280
    },
    {
      "epoch": 4.348478354050578,
      "grad_norm": 0.34085017442703247,
      "learning_rate": 1.4202028861265895e-05,
      "loss": 0.6631,
      "step": 20290
    },
    {
      "epoch": 4.350621517359623,
      "grad_norm": 0.36572933197021484,
      "learning_rate": 1.419917131018717e-05,
      "loss": 0.4937,
      "step": 20300
    },
    {
      "epoch": 4.352764680668667,
      "grad_norm": 29.850372314453125,
      "learning_rate": 1.4196313759108444e-05,
      "loss": 1.4299,
      "step": 20310
    },
    {
      "epoch": 4.354907843977711,
      "grad_norm": 0.45433303713798523,
      "learning_rate": 1.4193456208029718e-05,
      "loss": 0.1654,
      "step": 20320
    },
    {
      "epoch": 4.357051007286755,
      "grad_norm": 0.43377014994621277,
      "learning_rate": 1.4190598656950994e-05,
      "loss": 0.3185,
      "step": 20330
    },
    {
      "epoch": 4.359194170595799,
      "grad_norm": 14.87108325958252,
      "learning_rate": 1.4187741105872268e-05,
      "loss": 0.4794,
      "step": 20340
    },
    {
      "epoch": 4.361337333904844,
      "grad_norm": 0.42741629481315613,
      "learning_rate": 1.4184883554793543e-05,
      "loss": 0.9583,
      "step": 20350
    },
    {
      "epoch": 4.363480497213888,
      "grad_norm": 29.66650390625,
      "learning_rate": 1.4182026003714817e-05,
      "loss": 0.7817,
      "step": 20360
    },
    {
      "epoch": 4.365623660522932,
      "grad_norm": 0.5351297855377197,
      "learning_rate": 1.4179168452636091e-05,
      "loss": 0.6083,
      "step": 20370
    },
    {
      "epoch": 4.3677668238319765,
      "grad_norm": 0.5567952394485474,
      "learning_rate": 1.4176310901557367e-05,
      "loss": 0.4505,
      "step": 20380
    },
    {
      "epoch": 4.36990998714102,
      "grad_norm": 0.5836673974990845,
      "learning_rate": 1.417345335047864e-05,
      "loss": 0.7454,
      "step": 20390
    },
    {
      "epoch": 4.372053150450064,
      "grad_norm": 31.19855499267578,
      "learning_rate": 1.4170595799399915e-05,
      "loss": 1.0328,
      "step": 20400
    },
    {
      "epoch": 4.374196313759109,
      "grad_norm": 14.671024322509766,
      "learning_rate": 1.416773824832119e-05,
      "loss": 0.5789,
      "step": 20410
    },
    {
      "epoch": 4.376339477068153,
      "grad_norm": 14.69748306274414,
      "learning_rate": 1.4164880697242464e-05,
      "loss": 0.5803,
      "step": 20420
    },
    {
      "epoch": 4.378482640377197,
      "grad_norm": 14.687304496765137,
      "learning_rate": 1.416202314616374e-05,
      "loss": 0.9987,
      "step": 20430
    },
    {
      "epoch": 4.380625803686241,
      "grad_norm": 14.441481590270996,
      "learning_rate": 1.4159165595085014e-05,
      "loss": 0.4313,
      "step": 20440
    },
    {
      "epoch": 4.382768966995285,
      "grad_norm": 0.6516190767288208,
      "learning_rate": 1.4156308044006288e-05,
      "loss": 0.2987,
      "step": 20450
    },
    {
      "epoch": 4.384912130304329,
      "grad_norm": 29.526100158691406,
      "learning_rate": 1.4153450492927563e-05,
      "loss": 1.3013,
      "step": 20460
    },
    {
      "epoch": 4.387055293613374,
      "grad_norm": 0.5895775556564331,
      "learning_rate": 1.4150592941848837e-05,
      "loss": 0.5847,
      "step": 20470
    },
    {
      "epoch": 4.389198456922418,
      "grad_norm": 0.5629070997238159,
      "learning_rate": 1.4147735390770111e-05,
      "loss": 0.4414,
      "step": 20480
    },
    {
      "epoch": 4.3913416202314615,
      "grad_norm": 0.5439774394035339,
      "learning_rate": 1.4144877839691387e-05,
      "loss": 0.3048,
      "step": 20490
    },
    {
      "epoch": 4.393484783540506,
      "grad_norm": 0.553229808807373,
      "learning_rate": 1.414202028861266e-05,
      "loss": 1.1966,
      "step": 20500
    },
    {
      "epoch": 4.39562794684955,
      "grad_norm": 0.47256073355674744,
      "learning_rate": 1.4139162737533933e-05,
      "loss": 0.0113,
      "step": 20510
    },
    {
      "epoch": 4.397771110158594,
      "grad_norm": 0.37276211380958557,
      "learning_rate": 1.4136305186455209e-05,
      "loss": 0.0092,
      "step": 20520
    },
    {
      "epoch": 4.399914273467639,
      "grad_norm": 15.260842323303223,
      "learning_rate": 1.4133447635376483e-05,
      "loss": 0.3308,
      "step": 20530
    },
    {
      "epoch": 4.4020574367766825,
      "grad_norm": 0.2743706703186035,
      "learning_rate": 1.4130590084297757e-05,
      "loss": 0.5152,
      "step": 20540
    },
    {
      "epoch": 4.404200600085726,
      "grad_norm": 0.2917179763317108,
      "learning_rate": 1.4127732533219032e-05,
      "loss": 0.5174,
      "step": 20550
    },
    {
      "epoch": 4.406343763394771,
      "grad_norm": 15.255840301513672,
      "learning_rate": 1.4124874982140306e-05,
      "loss": 0.6801,
      "step": 20560
    },
    {
      "epoch": 4.408486926703815,
      "grad_norm": 0.34184157848358154,
      "learning_rate": 1.4122017431061582e-05,
      "loss": 0.3372,
      "step": 20570
    },
    {
      "epoch": 4.410630090012859,
      "grad_norm": 30.254240036010742,
      "learning_rate": 1.4119159879982856e-05,
      "loss": 0.8214,
      "step": 20580
    },
    {
      "epoch": 4.4127732533219035,
      "grad_norm": 0.36790236830711365,
      "learning_rate": 1.411630232890413e-05,
      "loss": 0.326,
      "step": 20590
    },
    {
      "epoch": 4.414916416630947,
      "grad_norm": 0.3626830279827118,
      "learning_rate": 1.4113444777825405e-05,
      "loss": 0.3305,
      "step": 20600
    },
    {
      "epoch": 4.417059579939991,
      "grad_norm": 0.32243263721466064,
      "learning_rate": 1.4110587226746679e-05,
      "loss": 0.8313,
      "step": 20610
    },
    {
      "epoch": 4.419202743249036,
      "grad_norm": 14.768257141113281,
      "learning_rate": 1.4107729675667953e-05,
      "loss": 0.6758,
      "step": 20620
    },
    {
      "epoch": 4.42134590655808,
      "grad_norm": 0.36064082384109497,
      "learning_rate": 1.4104872124589229e-05,
      "loss": 0.6531,
      "step": 20630
    },
    {
      "epoch": 4.423489069867124,
      "grad_norm": 0.4410130977630615,
      "learning_rate": 1.4102014573510503e-05,
      "loss": 0.9597,
      "step": 20640
    },
    {
      "epoch": 4.425632233176168,
      "grad_norm": 0.5020824074745178,
      "learning_rate": 1.4099157022431778e-05,
      "loss": 0.4672,
      "step": 20650
    },
    {
      "epoch": 4.427775396485212,
      "grad_norm": 0.5377492904663086,
      "learning_rate": 1.4096299471353052e-05,
      "loss": 0.7526,
      "step": 20660
    },
    {
      "epoch": 4.429918559794256,
      "grad_norm": 14.425997734069824,
      "learning_rate": 1.4093441920274326e-05,
      "loss": 0.7331,
      "step": 20670
    },
    {
      "epoch": 4.432061723103301,
      "grad_norm": 0.6174477338790894,
      "learning_rate": 1.4090584369195602e-05,
      "loss": 0.1564,
      "step": 20680
    },
    {
      "epoch": 4.434204886412345,
      "grad_norm": 14.522516250610352,
      "learning_rate": 1.4087726818116876e-05,
      "loss": 0.5923,
      "step": 20690
    },
    {
      "epoch": 4.4363480497213885,
      "grad_norm": 14.590240478515625,
      "learning_rate": 1.408486926703815e-05,
      "loss": 0.5993,
      "step": 20700
    },
    {
      "epoch": 4.438491213030433,
      "grad_norm": 0.5519073605537415,
      "learning_rate": 1.4082011715959425e-05,
      "loss": 0.5991,
      "step": 20710
    },
    {
      "epoch": 4.440634376339477,
      "grad_norm": 0.49144238233566284,
      "learning_rate": 1.4079154164880697e-05,
      "loss": 0.4613,
      "step": 20720
    },
    {
      "epoch": 4.442777539648521,
      "grad_norm": 14.642608642578125,
      "learning_rate": 1.4076296613801971e-05,
      "loss": 0.92,
      "step": 20730
    },
    {
      "epoch": 4.444920702957566,
      "grad_norm": 14.62095832824707,
      "learning_rate": 1.4073439062723247e-05,
      "loss": 0.4629,
      "step": 20740
    },
    {
      "epoch": 4.4470638662666095,
      "grad_norm": 14.638837814331055,
      "learning_rate": 1.4070581511644521e-05,
      "loss": 0.6223,
      "step": 20750
    },
    {
      "epoch": 4.449207029575653,
      "grad_norm": 0.4989565312862396,
      "learning_rate": 1.4067723960565795e-05,
      "loss": 0.4581,
      "step": 20760
    },
    {
      "epoch": 4.451350192884698,
      "grad_norm": 14.621297836303711,
      "learning_rate": 1.406486640948707e-05,
      "loss": 0.9055,
      "step": 20770
    },
    {
      "epoch": 4.453493356193742,
      "grad_norm": 0.4598410427570343,
      "learning_rate": 1.4062008858408344e-05,
      "loss": 0.3159,
      "step": 20780
    },
    {
      "epoch": 4.455636519502786,
      "grad_norm": 0.5600786805152893,
      "learning_rate": 1.405915130732962e-05,
      "loss": 0.6015,
      "step": 20790
    },
    {
      "epoch": 4.457779682811831,
      "grad_norm": 0.5314116477966309,
      "learning_rate": 1.4056293756250894e-05,
      "loss": 0.4518,
      "step": 20800
    },
    {
      "epoch": 4.459922846120874,
      "grad_norm": 14.537599563598633,
      "learning_rate": 1.4053436205172168e-05,
      "loss": 0.7559,
      "step": 20810
    },
    {
      "epoch": 4.462066009429918,
      "grad_norm": 0.5327552556991577,
      "learning_rate": 1.4050578654093443e-05,
      "loss": 0.4565,
      "step": 20820
    },
    {
      "epoch": 4.464209172738963,
      "grad_norm": 14.704447746276855,
      "learning_rate": 1.4047721103014717e-05,
      "loss": 0.315,
      "step": 20830
    },
    {
      "epoch": 4.466352336048007,
      "grad_norm": 0.3841879665851593,
      "learning_rate": 1.4044863551935991e-05,
      "loss": 0.6443,
      "step": 20840
    },
    {
      "epoch": 4.468495499357051,
      "grad_norm": 0.4089168608188629,
      "learning_rate": 1.4042006000857267e-05,
      "loss": 0.647,
      "step": 20850
    },
    {
      "epoch": 4.470638662666095,
      "grad_norm": 14.709818840026855,
      "learning_rate": 1.4039148449778541e-05,
      "loss": 0.3226,
      "step": 20860
    },
    {
      "epoch": 4.472781825975139,
      "grad_norm": 14.730466842651367,
      "learning_rate": 1.4036290898699816e-05,
      "loss": 0.7999,
      "step": 20870
    },
    {
      "epoch": 4.474924989284183,
      "grad_norm": 29.688467025756836,
      "learning_rate": 1.403343334762109e-05,
      "loss": 0.4757,
      "step": 20880
    },
    {
      "epoch": 4.477068152593228,
      "grad_norm": 0.4375481903553009,
      "learning_rate": 1.4030575796542364e-05,
      "loss": 0.3171,
      "step": 20890
    },
    {
      "epoch": 4.479211315902272,
      "grad_norm": 14.676403999328613,
      "learning_rate": 1.402771824546364e-05,
      "loss": 0.7931,
      "step": 20900
    },
    {
      "epoch": 4.481354479211316,
      "grad_norm": 14.647903442382812,
      "learning_rate": 1.4024860694384914e-05,
      "loss": 0.1651,
      "step": 20910
    },
    {
      "epoch": 4.48349764252036,
      "grad_norm": 0.4120473265647888,
      "learning_rate": 1.402200314330619e-05,
      "loss": 0.4751,
      "step": 20920
    },
    {
      "epoch": 4.485640805829404,
      "grad_norm": 0.405102014541626,
      "learning_rate": 1.4019145592227463e-05,
      "loss": 0.3256,
      "step": 20930
    },
    {
      "epoch": 4.487783969138448,
      "grad_norm": 14.699233055114746,
      "learning_rate": 1.4016288041148736e-05,
      "loss": 1.2658,
      "step": 20940
    },
    {
      "epoch": 4.489927132447493,
      "grad_norm": 0.45574501156806946,
      "learning_rate": 1.401343049007001e-05,
      "loss": 0.3177,
      "step": 20950
    },
    {
      "epoch": 4.492070295756537,
      "grad_norm": 0.44351276755332947,
      "learning_rate": 1.4010572938991285e-05,
      "loss": 0.3217,
      "step": 20960
    },
    {
      "epoch": 4.4942134590655805,
      "grad_norm": 0.46117475628852844,
      "learning_rate": 1.400771538791256e-05,
      "loss": 1.0962,
      "step": 20970
    },
    {
      "epoch": 4.496356622374625,
      "grad_norm": 0.5621563792228699,
      "learning_rate": 1.4004857836833833e-05,
      "loss": 0.9154,
      "step": 20980
    },
    {
      "epoch": 4.498499785683669,
      "grad_norm": 0.5820168852806091,
      "learning_rate": 1.4002000285755109e-05,
      "loss": 0.59,
      "step": 20990
    },
    {
      "epoch": 4.500642948992713,
      "grad_norm": 0.5387587547302246,
      "learning_rate": 1.3999142734676383e-05,
      "loss": 0.1587,
      "step": 21000
    },
    {
      "epoch": 4.502786112301758,
      "grad_norm": 0.47892525792121887,
      "learning_rate": 1.3996285183597658e-05,
      "loss": 0.1607,
      "step": 21010
    },
    {
      "epoch": 4.5049292756108015,
      "grad_norm": 0.45623451471328735,
      "learning_rate": 1.3993427632518932e-05,
      "loss": 0.9257,
      "step": 21020
    },
    {
      "epoch": 4.507072438919845,
      "grad_norm": 0.4004534184932709,
      "learning_rate": 1.3990570081440206e-05,
      "loss": 0.7773,
      "step": 21030
    },
    {
      "epoch": 4.50921560222889,
      "grad_norm": 15.064373016357422,
      "learning_rate": 1.3987712530361482e-05,
      "loss": 0.6321,
      "step": 21040
    },
    {
      "epoch": 4.511358765537934,
      "grad_norm": 0.34521886706352234,
      "learning_rate": 1.3984854979282756e-05,
      "loss": 0.0084,
      "step": 21050
    },
    {
      "epoch": 4.513501928846978,
      "grad_norm": 0.32154831290245056,
      "learning_rate": 1.3981997428204031e-05,
      "loss": 0.3377,
      "step": 21060
    },
    {
      "epoch": 4.5156450921560225,
      "grad_norm": 14.748519897460938,
      "learning_rate": 1.3979139877125305e-05,
      "loss": 0.5049,
      "step": 21070
    },
    {
      "epoch": 4.517788255465066,
      "grad_norm": 14.754386901855469,
      "learning_rate": 1.397628232604658e-05,
      "loss": 0.6714,
      "step": 21080
    },
    {
      "epoch": 4.51993141877411,
      "grad_norm": 14.762991905212402,
      "learning_rate": 1.3973424774967855e-05,
      "loss": 0.6591,
      "step": 21090
    },
    {
      "epoch": 4.522074582083155,
      "grad_norm": 0.3449108898639679,
      "learning_rate": 1.3970567223889129e-05,
      "loss": 0.3327,
      "step": 21100
    },
    {
      "epoch": 4.524217745392199,
      "grad_norm": 14.721089363098145,
      "learning_rate": 1.3967709672810403e-05,
      "loss": 0.4939,
      "step": 21110
    },
    {
      "epoch": 4.526360908701243,
      "grad_norm": 0.36787962913513184,
      "learning_rate": 1.3964852121731678e-05,
      "loss": 0.9743,
      "step": 21120
    },
    {
      "epoch": 4.528504072010287,
      "grad_norm": 0.3635426163673401,
      "learning_rate": 1.3961994570652952e-05,
      "loss": 0.0082,
      "step": 21130
    },
    {
      "epoch": 4.530647235319331,
      "grad_norm": 0.3623494803905487,
      "learning_rate": 1.3959137019574228e-05,
      "loss": 0.8152,
      "step": 21140
    },
    {
      "epoch": 4.532790398628375,
      "grad_norm": 0.3850112855434418,
      "learning_rate": 1.39562794684955e-05,
      "loss": 0.6491,
      "step": 21150
    },
    {
      "epoch": 4.53493356193742,
      "grad_norm": 14.676021575927734,
      "learning_rate": 1.3953421917416774e-05,
      "loss": 0.805,
      "step": 21160
    },
    {
      "epoch": 4.537076725246464,
      "grad_norm": 0.41768696904182434,
      "learning_rate": 1.3950564366338048e-05,
      "loss": 0.631,
      "step": 21170
    },
    {
      "epoch": 4.5392198885555075,
      "grad_norm": 0.4275256097316742,
      "learning_rate": 1.3947706815259324e-05,
      "loss": 0.1663,
      "step": 21180
    },
    {
      "epoch": 4.541363051864552,
      "grad_norm": 0.39251434803009033,
      "learning_rate": 1.3944849264180597e-05,
      "loss": 0.7873,
      "step": 21190
    },
    {
      "epoch": 4.543506215173596,
      "grad_norm": 0.4652461111545563,
      "learning_rate": 1.3941991713101873e-05,
      "loss": 0.9419,
      "step": 21200
    },
    {
      "epoch": 4.54564937848264,
      "grad_norm": 29.52824592590332,
      "learning_rate": 1.3939134162023147e-05,
      "loss": 0.9175,
      "step": 21210
    },
    {
      "epoch": 4.547792541791685,
      "grad_norm": 0.5974828600883484,
      "learning_rate": 1.3936276610944421e-05,
      "loss": 0.7513,
      "step": 21220
    },
    {
      "epoch": 4.5499357051007285,
      "grad_norm": 14.593428611755371,
      "learning_rate": 1.3933419059865697e-05,
      "loss": 0.7403,
      "step": 21230
    },
    {
      "epoch": 4.552078868409772,
      "grad_norm": 14.482629776000977,
      "learning_rate": 1.393056150878697e-05,
      "loss": 0.4464,
      "step": 21240
    },
    {
      "epoch": 4.554222031718817,
      "grad_norm": 0.5645182132720947,
      "learning_rate": 1.3927703957708244e-05,
      "loss": 0.4481,
      "step": 21250
    },
    {
      "epoch": 4.556365195027861,
      "grad_norm": 14.858482360839844,
      "learning_rate": 1.392484640662952e-05,
      "loss": 0.6037,
      "step": 21260
    },
    {
      "epoch": 4.558508358336905,
      "grad_norm": 0.48447632789611816,
      "learning_rate": 1.3921988855550794e-05,
      "loss": 0.4587,
      "step": 21270
    },
    {
      "epoch": 4.56065152164595,
      "grad_norm": 0.45744848251342773,
      "learning_rate": 1.391913130447207e-05,
      "loss": 0.6199,
      "step": 21280
    },
    {
      "epoch": 4.562794684954993,
      "grad_norm": 0.41810792684555054,
      "learning_rate": 1.3916273753393344e-05,
      "loss": 0.3177,
      "step": 21290
    },
    {
      "epoch": 4.564937848264037,
      "grad_norm": 14.81281852722168,
      "learning_rate": 1.3913416202314617e-05,
      "loss": 0.793,
      "step": 21300
    },
    {
      "epoch": 4.567081011573082,
      "grad_norm": 0.42267364263534546,
      "learning_rate": 1.3910558651235893e-05,
      "loss": 0.4791,
      "step": 21310
    },
    {
      "epoch": 4.569224174882126,
      "grad_norm": 0.4139850437641144,
      "learning_rate": 1.3907701100157167e-05,
      "loss": 0.3226,
      "step": 21320
    },
    {
      "epoch": 4.57136733819117,
      "grad_norm": 14.894302368164062,
      "learning_rate": 1.3904843549078441e-05,
      "loss": 0.6473,
      "step": 21330
    },
    {
      "epoch": 4.573510501500214,
      "grad_norm": 0.43002718687057495,
      "learning_rate": 1.3901985997999717e-05,
      "loss": 0.4795,
      "step": 21340
    },
    {
      "epoch": 4.575653664809258,
      "grad_norm": 14.74273681640625,
      "learning_rate": 1.389912844692099e-05,
      "loss": 0.7849,
      "step": 21350
    },
    {
      "epoch": 4.577796828118302,
      "grad_norm": 0.4229859411716461,
      "learning_rate": 1.3896270895842266e-05,
      "loss": 0.3178,
      "step": 21360
    },
    {
      "epoch": 4.579939991427347,
      "grad_norm": 0.4346788227558136,
      "learning_rate": 1.3893413344763538e-05,
      "loss": 0.7859,
      "step": 21370
    },
    {
      "epoch": 4.582083154736391,
      "grad_norm": 14.88230037689209,
      "learning_rate": 1.3890555793684812e-05,
      "loss": 0.3234,
      "step": 21380
    },
    {
      "epoch": 4.584226318045435,
      "grad_norm": 0.4322882890701294,
      "learning_rate": 1.3887698242606086e-05,
      "loss": 0.7893,
      "step": 21390
    },
    {
      "epoch": 4.586369481354479,
      "grad_norm": 0.5359028577804565,
      "learning_rate": 1.3884840691527362e-05,
      "loss": 0.934,
      "step": 21400
    },
    {
      "epoch": 4.588512644663523,
      "grad_norm": 0.5829206109046936,
      "learning_rate": 1.3881983140448636e-05,
      "loss": 0.7458,
      "step": 21410
    },
    {
      "epoch": 4.590655807972568,
      "grad_norm": 14.400739669799805,
      "learning_rate": 1.3879125589369911e-05,
      "loss": 0.8685,
      "step": 21420
    },
    {
      "epoch": 4.592798971281612,
      "grad_norm": 0.7399455904960632,
      "learning_rate": 1.3876268038291185e-05,
      "loss": 0.4358,
      "step": 21430
    },
    {
      "epoch": 4.594942134590656,
      "grad_norm": 0.6877113580703735,
      "learning_rate": 1.387341048721246e-05,
      "loss": 0.8512,
      "step": 21440
    },
    {
      "epoch": 4.5970852978997,
      "grad_norm": 0.7661160826683044,
      "learning_rate": 1.3870552936133735e-05,
      "loss": 0.7129,
      "step": 21450
    },
    {
      "epoch": 4.599228461208744,
      "grad_norm": 0.6101872324943542,
      "learning_rate": 1.3867695385055009e-05,
      "loss": 0.294,
      "step": 21460
    },
    {
      "epoch": 4.601371624517788,
      "grad_norm": 0.48776039481163025,
      "learning_rate": 1.3864837833976283e-05,
      "loss": 0.4608,
      "step": 21470
    },
    {
      "epoch": 4.603514787826833,
      "grad_norm": 0.4734570384025574,
      "learning_rate": 1.3861980282897558e-05,
      "loss": 0.46,
      "step": 21480
    },
    {
      "epoch": 4.605657951135877,
      "grad_norm": 0.40900668501853943,
      "learning_rate": 1.3859122731818832e-05,
      "loss": 0.3249,
      "step": 21490
    },
    {
      "epoch": 4.6078011144449205,
      "grad_norm": 0.4034205675125122,
      "learning_rate": 1.3856265180740108e-05,
      "loss": 0.6428,
      "step": 21500
    },
    {
      "epoch": 4.609944277753965,
      "grad_norm": 15.003819465637207,
      "learning_rate": 1.3853407629661382e-05,
      "loss": 1.0956,
      "step": 21510
    },
    {
      "epoch": 4.612087441063009,
      "grad_norm": 0.515842080116272,
      "learning_rate": 1.3850550078582656e-05,
      "loss": 0.1594,
      "step": 21520
    },
    {
      "epoch": 4.614230604372053,
      "grad_norm": 14.58842945098877,
      "learning_rate": 1.3847692527503931e-05,
      "loss": 0.4604,
      "step": 21530
    },
    {
      "epoch": 4.616373767681098,
      "grad_norm": 14.742701530456543,
      "learning_rate": 1.3844834976425205e-05,
      "loss": 0.6303,
      "step": 21540
    },
    {
      "epoch": 4.6185169309901415,
      "grad_norm": 14.827692031860352,
      "learning_rate": 1.384197742534648e-05,
      "loss": 0.3236,
      "step": 21550
    },
    {
      "epoch": 4.620660094299185,
      "grad_norm": 0.4444692134857178,
      "learning_rate": 1.3839119874267755e-05,
      "loss": 0.4754,
      "step": 21560
    },
    {
      "epoch": 4.62280325760823,
      "grad_norm": 14.680883407592773,
      "learning_rate": 1.3836262323189029e-05,
      "loss": 0.4771,
      "step": 21570
    },
    {
      "epoch": 4.624946420917274,
      "grad_norm": 0.44282543659210205,
      "learning_rate": 1.3833404772110301e-05,
      "loss": 0.6346,
      "step": 21580
    },
    {
      "epoch": 4.627089584226318,
      "grad_norm": 0.4091397523880005,
      "learning_rate": 1.3830547221031577e-05,
      "loss": 0.4767,
      "step": 21590
    },
    {
      "epoch": 4.6292327475353625,
      "grad_norm": 0.3908812403678894,
      "learning_rate": 1.382768966995285e-05,
      "loss": 0.3268,
      "step": 21600
    },
    {
      "epoch": 4.631375910844406,
      "grad_norm": 14.806131362915039,
      "learning_rate": 1.3824832118874124e-05,
      "loss": 0.3325,
      "step": 21610
    },
    {
      "epoch": 4.63351907415345,
      "grad_norm": 0.36564651131629944,
      "learning_rate": 1.38219745677954e-05,
      "loss": 0.8122,
      "step": 21620
    },
    {
      "epoch": 4.635662237462495,
      "grad_norm": 0.3695293962955475,
      "learning_rate": 1.3819117016716674e-05,
      "loss": 0.1681,
      "step": 21630
    },
    {
      "epoch": 4.637805400771539,
      "grad_norm": 0.38009020686149597,
      "learning_rate": 1.381625946563795e-05,
      "loss": 0.8079,
      "step": 21640
    },
    {
      "epoch": 4.639948564080583,
      "grad_norm": 14.831535339355469,
      "learning_rate": 1.3813401914559224e-05,
      "loss": 0.8027,
      "step": 21650
    },
    {
      "epoch": 4.642091727389627,
      "grad_norm": 0.4318925738334656,
      "learning_rate": 1.3810544363480497e-05,
      "loss": 0.9443,
      "step": 21660
    },
    {
      "epoch": 4.644234890698671,
      "grad_norm": 0.5571507215499878,
      "learning_rate": 1.3807686812401773e-05,
      "loss": 0.9041,
      "step": 21670
    },
    {
      "epoch": 4.646378054007715,
      "grad_norm": 0.6163012385368347,
      "learning_rate": 1.3804829261323047e-05,
      "loss": 0.8826,
      "step": 21680
    },
    {
      "epoch": 4.64852121731676,
      "grad_norm": 14.858848571777344,
      "learning_rate": 1.3801971710244321e-05,
      "loss": 0.8679,
      "step": 21690
    },
    {
      "epoch": 4.650664380625804,
      "grad_norm": 14.512333869934082,
      "learning_rate": 1.3799114159165597e-05,
      "loss": 0.298,
      "step": 21700
    },
    {
      "epoch": 4.6528075439348475,
      "grad_norm": 0.5735898017883301,
      "learning_rate": 1.379625660808687e-05,
      "loss": 0.4441,
      "step": 21710
    },
    {
      "epoch": 4.654950707243892,
      "grad_norm": 0.5662176012992859,
      "learning_rate": 1.3793399057008146e-05,
      "loss": 0.596,
      "step": 21720
    },
    {
      "epoch": 4.657093870552936,
      "grad_norm": 14.584769248962402,
      "learning_rate": 1.379054150592942e-05,
      "loss": 0.7549,
      "step": 21730
    },
    {
      "epoch": 4.65923703386198,
      "grad_norm": 0.5091883540153503,
      "learning_rate": 1.3787683954850694e-05,
      "loss": 0.3138,
      "step": 21740
    },
    {
      "epoch": 4.661380197171025,
      "grad_norm": 14.676798820495605,
      "learning_rate": 1.378482640377197e-05,
      "loss": 0.4738,
      "step": 21750
    },
    {
      "epoch": 4.6635233604800685,
      "grad_norm": 14.878767013549805,
      "learning_rate": 1.3781968852693244e-05,
      "loss": 1.2538,
      "step": 21760
    },
    {
      "epoch": 4.665666523789112,
      "grad_norm": 0.5470254421234131,
      "learning_rate": 1.377911130161452e-05,
      "loss": 1.0538,
      "step": 21770
    },
    {
      "epoch": 4.667809687098157,
      "grad_norm": 0.6286535859107971,
      "learning_rate": 1.3776253750535793e-05,
      "loss": 0.446,
      "step": 21780
    },
    {
      "epoch": 4.669952850407201,
      "grad_norm": 14.46081256866455,
      "learning_rate": 1.3773396199457067e-05,
      "loss": 0.8623,
      "step": 21790
    },
    {
      "epoch": 4.672096013716245,
      "grad_norm": 0.6391428112983704,
      "learning_rate": 1.377053864837834e-05,
      "loss": 0.152,
      "step": 21800
    },
    {
      "epoch": 4.67423917702529,
      "grad_norm": 0.5482365489006042,
      "learning_rate": 1.3767681097299615e-05,
      "loss": 0.3044,
      "step": 21810
    },
    {
      "epoch": 4.676382340334333,
      "grad_norm": 0.5774854421615601,
      "learning_rate": 1.3764823546220889e-05,
      "loss": 1.0386,
      "step": 21820
    },
    {
      "epoch": 4.678525503643377,
      "grad_norm": 0.5903295278549194,
      "learning_rate": 1.3761965995142163e-05,
      "loss": 0.4472,
      "step": 21830
    },
    {
      "epoch": 4.680668666952422,
      "grad_norm": 0.5866857171058655,
      "learning_rate": 1.3759108444063438e-05,
      "loss": 0.7436,
      "step": 21840
    },
    {
      "epoch": 4.682811830261466,
      "grad_norm": 0.5711619257926941,
      "learning_rate": 1.3756250892984712e-05,
      "loss": 0.3063,
      "step": 21850
    },
    {
      "epoch": 4.68495499357051,
      "grad_norm": 0.5456575155258179,
      "learning_rate": 1.3753393341905988e-05,
      "loss": 0.455,
      "step": 21860
    },
    {
      "epoch": 4.6870981568795544,
      "grad_norm": 0.4345003366470337,
      "learning_rate": 1.3750535790827262e-05,
      "loss": 0.1597,
      "step": 21870
    },
    {
      "epoch": 4.689241320188598,
      "grad_norm": 0.45856407284736633,
      "learning_rate": 1.3747678239748536e-05,
      "loss": 1.1068,
      "step": 21880
    },
    {
      "epoch": 4.691384483497642,
      "grad_norm": 0.4569160044193268,
      "learning_rate": 1.3744820688669811e-05,
      "loss": 0.6233,
      "step": 21890
    },
    {
      "epoch": 4.693527646806687,
      "grad_norm": 14.767088890075684,
      "learning_rate": 1.3741963137591085e-05,
      "loss": 0.4726,
      "step": 21900
    },
    {
      "epoch": 4.695670810115731,
      "grad_norm": 29.621774673461914,
      "learning_rate": 1.3739105586512361e-05,
      "loss": 0.7768,
      "step": 21910
    },
    {
      "epoch": 4.697813973424775,
      "grad_norm": 0.3858206272125244,
      "learning_rate": 1.3736248035433635e-05,
      "loss": 0.0094,
      "step": 21920
    },
    {
      "epoch": 4.699957136733819,
      "grad_norm": 0.33301037549972534,
      "learning_rate": 1.3733390484354909e-05,
      "loss": 0.3311,
      "step": 21930
    },
    {
      "epoch": 4.702100300042863,
      "grad_norm": 14.766194343566895,
      "learning_rate": 1.3730532933276184e-05,
      "loss": 1.1434,
      "step": 21940
    },
    {
      "epoch": 4.704243463351908,
      "grad_norm": 0.3997631072998047,
      "learning_rate": 1.3727675382197458e-05,
      "loss": 0.6473,
      "step": 21950
    },
    {
      "epoch": 4.706386626660952,
      "grad_norm": 0.4383276104927063,
      "learning_rate": 1.3724817831118732e-05,
      "loss": 0.7882,
      "step": 21960
    },
    {
      "epoch": 4.708529789969996,
      "grad_norm": 0.4590374231338501,
      "learning_rate": 1.3721960280040008e-05,
      "loss": 0.4707,
      "step": 21970
    },
    {
      "epoch": 4.71067295327904,
      "grad_norm": 0.46994224190711975,
      "learning_rate": 1.3719102728961282e-05,
      "loss": 0.6222,
      "step": 21980
    },
    {
      "epoch": 4.712816116588084,
      "grad_norm": 0.45148947834968567,
      "learning_rate": 1.3716245177882557e-05,
      "loss": 0.0103,
      "step": 21990
    },
    {
      "epoch": 4.714959279897128,
      "grad_norm": 0.418224573135376,
      "learning_rate": 1.3713387626803831e-05,
      "loss": 0.7898,
      "step": 22000
    },
    {
      "epoch": 4.717102443206173,
      "grad_norm": 0.4126543402671814,
      "learning_rate": 1.3710530075725104e-05,
      "loss": 0.4727,
      "step": 22010
    },
    {
      "epoch": 4.719245606515217,
      "grad_norm": 0.42963096499443054,
      "learning_rate": 1.3707672524646378e-05,
      "loss": 0.477,
      "step": 22020
    },
    {
      "epoch": 4.7213887698242605,
      "grad_norm": 29.685895919799805,
      "learning_rate": 1.3704814973567653e-05,
      "loss": 1.094,
      "step": 22030
    },
    {
      "epoch": 4.723531933133305,
      "grad_norm": 14.633427619934082,
      "learning_rate": 1.3701957422488927e-05,
      "loss": 0.4722,
      "step": 22040
    },
    {
      "epoch": 4.725675096442349,
      "grad_norm": 0.45111122727394104,
      "learning_rate": 1.3699099871410203e-05,
      "loss": 0.3164,
      "step": 22050
    },
    {
      "epoch": 4.727818259751393,
      "grad_norm": 0.4174051582813263,
      "learning_rate": 1.3696242320331477e-05,
      "loss": 0.3242,
      "step": 22060
    },
    {
      "epoch": 4.729961423060438,
      "grad_norm": 0.3515998423099518,
      "learning_rate": 1.369338476925275e-05,
      "loss": 0.1696,
      "step": 22070
    },
    {
      "epoch": 4.7321045863694815,
      "grad_norm": 0.34898194670677185,
      "learning_rate": 1.3690527218174026e-05,
      "loss": 0.9962,
      "step": 22080
    },
    {
      "epoch": 4.734247749678525,
      "grad_norm": 14.717697143554688,
      "learning_rate": 1.36876696670953e-05,
      "loss": 0.493,
      "step": 22090
    },
    {
      "epoch": 4.73639091298757,
      "grad_norm": 0.404527872800827,
      "learning_rate": 1.3684812116016574e-05,
      "loss": 0.6445,
      "step": 22100
    },
    {
      "epoch": 4.738534076296614,
      "grad_norm": 29.739307403564453,
      "learning_rate": 1.368195456493785e-05,
      "loss": 0.4816,
      "step": 22110
    },
    {
      "epoch": 4.740677239605658,
      "grad_norm": 0.3886106014251709,
      "learning_rate": 1.3679097013859124e-05,
      "loss": 0.6579,
      "step": 22120
    },
    {
      "epoch": 4.7428204029147025,
      "grad_norm": 14.693243026733398,
      "learning_rate": 1.36762394627804e-05,
      "loss": 0.6468,
      "step": 22130
    },
    {
      "epoch": 4.744963566223746,
      "grad_norm": 14.703153610229492,
      "learning_rate": 1.3673381911701673e-05,
      "loss": 1.1081,
      "step": 22140
    },
    {
      "epoch": 4.74710672953279,
      "grad_norm": 14.592109680175781,
      "learning_rate": 1.3670524360622947e-05,
      "loss": 0.7627,
      "step": 22150
    },
    {
      "epoch": 4.749249892841835,
      "grad_norm": 0.6458560228347778,
      "learning_rate": 1.3667666809544223e-05,
      "loss": 1.1691,
      "step": 22160
    },
    {
      "epoch": 4.751393056150879,
      "grad_norm": 14.43796157836914,
      "learning_rate": 1.3664809258465497e-05,
      "loss": 0.4368,
      "step": 22170
    },
    {
      "epoch": 4.753536219459923,
      "grad_norm": 29.420955657958984,
      "learning_rate": 1.366195170738677e-05,
      "loss": 0.303,
      "step": 22180
    },
    {
      "epoch": 4.755679382768967,
      "grad_norm": 0.5371605753898621,
      "learning_rate": 1.3659094156308046e-05,
      "loss": 0.4524,
      "step": 22190
    },
    {
      "epoch": 4.757822546078011,
      "grad_norm": 14.557098388671875,
      "learning_rate": 1.365623660522932e-05,
      "loss": 0.8978,
      "step": 22200
    },
    {
      "epoch": 4.759965709387055,
      "grad_norm": 0.498154878616333,
      "learning_rate": 1.3653379054150596e-05,
      "loss": 0.4575,
      "step": 22210
    },
    {
      "epoch": 4.7621088726961,
      "grad_norm": 0.48161444067955017,
      "learning_rate": 1.365052150307187e-05,
      "loss": 0.464,
      "step": 22220
    },
    {
      "epoch": 4.764252036005144,
      "grad_norm": 0.4537624418735504,
      "learning_rate": 1.3647663951993142e-05,
      "loss": 0.317,
      "step": 22230
    },
    {
      "epoch": 4.7663951993141875,
      "grad_norm": 0.45433056354522705,
      "learning_rate": 1.3644806400914416e-05,
      "loss": 0.6288,
      "step": 22240
    },
    {
      "epoch": 4.768538362623232,
      "grad_norm": 14.594280242919922,
      "learning_rate": 1.3641948849835691e-05,
      "loss": 1.0775,
      "step": 22250
    },
    {
      "epoch": 4.770681525932276,
      "grad_norm": 0.5019843578338623,
      "learning_rate": 1.3639091298756965e-05,
      "loss": 0.4605,
      "step": 22260
    },
    {
      "epoch": 4.77282468924132,
      "grad_norm": 0.5197471976280212,
      "learning_rate": 1.3636233747678241e-05,
      "loss": 0.4517,
      "step": 22270
    },
    {
      "epoch": 4.774967852550365,
      "grad_norm": 29.601993560791016,
      "learning_rate": 1.3633376196599515e-05,
      "loss": 0.9275,
      "step": 22280
    },
    {
      "epoch": 4.777111015859409,
      "grad_norm": 0.49919891357421875,
      "learning_rate": 1.3630518645520789e-05,
      "loss": 0.9211,
      "step": 22290
    },
    {
      "epoch": 4.779254179168452,
      "grad_norm": 0.512619137763977,
      "learning_rate": 1.3627661094442064e-05,
      "loss": 0.31,
      "step": 22300
    },
    {
      "epoch": 4.781397342477497,
      "grad_norm": 0.4766049087047577,
      "learning_rate": 1.3624803543363338e-05,
      "loss": 0.6142,
      "step": 22310
    },
    {
      "epoch": 4.783540505786541,
      "grad_norm": 0.49465540051460266,
      "learning_rate": 1.3621945992284612e-05,
      "loss": 0.7647,
      "step": 22320
    },
    {
      "epoch": 4.785683669095585,
      "grad_norm": 14.706418991088867,
      "learning_rate": 1.3619088441205888e-05,
      "loss": 0.3113,
      "step": 22330
    },
    {
      "epoch": 4.78782683240463,
      "grad_norm": 0.5083391666412354,
      "learning_rate": 1.3616230890127162e-05,
      "loss": 0.7654,
      "step": 22340
    },
    {
      "epoch": 4.789969995713673,
      "grad_norm": 0.5414832830429077,
      "learning_rate": 1.3613373339048438e-05,
      "loss": 0.9041,
      "step": 22350
    },
    {
      "epoch": 4.792113159022717,
      "grad_norm": 0.602968156337738,
      "learning_rate": 1.3610515787969711e-05,
      "loss": 1.0355,
      "step": 22360
    },
    {
      "epoch": 4.794256322331762,
      "grad_norm": 0.6377583742141724,
      "learning_rate": 1.3607658236890985e-05,
      "loss": 0.4406,
      "step": 22370
    },
    {
      "epoch": 4.796399485640806,
      "grad_norm": 0.5839810967445374,
      "learning_rate": 1.3604800685812261e-05,
      "loss": 0.2988,
      "step": 22380
    },
    {
      "epoch": 4.79854264894985,
      "grad_norm": 0.5578656196594238,
      "learning_rate": 1.3601943134733535e-05,
      "loss": 0.8867,
      "step": 22390
    },
    {
      "epoch": 4.8006858122588945,
      "grad_norm": 0.5241156220436096,
      "learning_rate": 1.359908558365481e-05,
      "loss": 0.3033,
      "step": 22400
    },
    {
      "epoch": 4.802828975567938,
      "grad_norm": 14.655515670776367,
      "learning_rate": 1.3596228032576084e-05,
      "loss": 1.0775,
      "step": 22410
    },
    {
      "epoch": 4.804972138876982,
      "grad_norm": 0.46774375438690186,
      "learning_rate": 1.3593370481497358e-05,
      "loss": 0.3148,
      "step": 22420
    },
    {
      "epoch": 4.807115302186027,
      "grad_norm": 0.5542727708816528,
      "learning_rate": 1.3590512930418634e-05,
      "loss": 1.2099,
      "step": 22430
    },
    {
      "epoch": 4.809258465495071,
      "grad_norm": 0.585896909236908,
      "learning_rate": 1.3587655379339906e-05,
      "loss": 0.5992,
      "step": 22440
    },
    {
      "epoch": 4.811401628804115,
      "grad_norm": 0.5604231357574463,
      "learning_rate": 1.358479782826118e-05,
      "loss": 0.5955,
      "step": 22450
    },
    {
      "epoch": 4.813544792113159,
      "grad_norm": 14.846187591552734,
      "learning_rate": 1.3581940277182454e-05,
      "loss": 0.6015,
      "step": 22460
    },
    {
      "epoch": 4.815687955422203,
      "grad_norm": 0.5152804851531982,
      "learning_rate": 1.357908272610373e-05,
      "loss": 0.3071,
      "step": 22470
    },
    {
      "epoch": 4.817831118731247,
      "grad_norm": 0.45540979504585266,
      "learning_rate": 1.3576225175025004e-05,
      "loss": 0.4684,
      "step": 22480
    },
    {
      "epoch": 4.819974282040292,
      "grad_norm": 0.47423437237739563,
      "learning_rate": 1.357336762394628e-05,
      "loss": 0.9278,
      "step": 22490
    },
    {
      "epoch": 4.822117445349336,
      "grad_norm": 0.4623620808124542,
      "learning_rate": 1.3570510072867553e-05,
      "loss": 0.1625,
      "step": 22500
    },
    {
      "epoch": 4.8242606086583795,
      "grad_norm": 0.39763501286506653,
      "learning_rate": 1.3567652521788827e-05,
      "loss": 0.1616,
      "step": 22510
    },
    {
      "epoch": 4.826403771967424,
      "grad_norm": 0.34642553329467773,
      "learning_rate": 1.3564794970710103e-05,
      "loss": 0.328,
      "step": 22520
    },
    {
      "epoch": 4.828546935276468,
      "grad_norm": 0.3024985194206238,
      "learning_rate": 1.3561937419631377e-05,
      "loss": 0.0072,
      "step": 22530
    },
    {
      "epoch": 4.830690098585512,
      "grad_norm": 0.2884286046028137,
      "learning_rate": 1.3559079868552652e-05,
      "loss": 0.5104,
      "step": 22540
    },
    {
      "epoch": 4.832833261894557,
      "grad_norm": 14.79425048828125,
      "learning_rate": 1.3556222317473926e-05,
      "loss": 0.5136,
      "step": 22550
    },
    {
      "epoch": 4.8349764252036005,
      "grad_norm": 0.2952752709388733,
      "learning_rate": 1.35533647663952e-05,
      "loss": 0.5138,
      "step": 22560
    },
    {
      "epoch": 4.837119588512644,
      "grad_norm": 14.785361289978027,
      "learning_rate": 1.3550507215316476e-05,
      "loss": 0.8455,
      "step": 22570
    },
    {
      "epoch": 4.839262751821689,
      "grad_norm": 0.3315804600715637,
      "learning_rate": 1.354764966423775e-05,
      "loss": 0.3359,
      "step": 22580
    },
    {
      "epoch": 4.841405915130733,
      "grad_norm": 0.374737024307251,
      "learning_rate": 1.3544792113159024e-05,
      "loss": 0.4922,
      "step": 22590
    },
    {
      "epoch": 4.843549078439777,
      "grad_norm": 0.3810185194015503,
      "learning_rate": 1.35419345620803e-05,
      "loss": 0.3284,
      "step": 22600
    },
    {
      "epoch": 4.8456922417488215,
      "grad_norm": 0.3720361590385437,
      "learning_rate": 1.3539077011001573e-05,
      "loss": 0.4886,
      "step": 22610
    },
    {
      "epoch": 4.847835405057865,
      "grad_norm": 0.37537363171577454,
      "learning_rate": 1.3536219459922849e-05,
      "loss": 0.8106,
      "step": 22620
    },
    {
      "epoch": 4.849978568366909,
      "grad_norm": 14.723950386047363,
      "learning_rate": 1.3533361908844123e-05,
      "loss": 0.806,
      "step": 22630
    },
    {
      "epoch": 4.852121731675954,
      "grad_norm": 0.4238136112689972,
      "learning_rate": 1.3530504357765397e-05,
      "loss": 0.4815,
      "step": 22640
    },
    {
      "epoch": 4.854264894984998,
      "grad_norm": 0.43649181723594666,
      "learning_rate": 1.3527646806686672e-05,
      "loss": 0.4747,
      "step": 22650
    },
    {
      "epoch": 4.856408058294042,
      "grad_norm": 14.64789867401123,
      "learning_rate": 1.3524789255607945e-05,
      "loss": 0.7842,
      "step": 22660
    },
    {
      "epoch": 4.858551221603086,
      "grad_norm": 14.603254318237305,
      "learning_rate": 1.3521931704529218e-05,
      "loss": 1.0826,
      "step": 22670
    },
    {
      "epoch": 4.86069438491213,
      "grad_norm": 0.49594175815582275,
      "learning_rate": 1.3519074153450494e-05,
      "loss": 0.3096,
      "step": 22680
    },
    {
      "epoch": 4.862837548221174,
      "grad_norm": 0.49847009778022766,
      "learning_rate": 1.3516216602371768e-05,
      "loss": 0.4607,
      "step": 22690
    },
    {
      "epoch": 4.864980711530219,
      "grad_norm": 0.4505501687526703,
      "learning_rate": 1.3513359051293042e-05,
      "loss": 0.3149,
      "step": 22700
    },
    {
      "epoch": 4.867123874839263,
      "grad_norm": 14.644745826721191,
      "learning_rate": 1.3510501500214318e-05,
      "loss": 0.4735,
      "step": 22710
    },
    {
      "epoch": 4.8692670381483065,
      "grad_norm": 0.43542587757110596,
      "learning_rate": 1.3507643949135591e-05,
      "loss": 0.7868,
      "step": 22720
    },
    {
      "epoch": 4.871410201457351,
      "grad_norm": 14.617959022521973,
      "learning_rate": 1.3504786398056865e-05,
      "loss": 0.9327,
      "step": 22730
    },
    {
      "epoch": 4.873553364766395,
      "grad_norm": 0.5014359951019287,
      "learning_rate": 1.3501928846978141e-05,
      "loss": 0.7692,
      "step": 22740
    },
    {
      "epoch": 4.875696528075439,
      "grad_norm": 0.5123369693756104,
      "learning_rate": 1.3499071295899415e-05,
      "loss": 0.1603,
      "step": 22750
    },
    {
      "epoch": 4.877839691384484,
      "grad_norm": 0.4916992485523224,
      "learning_rate": 1.349621374482069e-05,
      "loss": 0.4609,
      "step": 22760
    },
    {
      "epoch": 4.8799828546935275,
      "grad_norm": 14.58360481262207,
      "learning_rate": 1.3493356193741965e-05,
      "loss": 0.615,
      "step": 22770
    },
    {
      "epoch": 4.882126018002571,
      "grad_norm": 0.5169885754585266,
      "learning_rate": 1.3490498642663238e-05,
      "loss": 0.6135,
      "step": 22780
    },
    {
      "epoch": 4.884269181311616,
      "grad_norm": 14.547792434692383,
      "learning_rate": 1.3487641091584514e-05,
      "loss": 0.7525,
      "step": 22790
    },
    {
      "epoch": 4.88641234462066,
      "grad_norm": 0.588083028793335,
      "learning_rate": 1.3484783540505788e-05,
      "loss": 0.8922,
      "step": 22800
    },
    {
      "epoch": 4.888555507929704,
      "grad_norm": 0.5642709136009216,
      "learning_rate": 1.3481925989427062e-05,
      "loss": 0.1588,
      "step": 22810
    },
    {
      "epoch": 4.890698671238749,
      "grad_norm": 0.5278317928314209,
      "learning_rate": 1.3479068438348338e-05,
      "loss": 0.5983,
      "step": 22820
    },
    {
      "epoch": 4.892841834547792,
      "grad_norm": 14.609387397766113,
      "learning_rate": 1.3476210887269611e-05,
      "loss": 0.3085,
      "step": 22830
    },
    {
      "epoch": 4.894984997856836,
      "grad_norm": 0.48508337140083313,
      "learning_rate": 1.3473353336190887e-05,
      "loss": 0.6176,
      "step": 22840
    },
    {
      "epoch": 4.897128161165881,
      "grad_norm": 0.503563404083252,
      "learning_rate": 1.3470495785112161e-05,
      "loss": 1.0663,
      "step": 22850
    },
    {
      "epoch": 4.899271324474925,
      "grad_norm": 0.523704469203949,
      "learning_rate": 1.3467638234033435e-05,
      "loss": 0.6042,
      "step": 22860
    },
    {
      "epoch": 4.901414487783969,
      "grad_norm": 0.5342165231704712,
      "learning_rate": 1.3464780682954707e-05,
      "loss": 0.4561,
      "step": 22870
    },
    {
      "epoch": 4.9035576510930134,
      "grad_norm": 0.42985278367996216,
      "learning_rate": 1.3461923131875983e-05,
      "loss": 0.1587,
      "step": 22880
    },
    {
      "epoch": 4.905700814402057,
      "grad_norm": 15.23904037475586,
      "learning_rate": 1.3459065580797257e-05,
      "loss": 1.2627,
      "step": 22890
    },
    {
      "epoch": 4.907843977711101,
      "grad_norm": 0.45491984486579895,
      "learning_rate": 1.3456208029718532e-05,
      "loss": 0.3184,
      "step": 22900
    },
    {
      "epoch": 4.909987141020146,
      "grad_norm": 0.452385276556015,
      "learning_rate": 1.3453350478639806e-05,
      "loss": 0.469,
      "step": 22910
    },
    {
      "epoch": 4.91213030432919,
      "grad_norm": 14.6561918258667,
      "learning_rate": 1.345049292756108e-05,
      "loss": 0.7756,
      "step": 22920
    },
    {
      "epoch": 4.914273467638234,
      "grad_norm": 0.4590841829776764,
      "learning_rate": 1.3447635376482356e-05,
      "loss": 0.4721,
      "step": 22930
    },
    {
      "epoch": 4.916416630947278,
      "grad_norm": 14.821504592895508,
      "learning_rate": 1.344477782540363e-05,
      "loss": 0.4774,
      "step": 22940
    },
    {
      "epoch": 4.918559794256322,
      "grad_norm": 29.875638961791992,
      "learning_rate": 1.3441920274324904e-05,
      "loss": 1.1159,
      "step": 22950
    },
    {
      "epoch": 4.920702957565366,
      "grad_norm": 14.680060386657715,
      "learning_rate": 1.343906272324618e-05,
      "loss": 0.4884,
      "step": 22960
    },
    {
      "epoch": 4.922846120874411,
      "grad_norm": 0.38474321365356445,
      "learning_rate": 1.3436205172167453e-05,
      "loss": 0.488,
      "step": 22970
    },
    {
      "epoch": 4.924989284183455,
      "grad_norm": 0.4003770351409912,
      "learning_rate": 1.3433347621088729e-05,
      "loss": 0.6432,
      "step": 22980
    },
    {
      "epoch": 4.9271324474924985,
      "grad_norm": 0.43846580386161804,
      "learning_rate": 1.3430490070010003e-05,
      "loss": 0.6331,
      "step": 22990
    },
    {
      "epoch": 4.929275610801543,
      "grad_norm": 14.649428367614746,
      "learning_rate": 1.3427632518931277e-05,
      "loss": 0.3178,
      "step": 23000
    },
    {
      "epoch": 4.931418774110587,
      "grad_norm": 0.47268393635749817,
      "learning_rate": 1.3424774967852552e-05,
      "loss": 0.9334,
      "step": 23010
    },
    {
      "epoch": 4.933561937419631,
      "grad_norm": 0.47988614439964294,
      "learning_rate": 1.3421917416773826e-05,
      "loss": 0.4648,
      "step": 23020
    },
    {
      "epoch": 4.935705100728676,
      "grad_norm": 0.5016595125198364,
      "learning_rate": 1.34190598656951e-05,
      "loss": 0.7619,
      "step": 23030
    },
    {
      "epoch": 4.9378482640377195,
      "grad_norm": 14.599547386169434,
      "learning_rate": 1.3416202314616376e-05,
      "loss": 0.4618,
      "step": 23040
    },
    {
      "epoch": 4.939991427346763,
      "grad_norm": 14.650653839111328,
      "learning_rate": 1.341334476353765e-05,
      "loss": 0.6137,
      "step": 23050
    },
    {
      "epoch": 4.942134590655808,
      "grad_norm": 0.5013073682785034,
      "learning_rate": 1.3410487212458925e-05,
      "loss": 0.6121,
      "step": 23060
    },
    {
      "epoch": 4.944277753964852,
      "grad_norm": 14.629374504089355,
      "learning_rate": 1.34076296613802e-05,
      "loss": 1.2156,
      "step": 23070
    },
    {
      "epoch": 4.946420917273897,
      "grad_norm": 0.4834893047809601,
      "learning_rate": 1.3404772110301472e-05,
      "loss": 0.6141,
      "step": 23080
    },
    {
      "epoch": 4.9485640805829405,
      "grad_norm": 14.733207702636719,
      "learning_rate": 1.3401914559222745e-05,
      "loss": 0.7658,
      "step": 23090
    },
    {
      "epoch": 4.950707243891984,
      "grad_norm": 14.664360046386719,
      "learning_rate": 1.3399057008144021e-05,
      "loss": 0.4686,
      "step": 23100
    },
    {
      "epoch": 4.952850407201029,
      "grad_norm": 14.590140342712402,
      "learning_rate": 1.3396199457065295e-05,
      "loss": 0.9166,
      "step": 23110
    },
    {
      "epoch": 4.954993570510073,
      "grad_norm": 14.558469772338867,
      "learning_rate": 1.339334190598657e-05,
      "loss": 0.611,
      "step": 23120
    },
    {
      "epoch": 4.957136733819117,
      "grad_norm": 0.5246084332466125,
      "learning_rate": 1.3390484354907845e-05,
      "loss": 0.7553,
      "step": 23130
    },
    {
      "epoch": 4.9592798971281615,
      "grad_norm": 0.596609354019165,
      "learning_rate": 1.3387626803829118e-05,
      "loss": 1.034,
      "step": 23140
    },
    {
      "epoch": 4.961423060437205,
      "grad_norm": 14.484697341918945,
      "learning_rate": 1.3384769252750394e-05,
      "loss": 0.4405,
      "step": 23150
    },
    {
      "epoch": 4.963566223746249,
      "grad_norm": 0.6280112862586975,
      "learning_rate": 1.3381911701671668e-05,
      "loss": 0.4429,
      "step": 23160
    },
    {
      "epoch": 4.965709387055294,
      "grad_norm": 0.6551305651664734,
      "learning_rate": 1.3379054150592942e-05,
      "loss": 1.0106,
      "step": 23170
    },
    {
      "epoch": 4.967852550364338,
      "grad_norm": 14.919217109680176,
      "learning_rate": 1.3376196599514218e-05,
      "loss": 0.4395,
      "step": 23180
    },
    {
      "epoch": 4.969995713673382,
      "grad_norm": 0.5776498913764954,
      "learning_rate": 1.3373339048435492e-05,
      "loss": 0.4413,
      "step": 23190
    },
    {
      "epoch": 4.972138876982426,
      "grad_norm": 0.5688708424568176,
      "learning_rate": 1.3370481497356767e-05,
      "loss": 0.8902,
      "step": 23200
    },
    {
      "epoch": 4.97428204029147,
      "grad_norm": 0.6422631144523621,
      "learning_rate": 1.3367623946278041e-05,
      "loss": 0.5922,
      "step": 23210
    },
    {
      "epoch": 4.976425203600514,
      "grad_norm": 14.45261001586914,
      "learning_rate": 1.3364766395199315e-05,
      "loss": 0.8733,
      "step": 23220
    },
    {
      "epoch": 4.978568366909559,
      "grad_norm": 0.6516637206077576,
      "learning_rate": 1.336190884412059e-05,
      "loss": 0.5786,
      "step": 23230
    },
    {
      "epoch": 4.980711530218603,
      "grad_norm": 0.6804888248443604,
      "learning_rate": 1.3359051293041865e-05,
      "loss": 0.5796,
      "step": 23240
    },
    {
      "epoch": 4.9828546935276465,
      "grad_norm": 14.677934646606445,
      "learning_rate": 1.335619374196314e-05,
      "loss": 0.438,
      "step": 23250
    },
    {
      "epoch": 4.984997856836691,
      "grad_norm": 0.590282678604126,
      "learning_rate": 1.3353336190884414e-05,
      "loss": 0.444,
      "step": 23260
    },
    {
      "epoch": 4.987141020145735,
      "grad_norm": 0.5612471103668213,
      "learning_rate": 1.3350478639805688e-05,
      "loss": 0.3026,
      "step": 23270
    },
    {
      "epoch": 4.989284183454779,
      "grad_norm": 0.5097485780715942,
      "learning_rate": 1.3347621088726964e-05,
      "loss": 1.208,
      "step": 23280
    },
    {
      "epoch": 4.991427346763824,
      "grad_norm": 0.5556550025939941,
      "learning_rate": 1.3344763537648238e-05,
      "loss": 0.8942,
      "step": 23290
    },
    {
      "epoch": 4.993570510072868,
      "grad_norm": 0.4917646646499634,
      "learning_rate": 1.334190598656951e-05,
      "loss": 0.0119,
      "step": 23300
    },
    {
      "epoch": 4.995713673381911,
      "grad_norm": 0.3974018096923828,
      "learning_rate": 1.3339048435490784e-05,
      "loss": 0.3143,
      "step": 23310
    },
    {
      "epoch": 4.997856836690956,
      "grad_norm": 0.3502161204814911,
      "learning_rate": 1.333619088441206e-05,
      "loss": 0.3275,
      "step": 23320
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.3484623432159424,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.4983,
      "step": 23330
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.8736666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.6240381598472595,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 704.392,
      "eval_samples_per_second": 4.259,
      "eval_steps_per_second": 1.42,
      "step": 23330
    },
    {
      "epoch": 5.002143163309044,
      "grad_norm": 0.3424499034881592,
      "learning_rate": 1.3330475782254609e-05,
      "loss": 0.9848,
      "step": 23340
    },
    {
      "epoch": 5.004286326618089,
      "grad_norm": 0.37188416719436646,
      "learning_rate": 1.3327618231175883e-05,
      "loss": 0.0081,
      "step": 23350
    },
    {
      "epoch": 5.006429489927132,
      "grad_norm": 0.3341622054576874,
      "learning_rate": 1.3324760680097157e-05,
      "loss": 0.4963,
      "step": 23360
    },
    {
      "epoch": 5.008572653236176,
      "grad_norm": 14.780766487121582,
      "learning_rate": 1.3321903129018432e-05,
      "loss": 0.8238,
      "step": 23370
    },
    {
      "epoch": 5.010715816545221,
      "grad_norm": 0.35273265838623047,
      "learning_rate": 1.3319045577939706e-05,
      "loss": 0.3317,
      "step": 23380
    },
    {
      "epoch": 5.012858979854265,
      "grad_norm": 14.760529518127441,
      "learning_rate": 1.3316188026860982e-05,
      "loss": 0.3321,
      "step": 23390
    },
    {
      "epoch": 5.015002143163309,
      "grad_norm": 14.7455472946167,
      "learning_rate": 1.3313330475782256e-05,
      "loss": 0.6649,
      "step": 23400
    },
    {
      "epoch": 5.0171453064723535,
      "grad_norm": 14.75936508178711,
      "learning_rate": 1.331047292470353e-05,
      "loss": 1.1502,
      "step": 23410
    },
    {
      "epoch": 5.019288469781397,
      "grad_norm": 0.379474014043808,
      "learning_rate": 1.3307615373624805e-05,
      "loss": 0.3305,
      "step": 23420
    },
    {
      "epoch": 5.021431633090441,
      "grad_norm": 14.707674980163574,
      "learning_rate": 1.330475782254608e-05,
      "loss": 1.2758,
      "step": 23430
    },
    {
      "epoch": 5.023574796399486,
      "grad_norm": 14.821802139282227,
      "learning_rate": 1.3301900271467353e-05,
      "loss": 0.4763,
      "step": 23440
    },
    {
      "epoch": 5.02571795970853,
      "grad_norm": 0.4418356716632843,
      "learning_rate": 1.3299042720388629e-05,
      "loss": 0.3171,
      "step": 23450
    },
    {
      "epoch": 5.027861123017574,
      "grad_norm": 0.4321790337562561,
      "learning_rate": 1.3296185169309903e-05,
      "loss": 0.4708,
      "step": 23460
    },
    {
      "epoch": 5.030004286326618,
      "grad_norm": 0.4294615685939789,
      "learning_rate": 1.3293327618231178e-05,
      "loss": 1.2497,
      "step": 23470
    },
    {
      "epoch": 5.032147449635662,
      "grad_norm": 0.5111870765686035,
      "learning_rate": 1.3290470067152452e-05,
      "loss": 0.3116,
      "step": 23480
    },
    {
      "epoch": 5.034290612944706,
      "grad_norm": 14.60838508605957,
      "learning_rate": 1.3287612516073726e-05,
      "loss": 0.4604,
      "step": 23490
    },
    {
      "epoch": 5.036433776253751,
      "grad_norm": 0.4653139114379883,
      "learning_rate": 1.3284754964995002e-05,
      "loss": 0.3134,
      "step": 23500
    },
    {
      "epoch": 5.038576939562795,
      "grad_norm": 0.44931018352508545,
      "learning_rate": 1.3281897413916274e-05,
      "loss": 0.627,
      "step": 23510
    },
    {
      "epoch": 5.0407201028718385,
      "grad_norm": 0.4469554126262665,
      "learning_rate": 1.3279039862837548e-05,
      "loss": 0.4705,
      "step": 23520
    },
    {
      "epoch": 5.042863266180883,
      "grad_norm": 0.44977620244026184,
      "learning_rate": 1.3276182311758824e-05,
      "loss": 0.6185,
      "step": 23530
    },
    {
      "epoch": 5.045006429489927,
      "grad_norm": 0.46671542525291443,
      "learning_rate": 1.3273324760680098e-05,
      "loss": 1.0742,
      "step": 23540
    },
    {
      "epoch": 5.047149592798971,
      "grad_norm": 14.849855422973633,
      "learning_rate": 1.3270467209601372e-05,
      "loss": 0.7583,
      "step": 23550
    },
    {
      "epoch": 5.049292756108016,
      "grad_norm": 0.5724404454231262,
      "learning_rate": 1.3267609658522647e-05,
      "loss": 0.4556,
      "step": 23560
    },
    {
      "epoch": 5.0514359194170595,
      "grad_norm": 14.822874069213867,
      "learning_rate": 1.3264752107443921e-05,
      "loss": 0.7482,
      "step": 23570
    },
    {
      "epoch": 5.053579082726103,
      "grad_norm": 14.43265438079834,
      "learning_rate": 1.3261894556365195e-05,
      "loss": 0.8747,
      "step": 23580
    },
    {
      "epoch": 5.055722246035148,
      "grad_norm": 14.800811767578125,
      "learning_rate": 1.325903700528647e-05,
      "loss": 0.1578,
      "step": 23590
    },
    {
      "epoch": 5.057865409344192,
      "grad_norm": 0.5296086072921753,
      "learning_rate": 1.3256179454207745e-05,
      "loss": 0.4551,
      "step": 23600
    },
    {
      "epoch": 5.060008572653236,
      "grad_norm": 15.233514785766602,
      "learning_rate": 1.325332190312902e-05,
      "loss": 0.6072,
      "step": 23610
    },
    {
      "epoch": 5.0621517359622805,
      "grad_norm": 0.5361573696136475,
      "learning_rate": 1.3250464352050294e-05,
      "loss": 0.7624,
      "step": 23620
    },
    {
      "epoch": 5.064294899271324,
      "grad_norm": 0.5292485356330872,
      "learning_rate": 1.3247606800971568e-05,
      "loss": 0.455,
      "step": 23630
    },
    {
      "epoch": 5.066438062580368,
      "grad_norm": 14.622679710388184,
      "learning_rate": 1.3244749249892844e-05,
      "loss": 0.4587,
      "step": 23640
    },
    {
      "epoch": 5.068581225889413,
      "grad_norm": 0.4863525331020355,
      "learning_rate": 1.3241891698814118e-05,
      "loss": 0.7666,
      "step": 23650
    },
    {
      "epoch": 5.070724389198457,
      "grad_norm": 14.657611846923828,
      "learning_rate": 1.3239034147735392e-05,
      "loss": 0.7644,
      "step": 23660
    },
    {
      "epoch": 5.072867552507501,
      "grad_norm": 0.5401192307472229,
      "learning_rate": 1.3236176596656667e-05,
      "loss": 0.4602,
      "step": 23670
    },
    {
      "epoch": 5.075010715816545,
      "grad_norm": 0.4969896674156189,
      "learning_rate": 1.3233319045577941e-05,
      "loss": 0.4593,
      "step": 23680
    },
    {
      "epoch": 5.077153879125589,
      "grad_norm": 29.612348556518555,
      "learning_rate": 1.3230461494499217e-05,
      "loss": 0.7589,
      "step": 23690
    },
    {
      "epoch": 5.079297042434633,
      "grad_norm": 0.4588867425918579,
      "learning_rate": 1.322760394342049e-05,
      "loss": 0.6182,
      "step": 23700
    },
    {
      "epoch": 5.081440205743678,
      "grad_norm": 14.599496841430664,
      "learning_rate": 1.3224746392341765e-05,
      "loss": 0.7653,
      "step": 23710
    },
    {
      "epoch": 5.083583369052722,
      "grad_norm": 14.551677703857422,
      "learning_rate": 1.322188884126304e-05,
      "loss": 0.9077,
      "step": 23720
    },
    {
      "epoch": 5.085726532361766,
      "grad_norm": 0.5286470651626587,
      "learning_rate": 1.3219031290184312e-05,
      "loss": 0.1585,
      "step": 23730
    },
    {
      "epoch": 5.08786969567081,
      "grad_norm": 0.48144420981407166,
      "learning_rate": 1.3216173739105586e-05,
      "loss": 0.3154,
      "step": 23740
    },
    {
      "epoch": 5.090012858979854,
      "grad_norm": 14.635260581970215,
      "learning_rate": 1.3213316188026862e-05,
      "loss": 0.6206,
      "step": 23750
    },
    {
      "epoch": 5.092156022288899,
      "grad_norm": 0.4596666395664215,
      "learning_rate": 1.3210458636948136e-05,
      "loss": 0.6268,
      "step": 23760
    },
    {
      "epoch": 5.094299185597943,
      "grad_norm": 0.49247199296951294,
      "learning_rate": 1.320760108586941e-05,
      "loss": 1.0732,
      "step": 23770
    },
    {
      "epoch": 5.0964423489069866,
      "grad_norm": 14.534097671508789,
      "learning_rate": 1.3204743534790685e-05,
      "loss": 0.7569,
      "step": 23780
    },
    {
      "epoch": 5.098585512216031,
      "grad_norm": 0.5688930153846741,
      "learning_rate": 1.320188598371196e-05,
      "loss": 0.5983,
      "step": 23790
    },
    {
      "epoch": 5.100728675525075,
      "grad_norm": 0.5494940280914307,
      "learning_rate": 1.3199028432633233e-05,
      "loss": 0.4501,
      "step": 23800
    },
    {
      "epoch": 5.102871838834119,
      "grad_norm": 0.6167672872543335,
      "learning_rate": 1.3196170881554509e-05,
      "loss": 1.0402,
      "step": 23810
    },
    {
      "epoch": 5.105015002143164,
      "grad_norm": 0.5198974013328552,
      "learning_rate": 1.3193313330475783e-05,
      "loss": 0.4483,
      "step": 23820
    },
    {
      "epoch": 5.107158165452208,
      "grad_norm": 0.5750309824943542,
      "learning_rate": 1.3190455779397059e-05,
      "loss": 0.8871,
      "step": 23830
    },
    {
      "epoch": 5.109301328761251,
      "grad_norm": 0.6412577629089355,
      "learning_rate": 1.3187598228318332e-05,
      "loss": 0.448,
      "step": 23840
    },
    {
      "epoch": 5.111444492070296,
      "grad_norm": 30.270282745361328,
      "learning_rate": 1.3184740677239606e-05,
      "loss": 0.7276,
      "step": 23850
    },
    {
      "epoch": 5.11358765537934,
      "grad_norm": 0.6724918484687805,
      "learning_rate": 1.3181883126160882e-05,
      "loss": 0.8647,
      "step": 23860
    },
    {
      "epoch": 5.115730818688384,
      "grad_norm": 14.85216999053955,
      "learning_rate": 1.3179025575082156e-05,
      "loss": 0.4357,
      "step": 23870
    },
    {
      "epoch": 5.117873981997429,
      "grad_norm": 14.398510932922363,
      "learning_rate": 1.317616802400343e-05,
      "loss": 0.7104,
      "step": 23880
    },
    {
      "epoch": 5.1200171453064725,
      "grad_norm": 0.7190970778465271,
      "learning_rate": 1.3173310472924705e-05,
      "loss": 0.7127,
      "step": 23890
    },
    {
      "epoch": 5.122160308615516,
      "grad_norm": 0.6049991250038147,
      "learning_rate": 1.317045292184598e-05,
      "loss": 0.1564,
      "step": 23900
    },
    {
      "epoch": 5.124303471924561,
      "grad_norm": 44.503631591796875,
      "learning_rate": 1.3167595370767255e-05,
      "loss": 1.0298,
      "step": 23910
    },
    {
      "epoch": 5.126446635233605,
      "grad_norm": 14.494952201843262,
      "learning_rate": 1.3164737819688529e-05,
      "loss": 0.8807,
      "step": 23920
    },
    {
      "epoch": 5.128589798542649,
      "grad_norm": 14.463493347167969,
      "learning_rate": 1.3161880268609803e-05,
      "loss": 0.443,
      "step": 23930
    },
    {
      "epoch": 5.1307329618516935,
      "grad_norm": 0.6185041069984436,
      "learning_rate": 1.3159022717531075e-05,
      "loss": 0.5824,
      "step": 23940
    },
    {
      "epoch": 5.132876125160737,
      "grad_norm": 0.59999680519104,
      "learning_rate": 1.315616516645235e-05,
      "loss": 0.4441,
      "step": 23950
    },
    {
      "epoch": 5.135019288469781,
      "grad_norm": 0.559312641620636,
      "learning_rate": 1.3153307615373625e-05,
      "loss": 0.1628,
      "step": 23960
    },
    {
      "epoch": 5.137162451778826,
      "grad_norm": 14.862213134765625,
      "learning_rate": 1.31504500642949e-05,
      "loss": 0.6017,
      "step": 23970
    },
    {
      "epoch": 5.13930561508787,
      "grad_norm": 0.4988725185394287,
      "learning_rate": 1.3147592513216174e-05,
      "loss": 0.1603,
      "step": 23980
    },
    {
      "epoch": 5.141448778396914,
      "grad_norm": 14.595702171325684,
      "learning_rate": 1.3144734962137448e-05,
      "loss": 1.0714,
      "step": 23990
    },
    {
      "epoch": 5.143591941705958,
      "grad_norm": 0.456997275352478,
      "learning_rate": 1.3141877411058724e-05,
      "loss": 0.0104,
      "step": 24000
    },
    {
      "epoch": 5.145735105015002,
      "grad_norm": 0.3963181972503662,
      "learning_rate": 1.3139019859979998e-05,
      "loss": 0.3256,
      "step": 24010
    },
    {
      "epoch": 5.147878268324046,
      "grad_norm": 0.3578650653362274,
      "learning_rate": 1.3136162308901272e-05,
      "loss": 0.653,
      "step": 24020
    },
    {
      "epoch": 5.150021431633091,
      "grad_norm": 14.71635913848877,
      "learning_rate": 1.3133304757822547e-05,
      "loss": 0.8121,
      "step": 24030
    },
    {
      "epoch": 5.152164594942135,
      "grad_norm": 0.37581419944763184,
      "learning_rate": 1.3130447206743821e-05,
      "loss": 0.6475,
      "step": 24040
    },
    {
      "epoch": 5.1543077582511785,
      "grad_norm": 0.4044029712677002,
      "learning_rate": 1.3127589655665097e-05,
      "loss": 0.4833,
      "step": 24050
    },
    {
      "epoch": 5.156450921560223,
      "grad_norm": 0.391510546207428,
      "learning_rate": 1.312473210458637e-05,
      "loss": 0.1657,
      "step": 24060
    },
    {
      "epoch": 5.158594084869267,
      "grad_norm": 0.3641388714313507,
      "learning_rate": 1.3121874553507645e-05,
      "loss": 0.3261,
      "step": 24070
    },
    {
      "epoch": 5.160737248178311,
      "grad_norm": 0.3501777648925781,
      "learning_rate": 1.311901700242892e-05,
      "loss": 0.168,
      "step": 24080
    },
    {
      "epoch": 5.162880411487356,
      "grad_norm": 0.3165501654148102,
      "learning_rate": 1.3116159451350194e-05,
      "loss": 0.5029,
      "step": 24090
    },
    {
      "epoch": 5.1650235747963995,
      "grad_norm": 14.78143310546875,
      "learning_rate": 1.311330190027147e-05,
      "loss": 0.827,
      "step": 24100
    },
    {
      "epoch": 5.167166738105443,
      "grad_norm": 14.875717163085938,
      "learning_rate": 1.3110444349192744e-05,
      "loss": 0.9844,
      "step": 24110
    },
    {
      "epoch": 5.169309901414488,
      "grad_norm": 0.37589800357818604,
      "learning_rate": 1.3107586798114018e-05,
      "loss": 0.803,
      "step": 24120
    },
    {
      "epoch": 5.171453064723532,
      "grad_norm": 0.38404378294944763,
      "learning_rate": 1.3104729247035293e-05,
      "loss": 0.3244,
      "step": 24130
    },
    {
      "epoch": 5.173596228032576,
      "grad_norm": 14.647644996643066,
      "learning_rate": 1.3101871695956567e-05,
      "loss": 1.2669,
      "step": 24140
    },
    {
      "epoch": 5.1757393913416205,
      "grad_norm": 14.675060272216797,
      "learning_rate": 1.3099014144877841e-05,
      "loss": 0.9084,
      "step": 24150
    },
    {
      "epoch": 5.177882554650664,
      "grad_norm": 0.5542904138565063,
      "learning_rate": 1.3096156593799113e-05,
      "loss": 0.3047,
      "step": 24160
    },
    {
      "epoch": 5.180025717959708,
      "grad_norm": 0.5305556058883667,
      "learning_rate": 1.3093299042720389e-05,
      "loss": 0.1594,
      "step": 24170
    },
    {
      "epoch": 5.182168881268753,
      "grad_norm": 0.4488976001739502,
      "learning_rate": 1.3090441491641663e-05,
      "loss": 0.6025,
      "step": 24180
    },
    {
      "epoch": 5.184312044577797,
      "grad_norm": 14.950645446777344,
      "learning_rate": 1.3087583940562939e-05,
      "loss": 0.317,
      "step": 24190
    },
    {
      "epoch": 5.186455207886841,
      "grad_norm": 0.4367277920246124,
      "learning_rate": 1.3084726389484212e-05,
      "loss": 1.0942,
      "step": 24200
    },
    {
      "epoch": 5.188598371195885,
      "grad_norm": 14.878860473632812,
      "learning_rate": 1.3081868838405486e-05,
      "loss": 0.4709,
      "step": 24210
    },
    {
      "epoch": 5.190741534504929,
      "grad_norm": 0.446241170167923,
      "learning_rate": 1.3079011287326762e-05,
      "loss": 0.3171,
      "step": 24220
    },
    {
      "epoch": 5.192884697813973,
      "grad_norm": 29.637588500976562,
      "learning_rate": 1.3076153736248036e-05,
      "loss": 0.9396,
      "step": 24230
    },
    {
      "epoch": 5.195027861123018,
      "grad_norm": 0.45414605736732483,
      "learning_rate": 1.3073296185169312e-05,
      "loss": 0.1635,
      "step": 24240
    },
    {
      "epoch": 5.197171024432062,
      "grad_norm": 0.4082567095756531,
      "learning_rate": 1.3070438634090586e-05,
      "loss": 0.167,
      "step": 24250
    },
    {
      "epoch": 5.1993141877411055,
      "grad_norm": 0.3534753918647766,
      "learning_rate": 1.306758108301186e-05,
      "loss": 0.3292,
      "step": 24260
    },
    {
      "epoch": 5.20145735105015,
      "grad_norm": 0.33747509121894836,
      "learning_rate": 1.3064723531933135e-05,
      "loss": 0.3374,
      "step": 24270
    },
    {
      "epoch": 5.203600514359194,
      "grad_norm": 0.31844228506088257,
      "learning_rate": 1.3061865980854409e-05,
      "loss": 0.3359,
      "step": 24280
    },
    {
      "epoch": 5.205743677668238,
      "grad_norm": 0.35027652978897095,
      "learning_rate": 1.3059008429775683e-05,
      "loss": 1.155,
      "step": 24290
    },
    {
      "epoch": 5.207886840977283,
      "grad_norm": 0.37080028653144836,
      "learning_rate": 1.3056150878696959e-05,
      "loss": 0.6503,
      "step": 24300
    },
    {
      "epoch": 5.210030004286327,
      "grad_norm": 0.39408862590789795,
      "learning_rate": 1.3053293327618232e-05,
      "loss": 0.6445,
      "step": 24310
    },
    {
      "epoch": 5.21217316759537,
      "grad_norm": 0.4127114713191986,
      "learning_rate": 1.3050435776539508e-05,
      "loss": 0.6362,
      "step": 24320
    },
    {
      "epoch": 5.214316330904415,
      "grad_norm": 0.33193105459213257,
      "learning_rate": 1.3047578225460782e-05,
      "loss": 0.1721,
      "step": 24330
    },
    {
      "epoch": 5.216459494213459,
      "grad_norm": 0.32021015882492065,
      "learning_rate": 1.3044720674382056e-05,
      "loss": 0.6619,
      "step": 24340
    },
    {
      "epoch": 5.218602657522503,
      "grad_norm": 0.31081876158714294,
      "learning_rate": 1.3041863123303332e-05,
      "loss": 0.6638,
      "step": 24350
    },
    {
      "epoch": 5.220745820831548,
      "grad_norm": 0.3649418354034424,
      "learning_rate": 1.3039005572224606e-05,
      "loss": 0.4962,
      "step": 24360
    },
    {
      "epoch": 5.222888984140591,
      "grad_norm": 0.39689159393310547,
      "learning_rate": 1.3036148021145878e-05,
      "loss": 1.1482,
      "step": 24370
    },
    {
      "epoch": 5.225032147449635,
      "grad_norm": 0.40660661458969116,
      "learning_rate": 1.3033290470067153e-05,
      "loss": 0.1693,
      "step": 24380
    },
    {
      "epoch": 5.22717531075868,
      "grad_norm": 29.71799087524414,
      "learning_rate": 1.3030432918988427e-05,
      "loss": 1.1144,
      "step": 24390
    },
    {
      "epoch": 5.229318474067724,
      "grad_norm": 0.4808964431285858,
      "learning_rate": 1.3027575367909701e-05,
      "loss": 0.7763,
      "step": 24400
    },
    {
      "epoch": 5.231461637376768,
      "grad_norm": 0.5530580878257751,
      "learning_rate": 1.3024717816830977e-05,
      "loss": 1.0621,
      "step": 24410
    },
    {
      "epoch": 5.2336048006858125,
      "grad_norm": 0.5343154072761536,
      "learning_rate": 1.302186026575225e-05,
      "loss": 0.1569,
      "step": 24420
    },
    {
      "epoch": 5.235747963994856,
      "grad_norm": 0.4983890950679779,
      "learning_rate": 1.3019002714673525e-05,
      "loss": 0.4645,
      "step": 24430
    },
    {
      "epoch": 5.2378911273039,
      "grad_norm": 14.663840293884277,
      "learning_rate": 1.30161451635948e-05,
      "loss": 0.6169,
      "step": 24440
    },
    {
      "epoch": 5.240034290612945,
      "grad_norm": 0.43689632415771484,
      "learning_rate": 1.3013287612516074e-05,
      "loss": 0.3151,
      "step": 24450
    },
    {
      "epoch": 5.242177453921989,
      "grad_norm": 0.46188318729400635,
      "learning_rate": 1.301043006143735e-05,
      "loss": 0.7809,
      "step": 24460
    },
    {
      "epoch": 5.244320617231033,
      "grad_norm": 0.5088127851486206,
      "learning_rate": 1.3007572510358624e-05,
      "loss": 0.9107,
      "step": 24470
    },
    {
      "epoch": 5.246463780540077,
      "grad_norm": 0.6255550384521484,
      "learning_rate": 1.3004714959279898e-05,
      "loss": 0.4515,
      "step": 24480
    },
    {
      "epoch": 5.248606943849121,
      "grad_norm": 14.49135971069336,
      "learning_rate": 1.3001857408201173e-05,
      "loss": 0.7291,
      "step": 24490
    },
    {
      "epoch": 5.250750107158165,
      "grad_norm": 0.620130717754364,
      "learning_rate": 1.2998999857122447e-05,
      "loss": 0.2994,
      "step": 24500
    },
    {
      "epoch": 5.25289327046721,
      "grad_norm": 0.5588948726654053,
      "learning_rate": 1.2996142306043721e-05,
      "loss": 0.2989,
      "step": 24510
    },
    {
      "epoch": 5.255036433776254,
      "grad_norm": 0.5182068943977356,
      "learning_rate": 1.2993284754964997e-05,
      "loss": 0.6028,
      "step": 24520
    },
    {
      "epoch": 5.2571795970852975,
      "grad_norm": 0.48623284697532654,
      "learning_rate": 1.299042720388627e-05,
      "loss": 0.6157,
      "step": 24530
    },
    {
      "epoch": 5.259322760394342,
      "grad_norm": 0.46078723669052124,
      "learning_rate": 1.2987569652807546e-05,
      "loss": 0.4683,
      "step": 24540
    },
    {
      "epoch": 5.261465923703386,
      "grad_norm": 14.656455993652344,
      "learning_rate": 1.298471210172882e-05,
      "loss": 0.6161,
      "step": 24550
    },
    {
      "epoch": 5.26360908701243,
      "grad_norm": 0.49368816614151,
      "learning_rate": 1.2981854550650094e-05,
      "loss": 0.7699,
      "step": 24560
    },
    {
      "epoch": 5.265752250321475,
      "grad_norm": 0.468126118183136,
      "learning_rate": 1.297899699957137e-05,
      "loss": 0.1613,
      "step": 24570
    },
    {
      "epoch": 5.2678954136305185,
      "grad_norm": 29.63959503173828,
      "learning_rate": 1.2976139448492644e-05,
      "loss": 1.2526,
      "step": 24580
    },
    {
      "epoch": 5.270038576939562,
      "grad_norm": 0.4336327612400055,
      "learning_rate": 1.2973281897413916e-05,
      "loss": 0.6385,
      "step": 24590
    },
    {
      "epoch": 5.272181740248607,
      "grad_norm": 0.5212647318840027,
      "learning_rate": 1.2970424346335192e-05,
      "loss": 0.923,
      "step": 24600
    },
    {
      "epoch": 5.274324903557651,
      "grad_norm": 15.006507873535156,
      "learning_rate": 1.2967566795256466e-05,
      "loss": 0.4558,
      "step": 24610
    },
    {
      "epoch": 5.276468066866695,
      "grad_norm": 0.569004476070404,
      "learning_rate": 1.296470924417774e-05,
      "loss": 0.6055,
      "step": 24620
    },
    {
      "epoch": 5.2786112301757395,
      "grad_norm": 14.51626205444336,
      "learning_rate": 1.2961851693099015e-05,
      "loss": 0.8896,
      "step": 24630
    },
    {
      "epoch": 5.280754393484783,
      "grad_norm": 0.6658746004104614,
      "learning_rate": 1.2958994142020289e-05,
      "loss": 0.5841,
      "step": 24640
    },
    {
      "epoch": 5.282897556793827,
      "grad_norm": 0.5827406048774719,
      "learning_rate": 1.2956136590941563e-05,
      "loss": 0.7194,
      "step": 24650
    },
    {
      "epoch": 5.285040720102872,
      "grad_norm": 0.5570030212402344,
      "learning_rate": 1.2953279039862839e-05,
      "loss": 0.4482,
      "step": 24660
    },
    {
      "epoch": 5.287183883411916,
      "grad_norm": 14.634978294372559,
      "learning_rate": 1.2950421488784113e-05,
      "loss": 0.9048,
      "step": 24670
    },
    {
      "epoch": 5.2893270467209605,
      "grad_norm": 14.512706756591797,
      "learning_rate": 1.2947563937705388e-05,
      "loss": 1.0315,
      "step": 24680
    },
    {
      "epoch": 5.291470210030004,
      "grad_norm": 14.380941390991211,
      "learning_rate": 1.2944706386626662e-05,
      "loss": 0.5727,
      "step": 24690
    },
    {
      "epoch": 5.293613373339048,
      "grad_norm": 0.6799259185791016,
      "learning_rate": 1.2941848835547936e-05,
      "loss": 0.4271,
      "step": 24700
    },
    {
      "epoch": 5.295756536648093,
      "grad_norm": 0.7108444571495056,
      "learning_rate": 1.2938991284469212e-05,
      "loss": 0.9962,
      "step": 24710
    },
    {
      "epoch": 5.297899699957137,
      "grad_norm": 0.6070131659507751,
      "learning_rate": 1.2936133733390486e-05,
      "loss": 0.2926,
      "step": 24720
    },
    {
      "epoch": 5.300042863266181,
      "grad_norm": 0.460310697555542,
      "learning_rate": 1.293327618231176e-05,
      "loss": 0.7701,
      "step": 24730
    },
    {
      "epoch": 5.302186026575225,
      "grad_norm": 0.4723561108112335,
      "learning_rate": 1.2930418631233035e-05,
      "loss": 0.6209,
      "step": 24740
    },
    {
      "epoch": 5.304329189884269,
      "grad_norm": 0.5025717616081238,
      "learning_rate": 1.2927561080154309e-05,
      "loss": 0.9112,
      "step": 24750
    },
    {
      "epoch": 5.306472353193313,
      "grad_norm": 29.631359100341797,
      "learning_rate": 1.2924703529075585e-05,
      "loss": 0.7576,
      "step": 24760
    },
    {
      "epoch": 5.308615516502358,
      "grad_norm": 0.5134521126747131,
      "learning_rate": 1.2921845977996859e-05,
      "loss": 0.6036,
      "step": 24770
    },
    {
      "epoch": 5.310758679811402,
      "grad_norm": 0.527953565120697,
      "learning_rate": 1.2918988426918133e-05,
      "loss": 0.6001,
      "step": 24780
    },
    {
      "epoch": 5.3129018431204456,
      "grad_norm": 14.579113960266113,
      "learning_rate": 1.2916130875839408e-05,
      "loss": 1.0448,
      "step": 24790
    },
    {
      "epoch": 5.31504500642949,
      "grad_norm": 0.5885345935821533,
      "learning_rate": 1.291327332476068e-05,
      "loss": 0.8866,
      "step": 24800
    },
    {
      "epoch": 5.317188169738534,
      "grad_norm": 14.480320930480957,
      "learning_rate": 1.2910415773681954e-05,
      "loss": 0.3013,
      "step": 24810
    },
    {
      "epoch": 5.319331333047578,
      "grad_norm": 0.5631351470947266,
      "learning_rate": 1.290755822260323e-05,
      "loss": 0.4533,
      "step": 24820
    },
    {
      "epoch": 5.321474496356623,
      "grad_norm": 29.698410034179688,
      "learning_rate": 1.2904700671524504e-05,
      "loss": 0.7428,
      "step": 24830
    },
    {
      "epoch": 5.323617659665667,
      "grad_norm": 14.54550838470459,
      "learning_rate": 1.2901843120445778e-05,
      "loss": 0.7508,
      "step": 24840
    },
    {
      "epoch": 5.32576082297471,
      "grad_norm": 0.5460951924324036,
      "learning_rate": 1.2898985569367053e-05,
      "loss": 0.4534,
      "step": 24850
    },
    {
      "epoch": 5.327903986283755,
      "grad_norm": 29.616268157958984,
      "learning_rate": 1.2896128018288327e-05,
      "loss": 0.6002,
      "step": 24860
    },
    {
      "epoch": 5.330047149592799,
      "grad_norm": 0.5357818007469177,
      "learning_rate": 1.2893270467209601e-05,
      "loss": 0.749,
      "step": 24870
    },
    {
      "epoch": 5.332190312901843,
      "grad_norm": 0.5384818315505981,
      "learning_rate": 1.2890412916130877e-05,
      "loss": 0.4577,
      "step": 24880
    },
    {
      "epoch": 5.334333476210888,
      "grad_norm": 0.5979442596435547,
      "learning_rate": 1.288755536505215e-05,
      "loss": 0.8833,
      "step": 24890
    },
    {
      "epoch": 5.3364766395199315,
      "grad_norm": 14.475645065307617,
      "learning_rate": 1.2884697813973426e-05,
      "loss": 0.7311,
      "step": 24900
    },
    {
      "epoch": 5.338619802828975,
      "grad_norm": 0.6013691425323486,
      "learning_rate": 1.28818402628947e-05,
      "loss": 0.1562,
      "step": 24910
    },
    {
      "epoch": 5.34076296613802,
      "grad_norm": 0.5694540739059448,
      "learning_rate": 1.2878982711815974e-05,
      "loss": 0.4445,
      "step": 24920
    },
    {
      "epoch": 5.342906129447064,
      "grad_norm": 0.5096653699874878,
      "learning_rate": 1.287612516073725e-05,
      "loss": 0.1587,
      "step": 24930
    },
    {
      "epoch": 5.345049292756108,
      "grad_norm": 14.60865592956543,
      "learning_rate": 1.2873267609658524e-05,
      "loss": 1.0691,
      "step": 24940
    },
    {
      "epoch": 5.3471924560651525,
      "grad_norm": 0.5256417393684387,
      "learning_rate": 1.28704100585798e-05,
      "loss": 1.5106,
      "step": 24950
    },
    {
      "epoch": 5.349335619374196,
      "grad_norm": 0.6029574871063232,
      "learning_rate": 1.2867552507501073e-05,
      "loss": 0.3023,
      "step": 24960
    },
    {
      "epoch": 5.35147878268324,
      "grad_norm": 14.573967933654785,
      "learning_rate": 1.2864694956422347e-05,
      "loss": 0.4489,
      "step": 24970
    },
    {
      "epoch": 5.353621945992285,
      "grad_norm": 0.48934412002563477,
      "learning_rate": 1.2861837405343623e-05,
      "loss": 0.3087,
      "step": 24980
    },
    {
      "epoch": 5.355765109301329,
      "grad_norm": 14.883341789245605,
      "learning_rate": 1.2858979854264897e-05,
      "loss": 1.3711,
      "step": 24990
    },
    {
      "epoch": 5.357908272610373,
      "grad_norm": 0.4714749753475189,
      "learning_rate": 1.285612230318617e-05,
      "loss": 0.3124,
      "step": 25000
    },
    {
      "epoch": 5.360051435919417,
      "grad_norm": 14.58059024810791,
      "learning_rate": 1.2853264752107446e-05,
      "loss": 1.0685,
      "step": 25010
    },
    {
      "epoch": 5.362194599228461,
      "grad_norm": 14.602603912353516,
      "learning_rate": 1.2850407201028719e-05,
      "loss": 0.306,
      "step": 25020
    },
    {
      "epoch": 5.364337762537505,
      "grad_norm": 14.556821823120117,
      "learning_rate": 1.2847549649949993e-05,
      "loss": 0.7496,
      "step": 25030
    },
    {
      "epoch": 5.36648092584655,
      "grad_norm": 14.557279586791992,
      "learning_rate": 1.2844692098871268e-05,
      "loss": 0.3055,
      "step": 25040
    },
    {
      "epoch": 5.368624089155594,
      "grad_norm": 0.5406200885772705,
      "learning_rate": 1.2841834547792542e-05,
      "loss": 0.7501,
      "step": 25050
    },
    {
      "epoch": 5.3707672524646375,
      "grad_norm": 0.5634494423866272,
      "learning_rate": 1.2838976996713816e-05,
      "loss": 0.7491,
      "step": 25060
    },
    {
      "epoch": 5.372910415773682,
      "grad_norm": 14.490890502929688,
      "learning_rate": 1.2836119445635092e-05,
      "loss": 1.0251,
      "step": 25070
    },
    {
      "epoch": 5.375053579082726,
      "grad_norm": 0.7075262069702148,
      "learning_rate": 1.2833261894556366e-05,
      "loss": 0.8568,
      "step": 25080
    },
    {
      "epoch": 5.37719674239177,
      "grad_norm": 0.7849313616752625,
      "learning_rate": 1.2830404343477641e-05,
      "loss": 0.8391,
      "step": 25090
    },
    {
      "epoch": 5.379339905700815,
      "grad_norm": 0.8015022277832031,
      "learning_rate": 1.2827546792398915e-05,
      "loss": 0.8137,
      "step": 25100
    },
    {
      "epoch": 5.3814830690098585,
      "grad_norm": 0.9270004034042358,
      "learning_rate": 1.2824689241320189e-05,
      "loss": 0.671,
      "step": 25110
    },
    {
      "epoch": 5.383626232318902,
      "grad_norm": 14.383610725402832,
      "learning_rate": 1.2821831690241465e-05,
      "loss": 0.5425,
      "step": 25120
    },
    {
      "epoch": 5.385769395627947,
      "grad_norm": 0.7997390031814575,
      "learning_rate": 1.2818974139162739e-05,
      "loss": 0.4198,
      "step": 25130
    },
    {
      "epoch": 5.387912558936991,
      "grad_norm": 0.7874290943145752,
      "learning_rate": 1.2816116588084013e-05,
      "loss": 0.5564,
      "step": 25140
    },
    {
      "epoch": 5.390055722246035,
      "grad_norm": 0.7154895663261414,
      "learning_rate": 1.2813259037005288e-05,
      "loss": 0.7028,
      "step": 25150
    },
    {
      "epoch": 5.3921988855550795,
      "grad_norm": 0.6296614408493042,
      "learning_rate": 1.2810401485926562e-05,
      "loss": 0.4374,
      "step": 25160
    },
    {
      "epoch": 5.394342048864123,
      "grad_norm": 29.693342208862305,
      "learning_rate": 1.2807543934847838e-05,
      "loss": 0.7357,
      "step": 25170
    },
    {
      "epoch": 5.396485212173167,
      "grad_norm": 0.6340117454528809,
      "learning_rate": 1.2804686383769112e-05,
      "loss": 0.5922,
      "step": 25180
    },
    {
      "epoch": 5.398628375482212,
      "grad_norm": 0.5503639578819275,
      "learning_rate": 1.2801828832690386e-05,
      "loss": 0.1592,
      "step": 25190
    },
    {
      "epoch": 5.400771538791256,
      "grad_norm": 14.589902877807617,
      "learning_rate": 1.2798971281611661e-05,
      "loss": 0.7608,
      "step": 25200
    },
    {
      "epoch": 5.4029147021003,
      "grad_norm": 0.5219225287437439,
      "learning_rate": 1.2796113730532935e-05,
      "loss": 0.309,
      "step": 25210
    },
    {
      "epoch": 5.405057865409344,
      "grad_norm": 0.5195953845977783,
      "learning_rate": 1.2793256179454209e-05,
      "loss": 0.4612,
      "step": 25220
    },
    {
      "epoch": 5.407201028718388,
      "grad_norm": 0.5046972632408142,
      "learning_rate": 1.2790398628375483e-05,
      "loss": 1.0601,
      "step": 25230
    },
    {
      "epoch": 5.409344192027432,
      "grad_norm": 0.5100526809692383,
      "learning_rate": 1.2787541077296757e-05,
      "loss": 0.607,
      "step": 25240
    },
    {
      "epoch": 5.411487355336477,
      "grad_norm": 14.700748443603516,
      "learning_rate": 1.2784683526218031e-05,
      "loss": 0.6057,
      "step": 25250
    },
    {
      "epoch": 5.413630518645521,
      "grad_norm": 0.5248371958732605,
      "learning_rate": 1.2781825975139306e-05,
      "loss": 0.3088,
      "step": 25260
    },
    {
      "epoch": 5.4157736819545645,
      "grad_norm": 14.810690879821777,
      "learning_rate": 1.277896842406058e-05,
      "loss": 0.4678,
      "step": 25270
    },
    {
      "epoch": 5.417916845263609,
      "grad_norm": 14.589241981506348,
      "learning_rate": 1.2776110872981854e-05,
      "loss": 0.6141,
      "step": 25280
    },
    {
      "epoch": 5.420060008572653,
      "grad_norm": 0.48365843296051025,
      "learning_rate": 1.277325332190313e-05,
      "loss": 0.1603,
      "step": 25290
    },
    {
      "epoch": 5.422203171881697,
      "grad_norm": 0.44512566924095154,
      "learning_rate": 1.2770395770824404e-05,
      "loss": 0.4657,
      "step": 25300
    },
    {
      "epoch": 5.424346335190742,
      "grad_norm": 0.40457674860954285,
      "learning_rate": 1.276753821974568e-05,
      "loss": 0.1664,
      "step": 25310
    },
    {
      "epoch": 5.426489498499786,
      "grad_norm": 0.4005521833896637,
      "learning_rate": 1.2764680668666953e-05,
      "loss": 0.6387,
      "step": 25320
    },
    {
      "epoch": 5.428632661808829,
      "grad_norm": 14.697528839111328,
      "learning_rate": 1.2761823117588227e-05,
      "loss": 0.9494,
      "step": 25330
    },
    {
      "epoch": 5.430775825117874,
      "grad_norm": 0.4376966655254364,
      "learning_rate": 1.2758965566509503e-05,
      "loss": 0.3205,
      "step": 25340
    },
    {
      "epoch": 5.432918988426918,
      "grad_norm": 0.4176180958747864,
      "learning_rate": 1.2756108015430777e-05,
      "loss": 0.4774,
      "step": 25350
    },
    {
      "epoch": 5.435062151735963,
      "grad_norm": 0.409721314907074,
      "learning_rate": 1.2753250464352051e-05,
      "loss": 0.3222,
      "step": 25360
    },
    {
      "epoch": 5.437205315045007,
      "grad_norm": 0.3998066484928131,
      "learning_rate": 1.2750392913273326e-05,
      "loss": 0.3231,
      "step": 25370
    },
    {
      "epoch": 5.43934847835405,
      "grad_norm": 29.748924255371094,
      "learning_rate": 1.27475353621946e-05,
      "loss": 0.9563,
      "step": 25380
    },
    {
      "epoch": 5.441491641663095,
      "grad_norm": 14.657806396484375,
      "learning_rate": 1.2744677811115876e-05,
      "loss": 0.6349,
      "step": 25390
    },
    {
      "epoch": 5.443634804972139,
      "grad_norm": 14.626761436462402,
      "learning_rate": 1.274182026003715e-05,
      "loss": 0.7829,
      "step": 25400
    },
    {
      "epoch": 5.445777968281183,
      "grad_norm": 14.594771385192871,
      "learning_rate": 1.2738962708958424e-05,
      "loss": 1.066,
      "step": 25410
    },
    {
      "epoch": 5.447921131590228,
      "grad_norm": 0.5396082997322083,
      "learning_rate": 1.27361051578797e-05,
      "loss": 0.7524,
      "step": 25420
    },
    {
      "epoch": 5.4500642948992715,
      "grad_norm": 14.533907890319824,
      "learning_rate": 1.2733247606800973e-05,
      "loss": 0.597,
      "step": 25430
    },
    {
      "epoch": 5.452207458208315,
      "grad_norm": 30.470361709594727,
      "learning_rate": 1.2730390055722247e-05,
      "loss": 0.7398,
      "step": 25440
    },
    {
      "epoch": 5.45435062151736,
      "grad_norm": 0.5831178426742554,
      "learning_rate": 1.2727532504643521e-05,
      "loss": 0.4464,
      "step": 25450
    },
    {
      "epoch": 5.456493784826404,
      "grad_norm": 14.850095748901367,
      "learning_rate": 1.2724674953564795e-05,
      "loss": 0.8814,
      "step": 25460
    },
    {
      "epoch": 5.458636948135448,
      "grad_norm": 14.545900344848633,
      "learning_rate": 1.2721817402486069e-05,
      "loss": 0.588,
      "step": 25470
    },
    {
      "epoch": 5.4607801114444925,
      "grad_norm": 0.63545161485672,
      "learning_rate": 1.2718959851407345e-05,
      "loss": 0.2984,
      "step": 25480
    },
    {
      "epoch": 5.462923274753536,
      "grad_norm": 0.602428674697876,
      "learning_rate": 1.2716102300328619e-05,
      "loss": 0.8723,
      "step": 25490
    },
    {
      "epoch": 5.46506643806258,
      "grad_norm": 0.5457468628883362,
      "learning_rate": 1.2713244749249893e-05,
      "loss": 0.0129,
      "step": 25500
    },
    {
      "epoch": 5.467209601371625,
      "grad_norm": 0.46827930212020874,
      "learning_rate": 1.2710387198171168e-05,
      "loss": 0.7595,
      "step": 25510
    },
    {
      "epoch": 5.469352764680669,
      "grad_norm": 0.4643039405345917,
      "learning_rate": 1.2707529647092442e-05,
      "loss": 0.1631,
      "step": 25520
    },
    {
      "epoch": 5.471495927989713,
      "grad_norm": 14.665699005126953,
      "learning_rate": 1.2704672096013718e-05,
      "loss": 0.7807,
      "step": 25530
    },
    {
      "epoch": 5.473639091298757,
      "grad_norm": 14.714223861694336,
      "learning_rate": 1.2701814544934992e-05,
      "loss": 0.632,
      "step": 25540
    },
    {
      "epoch": 5.475782254607801,
      "grad_norm": 0.395668089389801,
      "learning_rate": 1.2698956993856266e-05,
      "loss": 0.3219,
      "step": 25550
    },
    {
      "epoch": 5.477925417916845,
      "grad_norm": 14.758407592773438,
      "learning_rate": 1.2696099442777541e-05,
      "loss": 0.4849,
      "step": 25560
    },
    {
      "epoch": 5.48006858122589,
      "grad_norm": 0.39854535460472107,
      "learning_rate": 1.2693241891698815e-05,
      "loss": 0.4826,
      "step": 25570
    },
    {
      "epoch": 5.482211744534934,
      "grad_norm": 0.4301965832710266,
      "learning_rate": 1.2690384340620089e-05,
      "loss": 0.8001,
      "step": 25580
    },
    {
      "epoch": 5.4843549078439775,
      "grad_norm": 0.3935822546482086,
      "learning_rate": 1.2687526789541365e-05,
      "loss": 0.0092,
      "step": 25590
    },
    {
      "epoch": 5.486498071153022,
      "grad_norm": 14.808473587036133,
      "learning_rate": 1.2684669238462639e-05,
      "loss": 0.1704,
      "step": 25600
    },
    {
      "epoch": 5.488641234462066,
      "grad_norm": 0.3252216577529907,
      "learning_rate": 1.2681811687383914e-05,
      "loss": 0.8268,
      "step": 25610
    },
    {
      "epoch": 5.49078439777111,
      "grad_norm": 0.3485773205757141,
      "learning_rate": 1.2678954136305188e-05,
      "loss": 0.6572,
      "step": 25620
    },
    {
      "epoch": 5.492927561080155,
      "grad_norm": 0.3747563064098358,
      "learning_rate": 1.2676096585226462e-05,
      "loss": 0.6512,
      "step": 25630
    },
    {
      "epoch": 5.4950707243891985,
      "grad_norm": 0.40513890981674194,
      "learning_rate": 1.2673239034147738e-05,
      "loss": 0.641,
      "step": 25640
    },
    {
      "epoch": 5.497213887698242,
      "grad_norm": 14.679386138916016,
      "learning_rate": 1.2670381483069012e-05,
      "loss": 0.3218,
      "step": 25650
    },
    {
      "epoch": 5.499357051007287,
      "grad_norm": 0.3882483243942261,
      "learning_rate": 1.2667523931990284e-05,
      "loss": 0.1662,
      "step": 25660
    },
    {
      "epoch": 5.501500214316331,
      "grad_norm": 14.694531440734863,
      "learning_rate": 1.266466638091156e-05,
      "loss": 0.798,
      "step": 25670
    },
    {
      "epoch": 5.503643377625375,
      "grad_norm": 14.693899154663086,
      "learning_rate": 1.2661808829832833e-05,
      "loss": 0.4842,
      "step": 25680
    },
    {
      "epoch": 5.5057865409344195,
      "grad_norm": 0.4243292212486267,
      "learning_rate": 1.2658951278754107e-05,
      "loss": 0.638,
      "step": 25690
    },
    {
      "epoch": 5.507929704243463,
      "grad_norm": 0.417185515165329,
      "learning_rate": 1.2656093727675383e-05,
      "loss": 0.321,
      "step": 25700
    },
    {
      "epoch": 5.510072867552507,
      "grad_norm": 0.3779265582561493,
      "learning_rate": 1.2653236176596657e-05,
      "loss": 0.4762,
      "step": 25710
    },
    {
      "epoch": 5.512216030861552,
      "grad_norm": 15.358319282531738,
      "learning_rate": 1.2650378625517931e-05,
      "loss": 0.9739,
      "step": 25720
    },
    {
      "epoch": 5.514359194170596,
      "grad_norm": 0.43659159541130066,
      "learning_rate": 1.2647521074439207e-05,
      "loss": 0.4751,
      "step": 25730
    },
    {
      "epoch": 5.51650235747964,
      "grad_norm": 0.4363131523132324,
      "learning_rate": 1.264466352336048e-05,
      "loss": 0.3176,
      "step": 25740
    },
    {
      "epoch": 5.518645520788684,
      "grad_norm": 0.44424882531166077,
      "learning_rate": 1.2641805972281756e-05,
      "loss": 0.9353,
      "step": 25750
    },
    {
      "epoch": 5.520788684097728,
      "grad_norm": 14.624466896057129,
      "learning_rate": 1.263894842120303e-05,
      "loss": 0.7788,
      "step": 25760
    },
    {
      "epoch": 5.522931847406772,
      "grad_norm": 0.48273468017578125,
      "learning_rate": 1.2636090870124304e-05,
      "loss": 0.618,
      "step": 25770
    },
    {
      "epoch": 5.525075010715817,
      "grad_norm": 14.569293975830078,
      "learning_rate": 1.263323331904558e-05,
      "loss": 0.612,
      "step": 25780
    },
    {
      "epoch": 5.527218174024861,
      "grad_norm": 14.721919059753418,
      "learning_rate": 1.2630375767966853e-05,
      "loss": 1.0574,
      "step": 25790
    },
    {
      "epoch": 5.529361337333905,
      "grad_norm": 14.494650840759277,
      "learning_rate": 1.2627518216888129e-05,
      "loss": 0.8814,
      "step": 25800
    },
    {
      "epoch": 5.531504500642949,
      "grad_norm": 0.6120069622993469,
      "learning_rate": 1.2624660665809403e-05,
      "loss": 0.448,
      "step": 25810
    },
    {
      "epoch": 5.533647663951993,
      "grad_norm": 14.49284839630127,
      "learning_rate": 1.2621803114730677e-05,
      "loss": 0.5839,
      "step": 25820
    },
    {
      "epoch": 5.535790827261037,
      "grad_norm": 0.5516234636306763,
      "learning_rate": 1.2618945563651953e-05,
      "loss": 0.8848,
      "step": 25830
    },
    {
      "epoch": 5.537933990570082,
      "grad_norm": 0.5732038617134094,
      "learning_rate": 1.2616088012573227e-05,
      "loss": 0.453,
      "step": 25840
    },
    {
      "epoch": 5.540077153879126,
      "grad_norm": 15.005059242248535,
      "learning_rate": 1.26132304614945e-05,
      "loss": 0.5903,
      "step": 25850
    },
    {
      "epoch": 5.542220317188169,
      "grad_norm": 0.5485929250717163,
      "learning_rate": 1.2610372910415776e-05,
      "loss": 0.0129,
      "step": 25860
    },
    {
      "epoch": 5.544363480497214,
      "grad_norm": 14.576127052307129,
      "learning_rate": 1.260751535933705e-05,
      "loss": 0.9073,
      "step": 25870
    },
    {
      "epoch": 5.546506643806258,
      "grad_norm": 0.49106231331825256,
      "learning_rate": 1.2604657808258322e-05,
      "loss": 0.1604,
      "step": 25880
    },
    {
      "epoch": 5.548649807115302,
      "grad_norm": 14.607460021972656,
      "learning_rate": 1.2601800257179598e-05,
      "loss": 0.7691,
      "step": 25890
    },
    {
      "epoch": 5.550792970424347,
      "grad_norm": 0.47994738817214966,
      "learning_rate": 1.2598942706100872e-05,
      "loss": 0.3127,
      "step": 25900
    },
    {
      "epoch": 5.5529361337333905,
      "grad_norm": 0.4555905759334564,
      "learning_rate": 1.2596085155022146e-05,
      "loss": 0.3151,
      "step": 25910
    },
    {
      "epoch": 5.555079297042434,
      "grad_norm": 0.42552807927131653,
      "learning_rate": 1.2593227603943421e-05,
      "loss": 0.3204,
      "step": 25920
    },
    {
      "epoch": 5.557222460351479,
      "grad_norm": 0.36451461911201477,
      "learning_rate": 1.2590370052864695e-05,
      "loss": 0.1656,
      "step": 25930
    },
    {
      "epoch": 5.559365623660523,
      "grad_norm": 0.3573484420776367,
      "learning_rate": 1.2587512501785971e-05,
      "loss": 0.33,
      "step": 25940
    },
    {
      "epoch": 5.561508786969567,
      "grad_norm": 0.34849652647972107,
      "learning_rate": 1.2584654950707245e-05,
      "loss": 0.9826,
      "step": 25950
    },
    {
      "epoch": 5.5636519502786115,
      "grad_norm": 0.3802938461303711,
      "learning_rate": 1.2581797399628519e-05,
      "loss": 0.4915,
      "step": 25960
    },
    {
      "epoch": 5.565795113587655,
      "grad_norm": 0.3573302626609802,
      "learning_rate": 1.2578939848549794e-05,
      "loss": 0.3282,
      "step": 25970
    },
    {
      "epoch": 5.567938276896699,
      "grad_norm": 14.767017364501953,
      "learning_rate": 1.2576082297471068e-05,
      "loss": 0.8119,
      "step": 25980
    },
    {
      "epoch": 5.570081440205744,
      "grad_norm": 0.4021187424659729,
      "learning_rate": 1.2573224746392342e-05,
      "loss": 0.6434,
      "step": 25990
    },
    {
      "epoch": 5.572224603514788,
      "grad_norm": 0.37506064772605896,
      "learning_rate": 1.2570367195313618e-05,
      "loss": 0.6385,
      "step": 26000
    },
    {
      "epoch": 5.574367766823832,
      "grad_norm": 0.42230790853500366,
      "learning_rate": 1.2567509644234892e-05,
      "loss": 0.6396,
      "step": 26010
    },
    {
      "epoch": 5.576510930132876,
      "grad_norm": 0.41470086574554443,
      "learning_rate": 1.2564652093156167e-05,
      "loss": 0.3241,
      "step": 26020
    },
    {
      "epoch": 5.57865409344192,
      "grad_norm": 14.942834854125977,
      "learning_rate": 1.2561794542077441e-05,
      "loss": 0.3332,
      "step": 26030
    },
    {
      "epoch": 5.580797256750964,
      "grad_norm": 0.3286180794239044,
      "learning_rate": 1.2558936990998715e-05,
      "loss": 0.3354,
      "step": 26040
    },
    {
      "epoch": 5.582940420060009,
      "grad_norm": 0.32537439465522766,
      "learning_rate": 1.2556079439919991e-05,
      "loss": 0.5004,
      "step": 26050
    },
    {
      "epoch": 5.585083583369053,
      "grad_norm": 0.3510076105594635,
      "learning_rate": 1.2553221888841265e-05,
      "loss": 0.9855,
      "step": 26060
    },
    {
      "epoch": 5.5872267466780965,
      "grad_norm": 14.721583366394043,
      "learning_rate": 1.2550364337762539e-05,
      "loss": 0.8073,
      "step": 26070
    },
    {
      "epoch": 5.589369909987141,
      "grad_norm": 0.416096031665802,
      "learning_rate": 1.2547506786683814e-05,
      "loss": 0.7941,
      "step": 26080
    },
    {
      "epoch": 5.591513073296185,
      "grad_norm": 14.664179801940918,
      "learning_rate": 1.2544649235605087e-05,
      "loss": 1.0933,
      "step": 26090
    },
    {
      "epoch": 5.593656236605229,
      "grad_norm": 0.4966285526752472,
      "learning_rate": 1.254179168452636e-05,
      "loss": 0.3153,
      "step": 26100
    },
    {
      "epoch": 5.595799399914274,
      "grad_norm": 0.5169832706451416,
      "learning_rate": 1.2538934133447636e-05,
      "loss": 0.9068,
      "step": 26110
    },
    {
      "epoch": 5.5979425632233175,
      "grad_norm": 0.5626433491706848,
      "learning_rate": 1.253607658236891e-05,
      "loss": 0.6021,
      "step": 26120
    },
    {
      "epoch": 5.600085726532361,
      "grad_norm": 0.5209043622016907,
      "learning_rate": 1.2533219031290184e-05,
      "loss": 0.1578,
      "step": 26130
    },
    {
      "epoch": 5.602228889841406,
      "grad_norm": 30.301362991333008,
      "learning_rate": 1.253036148021146e-05,
      "loss": 0.6149,
      "step": 26140
    },
    {
      "epoch": 5.60437205315045,
      "grad_norm": 14.638919830322266,
      "learning_rate": 1.2527503929132734e-05,
      "loss": 0.4648,
      "step": 26150
    },
    {
      "epoch": 5.606515216459494,
      "grad_norm": 29.75481605529785,
      "learning_rate": 1.252464637805401e-05,
      "loss": 0.4673,
      "step": 26160
    },
    {
      "epoch": 5.6086583797685385,
      "grad_norm": 14.66629409790039,
      "learning_rate": 1.2521788826975283e-05,
      "loss": 0.7905,
      "step": 26170
    },
    {
      "epoch": 5.610801543077582,
      "grad_norm": 16.2006893157959,
      "learning_rate": 1.2518931275896557e-05,
      "loss": 1.0883,
      "step": 26180
    },
    {
      "epoch": 5.612944706386626,
      "grad_norm": 0.5929781794548035,
      "learning_rate": 1.2516073724817833e-05,
      "loss": 0.4508,
      "step": 26190
    },
    {
      "epoch": 5.615087869695671,
      "grad_norm": 0.5451239347457886,
      "learning_rate": 1.2513216173739107e-05,
      "loss": 0.5939,
      "step": 26200
    },
    {
      "epoch": 5.617231033004715,
      "grad_norm": 14.921965599060059,
      "learning_rate": 1.251035862266038e-05,
      "loss": 0.6111,
      "step": 26210
    },
    {
      "epoch": 5.619374196313759,
      "grad_norm": 0.4950513243675232,
      "learning_rate": 1.2507501071581656e-05,
      "loss": 0.3097,
      "step": 26220
    },
    {
      "epoch": 5.621517359622803,
      "grad_norm": 0.3582362234592438,
      "learning_rate": 1.250464352050293e-05,
      "loss": 0.0097,
      "step": 26230
    },
    {
      "epoch": 5.623660522931847,
      "grad_norm": 14.777097702026367,
      "learning_rate": 1.2501785969424206e-05,
      "loss": 0.5006,
      "step": 26240
    },
    {
      "epoch": 5.625803686240891,
      "grad_norm": 0.2856254279613495,
      "learning_rate": 1.249892841834548e-05,
      "loss": 0.5089,
      "step": 26250
    },
    {
      "epoch": 5.627946849549936,
      "grad_norm": 15.465606689453125,
      "learning_rate": 1.2496070867266754e-05,
      "loss": 0.5153,
      "step": 26260
    },
    {
      "epoch": 5.63009001285898,
      "grad_norm": 0.3385990858078003,
      "learning_rate": 1.2493213316188029e-05,
      "loss": 0.3396,
      "step": 26270
    },
    {
      "epoch": 5.6322331761680235,
      "grad_norm": 0.32682254910469055,
      "learning_rate": 1.2490355765109303e-05,
      "loss": 0.3348,
      "step": 26280
    },
    {
      "epoch": 5.634376339477068,
      "grad_norm": 0.336712121963501,
      "learning_rate": 1.2487498214030577e-05,
      "loss": 0.5023,
      "step": 26290
    },
    {
      "epoch": 5.636519502786112,
      "grad_norm": 15.087602615356445,
      "learning_rate": 1.2484640662951851e-05,
      "loss": 0.6561,
      "step": 26300
    },
    {
      "epoch": 5.638662666095156,
      "grad_norm": 14.820364952087402,
      "learning_rate": 1.2481783111873125e-05,
      "loss": 0.966,
      "step": 26310
    },
    {
      "epoch": 5.640805829404201,
      "grad_norm": 14.651963233947754,
      "learning_rate": 1.2478925560794399e-05,
      "loss": 0.7884,
      "step": 26320
    },
    {
      "epoch": 5.642948992713245,
      "grad_norm": 0.4510578215122223,
      "learning_rate": 1.2476068009715674e-05,
      "loss": 0.4717,
      "step": 26330
    },
    {
      "epoch": 5.645092156022288,
      "grad_norm": 14.659738540649414,
      "learning_rate": 1.2473210458636948e-05,
      "loss": 0.9107,
      "step": 26340
    },
    {
      "epoch": 5.647235319331333,
      "grad_norm": 29.585657119750977,
      "learning_rate": 1.2470352907558222e-05,
      "loss": 0.7499,
      "step": 26350
    },
    {
      "epoch": 5.649378482640377,
      "grad_norm": 14.668397903442383,
      "learning_rate": 1.2467495356479498e-05,
      "loss": 0.9018,
      "step": 26360
    },
    {
      "epoch": 5.651521645949421,
      "grad_norm": 14.7386474609375,
      "learning_rate": 1.2464637805400772e-05,
      "loss": 0.6138,
      "step": 26370
    },
    {
      "epoch": 5.653664809258466,
      "grad_norm": 0.5172533392906189,
      "learning_rate": 1.2461780254322047e-05,
      "loss": 0.1663,
      "step": 26380
    },
    {
      "epoch": 5.6558079725675094,
      "grad_norm": 0.40232062339782715,
      "learning_rate": 1.2458922703243321e-05,
      "loss": 0.3167,
      "step": 26390
    },
    {
      "epoch": 5.657951135876553,
      "grad_norm": 0.43242400884628296,
      "learning_rate": 1.2456065152164595e-05,
      "loss": 1.1043,
      "step": 26400
    },
    {
      "epoch": 5.660094299185598,
      "grad_norm": 0.4334319531917572,
      "learning_rate": 1.2453207601085871e-05,
      "loss": 0.1631,
      "step": 26410
    },
    {
      "epoch": 5.662237462494642,
      "grad_norm": 0.3831759989261627,
      "learning_rate": 1.2450350050007145e-05,
      "loss": 0.6297,
      "step": 26420
    },
    {
      "epoch": 5.664380625803687,
      "grad_norm": 14.688793182373047,
      "learning_rate": 1.2447492498928419e-05,
      "loss": 0.9552,
      "step": 26430
    },
    {
      "epoch": 5.6665237891127305,
      "grad_norm": 0.45258140563964844,
      "learning_rate": 1.2444634947849694e-05,
      "loss": 0.4703,
      "step": 26440
    },
    {
      "epoch": 5.668666952421774,
      "grad_norm": 14.809670448303223,
      "learning_rate": 1.2441777396770968e-05,
      "loss": 0.6228,
      "step": 26450
    },
    {
      "epoch": 5.670810115730819,
      "grad_norm": 0.46794798970222473,
      "learning_rate": 1.2438919845692244e-05,
      "loss": 0.3128,
      "step": 26460
    },
    {
      "epoch": 5.672953279039863,
      "grad_norm": 0.43828046321868896,
      "learning_rate": 1.2436062294613518e-05,
      "loss": 0.626,
      "step": 26470
    },
    {
      "epoch": 5.675096442348907,
      "grad_norm": 15.17974853515625,
      "learning_rate": 1.2433204743534792e-05,
      "loss": 0.6189,
      "step": 26480
    },
    {
      "epoch": 5.6772396056579515,
      "grad_norm": 0.43313539028167725,
      "learning_rate": 1.2430347192456067e-05,
      "loss": 0.1626,
      "step": 26490
    },
    {
      "epoch": 5.679382768966995,
      "grad_norm": 0.516139030456543,
      "learning_rate": 1.2427489641377341e-05,
      "loss": 1.082,
      "step": 26500
    },
    {
      "epoch": 5.681525932276039,
      "grad_norm": 0.5175149440765381,
      "learning_rate": 1.2424632090298617e-05,
      "loss": 0.3101,
      "step": 26510
    },
    {
      "epoch": 5.683669095585084,
      "grad_norm": 14.652444839477539,
      "learning_rate": 1.242177453921989e-05,
      "loss": 0.4681,
      "step": 26520
    },
    {
      "epoch": 5.685812258894128,
      "grad_norm": 0.46301040053367615,
      "learning_rate": 1.2418916988141163e-05,
      "loss": 0.3142,
      "step": 26530
    },
    {
      "epoch": 5.687955422203172,
      "grad_norm": 0.44495904445648193,
      "learning_rate": 1.2416059437062437e-05,
      "loss": 0.3148,
      "step": 26540
    },
    {
      "epoch": 5.690098585512216,
      "grad_norm": 0.3848714232444763,
      "learning_rate": 1.2413201885983713e-05,
      "loss": 0.3165,
      "step": 26550
    },
    {
      "epoch": 5.69224174882126,
      "grad_norm": 0.3503133952617645,
      "learning_rate": 1.2410344334904987e-05,
      "loss": 0.6492,
      "step": 26560
    },
    {
      "epoch": 5.694384912130304,
      "grad_norm": 0.40779539942741394,
      "learning_rate": 1.240748678382626e-05,
      "loss": 1.1279,
      "step": 26570
    },
    {
      "epoch": 5.696528075439349,
      "grad_norm": 0.4651901423931122,
      "learning_rate": 1.2404629232747536e-05,
      "loss": 0.6251,
      "step": 26580
    },
    {
      "epoch": 5.698671238748393,
      "grad_norm": 0.48813873529434204,
      "learning_rate": 1.240177168166881e-05,
      "loss": 1.0794,
      "step": 26590
    },
    {
      "epoch": 5.7008144020574365,
      "grad_norm": 14.75772762298584,
      "learning_rate": 1.2398914130590086e-05,
      "loss": 0.6131,
      "step": 26600
    },
    {
      "epoch": 5.702957565366481,
      "grad_norm": 14.586540222167969,
      "learning_rate": 1.239605657951136e-05,
      "loss": 0.6106,
      "step": 26610
    },
    {
      "epoch": 5.705100728675525,
      "grad_norm": 0.5072964429855347,
      "learning_rate": 1.2393199028432634e-05,
      "loss": 0.4644,
      "step": 26620
    },
    {
      "epoch": 5.707243891984569,
      "grad_norm": 0.48979729413986206,
      "learning_rate": 1.239034147735391e-05,
      "loss": 0.3129,
      "step": 26630
    },
    {
      "epoch": 5.709387055293614,
      "grad_norm": 0.4549490809440613,
      "learning_rate": 1.2387483926275183e-05,
      "loss": 0.4613,
      "step": 26640
    },
    {
      "epoch": 5.7115302186026575,
      "grad_norm": 0.4469435513019562,
      "learning_rate": 1.2384626375196459e-05,
      "loss": 1.0925,
      "step": 26650
    },
    {
      "epoch": 5.713673381911701,
      "grad_norm": 0.48465684056282043,
      "learning_rate": 1.2381768824117733e-05,
      "loss": 0.4712,
      "step": 26660
    },
    {
      "epoch": 5.715816545220746,
      "grad_norm": 14.66632080078125,
      "learning_rate": 1.2378911273039007e-05,
      "loss": 0.9287,
      "step": 26670
    },
    {
      "epoch": 5.71795970852979,
      "grad_norm": 0.515701413154602,
      "learning_rate": 1.2376053721960282e-05,
      "loss": 0.3088,
      "step": 26680
    },
    {
      "epoch": 5.720102871838834,
      "grad_norm": 0.5248610377311707,
      "learning_rate": 1.2373196170881556e-05,
      "loss": 0.4571,
      "step": 26690
    },
    {
      "epoch": 5.7222460351478786,
      "grad_norm": 14.641207695007324,
      "learning_rate": 1.237033861980283e-05,
      "loss": 0.6142,
      "step": 26700
    },
    {
      "epoch": 5.724389198456922,
      "grad_norm": 0.5058627724647522,
      "learning_rate": 1.2367481068724106e-05,
      "loss": 0.4656,
      "step": 26710
    },
    {
      "epoch": 5.726532361765966,
      "grad_norm": 0.47522372007369995,
      "learning_rate": 1.236462351764538e-05,
      "loss": 0.3099,
      "step": 26720
    },
    {
      "epoch": 5.728675525075011,
      "grad_norm": 0.35572484135627747,
      "learning_rate": 1.2361765966566652e-05,
      "loss": 0.3295,
      "step": 26730
    },
    {
      "epoch": 5.730818688384055,
      "grad_norm": 0.34662488102912903,
      "learning_rate": 1.2358908415487927e-05,
      "loss": 0.6565,
      "step": 26740
    },
    {
      "epoch": 5.732961851693099,
      "grad_norm": 0.33829405903816223,
      "learning_rate": 1.2356050864409201e-05,
      "loss": 0.4954,
      "step": 26750
    },
    {
      "epoch": 5.735105015002143,
      "grad_norm": 0.33791500329971313,
      "learning_rate": 1.2353193313330475e-05,
      "loss": 0.3355,
      "step": 26760
    },
    {
      "epoch": 5.737248178311187,
      "grad_norm": 0.3403051495552063,
      "learning_rate": 1.2350335762251751e-05,
      "loss": 0.6613,
      "step": 26770
    },
    {
      "epoch": 5.739391341620231,
      "grad_norm": 14.805651664733887,
      "learning_rate": 1.2347478211173025e-05,
      "loss": 0.4957,
      "step": 26780
    },
    {
      "epoch": 5.741534504929276,
      "grad_norm": 0.34857410192489624,
      "learning_rate": 1.23446206600943e-05,
      "loss": 0.4954,
      "step": 26790
    },
    {
      "epoch": 5.74367766823832,
      "grad_norm": 0.37184247374534607,
      "learning_rate": 1.2341763109015574e-05,
      "loss": 0.4912,
      "step": 26800
    },
    {
      "epoch": 5.745820831547364,
      "grad_norm": 29.94978141784668,
      "learning_rate": 1.2338905557936848e-05,
      "loss": 0.3347,
      "step": 26810
    },
    {
      "epoch": 5.747963994856408,
      "grad_norm": 0.3231189250946045,
      "learning_rate": 1.2336048006858124e-05,
      "loss": 0.0074,
      "step": 26820
    },
    {
      "epoch": 5.750107158165452,
      "grad_norm": 0.28376832604408264,
      "learning_rate": 1.2333190455779398e-05,
      "loss": 0.5037,
      "step": 26830
    },
    {
      "epoch": 5.752250321474496,
      "grad_norm": 14.752575874328613,
      "learning_rate": 1.2330332904700672e-05,
      "loss": 0.8349,
      "step": 26840
    },
    {
      "epoch": 5.754393484783541,
      "grad_norm": 0.36809009313583374,
      "learning_rate": 1.2327475353621947e-05,
      "loss": 1.156,
      "step": 26850
    },
    {
      "epoch": 5.756536648092585,
      "grad_norm": 0.40563899278640747,
      "learning_rate": 1.2324617802543221e-05,
      "loss": 0.4843,
      "step": 26860
    },
    {
      "epoch": 5.758679811401628,
      "grad_norm": 0.41375285387039185,
      "learning_rate": 1.2321760251464497e-05,
      "loss": 0.3237,
      "step": 26870
    },
    {
      "epoch": 5.760822974710673,
      "grad_norm": 0.4128931760787964,
      "learning_rate": 1.2318902700385771e-05,
      "loss": 0.476,
      "step": 26880
    },
    {
      "epoch": 5.762966138019717,
      "grad_norm": 14.782624244689941,
      "learning_rate": 1.2316045149307045e-05,
      "loss": 0.4811,
      "step": 26890
    },
    {
      "epoch": 5.765109301328761,
      "grad_norm": 0.3908730745315552,
      "learning_rate": 1.231318759822832e-05,
      "loss": 0.6394,
      "step": 26900
    },
    {
      "epoch": 5.767252464637806,
      "grad_norm": 0.4320642948150635,
      "learning_rate": 1.2310330047149594e-05,
      "loss": 0.79,
      "step": 26910
    },
    {
      "epoch": 5.7693956279468495,
      "grad_norm": 14.621755599975586,
      "learning_rate": 1.2307472496070868e-05,
      "loss": 0.4744,
      "step": 26920
    },
    {
      "epoch": 5.771538791255894,
      "grad_norm": 14.669072151184082,
      "learning_rate": 1.2304614944992144e-05,
      "loss": 0.775,
      "step": 26930
    },
    {
      "epoch": 5.773681954564938,
      "grad_norm": 0.4880674481391907,
      "learning_rate": 1.2301757393913418e-05,
      "loss": 0.162,
      "step": 26940
    },
    {
      "epoch": 5.775825117873982,
      "grad_norm": 14.85749626159668,
      "learning_rate": 1.229889984283469e-05,
      "loss": 0.7824,
      "step": 26950
    },
    {
      "epoch": 5.777968281183027,
      "grad_norm": 0.4906136393547058,
      "learning_rate": 1.2296042291755966e-05,
      "loss": 0.6198,
      "step": 26960
    },
    {
      "epoch": 5.7801114444920705,
      "grad_norm": 0.5051807165145874,
      "learning_rate": 1.229318474067724e-05,
      "loss": 0.4586,
      "step": 26970
    },
    {
      "epoch": 5.782254607801114,
      "grad_norm": 0.5306496620178223,
      "learning_rate": 1.2290327189598514e-05,
      "loss": 0.9061,
      "step": 26980
    },
    {
      "epoch": 5.784397771110159,
      "grad_norm": 0.4905668795108795,
      "learning_rate": 1.228746963851979e-05,
      "loss": 0.3086,
      "step": 26990
    },
    {
      "epoch": 5.786540934419203,
      "grad_norm": 14.758882522583008,
      "learning_rate": 1.2284612087441063e-05,
      "loss": 0.7539,
      "step": 27000
    },
    {
      "epoch": 5.788684097728247,
      "grad_norm": 15.091109275817871,
      "learning_rate": 1.2281754536362339e-05,
      "loss": 0.7525,
      "step": 27010
    },
    {
      "epoch": 5.7908272610372915,
      "grad_norm": 0.6279197335243225,
      "learning_rate": 1.2278896985283613e-05,
      "loss": 0.899,
      "step": 27020
    },
    {
      "epoch": 5.792970424346335,
      "grad_norm": 0.60558021068573,
      "learning_rate": 1.2276039434204887e-05,
      "loss": 0.4398,
      "step": 27030
    },
    {
      "epoch": 5.795113587655379,
      "grad_norm": 14.50693416595459,
      "learning_rate": 1.2273181883126162e-05,
      "loss": 0.7172,
      "step": 27040
    },
    {
      "epoch": 5.797256750964424,
      "grad_norm": 14.558076858520508,
      "learning_rate": 1.2270324332047436e-05,
      "loss": 0.4455,
      "step": 27050
    },
    {
      "epoch": 5.799399914273468,
      "grad_norm": 0.562855064868927,
      "learning_rate": 1.226746678096871e-05,
      "loss": 0.3009,
      "step": 27060
    },
    {
      "epoch": 5.801543077582512,
      "grad_norm": 14.54330062866211,
      "learning_rate": 1.2264609229889986e-05,
      "loss": 0.7425,
      "step": 27070
    },
    {
      "epoch": 5.803686240891556,
      "grad_norm": 14.630105972290039,
      "learning_rate": 1.226175167881126e-05,
      "loss": 0.1665,
      "step": 27080
    },
    {
      "epoch": 5.8058294042006,
      "grad_norm": 0.45337167382240295,
      "learning_rate": 1.2258894127732535e-05,
      "loss": 0.6066,
      "step": 27090
    },
    {
      "epoch": 5.807972567509644,
      "grad_norm": 0.42892783880233765,
      "learning_rate": 1.225603657665381e-05,
      "loss": 0.1584,
      "step": 27100
    },
    {
      "epoch": 5.810115730818689,
      "grad_norm": 0.41314497590065,
      "learning_rate": 1.2253179025575083e-05,
      "loss": 0.4827,
      "step": 27110
    },
    {
      "epoch": 5.812258894127733,
      "grad_norm": 14.751029014587402,
      "learning_rate": 1.2250321474496359e-05,
      "loss": 0.6377,
      "step": 27120
    },
    {
      "epoch": 5.8144020574367765,
      "grad_norm": 14.682167053222656,
      "learning_rate": 1.2247463923417633e-05,
      "loss": 0.6348,
      "step": 27130
    },
    {
      "epoch": 5.816545220745821,
      "grad_norm": 0.4123528301715851,
      "learning_rate": 1.2244606372338907e-05,
      "loss": 0.4757,
      "step": 27140
    },
    {
      "epoch": 5.818688384054865,
      "grad_norm": 0.37720438838005066,
      "learning_rate": 1.2241748821260182e-05,
      "loss": 0.4711,
      "step": 27150
    },
    {
      "epoch": 5.820831547363909,
      "grad_norm": 0.40302664041519165,
      "learning_rate": 1.2238891270181454e-05,
      "loss": 0.3238,
      "step": 27160
    },
    {
      "epoch": 5.822974710672954,
      "grad_norm": 0.3421362042427063,
      "learning_rate": 1.2236033719102728e-05,
      "loss": 0.3323,
      "step": 27170
    },
    {
      "epoch": 5.8251178739819975,
      "grad_norm": 0.3373887836933136,
      "learning_rate": 1.2233176168024004e-05,
      "loss": 0.4933,
      "step": 27180
    },
    {
      "epoch": 5.827261037291041,
      "grad_norm": 29.844886779785156,
      "learning_rate": 1.2230318616945278e-05,
      "loss": 0.3348,
      "step": 27190
    },
    {
      "epoch": 5.829404200600086,
      "grad_norm": 0.3196716904640198,
      "learning_rate": 1.2227461065866552e-05,
      "loss": 0.338,
      "step": 27200
    },
    {
      "epoch": 5.83154736390913,
      "grad_norm": 14.773825645446777,
      "learning_rate": 1.2224603514787828e-05,
      "loss": 1.1629,
      "step": 27210
    },
    {
      "epoch": 5.833690527218174,
      "grad_norm": 15.00317668914795,
      "learning_rate": 1.2221745963709101e-05,
      "loss": 0.6538,
      "step": 27220
    },
    {
      "epoch": 5.835833690527219,
      "grad_norm": 0.4445084035396576,
      "learning_rate": 1.2218888412630377e-05,
      "loss": 0.6398,
      "step": 27230
    },
    {
      "epoch": 5.837976853836262,
      "grad_norm": 0.5073041319847107,
      "learning_rate": 1.2216030861551651e-05,
      "loss": 0.4688,
      "step": 27240
    },
    {
      "epoch": 5.840120017145306,
      "grad_norm": 0.5753786563873291,
      "learning_rate": 1.2213173310472925e-05,
      "loss": 0.7818,
      "step": 27250
    },
    {
      "epoch": 5.842263180454351,
      "grad_norm": 0.4476214647293091,
      "learning_rate": 1.22103157593942e-05,
      "loss": 0.4671,
      "step": 27260
    },
    {
      "epoch": 5.844406343763395,
      "grad_norm": 0.47718921303749084,
      "learning_rate": 1.2207458208315474e-05,
      "loss": 0.619,
      "step": 27270
    },
    {
      "epoch": 5.846549507072439,
      "grad_norm": 0.5356578826904297,
      "learning_rate": 1.2204600657236748e-05,
      "loss": 0.9171,
      "step": 27280
    },
    {
      "epoch": 5.848692670381483,
      "grad_norm": 14.569881439208984,
      "learning_rate": 1.2201743106158024e-05,
      "loss": 0.75,
      "step": 27290
    },
    {
      "epoch": 5.850835833690527,
      "grad_norm": 0.5433799624443054,
      "learning_rate": 1.2198885555079298e-05,
      "loss": 0.1584,
      "step": 27300
    },
    {
      "epoch": 5.852978996999571,
      "grad_norm": 0.5227927565574646,
      "learning_rate": 1.2196028004000574e-05,
      "loss": 0.7555,
      "step": 27310
    },
    {
      "epoch": 5.855122160308616,
      "grad_norm": 0.42992714047431946,
      "learning_rate": 1.2193170452921848e-05,
      "loss": 0.1641,
      "step": 27320
    },
    {
      "epoch": 5.85726532361766,
      "grad_norm": 0.436126172542572,
      "learning_rate": 1.2190312901843121e-05,
      "loss": 0.4689,
      "step": 27330
    },
    {
      "epoch": 5.859408486926704,
      "grad_norm": 15.902270317077637,
      "learning_rate": 1.2187455350764397e-05,
      "loss": 0.9382,
      "step": 27340
    },
    {
      "epoch": 5.861551650235748,
      "grad_norm": 0.5179041028022766,
      "learning_rate": 1.2184597799685671e-05,
      "loss": 0.7658,
      "step": 27350
    },
    {
      "epoch": 5.863694813544792,
      "grad_norm": 0.5444786548614502,
      "learning_rate": 1.2181740248606947e-05,
      "loss": 0.3066,
      "step": 27360
    },
    {
      "epoch": 5.865837976853836,
      "grad_norm": 0.5401694774627686,
      "learning_rate": 1.217888269752822e-05,
      "loss": 0.5992,
      "step": 27370
    },
    {
      "epoch": 5.867981140162881,
      "grad_norm": 14.555625915527344,
      "learning_rate": 1.2176025146449493e-05,
      "loss": 1.0425,
      "step": 27380
    },
    {
      "epoch": 5.870124303471925,
      "grad_norm": 0.5603577494621277,
      "learning_rate": 1.2173167595370767e-05,
      "loss": 0.5973,
      "step": 27390
    },
    {
      "epoch": 5.8722674667809684,
      "grad_norm": 16.468040466308594,
      "learning_rate": 1.2170310044292042e-05,
      "loss": 0.3145,
      "step": 27400
    },
    {
      "epoch": 5.874410630090013,
      "grad_norm": 0.347551167011261,
      "learning_rate": 1.2167452493213316e-05,
      "loss": 0.3269,
      "step": 27410
    },
    {
      "epoch": 5.876553793399057,
      "grad_norm": 0.40667182207107544,
      "learning_rate": 1.216459494213459e-05,
      "loss": 0.9665,
      "step": 27420
    },
    {
      "epoch": 5.878696956708101,
      "grad_norm": 0.40120407938957214,
      "learning_rate": 1.2161737391055866e-05,
      "loss": 0.6376,
      "step": 27430
    },
    {
      "epoch": 5.880840120017146,
      "grad_norm": 0.40999406576156616,
      "learning_rate": 1.215887983997714e-05,
      "loss": 0.48,
      "step": 27440
    },
    {
      "epoch": 5.8829832833261895,
      "grad_norm": 14.707635879516602,
      "learning_rate": 1.2156022288898415e-05,
      "loss": 0.6409,
      "step": 27450
    },
    {
      "epoch": 5.885126446635233,
      "grad_norm": 0.438512921333313,
      "learning_rate": 1.215316473781969e-05,
      "loss": 0.7848,
      "step": 27460
    },
    {
      "epoch": 5.887269609944278,
      "grad_norm": 14.64372730255127,
      "learning_rate": 1.2150307186740963e-05,
      "loss": 0.4717,
      "step": 27470
    },
    {
      "epoch": 5.889412773253322,
      "grad_norm": 14.813324928283691,
      "learning_rate": 1.2147449635662239e-05,
      "loss": 0.6247,
      "step": 27480
    },
    {
      "epoch": 5.891555936562366,
      "grad_norm": 0.4435340464115143,
      "learning_rate": 1.2144592084583513e-05,
      "loss": 0.4721,
      "step": 27490
    },
    {
      "epoch": 5.8936990998714105,
      "grad_norm": 0.4409621059894562,
      "learning_rate": 1.2141734533504788e-05,
      "loss": 0.6332,
      "step": 27500
    },
    {
      "epoch": 5.895842263180454,
      "grad_norm": 0.3850414454936981,
      "learning_rate": 1.2138876982426062e-05,
      "loss": 0.3272,
      "step": 27510
    },
    {
      "epoch": 5.897985426489498,
      "grad_norm": 14.71906852722168,
      "learning_rate": 1.2136019431347336e-05,
      "loss": 0.9591,
      "step": 27520
    },
    {
      "epoch": 5.900128589798543,
      "grad_norm": 14.741997718811035,
      "learning_rate": 1.2133161880268612e-05,
      "loss": 1.1146,
      "step": 27530
    },
    {
      "epoch": 5.902271753107587,
      "grad_norm": 0.4824731945991516,
      "learning_rate": 1.2130304329189886e-05,
      "loss": 0.3141,
      "step": 27540
    },
    {
      "epoch": 5.904414916416631,
      "grad_norm": 0.4192691445350647,
      "learning_rate": 1.212744677811116e-05,
      "loss": 0.4681,
      "step": 27550
    },
    {
      "epoch": 5.906558079725675,
      "grad_norm": 0.45450112223625183,
      "learning_rate": 1.2124589227032435e-05,
      "loss": 0.7752,
      "step": 27560
    },
    {
      "epoch": 5.908701243034719,
      "grad_norm": 14.621829986572266,
      "learning_rate": 1.212173167595371e-05,
      "loss": 0.6181,
      "step": 27570
    },
    {
      "epoch": 5.910844406343763,
      "grad_norm": 29.649009704589844,
      "learning_rate": 1.2118874124874985e-05,
      "loss": 0.9295,
      "step": 27580
    },
    {
      "epoch": 5.912987569652808,
      "grad_norm": 0.4486900269985199,
      "learning_rate": 1.2116016573796257e-05,
      "loss": 0.4629,
      "step": 27590
    },
    {
      "epoch": 5.915130732961852,
      "grad_norm": 0.5005866289138794,
      "learning_rate": 1.2113159022717531e-05,
      "loss": 0.3212,
      "step": 27600
    },
    {
      "epoch": 5.9172738962708955,
      "grad_norm": 14.849156379699707,
      "learning_rate": 1.2110301471638805e-05,
      "loss": 1.0814,
      "step": 27610
    },
    {
      "epoch": 5.91941705957994,
      "grad_norm": 0.4522157609462738,
      "learning_rate": 1.210744392056008e-05,
      "loss": 0.3173,
      "step": 27620
    },
    {
      "epoch": 5.921560222888984,
      "grad_norm": 0.42482754588127136,
      "learning_rate": 1.2104586369481355e-05,
      "loss": 0.3186,
      "step": 27630
    },
    {
      "epoch": 5.923703386198028,
      "grad_norm": 29.65907096862793,
      "learning_rate": 1.210172881840263e-05,
      "loss": 0.9466,
      "step": 27640
    },
    {
      "epoch": 5.925846549507073,
      "grad_norm": 14.670372009277344,
      "learning_rate": 1.2098871267323904e-05,
      "loss": 0.7928,
      "step": 27650
    },
    {
      "epoch": 5.9279897128161165,
      "grad_norm": 0.5344876646995544,
      "learning_rate": 1.2096013716245178e-05,
      "loss": 0.6176,
      "step": 27660
    },
    {
      "epoch": 5.93013287612516,
      "grad_norm": 0.5597254633903503,
      "learning_rate": 1.2093156165166454e-05,
      "loss": 0.6079,
      "step": 27670
    },
    {
      "epoch": 5.932276039434205,
      "grad_norm": 14.861833572387695,
      "learning_rate": 1.2090298614087728e-05,
      "loss": 0.5995,
      "step": 27680
    },
    {
      "epoch": 5.934419202743249,
      "grad_norm": 14.540689468383789,
      "learning_rate": 1.2087441063009001e-05,
      "loss": 0.596,
      "step": 27690
    },
    {
      "epoch": 5.936562366052293,
      "grad_norm": 0.5462749600410461,
      "learning_rate": 1.2084583511930277e-05,
      "loss": 0.1647,
      "step": 27700
    },
    {
      "epoch": 5.9387055293613376,
      "grad_norm": 0.484998881816864,
      "learning_rate": 1.2081725960851551e-05,
      "loss": 0.3093,
      "step": 27710
    },
    {
      "epoch": 5.940848692670381,
      "grad_norm": 14.592084884643555,
      "learning_rate": 1.2078868409772827e-05,
      "loss": 1.2226,
      "step": 27720
    },
    {
      "epoch": 5.942991855979425,
      "grad_norm": 0.5273150205612183,
      "learning_rate": 1.20760108586941e-05,
      "loss": 0.6093,
      "step": 27730
    },
    {
      "epoch": 5.94513501928847,
      "grad_norm": 14.605948448181152,
      "learning_rate": 1.2073153307615375e-05,
      "loss": 0.9032,
      "step": 27740
    },
    {
      "epoch": 5.947278182597514,
      "grad_norm": 14.548501968383789,
      "learning_rate": 1.207029575653665e-05,
      "loss": 0.4563,
      "step": 27750
    },
    {
      "epoch": 5.949421345906558,
      "grad_norm": 0.5813563466072083,
      "learning_rate": 1.2067438205457924e-05,
      "loss": 0.8952,
      "step": 27760
    },
    {
      "epoch": 5.951564509215602,
      "grad_norm": 14.58170223236084,
      "learning_rate": 1.2064580654379198e-05,
      "loss": 1.0028,
      "step": 27770
    },
    {
      "epoch": 5.953707672524646,
      "grad_norm": 0.6869748830795288,
      "learning_rate": 1.2061723103300474e-05,
      "loss": 0.5732,
      "step": 27780
    },
    {
      "epoch": 5.95585083583369,
      "grad_norm": 0.6952144503593445,
      "learning_rate": 1.2058865552221748e-05,
      "loss": 0.7115,
      "step": 27790
    },
    {
      "epoch": 5.957993999142735,
      "grad_norm": 14.543661117553711,
      "learning_rate": 1.2056008001143023e-05,
      "loss": 0.8498,
      "step": 27800
    },
    {
      "epoch": 5.960137162451779,
      "grad_norm": 0.8098621964454651,
      "learning_rate": 1.2053150450064295e-05,
      "loss": 0.9708,
      "step": 27810
    },
    {
      "epoch": 5.962280325760823,
      "grad_norm": 14.32528305053711,
      "learning_rate": 1.205029289898557e-05,
      "loss": 0.4131,
      "step": 27820
    },
    {
      "epoch": 5.964423489069867,
      "grad_norm": 0.715160071849823,
      "learning_rate": 1.2047435347906843e-05,
      "loss": 0.4314,
      "step": 27830
    },
    {
      "epoch": 5.966566652378911,
      "grad_norm": 14.716167449951172,
      "learning_rate": 1.2044577796828119e-05,
      "loss": 0.8509,
      "step": 27840
    },
    {
      "epoch": 5.968709815687955,
      "grad_norm": 0.7391695380210876,
      "learning_rate": 1.2041720245749393e-05,
      "loss": 0.8386,
      "step": 27850
    },
    {
      "epoch": 5.970852978997,
      "grad_norm": 14.448600769042969,
      "learning_rate": 1.2038862694670668e-05,
      "loss": 0.4267,
      "step": 27860
    },
    {
      "epoch": 5.972996142306044,
      "grad_norm": 0.6455646753311157,
      "learning_rate": 1.2036005143591942e-05,
      "loss": 0.5679,
      "step": 27870
    },
    {
      "epoch": 5.975139305615087,
      "grad_norm": 0.5507777333259583,
      "learning_rate": 1.2033147592513216e-05,
      "loss": 0.2974,
      "step": 27880
    },
    {
      "epoch": 5.977282468924132,
      "grad_norm": 0.4870871305465698,
      "learning_rate": 1.2030290041434492e-05,
      "loss": 0.4477,
      "step": 27890
    },
    {
      "epoch": 5.979425632233176,
      "grad_norm": 14.574897766113281,
      "learning_rate": 1.2027432490355766e-05,
      "loss": 0.608,
      "step": 27900
    },
    {
      "epoch": 5.98156879554222,
      "grad_norm": 0.5542707443237305,
      "learning_rate": 1.202457493927704e-05,
      "loss": 0.9022,
      "step": 27910
    },
    {
      "epoch": 5.983711958851265,
      "grad_norm": 0.54647296667099,
      "learning_rate": 1.2021717388198315e-05,
      "loss": 0.6097,
      "step": 27920
    },
    {
      "epoch": 5.9858551221603085,
      "grad_norm": 0.634388267993927,
      "learning_rate": 1.201885983711959e-05,
      "loss": 0.8819,
      "step": 27930
    },
    {
      "epoch": 5.987998285469352,
      "grad_norm": 0.5894747376441956,
      "learning_rate": 1.2016002286040865e-05,
      "loss": 0.7291,
      "step": 27940
    },
    {
      "epoch": 5.990141448778397,
      "grad_norm": 14.734285354614258,
      "learning_rate": 1.2013144734962139e-05,
      "loss": 0.5943,
      "step": 27950
    },
    {
      "epoch": 5.992284612087441,
      "grad_norm": 0.6154151558876038,
      "learning_rate": 1.2010287183883413e-05,
      "loss": 0.3014,
      "step": 27960
    },
    {
      "epoch": 5.994427775396485,
      "grad_norm": 0.5578338503837585,
      "learning_rate": 1.2007429632804688e-05,
      "loss": 0.4519,
      "step": 27970
    },
    {
      "epoch": 5.9965709387055295,
      "grad_norm": 14.55666446685791,
      "learning_rate": 1.2004572081725962e-05,
      "loss": 0.5982,
      "step": 27980
    },
    {
      "epoch": 5.998714102014573,
      "grad_norm": 0.47862255573272705,
      "learning_rate": 1.2001714530647236e-05,
      "loss": 0.161,
      "step": 27990
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.874,
      "eval_f1": 0.010471204188481676,
      "eval_loss": 0.5849413275718689,
      "eval_precision": 0.6666666666666666,
      "eval_recall": 0.005277044854881266,
      "eval_runtime": 704.9115,
      "eval_samples_per_second": 4.256,
      "eval_steps_per_second": 1.419,
      "step": 27996
    },
    {
      "epoch": 6.000857265323618,
      "grad_norm": 0.46500083804130554,
      "learning_rate": 1.1998856979568512e-05,
      "loss": 0.7697,
      "step": 28000
    },
    {
      "epoch": 6.003000428632662,
      "grad_norm": 0.42851245403289795,
      "learning_rate": 1.1995999428489786e-05,
      "loss": 0.9462,
      "step": 28010
    },
    {
      "epoch": 6.005143591941706,
      "grad_norm": 14.727603912353516,
      "learning_rate": 1.1993141877411058e-05,
      "loss": 0.4697,
      "step": 28020
    },
    {
      "epoch": 6.0072867552507505,
      "grad_norm": 0.5107513070106506,
      "learning_rate": 1.1990284326332334e-05,
      "loss": 0.7762,
      "step": 28030
    },
    {
      "epoch": 6.009429918559794,
      "grad_norm": 29.945192337036133,
      "learning_rate": 1.1987426775253608e-05,
      "loss": 0.8198,
      "step": 28040
    },
    {
      "epoch": 6.011573081868838,
      "grad_norm": 0.51702481508255,
      "learning_rate": 1.1984569224174882e-05,
      "loss": 0.1583,
      "step": 28050
    },
    {
      "epoch": 6.013716245177883,
      "grad_norm": 14.673285484313965,
      "learning_rate": 1.1981711673096157e-05,
      "loss": 1.2215,
      "step": 28060
    },
    {
      "epoch": 6.015859408486927,
      "grad_norm": 0.5629438161849976,
      "learning_rate": 1.1978854122017431e-05,
      "loss": 0.7481,
      "step": 28070
    },
    {
      "epoch": 6.018002571795971,
      "grad_norm": 0.599991500377655,
      "learning_rate": 1.1975996570938707e-05,
      "loss": 0.5902,
      "step": 28080
    },
    {
      "epoch": 6.020145735105015,
      "grad_norm": 0.5289722084999084,
      "learning_rate": 1.197313901985998e-05,
      "loss": 0.3076,
      "step": 28090
    },
    {
      "epoch": 6.022288898414059,
      "grad_norm": 14.64712905883789,
      "learning_rate": 1.1970281468781255e-05,
      "loss": 0.4581,
      "step": 28100
    },
    {
      "epoch": 6.024432061723103,
      "grad_norm": 0.4920084476470947,
      "learning_rate": 1.196742391770253e-05,
      "loss": 0.7688,
      "step": 28110
    },
    {
      "epoch": 6.026575225032148,
      "grad_norm": 14.779496192932129,
      "learning_rate": 1.1964566366623804e-05,
      "loss": 1.0587,
      "step": 28120
    },
    {
      "epoch": 6.028718388341192,
      "grad_norm": 14.676032066345215,
      "learning_rate": 1.1961708815545078e-05,
      "loss": 0.5963,
      "step": 28130
    },
    {
      "epoch": 6.0308615516502355,
      "grad_norm": 29.521299362182617,
      "learning_rate": 1.1958851264466354e-05,
      "loss": 0.4534,
      "step": 28140
    },
    {
      "epoch": 6.03300471495928,
      "grad_norm": 0.512978732585907,
      "learning_rate": 1.1955993713387628e-05,
      "loss": 0.0124,
      "step": 28150
    },
    {
      "epoch": 6.035147878268324,
      "grad_norm": 0.4707123339176178,
      "learning_rate": 1.1953136162308903e-05,
      "loss": 0.4634,
      "step": 28160
    },
    {
      "epoch": 6.037291041577368,
      "grad_norm": 0.4213305711746216,
      "learning_rate": 1.1950278611230177e-05,
      "loss": 0.1605,
      "step": 28170
    },
    {
      "epoch": 6.039434204886413,
      "grad_norm": 0.397687703371048,
      "learning_rate": 1.1947421060151451e-05,
      "loss": 0.4821,
      "step": 28180
    },
    {
      "epoch": 6.0415773681954565,
      "grad_norm": 0.4101944863796234,
      "learning_rate": 1.1944563509072727e-05,
      "loss": 0.4784,
      "step": 28190
    },
    {
      "epoch": 6.0437205315045,
      "grad_norm": 0.40680330991744995,
      "learning_rate": 1.1941705957994e-05,
      "loss": 0.3255,
      "step": 28200
    },
    {
      "epoch": 6.045863694813545,
      "grad_norm": 14.660270690917969,
      "learning_rate": 1.1938848406915276e-05,
      "loss": 0.6292,
      "step": 28210
    },
    {
      "epoch": 6.048006858122589,
      "grad_norm": 0.467835396528244,
      "learning_rate": 1.193599085583655e-05,
      "loss": 0.78,
      "step": 28220
    },
    {
      "epoch": 6.050150021431633,
      "grad_norm": 14.589780807495117,
      "learning_rate": 1.1933133304757824e-05,
      "loss": 0.6168,
      "step": 28230
    },
    {
      "epoch": 6.052293184740678,
      "grad_norm": 0.5174201726913452,
      "learning_rate": 1.1930275753679096e-05,
      "loss": 0.9175,
      "step": 28240
    },
    {
      "epoch": 6.054436348049721,
      "grad_norm": 0.5755574703216553,
      "learning_rate": 1.1927418202600372e-05,
      "loss": 1.0417,
      "step": 28250
    },
    {
      "epoch": 6.056579511358765,
      "grad_norm": 14.489497184753418,
      "learning_rate": 1.1924560651521646e-05,
      "loss": 0.5897,
      "step": 28260
    },
    {
      "epoch": 6.05872267466781,
      "grad_norm": 0.5756244659423828,
      "learning_rate": 1.192170310044292e-05,
      "loss": 0.5855,
      "step": 28270
    },
    {
      "epoch": 6.060865837976854,
      "grad_norm": 0.6297482848167419,
      "learning_rate": 1.1918845549364195e-05,
      "loss": 0.5877,
      "step": 28280
    },
    {
      "epoch": 6.063009001285898,
      "grad_norm": 14.466109275817871,
      "learning_rate": 1.191598799828547e-05,
      "loss": 0.8683,
      "step": 28290
    },
    {
      "epoch": 6.065152164594942,
      "grad_norm": 0.6867338418960571,
      "learning_rate": 1.1913130447206745e-05,
      "loss": 1.0019,
      "step": 28300
    },
    {
      "epoch": 6.067295327903986,
      "grad_norm": 0.7659258842468262,
      "learning_rate": 1.1910272896128019e-05,
      "loss": 0.4284,
      "step": 28310
    },
    {
      "epoch": 6.06943849121303,
      "grad_norm": 0.644554853439331,
      "learning_rate": 1.1907415345049293e-05,
      "loss": 0.707,
      "step": 28320
    },
    {
      "epoch": 6.071581654522075,
      "grad_norm": 0.7504780888557434,
      "learning_rate": 1.1904557793970568e-05,
      "loss": 0.9907,
      "step": 28330
    },
    {
      "epoch": 6.073724817831119,
      "grad_norm": 29.499460220336914,
      "learning_rate": 1.1901700242891842e-05,
      "loss": 1.2363,
      "step": 28340
    },
    {
      "epoch": 6.075867981140163,
      "grad_norm": 14.482070922851562,
      "learning_rate": 1.1898842691813118e-05,
      "loss": 0.154,
      "step": 28350
    },
    {
      "epoch": 6.078011144449207,
      "grad_norm": 0.6562936305999756,
      "learning_rate": 1.1895985140734392e-05,
      "loss": 0.5708,
      "step": 28360
    },
    {
      "epoch": 6.080154307758251,
      "grad_norm": 0.7520696520805359,
      "learning_rate": 1.1893127589655666e-05,
      "loss": 1.2722,
      "step": 28370
    },
    {
      "epoch": 6.082297471067295,
      "grad_norm": 0.8392730355262756,
      "learning_rate": 1.1890270038576942e-05,
      "loss": 0.8227,
      "step": 28380
    },
    {
      "epoch": 6.08444063437634,
      "grad_norm": 0.8225560784339905,
      "learning_rate": 1.1887412487498215e-05,
      "loss": 0.4184,
      "step": 28390
    },
    {
      "epoch": 6.086583797685384,
      "grad_norm": 14.448393821716309,
      "learning_rate": 1.188455493641949e-05,
      "loss": 0.8212,
      "step": 28400
    },
    {
      "epoch": 6.0887269609944275,
      "grad_norm": 14.888419151306152,
      "learning_rate": 1.1881697385340765e-05,
      "loss": 0.6877,
      "step": 28410
    },
    {
      "epoch": 6.090870124303472,
      "grad_norm": 14.281379699707031,
      "learning_rate": 1.1878839834262039e-05,
      "loss": 0.8135,
      "step": 28420
    },
    {
      "epoch": 6.093013287612516,
      "grad_norm": 0.8720571398735046,
      "learning_rate": 1.1875982283183315e-05,
      "loss": 0.8026,
      "step": 28430
    },
    {
      "epoch": 6.09515645092156,
      "grad_norm": 0.8255385160446167,
      "learning_rate": 1.1873124732104588e-05,
      "loss": 0.2809,
      "step": 28440
    },
    {
      "epoch": 6.097299614230605,
      "grad_norm": 0.6681673526763916,
      "learning_rate": 1.187026718102586e-05,
      "loss": 0.0164,
      "step": 28450
    },
    {
      "epoch": 6.0994427775396485,
      "grad_norm": 14.570473670959473,
      "learning_rate": 1.1867409629947135e-05,
      "loss": 0.4474,
      "step": 28460
    },
    {
      "epoch": 6.101585940848692,
      "grad_norm": 14.561280250549316,
      "learning_rate": 1.186455207886841e-05,
      "loss": 0.7558,
      "step": 28470
    },
    {
      "epoch": 6.103729104157737,
      "grad_norm": 0.5261144042015076,
      "learning_rate": 1.1861694527789684e-05,
      "loss": 0.6125,
      "step": 28480
    },
    {
      "epoch": 6.105872267466781,
      "grad_norm": 0.5334717035293579,
      "learning_rate": 1.185883697671096e-05,
      "loss": 0.6064,
      "step": 28490
    },
    {
      "epoch": 6.108015430775825,
      "grad_norm": 0.5435475707054138,
      "learning_rate": 1.1855979425632234e-05,
      "loss": 0.4549,
      "step": 28500
    },
    {
      "epoch": 6.1101585940848695,
      "grad_norm": 0.4707244038581848,
      "learning_rate": 1.1853121874553508e-05,
      "loss": 0.3158,
      "step": 28510
    },
    {
      "epoch": 6.112301757393913,
      "grad_norm": 0.5034289360046387,
      "learning_rate": 1.1850264323474783e-05,
      "loss": 0.7572,
      "step": 28520
    },
    {
      "epoch": 6.114444920702957,
      "grad_norm": 14.998834609985352,
      "learning_rate": 1.1847406772396057e-05,
      "loss": 0.6087,
      "step": 28530
    },
    {
      "epoch": 6.116588084012002,
      "grad_norm": 0.48195508122444153,
      "learning_rate": 1.1844549221317331e-05,
      "loss": 0.6091,
      "step": 28540
    },
    {
      "epoch": 6.118731247321046,
      "grad_norm": 0.42253831028938293,
      "learning_rate": 1.1841691670238607e-05,
      "loss": 0.0107,
      "step": 28550
    },
    {
      "epoch": 6.12087441063009,
      "grad_norm": 0.41177666187286377,
      "learning_rate": 1.183883411915988e-05,
      "loss": 0.7851,
      "step": 28560
    },
    {
      "epoch": 6.123017573939134,
      "grad_norm": 15.101784706115723,
      "learning_rate": 1.1835976568081156e-05,
      "loss": 0.779,
      "step": 28570
    },
    {
      "epoch": 6.125160737248178,
      "grad_norm": 0.4541361629962921,
      "learning_rate": 1.183311901700243e-05,
      "loss": 0.3227,
      "step": 28580
    },
    {
      "epoch": 6.127303900557222,
      "grad_norm": 0.4456649422645569,
      "learning_rate": 1.1830261465923704e-05,
      "loss": 0.3167,
      "step": 28590
    },
    {
      "epoch": 6.129447063866267,
      "grad_norm": 15.1255464553833,
      "learning_rate": 1.182740391484498e-05,
      "loss": 0.6426,
      "step": 28600
    },
    {
      "epoch": 6.131590227175311,
      "grad_norm": 0.39562052488327026,
      "learning_rate": 1.1824546363766254e-05,
      "loss": 0.328,
      "step": 28610
    },
    {
      "epoch": 6.1337333904843545,
      "grad_norm": 0.3763594925403595,
      "learning_rate": 1.1821688812687528e-05,
      "loss": 0.3254,
      "step": 28620
    },
    {
      "epoch": 6.135876553793399,
      "grad_norm": 0.3472078740596771,
      "learning_rate": 1.1818831261608803e-05,
      "loss": 0.4892,
      "step": 28630
    },
    {
      "epoch": 6.138019717102443,
      "grad_norm": 0.33172333240509033,
      "learning_rate": 1.1815973710530077e-05,
      "loss": 0.5015,
      "step": 28640
    },
    {
      "epoch": 6.140162880411487,
      "grad_norm": 0.36544671654701233,
      "learning_rate": 1.1813116159451353e-05,
      "loss": 1.1565,
      "step": 28650
    },
    {
      "epoch": 6.142306043720532,
      "grad_norm": 15.161675453186035,
      "learning_rate": 1.1810258608372627e-05,
      "loss": 0.485,
      "step": 28660
    },
    {
      "epoch": 6.1444492070295755,
      "grad_norm": 0.38049113750457764,
      "learning_rate": 1.1807401057293899e-05,
      "loss": 0.3281,
      "step": 28670
    },
    {
      "epoch": 6.146592370338619,
      "grad_norm": 0.3985842764377594,
      "learning_rate": 1.1804543506215173e-05,
      "loss": 0.8031,
      "step": 28680
    },
    {
      "epoch": 6.148735533647664,
      "grad_norm": 0.37883898615837097,
      "learning_rate": 1.1801685955136449e-05,
      "loss": 0.954,
      "step": 28690
    },
    {
      "epoch": 6.150878696956708,
      "grad_norm": 0.42255350947380066,
      "learning_rate": 1.1798828404057722e-05,
      "loss": 0.4749,
      "step": 28700
    },
    {
      "epoch": 6.153021860265753,
      "grad_norm": 0.4098086953163147,
      "learning_rate": 1.1795970852978998e-05,
      "loss": 0.4839,
      "step": 28710
    },
    {
      "epoch": 6.155165023574797,
      "grad_norm": 0.3528420031070709,
      "learning_rate": 1.1793113301900272e-05,
      "loss": 0.478,
      "step": 28720
    },
    {
      "epoch": 6.15730818688384,
      "grad_norm": 0.39894652366638184,
      "learning_rate": 1.1790255750821546e-05,
      "loss": 0.3247,
      "step": 28730
    },
    {
      "epoch": 6.159451350192885,
      "grad_norm": 14.959278106689453,
      "learning_rate": 1.1787398199742822e-05,
      "loss": 0.7997,
      "step": 28740
    },
    {
      "epoch": 6.161594513501929,
      "grad_norm": 0.38294753432273865,
      "learning_rate": 1.1784540648664095e-05,
      "loss": 0.1659,
      "step": 28750
    },
    {
      "epoch": 6.163737676810973,
      "grad_norm": 14.709073066711426,
      "learning_rate": 1.178168309758537e-05,
      "loss": 0.6381,
      "step": 28760
    },
    {
      "epoch": 6.165880840120018,
      "grad_norm": 0.3692757785320282,
      "learning_rate": 1.1778825546506645e-05,
      "loss": 0.3246,
      "step": 28770
    },
    {
      "epoch": 6.168024003429061,
      "grad_norm": 0.41669878363609314,
      "learning_rate": 1.1775967995427919e-05,
      "loss": 0.8051,
      "step": 28780
    },
    {
      "epoch": 6.170167166738105,
      "grad_norm": 0.44736912846565247,
      "learning_rate": 1.1773110444349195e-05,
      "loss": 0.4761,
      "step": 28790
    },
    {
      "epoch": 6.17231033004715,
      "grad_norm": 0.4334045946598053,
      "learning_rate": 1.1770252893270469e-05,
      "loss": 0.791,
      "step": 28800
    },
    {
      "epoch": 6.174453493356194,
      "grad_norm": 0.4791565537452698,
      "learning_rate": 1.1767395342191742e-05,
      "loss": 0.7727,
      "step": 28810
    },
    {
      "epoch": 6.176596656665238,
      "grad_norm": 0.49176326394081116,
      "learning_rate": 1.1764537791113018e-05,
      "loss": 0.3121,
      "step": 28820
    },
    {
      "epoch": 6.1787398199742825,
      "grad_norm": 0.5078129768371582,
      "learning_rate": 1.1761680240034292e-05,
      "loss": 0.7815,
      "step": 28830
    },
    {
      "epoch": 6.180882983283326,
      "grad_norm": 31.722763061523438,
      "learning_rate": 1.1758822688955568e-05,
      "loss": 1.0532,
      "step": 28840
    },
    {
      "epoch": 6.18302614659237,
      "grad_norm": 14.478423118591309,
      "learning_rate": 1.1755965137876842e-05,
      "loss": 1.3542,
      "step": 28850
    },
    {
      "epoch": 6.185169309901415,
      "grad_norm": 0.6484166383743286,
      "learning_rate": 1.1753107586798115e-05,
      "loss": 0.439,
      "step": 28860
    },
    {
      "epoch": 6.187312473210459,
      "grad_norm": 0.652237594127655,
      "learning_rate": 1.1750250035719391e-05,
      "loss": 0.5776,
      "step": 28870
    },
    {
      "epoch": 6.189455636519503,
      "grad_norm": 0.6246780157089233,
      "learning_rate": 1.1747392484640663e-05,
      "loss": 0.8661,
      "step": 28880
    },
    {
      "epoch": 6.191598799828547,
      "grad_norm": 0.6131266951560974,
      "learning_rate": 1.1744534933561937e-05,
      "loss": 0.2975,
      "step": 28890
    },
    {
      "epoch": 6.193741963137591,
      "grad_norm": 0.5852341055870056,
      "learning_rate": 1.1741677382483211e-05,
      "loss": 0.7327,
      "step": 28900
    },
    {
      "epoch": 6.195885126446635,
      "grad_norm": 29.53575897216797,
      "learning_rate": 1.1738819831404487e-05,
      "loss": 0.8907,
      "step": 28910
    },
    {
      "epoch": 6.19802828975568,
      "grad_norm": 0.5357365608215332,
      "learning_rate": 1.173596228032576e-05,
      "loss": 0.5975,
      "step": 28920
    },
    {
      "epoch": 6.200171453064724,
      "grad_norm": 0.5417364835739136,
      "learning_rate": 1.1733104729247036e-05,
      "loss": 0.4526,
      "step": 28930
    },
    {
      "epoch": 6.2023146163737675,
      "grad_norm": 0.5291879177093506,
      "learning_rate": 1.173024717816831e-05,
      "loss": 0.3087,
      "step": 28940
    },
    {
      "epoch": 6.204457779682812,
      "grad_norm": 0.5116634964942932,
      "learning_rate": 1.1727389627089584e-05,
      "loss": 0.6124,
      "step": 28950
    },
    {
      "epoch": 6.206600942991856,
      "grad_norm": 0.5048614740371704,
      "learning_rate": 1.172453207601086e-05,
      "loss": 0.4578,
      "step": 28960
    },
    {
      "epoch": 6.2087441063009,
      "grad_norm": 14.601110458374023,
      "learning_rate": 1.1721674524932134e-05,
      "loss": 0.4601,
      "step": 28970
    },
    {
      "epoch": 6.210887269609945,
      "grad_norm": 0.5027691125869751,
      "learning_rate": 1.171881697385341e-05,
      "loss": 0.4589,
      "step": 28980
    },
    {
      "epoch": 6.2130304329189885,
      "grad_norm": 0.48115435242652893,
      "learning_rate": 1.1715959422774683e-05,
      "loss": 0.6156,
      "step": 28990
    },
    {
      "epoch": 6.215173596228032,
      "grad_norm": 0.4869448244571686,
      "learning_rate": 1.1713101871695957e-05,
      "loss": 0.9245,
      "step": 29000
    },
    {
      "epoch": 6.217316759537077,
      "grad_norm": 29.64073944091797,
      "learning_rate": 1.1710244320617233e-05,
      "loss": 1.2062,
      "step": 29010
    },
    {
      "epoch": 6.219459922846121,
      "grad_norm": 0.6128807067871094,
      "learning_rate": 1.1707386769538507e-05,
      "loss": 0.7492,
      "step": 29020
    },
    {
      "epoch": 6.221603086155165,
      "grad_norm": 0.5503188967704773,
      "learning_rate": 1.170452921845978e-05,
      "loss": 0.1631,
      "step": 29030
    },
    {
      "epoch": 6.2237462494642095,
      "grad_norm": 14.656006813049316,
      "learning_rate": 1.1701671667381056e-05,
      "loss": 0.6054,
      "step": 29040
    },
    {
      "epoch": 6.225889412773253,
      "grad_norm": 0.532781720161438,
      "learning_rate": 1.169881411630233e-05,
      "loss": 0.1589,
      "step": 29050
    },
    {
      "epoch": 6.228032576082297,
      "grad_norm": 0.4426913857460022,
      "learning_rate": 1.1695956565223606e-05,
      "loss": 0.3242,
      "step": 29060
    },
    {
      "epoch": 6.230175739391342,
      "grad_norm": 17.50421714782715,
      "learning_rate": 1.169309901414488e-05,
      "loss": 0.3271,
      "step": 29070
    },
    {
      "epoch": 6.232318902700386,
      "grad_norm": 0.37544307112693787,
      "learning_rate": 1.1690241463066154e-05,
      "loss": 0.1668,
      "step": 29080
    },
    {
      "epoch": 6.23446206600943,
      "grad_norm": 14.735550880432129,
      "learning_rate": 1.168738391198743e-05,
      "loss": 1.1432,
      "step": 29090
    },
    {
      "epoch": 6.236605229318474,
      "grad_norm": 0.3790341019630432,
      "learning_rate": 1.1684526360908702e-05,
      "loss": 0.1674,
      "step": 29100
    },
    {
      "epoch": 6.238748392627518,
      "grad_norm": 0.35976308584213257,
      "learning_rate": 1.1681668809829976e-05,
      "loss": 0.1706,
      "step": 29110
    },
    {
      "epoch": 6.240891555936562,
      "grad_norm": 16.894506454467773,
      "learning_rate": 1.1678811258751251e-05,
      "loss": 0.5114,
      "step": 29120
    },
    {
      "epoch": 6.243034719245607,
      "grad_norm": 15.000524520874023,
      "learning_rate": 1.1675953707672525e-05,
      "loss": 0.84,
      "step": 29130
    },
    {
      "epoch": 6.245177882554651,
      "grad_norm": 0.3597690165042877,
      "learning_rate": 1.1673096156593799e-05,
      "loss": 0.825,
      "step": 29140
    },
    {
      "epoch": 6.2473210458636945,
      "grad_norm": 0.38318371772766113,
      "learning_rate": 1.1670238605515075e-05,
      "loss": 0.4858,
      "step": 29150
    },
    {
      "epoch": 6.249464209172739,
      "grad_norm": 0.4241286516189575,
      "learning_rate": 1.1667381054436349e-05,
      "loss": 0.7886,
      "step": 29160
    },
    {
      "epoch": 6.251607372481783,
      "grad_norm": 14.692700386047363,
      "learning_rate": 1.1664523503357623e-05,
      "loss": 0.781,
      "step": 29170
    },
    {
      "epoch": 6.253750535790827,
      "grad_norm": 14.631631851196289,
      "learning_rate": 1.1661665952278898e-05,
      "loss": 0.7767,
      "step": 29180
    },
    {
      "epoch": 6.255893699099872,
      "grad_norm": 0.5669000148773193,
      "learning_rate": 1.1658808401200172e-05,
      "loss": 0.9047,
      "step": 29190
    },
    {
      "epoch": 6.2580368624089155,
      "grad_norm": 0.6072565317153931,
      "learning_rate": 1.1655950850121448e-05,
      "loss": 0.4547,
      "step": 29200
    },
    {
      "epoch": 6.260180025717959,
      "grad_norm": 14.792271614074707,
      "learning_rate": 1.1653093299042722e-05,
      "loss": 0.5929,
      "step": 29210
    },
    {
      "epoch": 6.262323189027004,
      "grad_norm": 14.574498176574707,
      "learning_rate": 1.1650235747963996e-05,
      "loss": 0.1601,
      "step": 29220
    },
    {
      "epoch": 6.264466352336048,
      "grad_norm": 0.46183720231056213,
      "learning_rate": 1.1647378196885271e-05,
      "loss": 0.4631,
      "step": 29230
    },
    {
      "epoch": 6.266609515645092,
      "grad_norm": 14.797273635864258,
      "learning_rate": 1.1644520645806545e-05,
      "loss": 0.7731,
      "step": 29240
    },
    {
      "epoch": 6.268752678954137,
      "grad_norm": 0.46056967973709106,
      "learning_rate": 1.1641663094727819e-05,
      "loss": 0.3159,
      "step": 29250
    },
    {
      "epoch": 6.27089584226318,
      "grad_norm": 0.4418257474899292,
      "learning_rate": 1.1638805543649095e-05,
      "loss": 0.7796,
      "step": 29260
    },
    {
      "epoch": 6.273039005572224,
      "grad_norm": 14.689420700073242,
      "learning_rate": 1.1635947992570369e-05,
      "loss": 0.3203,
      "step": 29270
    },
    {
      "epoch": 6.275182168881269,
      "grad_norm": 0.4946995675563812,
      "learning_rate": 1.1633090441491644e-05,
      "loss": 0.7723,
      "step": 29280
    },
    {
      "epoch": 6.277325332190313,
      "grad_norm": 0.4941251575946808,
      "learning_rate": 1.1630232890412918e-05,
      "loss": 0.4604,
      "step": 29290
    },
    {
      "epoch": 6.279468495499357,
      "grad_norm": 14.80321216583252,
      "learning_rate": 1.1627375339334192e-05,
      "loss": 0.7623,
      "step": 29300
    },
    {
      "epoch": 6.281611658808401,
      "grad_norm": 0.5424706935882568,
      "learning_rate": 1.1624517788255464e-05,
      "loss": 0.3071,
      "step": 29310
    },
    {
      "epoch": 6.283754822117445,
      "grad_norm": 0.5321781039237976,
      "learning_rate": 1.162166023717674e-05,
      "loss": 0.4529,
      "step": 29320
    },
    {
      "epoch": 6.285897985426489,
      "grad_norm": 0.5157740116119385,
      "learning_rate": 1.1618802686098014e-05,
      "loss": 0.4549,
      "step": 29330
    },
    {
      "epoch": 6.288041148735534,
      "grad_norm": 14.61353588104248,
      "learning_rate": 1.161594513501929e-05,
      "loss": 1.052,
      "step": 29340
    },
    {
      "epoch": 6.290184312044578,
      "grad_norm": 0.5422242879867554,
      "learning_rate": 1.1613087583940563e-05,
      "loss": 0.6146,
      "step": 29350
    },
    {
      "epoch": 6.292327475353622,
      "grad_norm": 0.5744048357009888,
      "learning_rate": 1.1610230032861837e-05,
      "loss": 0.453,
      "step": 29360
    },
    {
      "epoch": 6.294470638662666,
      "grad_norm": 0.5426502823829651,
      "learning_rate": 1.1607372481783113e-05,
      "loss": 0.7503,
      "step": 29370
    },
    {
      "epoch": 6.29661380197171,
      "grad_norm": 14.508767127990723,
      "learning_rate": 1.1604514930704387e-05,
      "loss": 0.7472,
      "step": 29380
    },
    {
      "epoch": 6.298756965280754,
      "grad_norm": 0.5975646376609802,
      "learning_rate": 1.160165737962566e-05,
      "loss": 0.7401,
      "step": 29390
    },
    {
      "epoch": 6.300900128589799,
      "grad_norm": 0.6074676513671875,
      "learning_rate": 1.1598799828546936e-05,
      "loss": 0.7406,
      "step": 29400
    },
    {
      "epoch": 6.303043291898843,
      "grad_norm": 29.531314849853516,
      "learning_rate": 1.159594227746821e-05,
      "loss": 1.0232,
      "step": 29410
    },
    {
      "epoch": 6.3051864552078865,
      "grad_norm": 0.590058445930481,
      "learning_rate": 1.1593084726389486e-05,
      "loss": 0.451,
      "step": 29420
    },
    {
      "epoch": 6.307329618516931,
      "grad_norm": 15.093216896057129,
      "learning_rate": 1.159022717531076e-05,
      "loss": 0.595,
      "step": 29430
    },
    {
      "epoch": 6.309472781825975,
      "grad_norm": 14.640480041503906,
      "learning_rate": 1.1587369624232034e-05,
      "loss": 0.7416,
      "step": 29440
    },
    {
      "epoch": 6.311615945135019,
      "grad_norm": 29.47064208984375,
      "learning_rate": 1.158451207315331e-05,
      "loss": 0.8715,
      "step": 29450
    },
    {
      "epoch": 6.313759108444064,
      "grad_norm": 0.5984041094779968,
      "learning_rate": 1.1581654522074583e-05,
      "loss": 0.449,
      "step": 29460
    },
    {
      "epoch": 6.3159022717531075,
      "grad_norm": 15.454846382141113,
      "learning_rate": 1.1578796970995857e-05,
      "loss": 1.1558,
      "step": 29470
    },
    {
      "epoch": 6.318045435062151,
      "grad_norm": 0.8111007213592529,
      "learning_rate": 1.1575939419917133e-05,
      "loss": 0.6929,
      "step": 29480
    },
    {
      "epoch": 6.320188598371196,
      "grad_norm": 0.7377833724021912,
      "learning_rate": 1.1573081868838407e-05,
      "loss": 0.2927,
      "step": 29490
    },
    {
      "epoch": 6.32233176168024,
      "grad_norm": 0.6316826939582825,
      "learning_rate": 1.1570224317759682e-05,
      "loss": 0.4429,
      "step": 29500
    },
    {
      "epoch": 6.324474924989284,
      "grad_norm": 14.730865478515625,
      "learning_rate": 1.1567366766680956e-05,
      "loss": 0.7394,
      "step": 29510
    },
    {
      "epoch": 6.3266180882983285,
      "grad_norm": 0.5705299377441406,
      "learning_rate": 1.156450921560223e-05,
      "loss": 0.5945,
      "step": 29520
    },
    {
      "epoch": 6.328761251607372,
      "grad_norm": 0.5297669768333435,
      "learning_rate": 1.1561651664523503e-05,
      "loss": 0.5979,
      "step": 29530
    },
    {
      "epoch": 6.330904414916416,
      "grad_norm": 0.5143612623214722,
      "learning_rate": 1.1558794113444778e-05,
      "loss": 0.3034,
      "step": 29540
    },
    {
      "epoch": 6.333047578225461,
      "grad_norm": 0.5236628651618958,
      "learning_rate": 1.1555936562366052e-05,
      "loss": 0.7498,
      "step": 29550
    },
    {
      "epoch": 6.335190741534505,
      "grad_norm": 14.59200668334961,
      "learning_rate": 1.1553079011287328e-05,
      "loss": 0.3179,
      "step": 29560
    },
    {
      "epoch": 6.337333904843549,
      "grad_norm": 0.4120740592479706,
      "learning_rate": 1.1550221460208602e-05,
      "loss": 0.3168,
      "step": 29570
    },
    {
      "epoch": 6.339477068152593,
      "grad_norm": 14.675654411315918,
      "learning_rate": 1.1547363909129876e-05,
      "loss": 0.1662,
      "step": 29580
    },
    {
      "epoch": 6.341620231461637,
      "grad_norm": 0.4112206995487213,
      "learning_rate": 1.1544506358051151e-05,
      "loss": 0.7975,
      "step": 29590
    },
    {
      "epoch": 6.343763394770681,
      "grad_norm": 14.718186378479004,
      "learning_rate": 1.1541648806972425e-05,
      "loss": 0.4871,
      "step": 29600
    },
    {
      "epoch": 6.345906558079726,
      "grad_norm": 0.4015187919139862,
      "learning_rate": 1.1538791255893699e-05,
      "loss": 0.6479,
      "step": 29610
    },
    {
      "epoch": 6.34804972138877,
      "grad_norm": 14.721583366394043,
      "learning_rate": 1.1535933704814975e-05,
      "loss": 0.9537,
      "step": 29620
    },
    {
      "epoch": 6.3501928846978135,
      "grad_norm": 14.741074562072754,
      "learning_rate": 1.1533076153736249e-05,
      "loss": 0.6252,
      "step": 29630
    },
    {
      "epoch": 6.352336048006858,
      "grad_norm": 14.648083686828613,
      "learning_rate": 1.1530218602657524e-05,
      "loss": 0.7728,
      "step": 29640
    },
    {
      "epoch": 6.354479211315902,
      "grad_norm": 0.507063627243042,
      "learning_rate": 1.1527361051578798e-05,
      "loss": 0.6103,
      "step": 29650
    },
    {
      "epoch": 6.356622374624947,
      "grad_norm": 14.567511558532715,
      "learning_rate": 1.1524503500500072e-05,
      "loss": 0.7605,
      "step": 29660
    },
    {
      "epoch": 6.358765537933991,
      "grad_norm": 16.547603607177734,
      "learning_rate": 1.1521645949421348e-05,
      "loss": 0.7479,
      "step": 29670
    },
    {
      "epoch": 6.3609087012430345,
      "grad_norm": 0.5825951099395752,
      "learning_rate": 1.1518788398342622e-05,
      "loss": 0.1571,
      "step": 29680
    },
    {
      "epoch": 6.363051864552079,
      "grad_norm": 0.5448012351989746,
      "learning_rate": 1.1515930847263897e-05,
      "loss": 0.5982,
      "step": 29690
    },
    {
      "epoch": 6.365195027861123,
      "grad_norm": 0.5387764573097229,
      "learning_rate": 1.1513073296185171e-05,
      "loss": 0.4547,
      "step": 29700
    },
    {
      "epoch": 6.367338191170167,
      "grad_norm": 14.549850463867188,
      "learning_rate": 1.1510215745106445e-05,
      "loss": 0.7603,
      "step": 29710
    },
    {
      "epoch": 6.369481354479212,
      "grad_norm": 0.6258155703544617,
      "learning_rate": 1.150735819402772e-05,
      "loss": 0.8987,
      "step": 29720
    },
    {
      "epoch": 6.371624517788256,
      "grad_norm": 14.429018020629883,
      "learning_rate": 1.1504500642948995e-05,
      "loss": 1.009,
      "step": 29730
    },
    {
      "epoch": 6.373767681097299,
      "grad_norm": 0.6687580347061157,
      "learning_rate": 1.1501643091870267e-05,
      "loss": 0.1534,
      "step": 29740
    },
    {
      "epoch": 6.375910844406344,
      "grad_norm": 14.46951961517334,
      "learning_rate": 1.149878554079154e-05,
      "loss": 0.4412,
      "step": 29750
    },
    {
      "epoch": 6.378054007715388,
      "grad_norm": 14.503969192504883,
      "learning_rate": 1.1495927989712816e-05,
      "loss": 0.8809,
      "step": 29760
    },
    {
      "epoch": 6.380197171024432,
      "grad_norm": 0.592863917350769,
      "learning_rate": 1.149307043863409e-05,
      "loss": 0.448,
      "step": 29770
    },
    {
      "epoch": 6.382340334333477,
      "grad_norm": 0.5615133047103882,
      "learning_rate": 1.1490212887555366e-05,
      "loss": 0.457,
      "step": 29780
    },
    {
      "epoch": 6.38448349764252,
      "grad_norm": 14.638348579406738,
      "learning_rate": 1.148735533647664e-05,
      "loss": 0.6297,
      "step": 29790
    },
    {
      "epoch": 6.386626660951564,
      "grad_norm": 14.727712631225586,
      "learning_rate": 1.1484497785397914e-05,
      "loss": 0.9322,
      "step": 29800
    },
    {
      "epoch": 6.388769824260609,
      "grad_norm": 0.4884580969810486,
      "learning_rate": 1.148164023431919e-05,
      "loss": 0.6128,
      "step": 29810
    },
    {
      "epoch": 6.390912987569653,
      "grad_norm": 0.5083882808685303,
      "learning_rate": 1.1478782683240463e-05,
      "loss": 0.311,
      "step": 29820
    },
    {
      "epoch": 6.393056150878697,
      "grad_norm": 0.4606180489063263,
      "learning_rate": 1.1475925132161739e-05,
      "loss": 0.4687,
      "step": 29830
    },
    {
      "epoch": 6.3951993141877415,
      "grad_norm": 0.4449584186077118,
      "learning_rate": 1.1473067581083013e-05,
      "loss": 0.3128,
      "step": 29840
    },
    {
      "epoch": 6.397342477496785,
      "grad_norm": 14.70937442779541,
      "learning_rate": 1.1470210030004287e-05,
      "loss": 0.6424,
      "step": 29850
    },
    {
      "epoch": 6.399485640805829,
      "grad_norm": 0.3895440101623535,
      "learning_rate": 1.1467352478925563e-05,
      "loss": 0.6626,
      "step": 29860
    },
    {
      "epoch": 6.401628804114874,
      "grad_norm": 0.45090875029563904,
      "learning_rate": 1.1464494927846836e-05,
      "loss": 0.9517,
      "step": 29870
    },
    {
      "epoch": 6.403771967423918,
      "grad_norm": 14.750375747680664,
      "learning_rate": 1.146163737676811e-05,
      "loss": 0.7741,
      "step": 29880
    },
    {
      "epoch": 6.405915130732962,
      "grad_norm": 0.5022059082984924,
      "learning_rate": 1.1458779825689386e-05,
      "loss": 0.3095,
      "step": 29890
    },
    {
      "epoch": 6.408058294042006,
      "grad_norm": 0.4818960130214691,
      "learning_rate": 1.145592227461066e-05,
      "loss": 0.6106,
      "step": 29900
    },
    {
      "epoch": 6.41020145735105,
      "grad_norm": 0.4663847088813782,
      "learning_rate": 1.1453064723531936e-05,
      "loss": 0.1613,
      "step": 29910
    },
    {
      "epoch": 6.412344620660094,
      "grad_norm": 14.984623908996582,
      "learning_rate": 1.145020717245321e-05,
      "loss": 0.4777,
      "step": 29920
    },
    {
      "epoch": 6.414487783969139,
      "grad_norm": 0.47688478231430054,
      "learning_rate": 1.1447349621374483e-05,
      "loss": 0.6264,
      "step": 29930
    },
    {
      "epoch": 6.416630947278183,
      "grad_norm": 14.717143058776855,
      "learning_rate": 1.1444492070295759e-05,
      "loss": 0.921,
      "step": 29940
    },
    {
      "epoch": 6.4187741105872265,
      "grad_norm": 14.670660018920898,
      "learning_rate": 1.1441634519217031e-05,
      "loss": 0.7503,
      "step": 29950
    },
    {
      "epoch": 6.420917273896271,
      "grad_norm": 0.5310457348823547,
      "learning_rate": 1.1438776968138305e-05,
      "loss": 0.4554,
      "step": 29960
    },
    {
      "epoch": 6.423060437205315,
      "grad_norm": 14.656222343444824,
      "learning_rate": 1.143591941705958e-05,
      "loss": 0.3103,
      "step": 29970
    },
    {
      "epoch": 6.425203600514359,
      "grad_norm": 0.5130569934844971,
      "learning_rate": 1.1433061865980855e-05,
      "loss": 0.6178,
      "step": 29980
    },
    {
      "epoch": 6.427346763823404,
      "grad_norm": 14.511523246765137,
      "learning_rate": 1.1430204314902129e-05,
      "loss": 1.1979,
      "step": 29990
    },
    {
      "epoch": 6.4294899271324475,
      "grad_norm": 0.6162211894989014,
      "learning_rate": 1.1427346763823404e-05,
      "loss": 0.3017,
      "step": 30000
    },
    {
      "epoch": 6.431633090441491,
      "grad_norm": 0.6156100034713745,
      "learning_rate": 1.1424489212744678e-05,
      "loss": 0.7281,
      "step": 30010
    },
    {
      "epoch": 6.433776253750536,
      "grad_norm": 0.5727547407150269,
      "learning_rate": 1.1421631661665952e-05,
      "loss": 0.4459,
      "step": 30020
    },
    {
      "epoch": 6.43591941705958,
      "grad_norm": 0.5286458730697632,
      "learning_rate": 1.1418774110587228e-05,
      "loss": 0.4556,
      "step": 30030
    },
    {
      "epoch": 6.438062580368624,
      "grad_norm": 0.4829840362071991,
      "learning_rate": 1.1415916559508502e-05,
      "loss": 0.1604,
      "step": 30040
    },
    {
      "epoch": 6.4402057436776685,
      "grad_norm": 0.4981827735900879,
      "learning_rate": 1.1413059008429777e-05,
      "loss": 0.9179,
      "step": 30050
    },
    {
      "epoch": 6.442348906986712,
      "grad_norm": 0.4745893180370331,
      "learning_rate": 1.1410201457351051e-05,
      "loss": 0.4653,
      "step": 30060
    },
    {
      "epoch": 6.444492070295756,
      "grad_norm": 14.600468635559082,
      "learning_rate": 1.1407343906272325e-05,
      "loss": 0.6113,
      "step": 30070
    },
    {
      "epoch": 6.446635233604801,
      "grad_norm": 16.63033103942871,
      "learning_rate": 1.14044863551936e-05,
      "loss": 0.3153,
      "step": 30080
    },
    {
      "epoch": 6.448778396913845,
      "grad_norm": 14.595298767089844,
      "learning_rate": 1.1401628804114875e-05,
      "loss": 0.6164,
      "step": 30090
    },
    {
      "epoch": 6.450921560222889,
      "grad_norm": 0.46947935223579407,
      "learning_rate": 1.1398771253036149e-05,
      "loss": 0.4684,
      "step": 30100
    },
    {
      "epoch": 6.453064723531933,
      "grad_norm": 0.5219476222991943,
      "learning_rate": 1.1395913701957424e-05,
      "loss": 0.9206,
      "step": 30110
    },
    {
      "epoch": 6.455207886840977,
      "grad_norm": 14.781586647033691,
      "learning_rate": 1.1393056150878698e-05,
      "loss": 0.4568,
      "step": 30120
    },
    {
      "epoch": 6.457351050150021,
      "grad_norm": 14.565364837646484,
      "learning_rate": 1.1390198599799974e-05,
      "loss": 0.4585,
      "step": 30130
    },
    {
      "epoch": 6.459494213459066,
      "grad_norm": 0.5415990352630615,
      "learning_rate": 1.1387341048721248e-05,
      "loss": 0.7551,
      "step": 30140
    },
    {
      "epoch": 6.46163737676811,
      "grad_norm": 29.724437713623047,
      "learning_rate": 1.1384483497642522e-05,
      "loss": 0.6011,
      "step": 30150
    },
    {
      "epoch": 6.4637805400771535,
      "grad_norm": 14.540623664855957,
      "learning_rate": 1.1381625946563797e-05,
      "loss": 0.7485,
      "step": 30160
    },
    {
      "epoch": 6.465923703386198,
      "grad_norm": 14.78140640258789,
      "learning_rate": 1.137876839548507e-05,
      "loss": 0.8908,
      "step": 30170
    },
    {
      "epoch": 6.468066866695242,
      "grad_norm": 14.528995513916016,
      "learning_rate": 1.1375910844406343e-05,
      "loss": 0.3038,
      "step": 30180
    },
    {
      "epoch": 6.470210030004286,
      "grad_norm": 0.5354053974151611,
      "learning_rate": 1.1373053293327619e-05,
      "loss": 0.3051,
      "step": 30190
    },
    {
      "epoch": 6.472353193313331,
      "grad_norm": 0.5191097259521484,
      "learning_rate": 1.1370195742248893e-05,
      "loss": 0.461,
      "step": 30200
    },
    {
      "epoch": 6.4744963566223745,
      "grad_norm": 0.4894154369831085,
      "learning_rate": 1.1367338191170167e-05,
      "loss": 0.3088,
      "step": 30210
    },
    {
      "epoch": 6.476639519931418,
      "grad_norm": 29.607303619384766,
      "learning_rate": 1.1364480640091443e-05,
      "loss": 0.7706,
      "step": 30220
    },
    {
      "epoch": 6.478782683240463,
      "grad_norm": 0.469978928565979,
      "learning_rate": 1.1361623089012716e-05,
      "loss": 0.3199,
      "step": 30230
    },
    {
      "epoch": 6.480925846549507,
      "grad_norm": 0.45039862394332886,
      "learning_rate": 1.135876553793399e-05,
      "loss": 0.6213,
      "step": 30240
    },
    {
      "epoch": 6.483069009858551,
      "grad_norm": 14.616703987121582,
      "learning_rate": 1.1355907986855266e-05,
      "loss": 0.3218,
      "step": 30250
    },
    {
      "epoch": 6.485212173167596,
      "grad_norm": 0.4127621352672577,
      "learning_rate": 1.135305043577654e-05,
      "loss": 0.1638,
      "step": 30260
    },
    {
      "epoch": 6.487355336476639,
      "grad_norm": 0.44120535254478455,
      "learning_rate": 1.1350192884697816e-05,
      "loss": 0.7835,
      "step": 30270
    },
    {
      "epoch": 6.489498499785683,
      "grad_norm": 0.43975594639778137,
      "learning_rate": 1.134733533361909e-05,
      "loss": 0.9282,
      "step": 30280
    },
    {
      "epoch": 6.491641663094728,
      "grad_norm": 0.4761817753314972,
      "learning_rate": 1.1344477782540363e-05,
      "loss": 0.4666,
      "step": 30290
    },
    {
      "epoch": 6.493784826403772,
      "grad_norm": 14.589425086975098,
      "learning_rate": 1.1341620231461639e-05,
      "loss": 0.9165,
      "step": 30300
    },
    {
      "epoch": 6.495927989712816,
      "grad_norm": 0.4966086745262146,
      "learning_rate": 1.1338762680382913e-05,
      "loss": 0.469,
      "step": 30310
    },
    {
      "epoch": 6.4980711530218604,
      "grad_norm": 0.4447213411331177,
      "learning_rate": 1.1335905129304187e-05,
      "loss": 0.6194,
      "step": 30320
    },
    {
      "epoch": 6.500214316330904,
      "grad_norm": 0.5012321472167969,
      "learning_rate": 1.1333047578225463e-05,
      "loss": 0.4618,
      "step": 30330
    },
    {
      "epoch": 6.502357479639949,
      "grad_norm": 0.5011755228042603,
      "learning_rate": 1.1330190027146736e-05,
      "loss": 0.6106,
      "step": 30340
    },
    {
      "epoch": 6.504500642948993,
      "grad_norm": 0.4694822132587433,
      "learning_rate": 1.1327332476068012e-05,
      "loss": 0.6095,
      "step": 30350
    },
    {
      "epoch": 6.506643806258037,
      "grad_norm": 0.4017361104488373,
      "learning_rate": 1.1324474924989286e-05,
      "loss": 0.4742,
      "step": 30360
    },
    {
      "epoch": 6.5087869695670815,
      "grad_norm": 0.4185946583747864,
      "learning_rate": 1.132161737391056e-05,
      "loss": 0.6428,
      "step": 30370
    },
    {
      "epoch": 6.510930132876125,
      "grad_norm": 14.654047012329102,
      "learning_rate": 1.1318759822831832e-05,
      "loss": 1.1032,
      "step": 30380
    },
    {
      "epoch": 6.513073296185169,
      "grad_norm": 0.4503667652606964,
      "learning_rate": 1.1315902271753108e-05,
      "loss": 0.4732,
      "step": 30390
    },
    {
      "epoch": 6.515216459494214,
      "grad_norm": 0.4530801773071289,
      "learning_rate": 1.1313044720674382e-05,
      "loss": 0.6177,
      "step": 30400
    },
    {
      "epoch": 6.517359622803258,
      "grad_norm": 14.57182502746582,
      "learning_rate": 1.1310187169595657e-05,
      "loss": 0.6112,
      "step": 30410
    },
    {
      "epoch": 6.519502786112302,
      "grad_norm": 14.545560836791992,
      "learning_rate": 1.1307329618516931e-05,
      "loss": 0.9183,
      "step": 30420
    },
    {
      "epoch": 6.521645949421346,
      "grad_norm": 14.54658317565918,
      "learning_rate": 1.1304472067438205e-05,
      "loss": 0.4523,
      "step": 30430
    },
    {
      "epoch": 6.52378911273039,
      "grad_norm": 0.5583935976028442,
      "learning_rate": 1.1301614516359481e-05,
      "loss": 0.7474,
      "step": 30440
    },
    {
      "epoch": 6.525932276039434,
      "grad_norm": 0.5527199506759644,
      "learning_rate": 1.1298756965280755e-05,
      "loss": 0.1637,
      "step": 30450
    },
    {
      "epoch": 6.528075439348479,
      "grad_norm": 0.4955853521823883,
      "learning_rate": 1.1295899414202029e-05,
      "loss": 0.1609,
      "step": 30460
    },
    {
      "epoch": 6.530218602657523,
      "grad_norm": 0.4091620147228241,
      "learning_rate": 1.1293041863123304e-05,
      "loss": 0.1638,
      "step": 30470
    },
    {
      "epoch": 6.5323617659665665,
      "grad_norm": 14.940264701843262,
      "learning_rate": 1.1290184312044578e-05,
      "loss": 1.4052,
      "step": 30480
    },
    {
      "epoch": 6.534504929275611,
      "grad_norm": 14.647612571716309,
      "learning_rate": 1.1287326760965854e-05,
      "loss": 0.4688,
      "step": 30490
    },
    {
      "epoch": 6.536648092584655,
      "grad_norm": 29.75649642944336,
      "learning_rate": 1.1284469209887128e-05,
      "loss": 0.4729,
      "step": 30500
    },
    {
      "epoch": 6.538791255893699,
      "grad_norm": 0.4378677308559418,
      "learning_rate": 1.1281611658808402e-05,
      "loss": 0.7793,
      "step": 30510
    },
    {
      "epoch": 6.540934419202744,
      "grad_norm": 0.4736057221889496,
      "learning_rate": 1.1278754107729677e-05,
      "loss": 0.3145,
      "step": 30520
    },
    {
      "epoch": 6.5430775825117875,
      "grad_norm": 14.607926368713379,
      "learning_rate": 1.1275896556650951e-05,
      "loss": 0.7743,
      "step": 30530
    },
    {
      "epoch": 6.545220745820831,
      "grad_norm": 14.599123001098633,
      "learning_rate": 1.1273039005572227e-05,
      "loss": 0.6224,
      "step": 30540
    },
    {
      "epoch": 6.547363909129876,
      "grad_norm": 14.606988906860352,
      "learning_rate": 1.12701814544935e-05,
      "loss": 0.3108,
      "step": 30550
    },
    {
      "epoch": 6.54950707243892,
      "grad_norm": 14.595145225524902,
      "learning_rate": 1.1267323903414775e-05,
      "loss": 0.7619,
      "step": 30560
    },
    {
      "epoch": 6.551650235747964,
      "grad_norm": 0.5105727910995483,
      "learning_rate": 1.126446635233605e-05,
      "loss": 0.7577,
      "step": 30570
    },
    {
      "epoch": 6.5537933990570085,
      "grad_norm": 0.5211828351020813,
      "learning_rate": 1.1261608801257324e-05,
      "loss": 1.0498,
      "step": 30580
    },
    {
      "epoch": 6.555936562366052,
      "grad_norm": 0.5878549218177795,
      "learning_rate": 1.1258751250178598e-05,
      "loss": 0.891,
      "step": 30590
    },
    {
      "epoch": 6.558079725675096,
      "grad_norm": 0.6066914200782776,
      "learning_rate": 1.125589369909987e-05,
      "loss": 0.301,
      "step": 30600
    },
    {
      "epoch": 6.560222888984141,
      "grad_norm": 14.547708511352539,
      "learning_rate": 1.1253036148021146e-05,
      "loss": 0.4532,
      "step": 30610
    },
    {
      "epoch": 6.562366052293185,
      "grad_norm": 0.5036593079566956,
      "learning_rate": 1.125017859694242e-05,
      "loss": 0.305,
      "step": 30620
    },
    {
      "epoch": 6.564509215602229,
      "grad_norm": 14.685320854187012,
      "learning_rate": 1.1247321045863696e-05,
      "loss": 0.7656,
      "step": 30630
    },
    {
      "epoch": 6.566652378911273,
      "grad_norm": 0.4635963439941406,
      "learning_rate": 1.124446349478497e-05,
      "loss": 0.3126,
      "step": 30640
    },
    {
      "epoch": 6.568795542220317,
      "grad_norm": 14.698089599609375,
      "learning_rate": 1.1241605943706244e-05,
      "loss": 0.623,
      "step": 30650
    },
    {
      "epoch": 6.570938705529361,
      "grad_norm": 14.702534675598145,
      "learning_rate": 1.1238748392627519e-05,
      "loss": 0.4739,
      "step": 30660
    },
    {
      "epoch": 6.573081868838406,
      "grad_norm": 14.772407531738281,
      "learning_rate": 1.1235890841548793e-05,
      "loss": 0.7737,
      "step": 30670
    },
    {
      "epoch": 6.57522503214745,
      "grad_norm": 0.47779083251953125,
      "learning_rate": 1.1233033290470069e-05,
      "loss": 0.3216,
      "step": 30680
    },
    {
      "epoch": 6.5773681954564935,
      "grad_norm": 0.48930269479751587,
      "learning_rate": 1.1230175739391343e-05,
      "loss": 0.4693,
      "step": 30690
    },
    {
      "epoch": 6.579511358765538,
      "grad_norm": 14.722576141357422,
      "learning_rate": 1.1227318188312617e-05,
      "loss": 0.4885,
      "step": 30700
    },
    {
      "epoch": 6.581654522074582,
      "grad_norm": 14.686491966247559,
      "learning_rate": 1.1224460637233892e-05,
      "loss": 1.2821,
      "step": 30710
    },
    {
      "epoch": 6.583797685383626,
      "grad_norm": 0.452413946390152,
      "learning_rate": 1.1221603086155166e-05,
      "loss": 0.4738,
      "step": 30720
    },
    {
      "epoch": 6.585940848692671,
      "grad_norm": 0.45456984639167786,
      "learning_rate": 1.121874553507644e-05,
      "loss": 0.1629,
      "step": 30730
    },
    {
      "epoch": 6.588084012001715,
      "grad_norm": 0.44449082016944885,
      "learning_rate": 1.1215887983997716e-05,
      "loss": 0.325,
      "step": 30740
    },
    {
      "epoch": 6.590227175310758,
      "grad_norm": 0.43672335147857666,
      "learning_rate": 1.121303043291899e-05,
      "loss": 0.9393,
      "step": 30750
    },
    {
      "epoch": 6.592370338619803,
      "grad_norm": 0.4465848505496979,
      "learning_rate": 1.1210172881840265e-05,
      "loss": 0.4711,
      "step": 30760
    },
    {
      "epoch": 6.594513501928847,
      "grad_norm": 14.664299011230469,
      "learning_rate": 1.1207315330761539e-05,
      "loss": 0.3166,
      "step": 30770
    },
    {
      "epoch": 6.596656665237891,
      "grad_norm": 0.39060014486312866,
      "learning_rate": 1.1204457779682813e-05,
      "loss": 0.3203,
      "step": 30780
    },
    {
      "epoch": 6.598799828546936,
      "grad_norm": 14.670985221862793,
      "learning_rate": 1.1201600228604089e-05,
      "loss": 0.8003,
      "step": 30790
    },
    {
      "epoch": 6.600942991855979,
      "grad_norm": 0.4387228488922119,
      "learning_rate": 1.1198742677525363e-05,
      "loss": 0.7969,
      "step": 30800
    },
    {
      "epoch": 6.603086155165023,
      "grad_norm": 0.4496949315071106,
      "learning_rate": 1.1195885126446635e-05,
      "loss": 0.7855,
      "step": 30810
    },
    {
      "epoch": 6.605229318474068,
      "grad_norm": 0.42123866081237793,
      "learning_rate": 1.119302757536791e-05,
      "loss": 0.0101,
      "step": 30820
    },
    {
      "epoch": 6.607372481783112,
      "grad_norm": 14.657639503479004,
      "learning_rate": 1.1190170024289184e-05,
      "loss": 0.4761,
      "step": 30830
    },
    {
      "epoch": 6.609515645092156,
      "grad_norm": 0.39496153593063354,
      "learning_rate": 1.1187312473210458e-05,
      "loss": 0.009,
      "step": 30840
    },
    {
      "epoch": 6.6116588084012005,
      "grad_norm": 0.3631916642189026,
      "learning_rate": 1.1184454922131734e-05,
      "loss": 0.3304,
      "step": 30850
    },
    {
      "epoch": 6.613801971710244,
      "grad_norm": 0.3778626024723053,
      "learning_rate": 1.1181597371053008e-05,
      "loss": 1.1331,
      "step": 30860
    },
    {
      "epoch": 6.615945135019288,
      "grad_norm": 14.678771018981934,
      "learning_rate": 1.1178739819974282e-05,
      "loss": 0.8045,
      "step": 30870
    },
    {
      "epoch": 6.618088298328333,
      "grad_norm": 14.654762268066406,
      "learning_rate": 1.1175882268895557e-05,
      "loss": 0.6354,
      "step": 30880
    },
    {
      "epoch": 6.620231461637377,
      "grad_norm": 15.448653221130371,
      "learning_rate": 1.1173024717816831e-05,
      "loss": 0.785,
      "step": 30890
    },
    {
      "epoch": 6.622374624946421,
      "grad_norm": 0.4771031439304352,
      "learning_rate": 1.1170167166738107e-05,
      "loss": 0.3143,
      "step": 30900
    },
    {
      "epoch": 6.624517788255465,
      "grad_norm": 14.646772384643555,
      "learning_rate": 1.1167309615659381e-05,
      "loss": 0.3136,
      "step": 30910
    },
    {
      "epoch": 6.626660951564509,
      "grad_norm": 29.801254272460938,
      "learning_rate": 1.1164452064580655e-05,
      "loss": 0.9305,
      "step": 30920
    },
    {
      "epoch": 6.628804114873553,
      "grad_norm": 0.44960156083106995,
      "learning_rate": 1.116159451350193e-05,
      "loss": 0.785,
      "step": 30930
    },
    {
      "epoch": 6.630947278182598,
      "grad_norm": 0.4720403552055359,
      "learning_rate": 1.1158736962423204e-05,
      "loss": 0.6286,
      "step": 30940
    },
    {
      "epoch": 6.633090441491642,
      "grad_norm": 14.844348907470703,
      "learning_rate": 1.1155879411344478e-05,
      "loss": 0.7768,
      "step": 30950
    },
    {
      "epoch": 6.6352336048006855,
      "grad_norm": 0.4465640187263489,
      "learning_rate": 1.1153021860265754e-05,
      "loss": 0.0101,
      "step": 30960
    },
    {
      "epoch": 6.63737676810973,
      "grad_norm": 0.4127136468887329,
      "learning_rate": 1.1150164309187028e-05,
      "loss": 0.1641,
      "step": 30970
    },
    {
      "epoch": 6.639519931418774,
      "grad_norm": 0.3706444501876831,
      "learning_rate": 1.1147306758108303e-05,
      "loss": 0.4846,
      "step": 30980
    },
    {
      "epoch": 6.641663094727818,
      "grad_norm": 14.85109806060791,
      "learning_rate": 1.1144449207029577e-05,
      "loss": 0.8141,
      "step": 30990
    },
    {
      "epoch": 6.643806258036863,
      "grad_norm": 0.34770116209983826,
      "learning_rate": 1.1141591655950851e-05,
      "loss": 0.491,
      "step": 31000
    },
    {
      "epoch": 6.6459494213459065,
      "grad_norm": 0.35267749428749084,
      "learning_rate": 1.1138734104872127e-05,
      "loss": 0.1673,
      "step": 31010
    },
    {
      "epoch": 6.64809258465495,
      "grad_norm": 0.3341083228588104,
      "learning_rate": 1.1135876553793401e-05,
      "loss": 0.3343,
      "step": 31020
    },
    {
      "epoch": 6.650235747963995,
      "grad_norm": 0.3283856511116028,
      "learning_rate": 1.1133019002714673e-05,
      "loss": 0.499,
      "step": 31030
    },
    {
      "epoch": 6.652378911273039,
      "grad_norm": 14.761571884155273,
      "learning_rate": 1.1130161451635949e-05,
      "loss": 0.8295,
      "step": 31040
    },
    {
      "epoch": 6.654522074582083,
      "grad_norm": 15.022954940795898,
      "learning_rate": 1.1127303900557223e-05,
      "loss": 0.3359,
      "step": 31050
    },
    {
      "epoch": 6.6566652378911275,
      "grad_norm": 14.748201370239258,
      "learning_rate": 1.1124446349478497e-05,
      "loss": 0.6621,
      "step": 31060
    },
    {
      "epoch": 6.658808401200171,
      "grad_norm": 0.34536051750183105,
      "learning_rate": 1.1121588798399772e-05,
      "loss": 0.9901,
      "step": 31070
    },
    {
      "epoch": 6.660951564509215,
      "grad_norm": 14.731091499328613,
      "learning_rate": 1.1118731247321046e-05,
      "loss": 0.6499,
      "step": 31080
    },
    {
      "epoch": 6.66309472781826,
      "grad_norm": 14.837332725524902,
      "learning_rate": 1.111587369624232e-05,
      "loss": 0.6389,
      "step": 31090
    },
    {
      "epoch": 6.665237891127304,
      "grad_norm": 0.4132936894893646,
      "learning_rate": 1.1113016145163596e-05,
      "loss": 0.322,
      "step": 31100
    },
    {
      "epoch": 6.667381054436348,
      "grad_norm": 14.665823936462402,
      "learning_rate": 1.111015859408487e-05,
      "loss": 0.9494,
      "step": 31110
    },
    {
      "epoch": 6.669524217745392,
      "grad_norm": 14.814287185668945,
      "learning_rate": 1.1107301043006145e-05,
      "loss": 0.6341,
      "step": 31120
    },
    {
      "epoch": 6.671667381054436,
      "grad_norm": 14.925833702087402,
      "learning_rate": 1.110444349192742e-05,
      "loss": 0.4755,
      "step": 31130
    },
    {
      "epoch": 6.67381054436348,
      "grad_norm": 0.4410550594329834,
      "learning_rate": 1.1101585940848693e-05,
      "loss": 0.3132,
      "step": 31140
    },
    {
      "epoch": 6.675953707672525,
      "grad_norm": 0.4890129268169403,
      "learning_rate": 1.1098728389769969e-05,
      "loss": 0.9246,
      "step": 31150
    },
    {
      "epoch": 6.678096870981569,
      "grad_norm": 0.5455484390258789,
      "learning_rate": 1.1095870838691243e-05,
      "loss": 0.4565,
      "step": 31160
    },
    {
      "epoch": 6.6802400342906125,
      "grad_norm": 29.491777420043945,
      "learning_rate": 1.1093013287612517e-05,
      "loss": 0.6113,
      "step": 31170
    },
    {
      "epoch": 6.682383197599657,
      "grad_norm": 0.5036044716835022,
      "learning_rate": 1.1090155736533792e-05,
      "loss": 0.6114,
      "step": 31180
    },
    {
      "epoch": 6.684526360908701,
      "grad_norm": 0.49285629391670227,
      "learning_rate": 1.1087298185455066e-05,
      "loss": 0.7661,
      "step": 31190
    },
    {
      "epoch": 6.686669524217745,
      "grad_norm": 14.655550956726074,
      "learning_rate": 1.1084440634376342e-05,
      "loss": 0.4643,
      "step": 31200
    },
    {
      "epoch": 6.68881268752679,
      "grad_norm": 0.5059229731559753,
      "learning_rate": 1.1081583083297616e-05,
      "loss": 0.4609,
      "step": 31210
    },
    {
      "epoch": 6.6909558508358336,
      "grad_norm": 14.76233959197998,
      "learning_rate": 1.107872553221889e-05,
      "loss": 0.4657,
      "step": 31220
    },
    {
      "epoch": 6.693099014144877,
      "grad_norm": 29.819162368774414,
      "learning_rate": 1.1075867981140165e-05,
      "loss": 1.2275,
      "step": 31230
    },
    {
      "epoch": 6.695242177453922,
      "grad_norm": 0.48045507073402405,
      "learning_rate": 1.1073010430061437e-05,
      "loss": 0.3078,
      "step": 31240
    },
    {
      "epoch": 6.697385340762966,
      "grad_norm": 14.588285446166992,
      "learning_rate": 1.1070152878982711e-05,
      "loss": 0.6101,
      "step": 31250
    },
    {
      "epoch": 6.69952850407201,
      "grad_norm": 0.4549672603607178,
      "learning_rate": 1.1067295327903987e-05,
      "loss": 0.317,
      "step": 31260
    },
    {
      "epoch": 6.701671667381055,
      "grad_norm": 0.43880292773246765,
      "learning_rate": 1.1064437776825261e-05,
      "loss": 0.4666,
      "step": 31270
    },
    {
      "epoch": 6.703814830690098,
      "grad_norm": 0.427988201379776,
      "learning_rate": 1.1061580225746535e-05,
      "loss": 0.4683,
      "step": 31280
    },
    {
      "epoch": 6.705957993999142,
      "grad_norm": 0.3624856173992157,
      "learning_rate": 1.105872267466781e-05,
      "loss": 0.1676,
      "step": 31290
    },
    {
      "epoch": 6.708101157308187,
      "grad_norm": 15.558077812194824,
      "learning_rate": 1.1055865123589084e-05,
      "loss": 0.9737,
      "step": 31300
    },
    {
      "epoch": 6.710244320617231,
      "grad_norm": 0.41701751947402954,
      "learning_rate": 1.1053007572510358e-05,
      "loss": 0.6328,
      "step": 31310
    },
    {
      "epoch": 6.712387483926275,
      "grad_norm": 0.4272657632827759,
      "learning_rate": 1.1050150021431634e-05,
      "loss": 0.1638,
      "step": 31320
    },
    {
      "epoch": 6.7145306472353194,
      "grad_norm": 0.3880467712879181,
      "learning_rate": 1.1047292470352908e-05,
      "loss": 0.3283,
      "step": 31330
    },
    {
      "epoch": 6.716673810544363,
      "grad_norm": 0.35761475563049316,
      "learning_rate": 1.1044434919274184e-05,
      "loss": 0.3311,
      "step": 31340
    },
    {
      "epoch": 6.718816973853407,
      "grad_norm": 0.33552560210227966,
      "learning_rate": 1.1041577368195457e-05,
      "loss": 0.496,
      "step": 31350
    },
    {
      "epoch": 6.720960137162452,
      "grad_norm": 0.3133305311203003,
      "learning_rate": 1.1038719817116731e-05,
      "loss": 0.1705,
      "step": 31360
    },
    {
      "epoch": 6.723103300471496,
      "grad_norm": 0.31391751766204834,
      "learning_rate": 1.1035862266038007e-05,
      "loss": 1.1624,
      "step": 31370
    },
    {
      "epoch": 6.72524646378054,
      "grad_norm": 14.717130661010742,
      "learning_rate": 1.1033004714959281e-05,
      "loss": 1.8059,
      "step": 31380
    },
    {
      "epoch": 6.727389627089584,
      "grad_norm": 14.655757904052734,
      "learning_rate": 1.1030147163880557e-05,
      "loss": 0.7907,
      "step": 31390
    },
    {
      "epoch": 6.729532790398628,
      "grad_norm": 0.5053460597991943,
      "learning_rate": 1.102728961280183e-05,
      "loss": 0.6158,
      "step": 31400
    },
    {
      "epoch": 6.731675953707673,
      "grad_norm": 0.49141475558280945,
      "learning_rate": 1.1024432061723104e-05,
      "loss": 0.3113,
      "step": 31410
    },
    {
      "epoch": 6.733819117016717,
      "grad_norm": 0.42836347222328186,
      "learning_rate": 1.102157451064438e-05,
      "loss": 0.3185,
      "step": 31420
    },
    {
      "epoch": 6.735962280325761,
      "grad_norm": 0.4009558856487274,
      "learning_rate": 1.1018716959565654e-05,
      "loss": 0.7833,
      "step": 31430
    },
    {
      "epoch": 6.738105443634805,
      "grad_norm": 0.42996934056282043,
      "learning_rate": 1.1015859408486928e-05,
      "loss": 0.3174,
      "step": 31440
    },
    {
      "epoch": 6.740248606943849,
      "grad_norm": 0.463273823261261,
      "learning_rate": 1.1013001857408204e-05,
      "loss": 1.0883,
      "step": 31450
    },
    {
      "epoch": 6.742391770252893,
      "grad_norm": 0.4927229881286621,
      "learning_rate": 1.1010144306329476e-05,
      "loss": 0.613,
      "step": 31460
    },
    {
      "epoch": 6.744534933561938,
      "grad_norm": 14.985631942749023,
      "learning_rate": 1.100728675525075e-05,
      "loss": 0.4579,
      "step": 31470
    },
    {
      "epoch": 6.746678096870982,
      "grad_norm": 0.4791608452796936,
      "learning_rate": 1.1004429204172025e-05,
      "loss": 0.3177,
      "step": 31480
    },
    {
      "epoch": 6.7488212601800255,
      "grad_norm": 0.5191590189933777,
      "learning_rate": 1.10015716530933e-05,
      "loss": 0.9089,
      "step": 31490
    },
    {
      "epoch": 6.75096442348907,
      "grad_norm": 0.48070859909057617,
      "learning_rate": 1.0998714102014573e-05,
      "loss": 0.3087,
      "step": 31500
    },
    {
      "epoch": 6.753107586798114,
      "grad_norm": 0.5755937695503235,
      "learning_rate": 1.0995856550935849e-05,
      "loss": 1.0381,
      "step": 31510
    },
    {
      "epoch": 6.755250750107158,
      "grad_norm": 0.5160882472991943,
      "learning_rate": 1.0992998999857123e-05,
      "loss": 0.4505,
      "step": 31520
    },
    {
      "epoch": 6.757393913416203,
      "grad_norm": 0.5050685405731201,
      "learning_rate": 1.0990141448778398e-05,
      "loss": 0.6013,
      "step": 31530
    },
    {
      "epoch": 6.7595370767252465,
      "grad_norm": 0.6061302423477173,
      "learning_rate": 1.0987283897699672e-05,
      "loss": 1.0402,
      "step": 31540
    },
    {
      "epoch": 6.76168024003429,
      "grad_norm": 0.5116879343986511,
      "learning_rate": 1.0984426346620946e-05,
      "loss": 0.3087,
      "step": 31550
    },
    {
      "epoch": 6.763823403343335,
      "grad_norm": 0.5286787748336792,
      "learning_rate": 1.0981568795542222e-05,
      "loss": 0.7542,
      "step": 31560
    },
    {
      "epoch": 6.765966566652379,
      "grad_norm": 15.169853210449219,
      "learning_rate": 1.0978711244463496e-05,
      "loss": 0.7607,
      "step": 31570
    },
    {
      "epoch": 6.768109729961423,
      "grad_norm": 15.005145072937012,
      "learning_rate": 1.097585369338477e-05,
      "loss": 0.7319,
      "step": 31580
    },
    {
      "epoch": 6.7702528932704675,
      "grad_norm": 0.6131864786148071,
      "learning_rate": 1.0972996142306045e-05,
      "loss": 0.4307,
      "step": 31590
    },
    {
      "epoch": 6.772396056579511,
      "grad_norm": 0.6259485483169556,
      "learning_rate": 1.097013859122732e-05,
      "loss": 0.8609,
      "step": 31600
    },
    {
      "epoch": 6.774539219888555,
      "grad_norm": 0.5425589084625244,
      "learning_rate": 1.0967281040148595e-05,
      "loss": 0.2905,
      "step": 31610
    },
    {
      "epoch": 6.7766823831976,
      "grad_norm": 15.417189598083496,
      "learning_rate": 1.0964423489069869e-05,
      "loss": 0.272,
      "step": 31620
    },
    {
      "epoch": 6.778825546506644,
      "grad_norm": 0.4447689652442932,
      "learning_rate": 1.0961565937991143e-05,
      "loss": 0.4165,
      "step": 31630
    },
    {
      "epoch": 6.780968709815688,
      "grad_norm": 0.4119047522544861,
      "learning_rate": 1.0958708386912418e-05,
      "loss": 0.43,
      "step": 31640
    },
    {
      "epoch": 6.783111873124732,
      "grad_norm": 15.864601135253906,
      "learning_rate": 1.0955850835833692e-05,
      "loss": 0.6356,
      "step": 31650
    },
    {
      "epoch": 6.785255036433776,
      "grad_norm": 0.4634689688682556,
      "learning_rate": 1.0952993284754966e-05,
      "loss": 0.473,
      "step": 31660
    },
    {
      "epoch": 6.78739819974282,
      "grad_norm": 0.7188987731933594,
      "learning_rate": 1.095013573367624e-05,
      "loss": 0.6372,
      "step": 31670
    },
    {
      "epoch": 6.789541363051865,
      "grad_norm": 0.44103923439979553,
      "learning_rate": 1.0947278182597514e-05,
      "loss": 0.7708,
      "step": 31680
    },
    {
      "epoch": 6.791684526360909,
      "grad_norm": 0.43381384015083313,
      "learning_rate": 1.0944420631518788e-05,
      "loss": 1.0694,
      "step": 31690
    },
    {
      "epoch": 6.7938276896699525,
      "grad_norm": 15.217677116394043,
      "learning_rate": 1.0941563080440064e-05,
      "loss": 0.8649,
      "step": 31700
    },
    {
      "epoch": 6.795970852978997,
      "grad_norm": 0.4521616995334625,
      "learning_rate": 1.0938705529361338e-05,
      "loss": 0.2923,
      "step": 31710
    },
    {
      "epoch": 6.798114016288041,
      "grad_norm": 0.48298022150993347,
      "learning_rate": 1.0935847978282611e-05,
      "loss": 0.5935,
      "step": 31720
    },
    {
      "epoch": 6.800257179597085,
      "grad_norm": 0.29429957270622253,
      "learning_rate": 1.0932990427203887e-05,
      "loss": 0.4808,
      "step": 31730
    },
    {
      "epoch": 6.80240034290613,
      "grad_norm": 0.360569566488266,
      "learning_rate": 1.0930132876125161e-05,
      "loss": 0.6384,
      "step": 31740
    },
    {
      "epoch": 6.804543506215174,
      "grad_norm": 0.303263783454895,
      "learning_rate": 1.0927275325046437e-05,
      "loss": 0.3329,
      "step": 31750
    },
    {
      "epoch": 6.806686669524217,
      "grad_norm": 15.669478416442871,
      "learning_rate": 1.092441777396771e-05,
      "loss": 0.6627,
      "step": 31760
    },
    {
      "epoch": 6.808829832833262,
      "grad_norm": 0.36555445194244385,
      "learning_rate": 1.0921560222888984e-05,
      "loss": 0.7959,
      "step": 31770
    },
    {
      "epoch": 6.810972996142306,
      "grad_norm": 0.5437179803848267,
      "learning_rate": 1.091870267181026e-05,
      "loss": 1.1202,
      "step": 31780
    },
    {
      "epoch": 6.81311615945135,
      "grad_norm": 0.5187035202980042,
      "learning_rate": 1.0915845120731534e-05,
      "loss": 0.3074,
      "step": 31790
    },
    {
      "epoch": 6.815259322760395,
      "grad_norm": 0.5441744923591614,
      "learning_rate": 1.0912987569652808e-05,
      "loss": 0.8941,
      "step": 31800
    },
    {
      "epoch": 6.817402486069438,
      "grad_norm": 0.4629128873348236,
      "learning_rate": 1.0910130018574084e-05,
      "loss": 0.534,
      "step": 31810
    },
    {
      "epoch": 6.819545649378482,
      "grad_norm": 0.4749937951564789,
      "learning_rate": 1.0907272467495357e-05,
      "loss": 0.2812,
      "step": 31820
    },
    {
      "epoch": 6.821688812687527,
      "grad_norm": 16.051420211791992,
      "learning_rate": 1.0904414916416633e-05,
      "loss": 0.7486,
      "step": 31830
    },
    {
      "epoch": 6.823831975996571,
      "grad_norm": 2.873476266860962,
      "learning_rate": 1.0901557365337907e-05,
      "loss": 0.4364,
      "step": 31840
    },
    {
      "epoch": 6.825975139305615,
      "grad_norm": 0.48862093687057495,
      "learning_rate": 1.0898699814259181e-05,
      "loss": 0.8034,
      "step": 31850
    },
    {
      "epoch": 6.8281183026146595,
      "grad_norm": 0.43044427037239075,
      "learning_rate": 1.0895842263180457e-05,
      "loss": 0.2911,
      "step": 31860
    },
    {
      "epoch": 6.830261465923703,
      "grad_norm": 0.4917697608470917,
      "learning_rate": 1.089298471210173e-05,
      "loss": 0.1789,
      "step": 31870
    },
    {
      "epoch": 6.832404629232747,
      "grad_norm": 0.5069199204444885,
      "learning_rate": 1.0890127161023004e-05,
      "loss": 0.6151,
      "step": 31880
    },
    {
      "epoch": 6.834547792541792,
      "grad_norm": 30.550745010375977,
      "learning_rate": 1.0887269609944278e-05,
      "loss": 0.9875,
      "step": 31890
    },
    {
      "epoch": 6.836690955850836,
      "grad_norm": 1.0991148948669434,
      "learning_rate": 1.0884412058865552e-05,
      "loss": 0.6253,
      "step": 31900
    },
    {
      "epoch": 6.83883411915988,
      "grad_norm": 0.24072690308094025,
      "learning_rate": 1.0881554507786826e-05,
      "loss": 0.5915,
      "step": 31910
    },
    {
      "epoch": 6.840977282468924,
      "grad_norm": 0.004341672174632549,
      "learning_rate": 1.0878696956708102e-05,
      "loss": 0.4873,
      "step": 31920
    },
    {
      "epoch": 6.843120445777968,
      "grad_norm": 0.0037767807953059673,
      "learning_rate": 1.0875839405629376e-05,
      "loss": 0.3631,
      "step": 31930
    },
    {
      "epoch": 6.845263609087013,
      "grad_norm": 14.650845527648926,
      "learning_rate": 1.087298185455065e-05,
      "loss": 0.6798,
      "step": 31940
    },
    {
      "epoch": 6.847406772396057,
      "grad_norm": 0.7646932601928711,
      "learning_rate": 1.0870124303471925e-05,
      "loss": 0.4953,
      "step": 31950
    },
    {
      "epoch": 6.849549935705101,
      "grad_norm": 17.829700469970703,
      "learning_rate": 1.08672667523932e-05,
      "loss": 0.6323,
      "step": 31960
    },
    {
      "epoch": 6.851693099014145,
      "grad_norm": 0.45580580830574036,
      "learning_rate": 1.0864409201314475e-05,
      "loss": 0.4039,
      "step": 31970
    },
    {
      "epoch": 6.853836262323189,
      "grad_norm": 0.007918796502053738,
      "learning_rate": 1.0861551650235749e-05,
      "loss": 0.3112,
      "step": 31980
    },
    {
      "epoch": 6.855979425632233,
      "grad_norm": 0.002339859027415514,
      "learning_rate": 1.0858694099157023e-05,
      "loss": 0.2944,
      "step": 31990
    },
    {
      "epoch": 6.858122588941278,
      "grad_norm": 0.0003565786755643785,
      "learning_rate": 1.0855836548078298e-05,
      "loss": 0.3228,
      "step": 32000
    },
    {
      "epoch": 6.860265752250322,
      "grad_norm": 0.005384683143347502,
      "learning_rate": 1.0852978996999572e-05,
      "loss": 0.7693,
      "step": 32010
    },
    {
      "epoch": 6.8624089155593655,
      "grad_norm": 17.45979881286621,
      "learning_rate": 1.0850121445920846e-05,
      "loss": 0.8246,
      "step": 32020
    },
    {
      "epoch": 6.86455207886841,
      "grad_norm": 17.509279251098633,
      "learning_rate": 1.0847263894842122e-05,
      "loss": 0.7401,
      "step": 32030
    },
    {
      "epoch": 6.866695242177454,
      "grad_norm": 0.37240245938301086,
      "learning_rate": 1.0844406343763396e-05,
      "loss": 0.2497,
      "step": 32040
    },
    {
      "epoch": 6.868838405486498,
      "grad_norm": 0.4758233428001404,
      "learning_rate": 1.0841548792684671e-05,
      "loss": 1.2768,
      "step": 32050
    },
    {
      "epoch": 6.870981568795543,
      "grad_norm": 32.87470626831055,
      "learning_rate": 1.0838691241605945e-05,
      "loss": 0.4575,
      "step": 32060
    },
    {
      "epoch": 6.8731247321045865,
      "grad_norm": 0.36265328526496887,
      "learning_rate": 1.083583369052722e-05,
      "loss": 0.2854,
      "step": 32070
    },
    {
      "epoch": 6.87526789541363,
      "grad_norm": 87.6250991821289,
      "learning_rate": 1.0832976139448495e-05,
      "loss": 0.5772,
      "step": 32080
    },
    {
      "epoch": 6.877411058722675,
      "grad_norm": 21.630521774291992,
      "learning_rate": 1.0830118588369769e-05,
      "loss": 0.591,
      "step": 32090
    },
    {
      "epoch": 6.879554222031719,
      "grad_norm": 0.6409958004951477,
      "learning_rate": 1.0827261037291041e-05,
      "loss": 0.7046,
      "step": 32100
    },
    {
      "epoch": 6.881697385340763,
      "grad_norm": 0.16901224851608276,
      "learning_rate": 1.0824403486212317e-05,
      "loss": 0.745,
      "step": 32110
    },
    {
      "epoch": 6.8838405486498075,
      "grad_norm": 0.9098384380340576,
      "learning_rate": 1.082154593513359e-05,
      "loss": 0.2298,
      "step": 32120
    },
    {
      "epoch": 6.885983711958851,
      "grad_norm": 1.1554394960403442,
      "learning_rate": 1.0818688384054865e-05,
      "loss": 0.8561,
      "step": 32130
    },
    {
      "epoch": 6.888126875267895,
      "grad_norm": 0.8613840937614441,
      "learning_rate": 1.081583083297614e-05,
      "loss": 0.9236,
      "step": 32140
    },
    {
      "epoch": 6.89027003857694,
      "grad_norm": 15.326176643371582,
      "learning_rate": 1.0812973281897414e-05,
      "loss": 1.1224,
      "step": 32150
    },
    {
      "epoch": 6.892413201885984,
      "grad_norm": 0.39429590106010437,
      "learning_rate": 1.0810115730818688e-05,
      "loss": 0.4914,
      "step": 32160
    },
    {
      "epoch": 6.894556365195028,
      "grad_norm": 0.29765933752059937,
      "learning_rate": 1.0807258179739964e-05,
      "loss": 0.2743,
      "step": 32170
    },
    {
      "epoch": 6.896699528504072,
      "grad_norm": 11.974226951599121,
      "learning_rate": 1.0804400628661238e-05,
      "loss": 0.7669,
      "step": 32180
    },
    {
      "epoch": 6.898842691813116,
      "grad_norm": 20.1080265045166,
      "learning_rate": 1.0801543077582513e-05,
      "loss": 0.528,
      "step": 32190
    },
    {
      "epoch": 6.90098585512216,
      "grad_norm": 0.3215259611606598,
      "learning_rate": 1.0798685526503787e-05,
      "loss": 0.4449,
      "step": 32200
    },
    {
      "epoch": 6.903129018431205,
      "grad_norm": 15.006359100341797,
      "learning_rate": 1.0795827975425061e-05,
      "loss": 0.169,
      "step": 32210
    },
    {
      "epoch": 6.905272181740249,
      "grad_norm": 0.39312660694122314,
      "learning_rate": 1.0792970424346337e-05,
      "loss": 0.5182,
      "step": 32220
    },
    {
      "epoch": 6.9074153450492926,
      "grad_norm": 0.22374925017356873,
      "learning_rate": 1.079011287326761e-05,
      "loss": 0.261,
      "step": 32230
    },
    {
      "epoch": 6.909558508358337,
      "grad_norm": 0.19738289713859558,
      "learning_rate": 1.0787255322188886e-05,
      "loss": 0.0109,
      "step": 32240
    },
    {
      "epoch": 6.911701671667381,
      "grad_norm": 0.16042973101139069,
      "learning_rate": 1.078439777111016e-05,
      "loss": 1.0871,
      "step": 32250
    },
    {
      "epoch": 6.913844834976425,
      "grad_norm": 0.2513229250907898,
      "learning_rate": 1.0781540220031434e-05,
      "loss": 0.9745,
      "step": 32260
    },
    {
      "epoch": 6.91598799828547,
      "grad_norm": 0.30237433314323425,
      "learning_rate": 1.077868266895271e-05,
      "loss": 0.2431,
      "step": 32270
    },
    {
      "epoch": 6.918131161594514,
      "grad_norm": 0.27575090527534485,
      "learning_rate": 1.0775825117873984e-05,
      "loss": 0.4799,
      "step": 32280
    },
    {
      "epoch": 6.920274324903557,
      "grad_norm": 0.3855806291103363,
      "learning_rate": 1.0772967566795258e-05,
      "loss": 0.7411,
      "step": 32290
    },
    {
      "epoch": 6.922417488212602,
      "grad_norm": 3.639380693435669,
      "learning_rate": 1.0770110015716533e-05,
      "loss": 0.4601,
      "step": 32300
    },
    {
      "epoch": 6.924560651521646,
      "grad_norm": 0.3294590711593628,
      "learning_rate": 1.0767252464637807e-05,
      "loss": 0.3638,
      "step": 32310
    },
    {
      "epoch": 6.92670381483069,
      "grad_norm": 0.5165048837661743,
      "learning_rate": 1.076439491355908e-05,
      "loss": 0.5003,
      "step": 32320
    },
    {
      "epoch": 6.928846978139735,
      "grad_norm": 0.26159828901290894,
      "learning_rate": 1.0761537362480355e-05,
      "loss": 0.4244,
      "step": 32330
    },
    {
      "epoch": 6.9309901414487785,
      "grad_norm": 0.2819521427154541,
      "learning_rate": 1.0758679811401629e-05,
      "loss": 0.6419,
      "step": 32340
    },
    {
      "epoch": 6.933133304757822,
      "grad_norm": 0.25923070311546326,
      "learning_rate": 1.0755822260322903e-05,
      "loss": 0.0099,
      "step": 32350
    },
    {
      "epoch": 6.935276468066867,
      "grad_norm": 0.25760138034820557,
      "learning_rate": 1.0752964709244178e-05,
      "loss": 0.1673,
      "step": 32360
    },
    {
      "epoch": 6.937419631375911,
      "grad_norm": 0.29230430722236633,
      "learning_rate": 1.0750107158165452e-05,
      "loss": 0.3578,
      "step": 32370
    },
    {
      "epoch": 6.939562794684955,
      "grad_norm": 16.34713363647461,
      "learning_rate": 1.0747249607086728e-05,
      "loss": 0.8011,
      "step": 32380
    },
    {
      "epoch": 6.9417059579939995,
      "grad_norm": 0.32437682151794434,
      "learning_rate": 1.0744392056008002e-05,
      "loss": 0.8276,
      "step": 32390
    },
    {
      "epoch": 6.943849121303043,
      "grad_norm": 2.3492326736450195,
      "learning_rate": 1.0741534504929276e-05,
      "loss": 0.7875,
      "step": 32400
    },
    {
      "epoch": 6.945992284612087,
      "grad_norm": 0.36145251989364624,
      "learning_rate": 1.0738676953850551e-05,
      "loss": 0.1375,
      "step": 32410
    },
    {
      "epoch": 6.948135447921132,
      "grad_norm": 0.26376771926879883,
      "learning_rate": 1.0735819402771825e-05,
      "loss": 0.3741,
      "step": 32420
    },
    {
      "epoch": 6.950278611230176,
      "grad_norm": 0.37253811955451965,
      "learning_rate": 1.07329618516931e-05,
      "loss": 0.951,
      "step": 32430
    },
    {
      "epoch": 6.95242177453922,
      "grad_norm": 15.098142623901367,
      "learning_rate": 1.0730104300614375e-05,
      "loss": 0.4962,
      "step": 32440
    },
    {
      "epoch": 6.954564937848264,
      "grad_norm": 13.923334121704102,
      "learning_rate": 1.0727246749535649e-05,
      "loss": 0.4108,
      "step": 32450
    },
    {
      "epoch": 6.956708101157308,
      "grad_norm": 13.065964698791504,
      "learning_rate": 1.0724389198456924e-05,
      "loss": 0.1912,
      "step": 32460
    },
    {
      "epoch": 6.958851264466352,
      "grad_norm": 0.445250928401947,
      "learning_rate": 1.0721531647378198e-05,
      "loss": 0.8991,
      "step": 32470
    },
    {
      "epoch": 6.960994427775397,
      "grad_norm": 0.25699859857559204,
      "learning_rate": 1.0718674096299472e-05,
      "loss": 0.0613,
      "step": 32480
    },
    {
      "epoch": 6.963137591084441,
      "grad_norm": 3.2534687519073486,
      "learning_rate": 1.0715816545220748e-05,
      "loss": 0.3537,
      "step": 32490
    },
    {
      "epoch": 6.9652807543934845,
      "grad_norm": 0.23816795647144318,
      "learning_rate": 1.0712958994142022e-05,
      "loss": 0.4052,
      "step": 32500
    },
    {
      "epoch": 6.967423917702529,
      "grad_norm": 0.2812669575214386,
      "learning_rate": 1.0710101443063296e-05,
      "loss": 0.2106,
      "step": 32510
    },
    {
      "epoch": 6.969567081011573,
      "grad_norm": 0.23744577169418335,
      "learning_rate": 1.0707243891984571e-05,
      "loss": 0.3901,
      "step": 32520
    },
    {
      "epoch": 6.971710244320617,
      "grad_norm": 0.15333428978919983,
      "learning_rate": 1.0704386340905844e-05,
      "loss": 0.3169,
      "step": 32530
    },
    {
      "epoch": 6.973853407629662,
      "grad_norm": 28.507455825805664,
      "learning_rate": 1.0701528789827118e-05,
      "loss": 0.5337,
      "step": 32540
    },
    {
      "epoch": 6.9759965709387055,
      "grad_norm": 14.89377498626709,
      "learning_rate": 1.0698671238748393e-05,
      "loss": 0.3977,
      "step": 32550
    },
    {
      "epoch": 6.978139734247749,
      "grad_norm": 0.27647244930267334,
      "learning_rate": 1.0695813687669667e-05,
      "loss": 0.1232,
      "step": 32560
    },
    {
      "epoch": 6.980282897556794,
      "grad_norm": 0.5033677816390991,
      "learning_rate": 1.0692956136590941e-05,
      "loss": 0.5825,
      "step": 32570
    },
    {
      "epoch": 6.982426060865838,
      "grad_norm": 0.08994585275650024,
      "learning_rate": 1.0690098585512217e-05,
      "loss": 0.4064,
      "step": 32580
    },
    {
      "epoch": 6.984569224174882,
      "grad_norm": 0.11102510243654251,
      "learning_rate": 1.068724103443349e-05,
      "loss": 0.6081,
      "step": 32590
    },
    {
      "epoch": 6.9867123874839265,
      "grad_norm": 0.16579420864582062,
      "learning_rate": 1.0684383483354766e-05,
      "loss": 0.4426,
      "step": 32600
    },
    {
      "epoch": 6.98885555079297,
      "grad_norm": 16.314924240112305,
      "learning_rate": 1.068152593227604e-05,
      "loss": 1.0214,
      "step": 32610
    },
    {
      "epoch": 6.990998714102014,
      "grad_norm": 0.6125161647796631,
      "learning_rate": 1.0678668381197314e-05,
      "loss": 0.3157,
      "step": 32620
    },
    {
      "epoch": 6.993141877411059,
      "grad_norm": 0.5850809216499329,
      "learning_rate": 1.067581083011859e-05,
      "loss": 0.4287,
      "step": 32630
    },
    {
      "epoch": 6.995285040720103,
      "grad_norm": 6.641164779663086,
      "learning_rate": 1.0672953279039864e-05,
      "loss": 0.1986,
      "step": 32640
    },
    {
      "epoch": 6.997428204029147,
      "grad_norm": 13.122706413269043,
      "learning_rate": 1.0670095727961138e-05,
      "loss": 0.4761,
      "step": 32650
    },
    {
      "epoch": 6.999571367338191,
      "grad_norm": 0.3348774015903473,
      "learning_rate": 1.0667238176882413e-05,
      "loss": 0.5625,
      "step": 32660
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.8736666666666667,
      "eval_f1": 0.0,
      "eval_loss": 0.3719697594642639,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 704.1072,
      "eval_samples_per_second": 4.261,
      "eval_steps_per_second": 1.42,
      "step": 32662
    },
    {
      "epoch": 7.001714530647235,
      "grad_norm": 6.074079990386963,
      "learning_rate": 1.0664380625803687e-05,
      "loss": 0.3954,
      "step": 32670
    },
    {
      "epoch": 7.003857693956279,
      "grad_norm": 0.26724740862846375,
      "learning_rate": 1.0661523074724963e-05,
      "loss": 0.2007,
      "step": 32680
    },
    {
      "epoch": 7.006000857265324,
      "grad_norm": 17.28325080871582,
      "learning_rate": 1.0658665523646237e-05,
      "loss": 0.5587,
      "step": 32690
    },
    {
      "epoch": 7.008144020574368,
      "grad_norm": 0.32044172286987305,
      "learning_rate": 1.065580797256751e-05,
      "loss": 0.2851,
      "step": 32700
    },
    {
      "epoch": 7.0102871838834115,
      "grad_norm": 0.4272401034832001,
      "learning_rate": 1.0652950421488786e-05,
      "loss": 0.3324,
      "step": 32710
    },
    {
      "epoch": 7.012430347192456,
      "grad_norm": 89.30499267578125,
      "learning_rate": 1.065009287041006e-05,
      "loss": 0.8001,
      "step": 32720
    },
    {
      "epoch": 7.0145735105015,
      "grad_norm": 25.14073944091797,
      "learning_rate": 1.0647235319331334e-05,
      "loss": 0.653,
      "step": 32730
    },
    {
      "epoch": 7.016716673810544,
      "grad_norm": 23.052608489990234,
      "learning_rate": 1.064437776825261e-05,
      "loss": 0.7095,
      "step": 32740
    },
    {
      "epoch": 7.018859837119589,
      "grad_norm": 24.426721572875977,
      "learning_rate": 1.0641520217173882e-05,
      "loss": 0.5809,
      "step": 32750
    },
    {
      "epoch": 7.021003000428633,
      "grad_norm": 19.837671279907227,
      "learning_rate": 1.0638662666095156e-05,
      "loss": 0.2202,
      "step": 32760
    },
    {
      "epoch": 7.023146163737676,
      "grad_norm": 8.266400337219238,
      "learning_rate": 1.0635805115016431e-05,
      "loss": 0.1869,
      "step": 32770
    },
    {
      "epoch": 7.025289327046721,
      "grad_norm": 0.3210415542125702,
      "learning_rate": 1.0632947563937705e-05,
      "loss": 0.1751,
      "step": 32780
    },
    {
      "epoch": 7.027432490355765,
      "grad_norm": 0.18890629708766937,
      "learning_rate": 1.063009001285898e-05,
      "loss": 0.4307,
      "step": 32790
    },
    {
      "epoch": 7.029575653664809,
      "grad_norm": 16.602739334106445,
      "learning_rate": 1.0627232461780255e-05,
      "loss": 0.5145,
      "step": 32800
    },
    {
      "epoch": 7.031718816973854,
      "grad_norm": 1.21589994430542,
      "learning_rate": 1.0624374910701529e-05,
      "loss": 0.213,
      "step": 32810
    },
    {
      "epoch": 7.033861980282897,
      "grad_norm": 16.38780403137207,
      "learning_rate": 1.0621517359622805e-05,
      "loss": 0.517,
      "step": 32820
    },
    {
      "epoch": 7.036005143591941,
      "grad_norm": 23.06351089477539,
      "learning_rate": 1.0618659808544078e-05,
      "loss": 0.3333,
      "step": 32830
    },
    {
      "epoch": 7.038148306900986,
      "grad_norm": 0.39775052666664124,
      "learning_rate": 1.0615802257465352e-05,
      "loss": 0.3176,
      "step": 32840
    },
    {
      "epoch": 7.04029147021003,
      "grad_norm": 0.14660125970840454,
      "learning_rate": 1.0612944706386628e-05,
      "loss": 0.489,
      "step": 32850
    },
    {
      "epoch": 7.042434633519074,
      "grad_norm": 0.10990817844867706,
      "learning_rate": 1.0610087155307902e-05,
      "loss": 0.3535,
      "step": 32860
    },
    {
      "epoch": 7.0445777968281185,
      "grad_norm": 0.18058237433433533,
      "learning_rate": 1.0607229604229176e-05,
      "loss": 0.6361,
      "step": 32870
    },
    {
      "epoch": 7.046720960137162,
      "grad_norm": 0.11379116773605347,
      "learning_rate": 1.0604372053150451e-05,
      "loss": 0.4088,
      "step": 32880
    },
    {
      "epoch": 7.048864123446206,
      "grad_norm": 0.9998680949211121,
      "learning_rate": 1.0601514502071725e-05,
      "loss": 0.8763,
      "step": 32890
    },
    {
      "epoch": 7.051007286755251,
      "grad_norm": 0.1161007434129715,
      "learning_rate": 1.0598656950993001e-05,
      "loss": 0.1147,
      "step": 32900
    },
    {
      "epoch": 7.053150450064295,
      "grad_norm": 0.4483764171600342,
      "learning_rate": 1.0595799399914275e-05,
      "loss": 0.283,
      "step": 32910
    },
    {
      "epoch": 7.055293613373339,
      "grad_norm": 0.767991840839386,
      "learning_rate": 1.0592941848835549e-05,
      "loss": 0.5473,
      "step": 32920
    },
    {
      "epoch": 7.057436776682383,
      "grad_norm": 17.83175277709961,
      "learning_rate": 1.0590084297756825e-05,
      "loss": 0.7852,
      "step": 32930
    },
    {
      "epoch": 7.059579939991427,
      "grad_norm": 0.16546101868152618,
      "learning_rate": 1.0587226746678098e-05,
      "loss": 0.3455,
      "step": 32940
    },
    {
      "epoch": 7.061723103300472,
      "grad_norm": 0.1393367201089859,
      "learning_rate": 1.0584369195599374e-05,
      "loss": 0.4284,
      "step": 32950
    },
    {
      "epoch": 7.063866266609516,
      "grad_norm": 0.1883241832256317,
      "learning_rate": 1.0581511644520646e-05,
      "loss": 0.2293,
      "step": 32960
    },
    {
      "epoch": 7.06600942991856,
      "grad_norm": 0.2487800568342209,
      "learning_rate": 1.057865409344192e-05,
      "loss": 0.3559,
      "step": 32970
    },
    {
      "epoch": 7.068152593227604,
      "grad_norm": 0.25996407866477966,
      "learning_rate": 1.0575796542363194e-05,
      "loss": 0.5417,
      "step": 32980
    },
    {
      "epoch": 7.070295756536648,
      "grad_norm": 19.95904541015625,
      "learning_rate": 1.057293899128447e-05,
      "loss": 0.2944,
      "step": 32990
    },
    {
      "epoch": 7.072438919845692,
      "grad_norm": 16.873348236083984,
      "learning_rate": 1.0570081440205744e-05,
      "loss": 0.7409,
      "step": 33000
    },
    {
      "epoch": 7.074582083154737,
      "grad_norm": 16.86246681213379,
      "learning_rate": 1.0567223889127018e-05,
      "loss": 0.4341,
      "step": 33010
    },
    {
      "epoch": 7.076725246463781,
      "grad_norm": 23.691967010498047,
      "learning_rate": 1.0564366338048293e-05,
      "loss": 0.4704,
      "step": 33020
    },
    {
      "epoch": 7.0788684097728245,
      "grad_norm": 0.17145384848117828,
      "learning_rate": 1.0561508786969567e-05,
      "loss": 0.1884,
      "step": 33030
    },
    {
      "epoch": 7.081011573081869,
      "grad_norm": 15.174986839294434,
      "learning_rate": 1.0558651235890843e-05,
      "loss": 0.7741,
      "step": 33040
    },
    {
      "epoch": 7.083154736390913,
      "grad_norm": 24.75800132751465,
      "learning_rate": 1.0555793684812117e-05,
      "loss": 0.1658,
      "step": 33050
    },
    {
      "epoch": 7.085297899699957,
      "grad_norm": 29.967443466186523,
      "learning_rate": 1.055293613373339e-05,
      "loss": 0.6446,
      "step": 33060
    },
    {
      "epoch": 7.087441063009002,
      "grad_norm": 13.586204528808594,
      "learning_rate": 1.0550078582654666e-05,
      "loss": 0.1823,
      "step": 33070
    },
    {
      "epoch": 7.0895842263180455,
      "grad_norm": 15.996440887451172,
      "learning_rate": 1.054722103157594e-05,
      "loss": 0.2815,
      "step": 33080
    },
    {
      "epoch": 7.091727389627089,
      "grad_norm": 0.1780339926481247,
      "learning_rate": 1.0544363480497216e-05,
      "loss": 0.4159,
      "step": 33090
    },
    {
      "epoch": 7.093870552936134,
      "grad_norm": 4.258251667022705,
      "learning_rate": 1.054150592941849e-05,
      "loss": 0.6213,
      "step": 33100
    },
    {
      "epoch": 7.096013716245178,
      "grad_norm": 0.20436276495456696,
      "learning_rate": 1.0538648378339764e-05,
      "loss": 0.2561,
      "step": 33110
    },
    {
      "epoch": 7.098156879554222,
      "grad_norm": 0.10575558245182037,
      "learning_rate": 1.053579082726104e-05,
      "loss": 0.2534,
      "step": 33120
    },
    {
      "epoch": 7.1003000428632665,
      "grad_norm": 15.834528923034668,
      "learning_rate": 1.0532933276182313e-05,
      "loss": 0.5229,
      "step": 33130
    },
    {
      "epoch": 7.10244320617231,
      "grad_norm": 0.14895471930503845,
      "learning_rate": 1.0530075725103587e-05,
      "loss": 0.6969,
      "step": 33140
    },
    {
      "epoch": 7.104586369481354,
      "grad_norm": 0.20135772228240967,
      "learning_rate": 1.0527218174024863e-05,
      "loss": 0.4847,
      "step": 33150
    },
    {
      "epoch": 7.106729532790399,
      "grad_norm": 0.3598339557647705,
      "learning_rate": 1.0524360622946137e-05,
      "loss": 0.3313,
      "step": 33160
    },
    {
      "epoch": 7.108872696099443,
      "grad_norm": 14.596010208129883,
      "learning_rate": 1.0521503071867412e-05,
      "loss": 0.1152,
      "step": 33170
    },
    {
      "epoch": 7.111015859408487,
      "grad_norm": 7.534150123596191,
      "learning_rate": 1.0518645520788685e-05,
      "loss": 0.1131,
      "step": 33180
    },
    {
      "epoch": 7.113159022717531,
      "grad_norm": 0.07935424894094467,
      "learning_rate": 1.0515787969709959e-05,
      "loss": 0.4905,
      "step": 33190
    },
    {
      "epoch": 7.115302186026575,
      "grad_norm": 46.076995849609375,
      "learning_rate": 1.0512930418631232e-05,
      "loss": 0.4704,
      "step": 33200
    },
    {
      "epoch": 7.117445349335619,
      "grad_norm": 30.670459747314453,
      "learning_rate": 1.0510072867552508e-05,
      "loss": 0.3381,
      "step": 33210
    },
    {
      "epoch": 7.119588512644664,
      "grad_norm": 0.1108432486653328,
      "learning_rate": 1.0507215316473782e-05,
      "loss": 0.0329,
      "step": 33220
    },
    {
      "epoch": 7.121731675953708,
      "grad_norm": 0.15187889337539673,
      "learning_rate": 1.0504357765395058e-05,
      "loss": 0.1855,
      "step": 33230
    },
    {
      "epoch": 7.123874839262752,
      "grad_norm": 0.03951452672481537,
      "learning_rate": 1.0501500214316332e-05,
      "loss": 0.1857,
      "step": 33240
    },
    {
      "epoch": 7.126018002571796,
      "grad_norm": 0.1533997356891632,
      "learning_rate": 1.0498642663237605e-05,
      "loss": 0.1935,
      "step": 33250
    },
    {
      "epoch": 7.12816116588084,
      "grad_norm": 0.18905331194400787,
      "learning_rate": 1.0495785112158881e-05,
      "loss": 0.0165,
      "step": 33260
    },
    {
      "epoch": 7.130304329189884,
      "grad_norm": 0.03496682643890381,
      "learning_rate": 1.0492927561080155e-05,
      "loss": 0.7286,
      "step": 33270
    },
    {
      "epoch": 7.132447492498929,
      "grad_norm": 0.0554070882499218,
      "learning_rate": 1.0490070010001429e-05,
      "loss": 0.2737,
      "step": 33280
    },
    {
      "epoch": 7.134590655807973,
      "grad_norm": 0.18088200688362122,
      "learning_rate": 1.0487212458922705e-05,
      "loss": 0.3784,
      "step": 33290
    },
    {
      "epoch": 7.136733819117016,
      "grad_norm": 14.498167037963867,
      "learning_rate": 1.0484354907843978e-05,
      "loss": 0.5083,
      "step": 33300
    },
    {
      "epoch": 7.138876982426061,
      "grad_norm": 19.889623641967773,
      "learning_rate": 1.0481497356765254e-05,
      "loss": 0.5019,
      "step": 33310
    },
    {
      "epoch": 7.141020145735105,
      "grad_norm": 0.14620958268642426,
      "learning_rate": 1.0478639805686528e-05,
      "loss": 0.1442,
      "step": 33320
    },
    {
      "epoch": 7.143163309044149,
      "grad_norm": 0.15371713042259216,
      "learning_rate": 1.0475782254607802e-05,
      "loss": 0.2235,
      "step": 33330
    },
    {
      "epoch": 7.145306472353194,
      "grad_norm": 0.44087907671928406,
      "learning_rate": 1.0472924703529078e-05,
      "loss": 0.4839,
      "step": 33340
    },
    {
      "epoch": 7.1474496356622375,
      "grad_norm": 0.16471651196479797,
      "learning_rate": 1.0470067152450352e-05,
      "loss": 0.9318,
      "step": 33350
    },
    {
      "epoch": 7.149592798971281,
      "grad_norm": 16.918243408203125,
      "learning_rate": 1.0467209601371625e-05,
      "loss": 0.5855,
      "step": 33360
    },
    {
      "epoch": 7.151735962280326,
      "grad_norm": 19.9472713470459,
      "learning_rate": 1.0464352050292901e-05,
      "loss": 0.1098,
      "step": 33370
    },
    {
      "epoch": 7.15387912558937,
      "grad_norm": 0.04267054796218872,
      "learning_rate": 1.0461494499214175e-05,
      "loss": 0.3642,
      "step": 33380
    },
    {
      "epoch": 7.156022288898414,
      "grad_norm": 19.41486167907715,
      "learning_rate": 1.0458636948135447e-05,
      "loss": 0.4336,
      "step": 33390
    },
    {
      "epoch": 7.1581654522074585,
      "grad_norm": 0.05342038720846176,
      "learning_rate": 1.0455779397056723e-05,
      "loss": 0.2922,
      "step": 33400
    },
    {
      "epoch": 7.160308615516502,
      "grad_norm": 7.013434410095215,
      "learning_rate": 1.0452921845977997e-05,
      "loss": 0.4342,
      "step": 33410
    },
    {
      "epoch": 7.162451778825546,
      "grad_norm": 31.569408416748047,
      "learning_rate": 1.045006429489927e-05,
      "loss": 0.9356,
      "step": 33420
    },
    {
      "epoch": 7.164594942134591,
      "grad_norm": 15.081374168395996,
      "learning_rate": 1.0447206743820546e-05,
      "loss": 0.5748,
      "step": 33430
    },
    {
      "epoch": 7.166738105443635,
      "grad_norm": 88.49300384521484,
      "learning_rate": 1.044434919274182e-05,
      "loss": 0.2544,
      "step": 33440
    },
    {
      "epoch": 7.168881268752679,
      "grad_norm": 10.241125106811523,
      "learning_rate": 1.0441491641663096e-05,
      "loss": 0.3282,
      "step": 33450
    },
    {
      "epoch": 7.171024432061723,
      "grad_norm": 30.780235290527344,
      "learning_rate": 1.043863409058437e-05,
      "loss": 0.5634,
      "step": 33460
    },
    {
      "epoch": 7.173167595370767,
      "grad_norm": 30.7669734954834,
      "learning_rate": 1.0435776539505644e-05,
      "loss": 0.7682,
      "step": 33470
    },
    {
      "epoch": 7.175310758679811,
      "grad_norm": 21.18197250366211,
      "learning_rate": 1.043291898842692e-05,
      "loss": 0.5146,
      "step": 33480
    },
    {
      "epoch": 7.177453921988856,
      "grad_norm": 0.06412917375564575,
      "learning_rate": 1.0430061437348193e-05,
      "loss": 0.3845,
      "step": 33490
    },
    {
      "epoch": 7.1795970852979,
      "grad_norm": 0.07300032675266266,
      "learning_rate": 1.0427203886269467e-05,
      "loss": 0.2233,
      "step": 33500
    },
    {
      "epoch": 7.1817402486069435,
      "grad_norm": 0.0691327229142189,
      "learning_rate": 1.0424346335190743e-05,
      "loss": 0.7653,
      "step": 33510
    },
    {
      "epoch": 7.183883411915988,
      "grad_norm": 0.106452576816082,
      "learning_rate": 1.0421488784112017e-05,
      "loss": 0.3439,
      "step": 33520
    },
    {
      "epoch": 7.186026575225032,
      "grad_norm": 0.04023785516619682,
      "learning_rate": 1.0418631233033292e-05,
      "loss": 0.3614,
      "step": 33530
    },
    {
      "epoch": 7.188169738534076,
      "grad_norm": 29.826753616333008,
      "learning_rate": 1.0415773681954566e-05,
      "loss": 0.3575,
      "step": 33540
    },
    {
      "epoch": 7.190312901843121,
      "grad_norm": 1648.7647705078125,
      "learning_rate": 1.041291613087584e-05,
      "loss": 0.4862,
      "step": 33550
    },
    {
      "epoch": 7.1924560651521645,
      "grad_norm": 0.27796897292137146,
      "learning_rate": 1.0410058579797116e-05,
      "loss": 0.2057,
      "step": 33560
    },
    {
      "epoch": 7.194599228461208,
      "grad_norm": 2.885776996612549,
      "learning_rate": 1.040720102871839e-05,
      "loss": 0.4417,
      "step": 33570
    },
    {
      "epoch": 7.196742391770253,
      "grad_norm": 34.13617706298828,
      "learning_rate": 1.0404343477639664e-05,
      "loss": 0.3153,
      "step": 33580
    },
    {
      "epoch": 7.198885555079297,
      "grad_norm": 0.13505326211452484,
      "learning_rate": 1.040148592656094e-05,
      "loss": 0.1243,
      "step": 33590
    },
    {
      "epoch": 7.201028718388341,
      "grad_norm": 4.451247215270996,
      "learning_rate": 1.0398628375482212e-05,
      "loss": 0.5402,
      "step": 33600
    },
    {
      "epoch": 7.2031718816973855,
      "grad_norm": 0.057745933532714844,
      "learning_rate": 1.0395770824403486e-05,
      "loss": 0.9209,
      "step": 33610
    },
    {
      "epoch": 7.205315045006429,
      "grad_norm": 1752.747802734375,
      "learning_rate": 1.0392913273324761e-05,
      "loss": 0.4846,
      "step": 33620
    },
    {
      "epoch": 7.207458208315473,
      "grad_norm": 17.319486618041992,
      "learning_rate": 1.0390055722246035e-05,
      "loss": 0.5394,
      "step": 33630
    },
    {
      "epoch": 7.209601371624518,
      "grad_norm": 0.16283872723579407,
      "learning_rate": 1.0387198171167309e-05,
      "loss": 0.1527,
      "step": 33640
    },
    {
      "epoch": 7.211744534933562,
      "grad_norm": 0.11521408706903458,
      "learning_rate": 1.0384340620088585e-05,
      "loss": 0.209,
      "step": 33650
    },
    {
      "epoch": 7.213887698242606,
      "grad_norm": 4.015092372894287,
      "learning_rate": 1.0381483069009859e-05,
      "loss": 0.7613,
      "step": 33660
    },
    {
      "epoch": 7.21603086155165,
      "grad_norm": 0.2942759096622467,
      "learning_rate": 1.0378625517931134e-05,
      "loss": 0.233,
      "step": 33670
    },
    {
      "epoch": 7.218174024860694,
      "grad_norm": 0.6815804243087769,
      "learning_rate": 1.0375767966852408e-05,
      "loss": 0.0964,
      "step": 33680
    },
    {
      "epoch": 7.220317188169738,
      "grad_norm": 0.12346407026052475,
      "learning_rate": 1.0372910415773682e-05,
      "loss": 0.1006,
      "step": 33690
    },
    {
      "epoch": 7.222460351478783,
      "grad_norm": 0.9972448945045471,
      "learning_rate": 1.0370052864694958e-05,
      "loss": 0.2242,
      "step": 33700
    },
    {
      "epoch": 7.224603514787827,
      "grad_norm": 0.059595853090286255,
      "learning_rate": 1.0367195313616232e-05,
      "loss": 0.6571,
      "step": 33710
    },
    {
      "epoch": 7.226746678096871,
      "grad_norm": 32.67827224731445,
      "learning_rate": 1.0364337762537506e-05,
      "loss": 1.1908,
      "step": 33720
    },
    {
      "epoch": 7.228889841405915,
      "grad_norm": 0.2486819475889206,
      "learning_rate": 1.0361480211458781e-05,
      "loss": 0.3163,
      "step": 33730
    },
    {
      "epoch": 7.231033004714959,
      "grad_norm": 0.4574880003929138,
      "learning_rate": 1.0358622660380055e-05,
      "loss": 0.1518,
      "step": 33740
    },
    {
      "epoch": 7.233176168024004,
      "grad_norm": 220.29098510742188,
      "learning_rate": 1.035576510930133e-05,
      "loss": 0.2314,
      "step": 33750
    },
    {
      "epoch": 7.235319331333048,
      "grad_norm": 0.1254730522632599,
      "learning_rate": 1.0352907558222605e-05,
      "loss": 0.2982,
      "step": 33760
    },
    {
      "epoch": 7.237462494642092,
      "grad_norm": 0.049133408814668655,
      "learning_rate": 1.0350050007143879e-05,
      "loss": 0.5916,
      "step": 33770
    },
    {
      "epoch": 7.239605657951136,
      "grad_norm": 16.8958683013916,
      "learning_rate": 1.0347192456065154e-05,
      "loss": 0.3583,
      "step": 33780
    },
    {
      "epoch": 7.24174882126018,
      "grad_norm": 0.11385403573513031,
      "learning_rate": 1.0344334904986428e-05,
      "loss": 0.6939,
      "step": 33790
    },
    {
      "epoch": 7.243891984569224,
      "grad_norm": 0.3203745484352112,
      "learning_rate": 1.0341477353907704e-05,
      "loss": 0.4175,
      "step": 33800
    },
    {
      "epoch": 7.246035147878269,
      "grad_norm": 0.1282998025417328,
      "learning_rate": 1.0338619802828978e-05,
      "loss": 0.1176,
      "step": 33810
    },
    {
      "epoch": 7.248178311187313,
      "grad_norm": 16.743907928466797,
      "learning_rate": 1.033576225175025e-05,
      "loss": 0.5829,
      "step": 33820
    },
    {
      "epoch": 7.2503214744963564,
      "grad_norm": 0.3014776110649109,
      "learning_rate": 1.0332904700671524e-05,
      "loss": 0.3303,
      "step": 33830
    },
    {
      "epoch": 7.252464637805401,
      "grad_norm": 0.2903091609477997,
      "learning_rate": 1.03300471495928e-05,
      "loss": 0.1259,
      "step": 33840
    },
    {
      "epoch": 7.254607801114445,
      "grad_norm": 0.5573859810829163,
      "learning_rate": 1.0327189598514073e-05,
      "loss": 0.778,
      "step": 33850
    },
    {
      "epoch": 7.256750964423489,
      "grad_norm": 19.06195640563965,
      "learning_rate": 1.0324332047435347e-05,
      "loss": 0.2067,
      "step": 33860
    },
    {
      "epoch": 7.258894127732534,
      "grad_norm": 0.2041417807340622,
      "learning_rate": 1.0321474496356623e-05,
      "loss": 0.6927,
      "step": 33870
    },
    {
      "epoch": 7.2610372910415775,
      "grad_norm": 17.55763816833496,
      "learning_rate": 1.0318616945277897e-05,
      "loss": 0.6422,
      "step": 33880
    },
    {
      "epoch": 7.263180454350621,
      "grad_norm": 0.7923210859298706,
      "learning_rate": 1.0315759394199172e-05,
      "loss": 0.039,
      "step": 33890
    },
    {
      "epoch": 7.265323617659666,
      "grad_norm": 0.19585531949996948,
      "learning_rate": 1.0312901843120446e-05,
      "loss": 0.2677,
      "step": 33900
    },
    {
      "epoch": 7.26746678096871,
      "grad_norm": 20.869121551513672,
      "learning_rate": 1.031004429204172e-05,
      "loss": 0.977,
      "step": 33910
    },
    {
      "epoch": 7.269609944277754,
      "grad_norm": 0.27806565165519714,
      "learning_rate": 1.0307186740962996e-05,
      "loss": 0.2875,
      "step": 33920
    },
    {
      "epoch": 7.2717531075867985,
      "grad_norm": 0.22153040766716003,
      "learning_rate": 1.030432918988427e-05,
      "loss": 0.3113,
      "step": 33930
    },
    {
      "epoch": 7.273896270895842,
      "grad_norm": 0.04721025750041008,
      "learning_rate": 1.0301471638805545e-05,
      "loss": 0.3794,
      "step": 33940
    },
    {
      "epoch": 7.276039434204886,
      "grad_norm": 1.6924594640731812,
      "learning_rate": 1.029861408772682e-05,
      "loss": 0.6481,
      "step": 33950
    },
    {
      "epoch": 7.278182597513931,
      "grad_norm": 18.255966186523438,
      "learning_rate": 1.0295756536648093e-05,
      "loss": 0.111,
      "step": 33960
    },
    {
      "epoch": 7.280325760822975,
      "grad_norm": 0.7043874859809875,
      "learning_rate": 1.0292898985569369e-05,
      "loss": 0.2006,
      "step": 33970
    },
    {
      "epoch": 7.282468924132019,
      "grad_norm": 0.09575854241847992,
      "learning_rate": 1.0290041434490643e-05,
      "loss": 0.5678,
      "step": 33980
    },
    {
      "epoch": 7.284612087441063,
      "grad_norm": 0.2165149301290512,
      "learning_rate": 1.0287183883411917e-05,
      "loss": 0.419,
      "step": 33990
    },
    {
      "epoch": 7.286755250750107,
      "grad_norm": 0.1617538183927536,
      "learning_rate": 1.0284326332333192e-05,
      "loss": 0.7383,
      "step": 34000
    },
    {
      "epoch": 7.288898414059151,
      "grad_norm": 0.180083766579628,
      "learning_rate": 1.0281468781254466e-05,
      "loss": 0.1715,
      "step": 34010
    },
    {
      "epoch": 7.291041577368196,
      "grad_norm": 21.323293685913086,
      "learning_rate": 1.0278611230175742e-05,
      "loss": 0.6376,
      "step": 34020
    },
    {
      "epoch": 7.29318474067724,
      "grad_norm": 0.15959495306015015,
      "learning_rate": 1.0275753679097014e-05,
      "loss": 0.4772,
      "step": 34030
    },
    {
      "epoch": 7.2953279039862835,
      "grad_norm": 19.5573673248291,
      "learning_rate": 1.0272896128018288e-05,
      "loss": 1.2108,
      "step": 34040
    },
    {
      "epoch": 7.297471067295328,
      "grad_norm": 0.2410523146390915,
      "learning_rate": 1.0270038576939562e-05,
      "loss": 0.3221,
      "step": 34050
    },
    {
      "epoch": 7.299614230604372,
      "grad_norm": 0.04990636184811592,
      "learning_rate": 1.0267181025860838e-05,
      "loss": 0.1929,
      "step": 34060
    },
    {
      "epoch": 7.301757393913416,
      "grad_norm": 0.09592385590076447,
      "learning_rate": 1.0264323474782112e-05,
      "loss": 0.2662,
      "step": 34070
    },
    {
      "epoch": 7.303900557222461,
      "grad_norm": 0.09321004897356033,
      "learning_rate": 1.0261465923703387e-05,
      "loss": 0.1506,
      "step": 34080
    },
    {
      "epoch": 7.3060437205315045,
      "grad_norm": 0.08104822784662247,
      "learning_rate": 1.0258608372624661e-05,
      "loss": 0.6637,
      "step": 34090
    },
    {
      "epoch": 7.308186883840548,
      "grad_norm": 18.47930145263672,
      "learning_rate": 1.0255750821545935e-05,
      "loss": 1.4249,
      "step": 34100
    },
    {
      "epoch": 7.310330047149593,
      "grad_norm": 0.8738651275634766,
      "learning_rate": 1.025289327046721e-05,
      "loss": 0.6235,
      "step": 34110
    },
    {
      "epoch": 7.312473210458637,
      "grad_norm": 2.146068811416626,
      "learning_rate": 1.0250035719388485e-05,
      "loss": 0.4254,
      "step": 34120
    },
    {
      "epoch": 7.314616373767681,
      "grad_norm": 0.4235624372959137,
      "learning_rate": 1.0247178168309759e-05,
      "loss": 0.2946,
      "step": 34130
    },
    {
      "epoch": 7.3167595370767256,
      "grad_norm": 0.436049222946167,
      "learning_rate": 1.0244320617231034e-05,
      "loss": 0.0064,
      "step": 34140
    },
    {
      "epoch": 7.318902700385769,
      "grad_norm": 23.26209831237793,
      "learning_rate": 1.0241463066152308e-05,
      "loss": 0.5948,
      "step": 34150
    },
    {
      "epoch": 7.321045863694813,
      "grad_norm": 0.08983224630355835,
      "learning_rate": 1.0238605515073584e-05,
      "loss": 0.3712,
      "step": 34160
    },
    {
      "epoch": 7.323189027003858,
      "grad_norm": 0.4119653105735779,
      "learning_rate": 1.0235747963994858e-05,
      "loss": 0.9725,
      "step": 34170
    },
    {
      "epoch": 7.325332190312902,
      "grad_norm": 0.40034544467926025,
      "learning_rate": 1.0232890412916132e-05,
      "loss": 0.16,
      "step": 34180
    },
    {
      "epoch": 7.327475353621946,
      "grad_norm": 0.633832573890686,
      "learning_rate": 1.0230032861837407e-05,
      "loss": 0.008,
      "step": 34190
    },
    {
      "epoch": 7.32961851693099,
      "grad_norm": 0.16849885880947113,
      "learning_rate": 1.0227175310758681e-05,
      "loss": 0.7719,
      "step": 34200
    },
    {
      "epoch": 7.331761680240034,
      "grad_norm": 0.09759479016065598,
      "learning_rate": 1.0224317759679955e-05,
      "loss": 0.3405,
      "step": 34210
    },
    {
      "epoch": 7.333904843549078,
      "grad_norm": 0.35275235772132874,
      "learning_rate": 1.022146020860123e-05,
      "loss": 0.1959,
      "step": 34220
    },
    {
      "epoch": 7.336048006858123,
      "grad_norm": 0.09011103212833405,
      "learning_rate": 1.0218602657522505e-05,
      "loss": 0.2872,
      "step": 34230
    },
    {
      "epoch": 7.338191170167167,
      "grad_norm": 0.19448734819889069,
      "learning_rate": 1.021574510644378e-05,
      "loss": 0.4734,
      "step": 34240
    },
    {
      "epoch": 7.340334333476211,
      "grad_norm": 1.023585557937622,
      "learning_rate": 1.0212887555365053e-05,
      "loss": 0.1738,
      "step": 34250
    },
    {
      "epoch": 7.342477496785255,
      "grad_norm": 0.4297999441623688,
      "learning_rate": 1.0210030004286326e-05,
      "loss": 0.5591,
      "step": 34260
    },
    {
      "epoch": 7.344620660094299,
      "grad_norm": 0.20271724462509155,
      "learning_rate": 1.02071724532076e-05,
      "loss": 0.0044,
      "step": 34270
    },
    {
      "epoch": 7.346763823403343,
      "grad_norm": 17.16227149963379,
      "learning_rate": 1.0204314902128876e-05,
      "loss": 0.3511,
      "step": 34280
    },
    {
      "epoch": 7.348906986712388,
      "grad_norm": 19.92009735107422,
      "learning_rate": 1.020145735105015e-05,
      "loss": 0.717,
      "step": 34290
    },
    {
      "epoch": 7.351050150021432,
      "grad_norm": 0.05688294768333435,
      "learning_rate": 1.0198599799971426e-05,
      "loss": 0.6144,
      "step": 34300
    },
    {
      "epoch": 7.353193313330475,
      "grad_norm": 0.059678077697753906,
      "learning_rate": 1.01957422488927e-05,
      "loss": 0.555,
      "step": 34310
    },
    {
      "epoch": 7.35533647663952,
      "grad_norm": 23.038236618041992,
      "learning_rate": 1.0192884697813973e-05,
      "loss": 0.5482,
      "step": 34320
    },
    {
      "epoch": 7.357479639948564,
      "grad_norm": 0.36084848642349243,
      "learning_rate": 1.0190027146735249e-05,
      "loss": 0.1578,
      "step": 34330
    },
    {
      "epoch": 7.359622803257608,
      "grad_norm": 21.891586303710938,
      "learning_rate": 1.0187169595656523e-05,
      "loss": 0.7156,
      "step": 34340
    },
    {
      "epoch": 7.361765966566653,
      "grad_norm": 0.10144757479429245,
      "learning_rate": 1.0184312044577797e-05,
      "loss": 0.3495,
      "step": 34350
    },
    {
      "epoch": 7.3639091298756965,
      "grad_norm": 82.60919952392578,
      "learning_rate": 1.0181454493499072e-05,
      "loss": 0.3452,
      "step": 34360
    },
    {
      "epoch": 7.36605229318474,
      "grad_norm": 0.8003581166267395,
      "learning_rate": 1.0178596942420346e-05,
      "loss": 0.3198,
      "step": 34370
    },
    {
      "epoch": 7.368195456493785,
      "grad_norm": 352.10162353515625,
      "learning_rate": 1.0175739391341622e-05,
      "loss": 0.2366,
      "step": 34380
    },
    {
      "epoch": 7.370338619802829,
      "grad_norm": 0.09310553222894669,
      "learning_rate": 1.0172881840262896e-05,
      "loss": 0.0045,
      "step": 34390
    },
    {
      "epoch": 7.372481783111873,
      "grad_norm": 0.08520810306072235,
      "learning_rate": 1.017002428918417e-05,
      "loss": 0.5865,
      "step": 34400
    },
    {
      "epoch": 7.3746249464209175,
      "grad_norm": 0.062750443816185,
      "learning_rate": 1.0167166738105446e-05,
      "loss": 0.1985,
      "step": 34410
    },
    {
      "epoch": 7.376768109729961,
      "grad_norm": 275.2089538574219,
      "learning_rate": 1.016430918702672e-05,
      "loss": 0.4174,
      "step": 34420
    },
    {
      "epoch": 7.378911273039005,
      "grad_norm": 0.2211397886276245,
      "learning_rate": 1.0161451635947993e-05,
      "loss": 0.5752,
      "step": 34430
    },
    {
      "epoch": 7.38105443634805,
      "grad_norm": 43.968441009521484,
      "learning_rate": 1.0158594084869269e-05,
      "loss": 0.1432,
      "step": 34440
    },
    {
      "epoch": 7.383197599657094,
      "grad_norm": 30.854633331298828,
      "learning_rate": 1.0155736533790543e-05,
      "loss": 0.3664,
      "step": 34450
    },
    {
      "epoch": 7.385340762966138,
      "grad_norm": 0.06212690472602844,
      "learning_rate": 1.0152878982711815e-05,
      "loss": 0.2484,
      "step": 34460
    },
    {
      "epoch": 7.387483926275182,
      "grad_norm": 19.714221954345703,
      "learning_rate": 1.015002143163309e-05,
      "loss": 0.509,
      "step": 34470
    },
    {
      "epoch": 7.389627089584226,
      "grad_norm": 27.69693374633789,
      "learning_rate": 1.0147163880554365e-05,
      "loss": 0.5274,
      "step": 34480
    },
    {
      "epoch": 7.39177025289327,
      "grad_norm": 0.19490811228752136,
      "learning_rate": 1.0144306329475639e-05,
      "loss": 0.1549,
      "step": 34490
    },
    {
      "epoch": 7.393913416202315,
      "grad_norm": 0.3280474841594696,
      "learning_rate": 1.0141448778396914e-05,
      "loss": 0.8389,
      "step": 34500
    },
    {
      "epoch": 7.396056579511359,
      "grad_norm": 0.1513078659772873,
      "learning_rate": 1.0138591227318188e-05,
      "loss": 0.1328,
      "step": 34510
    },
    {
      "epoch": 7.3981997428204025,
      "grad_norm": 0.6060652732849121,
      "learning_rate": 1.0135733676239464e-05,
      "loss": 0.2085,
      "step": 34520
    },
    {
      "epoch": 7.400342906129447,
      "grad_norm": 20.822643280029297,
      "learning_rate": 1.0132876125160738e-05,
      "loss": 0.8629,
      "step": 34530
    },
    {
      "epoch": 7.402486069438491,
      "grad_norm": 16.103174209594727,
      "learning_rate": 1.0130018574082012e-05,
      "loss": 0.1285,
      "step": 34540
    },
    {
      "epoch": 7.404629232747535,
      "grad_norm": 0.09183128923177719,
      "learning_rate": 1.0127161023003287e-05,
      "loss": 0.2727,
      "step": 34550
    },
    {
      "epoch": 7.40677239605658,
      "grad_norm": 0.05898376926779747,
      "learning_rate": 1.0124303471924561e-05,
      "loss": 0.5206,
      "step": 34560
    },
    {
      "epoch": 7.4089155593656235,
      "grad_norm": 0.06847696751356125,
      "learning_rate": 1.0121445920845835e-05,
      "loss": 0.3579,
      "step": 34570
    },
    {
      "epoch": 7.411058722674667,
      "grad_norm": 0.34279510378837585,
      "learning_rate": 1.011858836976711e-05,
      "loss": 0.0092,
      "step": 34580
    },
    {
      "epoch": 7.413201885983712,
      "grad_norm": 35.28030014038086,
      "learning_rate": 1.0115730818688385e-05,
      "loss": 1.3267,
      "step": 34590
    },
    {
      "epoch": 7.415345049292756,
      "grad_norm": 0.1710316389799118,
      "learning_rate": 1.011287326760966e-05,
      "loss": 0.1264,
      "step": 34600
    },
    {
      "epoch": 7.4174882126018,
      "grad_norm": 0.11328030377626419,
      "learning_rate": 1.0110015716530934e-05,
      "loss": 0.2935,
      "step": 34610
    },
    {
      "epoch": 7.4196313759108445,
      "grad_norm": 18.606800079345703,
      "learning_rate": 1.0107158165452208e-05,
      "loss": 0.3583,
      "step": 34620
    },
    {
      "epoch": 7.421774539219888,
      "grad_norm": 0.3482700288295746,
      "learning_rate": 1.0104300614373484e-05,
      "loss": 0.6226,
      "step": 34630
    },
    {
      "epoch": 7.423917702528932,
      "grad_norm": 0.25271710753440857,
      "learning_rate": 1.0101443063294758e-05,
      "loss": 0.7652,
      "step": 34640
    },
    {
      "epoch": 7.426060865837977,
      "grad_norm": 0.27379393577575684,
      "learning_rate": 1.0098585512216033e-05,
      "loss": 0.3095,
      "step": 34650
    },
    {
      "epoch": 7.428204029147021,
      "grad_norm": 0.6377800107002258,
      "learning_rate": 1.0095727961137307e-05,
      "loss": 0.2441,
      "step": 34660
    },
    {
      "epoch": 7.430347192456066,
      "grad_norm": 18.57965850830078,
      "learning_rate": 1.0092870410058581e-05,
      "loss": 0.4663,
      "step": 34670
    },
    {
      "epoch": 7.432490355765109,
      "grad_norm": 0.07210104167461395,
      "learning_rate": 1.0090012858979853e-05,
      "loss": 0.0038,
      "step": 34680
    },
    {
      "epoch": 7.434633519074153,
      "grad_norm": 17.082931518554688,
      "learning_rate": 1.0087155307901129e-05,
      "loss": 0.7447,
      "step": 34690
    },
    {
      "epoch": 7.436776682383198,
      "grad_norm": 0.07153546065092087,
      "learning_rate": 1.0084297756822403e-05,
      "loss": 0.3743,
      "step": 34700
    },
    {
      "epoch": 7.438919845692242,
      "grad_norm": 0.11025220900774002,
      "learning_rate": 1.0081440205743677e-05,
      "loss": 0.2939,
      "step": 34710
    },
    {
      "epoch": 7.441063009001286,
      "grad_norm": 0.2422400265932083,
      "learning_rate": 1.0078582654664953e-05,
      "loss": 0.5464,
      "step": 34720
    },
    {
      "epoch": 7.44320617231033,
      "grad_norm": 0.17570970952510834,
      "learning_rate": 1.0075725103586226e-05,
      "loss": 0.4257,
      "step": 34730
    },
    {
      "epoch": 7.445349335619374,
      "grad_norm": 0.09004905074834824,
      "learning_rate": 1.0072867552507502e-05,
      "loss": 0.3586,
      "step": 34740
    },
    {
      "epoch": 7.447492498928418,
      "grad_norm": 0.42948442697525024,
      "learning_rate": 1.0070010001428776e-05,
      "loss": 0.1296,
      "step": 34750
    },
    {
      "epoch": 7.449635662237463,
      "grad_norm": 0.10199084877967834,
      "learning_rate": 1.006715245035005e-05,
      "loss": 0.2412,
      "step": 34760
    },
    {
      "epoch": 7.451778825546507,
      "grad_norm": 0.7163175940513611,
      "learning_rate": 1.0064294899271326e-05,
      "loss": 0.7454,
      "step": 34770
    },
    {
      "epoch": 7.453921988855551,
      "grad_norm": 0.06638956815004349,
      "learning_rate": 1.00614373481926e-05,
      "loss": 0.1874,
      "step": 34780
    },
    {
      "epoch": 7.456065152164595,
      "grad_norm": 0.3241821527481079,
      "learning_rate": 1.0058579797113875e-05,
      "loss": 0.5743,
      "step": 34790
    },
    {
      "epoch": 7.458208315473639,
      "grad_norm": 0.3956294059753418,
      "learning_rate": 1.0055722246035149e-05,
      "loss": 0.2605,
      "step": 34800
    },
    {
      "epoch": 7.460351478782683,
      "grad_norm": 0.7381700873374939,
      "learning_rate": 1.0052864694956423e-05,
      "loss": 0.1273,
      "step": 34810
    },
    {
      "epoch": 7.462494642091728,
      "grad_norm": 0.06401348859071732,
      "learning_rate": 1.0050007143877699e-05,
      "loss": 0.1443,
      "step": 34820
    },
    {
      "epoch": 7.464637805400772,
      "grad_norm": 0.11132163554430008,
      "learning_rate": 1.0047149592798973e-05,
      "loss": 0.2761,
      "step": 34830
    },
    {
      "epoch": 7.4667809687098154,
      "grad_norm": 19.966157913208008,
      "learning_rate": 1.0044292041720246e-05,
      "loss": 0.6708,
      "step": 34840
    },
    {
      "epoch": 7.46892413201886,
      "grad_norm": 0.43235495686531067,
      "learning_rate": 1.0041434490641522e-05,
      "loss": 0.3563,
      "step": 34850
    },
    {
      "epoch": 7.471067295327904,
      "grad_norm": 0.044775042682886124,
      "learning_rate": 1.0038576939562796e-05,
      "loss": 0.2943,
      "step": 34860
    },
    {
      "epoch": 7.473210458636948,
      "grad_norm": 15.086475372314453,
      "learning_rate": 1.0035719388484072e-05,
      "loss": 0.5218,
      "step": 34870
    },
    {
      "epoch": 7.475353621945993,
      "grad_norm": 0.13226445019245148,
      "learning_rate": 1.0032861837405346e-05,
      "loss": 0.2938,
      "step": 34880
    },
    {
      "epoch": 7.4774967852550365,
      "grad_norm": 0.024515261873602867,
      "learning_rate": 1.0030004286326618e-05,
      "loss": 0.1849,
      "step": 34890
    },
    {
      "epoch": 7.47963994856408,
      "grad_norm": 0.4258352220058441,
      "learning_rate": 1.0027146735247892e-05,
      "loss": 0.4997,
      "step": 34900
    },
    {
      "epoch": 7.481783111873125,
      "grad_norm": 0.20186381042003632,
      "learning_rate": 1.0024289184169167e-05,
      "loss": 0.8397,
      "step": 34910
    },
    {
      "epoch": 7.483926275182169,
      "grad_norm": 18.72006607055664,
      "learning_rate": 1.0021431633090441e-05,
      "loss": 0.8043,
      "step": 34920
    },
    {
      "epoch": 7.486069438491213,
      "grad_norm": 0.3815477192401886,
      "learning_rate": 1.0018574082011717e-05,
      "loss": 0.1632,
      "step": 34930
    },
    {
      "epoch": 7.4882126018002575,
      "grad_norm": 0.16822350025177002,
      "learning_rate": 1.001571653093299e-05,
      "loss": 0.5647,
      "step": 34940
    },
    {
      "epoch": 7.490355765109301,
      "grad_norm": 17.25450325012207,
      "learning_rate": 1.0012858979854265e-05,
      "loss": 0.7567,
      "step": 34950
    },
    {
      "epoch": 7.492498928418345,
      "grad_norm": 0.7069227695465088,
      "learning_rate": 1.001000142877554e-05,
      "loss": 0.3515,
      "step": 34960
    },
    {
      "epoch": 7.49464209172739,
      "grad_norm": 19.005861282348633,
      "learning_rate": 1.0007143877696814e-05,
      "loss": 0.8659,
      "step": 34970
    },
    {
      "epoch": 7.496785255036434,
      "grad_norm": 2.531782627105713,
      "learning_rate": 1.0004286326618088e-05,
      "loss": 0.4104,
      "step": 34980
    },
    {
      "epoch": 7.498928418345478,
      "grad_norm": 0.1558157503604889,
      "learning_rate": 1.0001428775539364e-05,
      "loss": 0.2172,
      "step": 34990
    },
    {
      "epoch": 7.501071581654522,
      "grad_norm": 7.427639961242676,
      "learning_rate": 9.998571224460638e-06,
      "loss": 0.6252,
      "step": 35000
    },
    {
      "epoch": 7.503214744963566,
      "grad_norm": 15.136683464050293,
      "learning_rate": 9.995713673381913e-06,
      "loss": 0.7314,
      "step": 35010
    },
    {
      "epoch": 7.50535790827261,
      "grad_norm": 0.26301637291908264,
      "learning_rate": 9.992856122303187e-06,
      "loss": 0.1981,
      "step": 35020
    },
    {
      "epoch": 7.507501071581655,
      "grad_norm": 0.22331152856349945,
      "learning_rate": 9.989998571224461e-06,
      "loss": 0.4943,
      "step": 35030
    },
    {
      "epoch": 7.509644234890699,
      "grad_norm": 0.08212772011756897,
      "learning_rate": 9.987141020145737e-06,
      "loss": 0.2086,
      "step": 35040
    },
    {
      "epoch": 7.5117873981997425,
      "grad_norm": 15.775248527526855,
      "learning_rate": 9.984283469067009e-06,
      "loss": 0.2137,
      "step": 35050
    },
    {
      "epoch": 7.513930561508787,
      "grad_norm": 1.139341950416565,
      "learning_rate": 9.981425917988285e-06,
      "loss": 0.4534,
      "step": 35060
    },
    {
      "epoch": 7.516073724817831,
      "grad_norm": 18.570938110351562,
      "learning_rate": 9.978568366909559e-06,
      "loss": 0.3807,
      "step": 35070
    },
    {
      "epoch": 7.518216888126875,
      "grad_norm": 0.043296314775943756,
      "learning_rate": 9.975710815830834e-06,
      "loss": 0.3243,
      "step": 35080
    },
    {
      "epoch": 7.52036005143592,
      "grad_norm": 1.9118375778198242,
      "learning_rate": 9.972853264752108e-06,
      "loss": 0.428,
      "step": 35090
    },
    {
      "epoch": 7.5225032147449635,
      "grad_norm": 0.6643753051757812,
      "learning_rate": 9.969995713673382e-06,
      "loss": 0.2212,
      "step": 35100
    },
    {
      "epoch": 7.524646378054007,
      "grad_norm": 0.22718986868858337,
      "learning_rate": 9.967138162594658e-06,
      "loss": 0.3011,
      "step": 35110
    },
    {
      "epoch": 7.526789541363052,
      "grad_norm": 0.06117488071322441,
      "learning_rate": 9.964280611515932e-06,
      "loss": 0.442,
      "step": 35120
    },
    {
      "epoch": 7.528932704672096,
      "grad_norm": 0.1228722408413887,
      "learning_rate": 9.961423060437206e-06,
      "loss": 0.5255,
      "step": 35130
    },
    {
      "epoch": 7.53107586798114,
      "grad_norm": 0.47055739164352417,
      "learning_rate": 9.958565509358481e-06,
      "loss": 0.5456,
      "step": 35140
    },
    {
      "epoch": 7.5332190312901846,
      "grad_norm": 0.5799245238304138,
      "learning_rate": 9.955707958279755e-06,
      "loss": 0.3507,
      "step": 35150
    },
    {
      "epoch": 7.535362194599228,
      "grad_norm": 0.0426441952586174,
      "learning_rate": 9.952850407201029e-06,
      "loss": 0.1264,
      "step": 35160
    },
    {
      "epoch": 7.537505357908272,
      "grad_norm": 0.17928536236286163,
      "learning_rate": 9.949992856122303e-06,
      "loss": 0.2433,
      "step": 35170
    },
    {
      "epoch": 7.539648521217317,
      "grad_norm": 0.04453963413834572,
      "learning_rate": 9.947135305043579e-06,
      "loss": 0.0036,
      "step": 35180
    },
    {
      "epoch": 7.541791684526361,
      "grad_norm": 0.05572127178311348,
      "learning_rate": 9.944277753964853e-06,
      "loss": 0.22,
      "step": 35190
    },
    {
      "epoch": 7.543934847835405,
      "grad_norm": 20.163684844970703,
      "learning_rate": 9.941420202886127e-06,
      "loss": 0.6048,
      "step": 35200
    },
    {
      "epoch": 7.546078011144449,
      "grad_norm": 0.2969134449958801,
      "learning_rate": 9.938562651807402e-06,
      "loss": 0.3332,
      "step": 35210
    },
    {
      "epoch": 7.548221174453493,
      "grad_norm": 0.3674446940422058,
      "learning_rate": 9.935705100728676e-06,
      "loss": 0.5433,
      "step": 35220
    },
    {
      "epoch": 7.550364337762537,
      "grad_norm": 19.157588958740234,
      "learning_rate": 9.932847549649952e-06,
      "loss": 0.63,
      "step": 35230
    },
    {
      "epoch": 7.552507501071582,
      "grad_norm": 0.43005526065826416,
      "learning_rate": 9.929989998571226e-06,
      "loss": 0.1732,
      "step": 35240
    },
    {
      "epoch": 7.554650664380626,
      "grad_norm": 27.540576934814453,
      "learning_rate": 9.9271324474925e-06,
      "loss": 0.283,
      "step": 35250
    },
    {
      "epoch": 7.5567938276896705,
      "grad_norm": 0.22086188197135925,
      "learning_rate": 9.924274896413775e-06,
      "loss": 0.1533,
      "step": 35260
    },
    {
      "epoch": 7.558936990998714,
      "grad_norm": 0.102082259953022,
      "learning_rate": 9.921417345335047e-06,
      "loss": 0.4848,
      "step": 35270
    },
    {
      "epoch": 7.561080154307758,
      "grad_norm": 0.0637950748205185,
      "learning_rate": 9.918559794256323e-06,
      "loss": 0.6762,
      "step": 35280
    },
    {
      "epoch": 7.563223317616803,
      "grad_norm": 0.06168339028954506,
      "learning_rate": 9.915702243177597e-06,
      "loss": 0.3473,
      "step": 35290
    },
    {
      "epoch": 7.565366480925847,
      "grad_norm": 16.291349411010742,
      "learning_rate": 9.912844692098873e-06,
      "loss": 0.415,
      "step": 35300
    },
    {
      "epoch": 7.567509644234891,
      "grad_norm": 0.22025570273399353,
      "learning_rate": 9.909987141020146e-06,
      "loss": 0.6456,
      "step": 35310
    },
    {
      "epoch": 7.569652807543935,
      "grad_norm": 0.5948153734207153,
      "learning_rate": 9.90712958994142e-06,
      "loss": 0.0074,
      "step": 35320
    },
    {
      "epoch": 7.571795970852979,
      "grad_norm": 0.155206561088562,
      "learning_rate": 9.904272038862696e-06,
      "loss": 0.3659,
      "step": 35330
    },
    {
      "epoch": 7.573939134162023,
      "grad_norm": 0.24220481514930725,
      "learning_rate": 9.90141448778397e-06,
      "loss": 0.5137,
      "step": 35340
    },
    {
      "epoch": 7.576082297471068,
      "grad_norm": 2.250046730041504,
      "learning_rate": 9.898556936705246e-06,
      "loss": 0.1423,
      "step": 35350
    },
    {
      "epoch": 7.578225460780112,
      "grad_norm": 0.04305621609091759,
      "learning_rate": 9.89569938562652e-06,
      "loss": 0.1752,
      "step": 35360
    },
    {
      "epoch": 7.5803686240891555,
      "grad_norm": 0.029179781675338745,
      "learning_rate": 9.892841834547793e-06,
      "loss": 0.1844,
      "step": 35370
    },
    {
      "epoch": 7.5825117873982,
      "grad_norm": 0.24246925115585327,
      "learning_rate": 9.889984283469067e-06,
      "loss": 0.3188,
      "step": 35380
    },
    {
      "epoch": 7.584654950707244,
      "grad_norm": 17.135990142822266,
      "learning_rate": 9.887126732390341e-06,
      "loss": 0.531,
      "step": 35390
    },
    {
      "epoch": 7.586798114016288,
      "grad_norm": 0.2784140408039093,
      "learning_rate": 9.884269181311617e-06,
      "loss": 0.1367,
      "step": 35400
    },
    {
      "epoch": 7.588941277325333,
      "grad_norm": 0.043506041169166565,
      "learning_rate": 9.881411630232891e-06,
      "loss": 0.1494,
      "step": 35410
    },
    {
      "epoch": 7.5910844406343765,
      "grad_norm": 0.11833212524652481,
      "learning_rate": 9.878554079154166e-06,
      "loss": 0.1287,
      "step": 35420
    },
    {
      "epoch": 7.59322760394342,
      "grad_norm": 0.029314616695046425,
      "learning_rate": 9.87569652807544e-06,
      "loss": 0.2486,
      "step": 35430
    },
    {
      "epoch": 7.595370767252465,
      "grad_norm": 0.4498506188392639,
      "learning_rate": 9.872838976996714e-06,
      "loss": 0.4104,
      "step": 35440
    },
    {
      "epoch": 7.597513930561509,
      "grad_norm": 87.68867492675781,
      "learning_rate": 9.86998142591799e-06,
      "loss": 0.4138,
      "step": 35450
    },
    {
      "epoch": 7.599657093870553,
      "grad_norm": 0.126013845205307,
      "learning_rate": 9.867123874839264e-06,
      "loss": 0.3792,
      "step": 35460
    },
    {
      "epoch": 7.6018002571795975,
      "grad_norm": 0.29671066999435425,
      "learning_rate": 9.864266323760538e-06,
      "loss": 0.5635,
      "step": 35470
    },
    {
      "epoch": 7.603943420488641,
      "grad_norm": 0.39785924553871155,
      "learning_rate": 9.861408772681812e-06,
      "loss": 0.4705,
      "step": 35480
    },
    {
      "epoch": 7.606086583797685,
      "grad_norm": 172.4239501953125,
      "learning_rate": 9.858551221603087e-06,
      "loss": 0.6052,
      "step": 35490
    },
    {
      "epoch": 7.60822974710673,
      "grad_norm": 32.123046875,
      "learning_rate": 9.855693670524361e-06,
      "loss": 0.4302,
      "step": 35500
    },
    {
      "epoch": 7.610372910415774,
      "grad_norm": 0.27044257521629333,
      "learning_rate": 9.852836119445635e-06,
      "loss": 0.5851,
      "step": 35510
    },
    {
      "epoch": 7.612516073724818,
      "grad_norm": 0.2995755970478058,
      "learning_rate": 9.849978568366911e-06,
      "loss": 1.0777,
      "step": 35520
    },
    {
      "epoch": 7.614659237033862,
      "grad_norm": 0.33083829283714294,
      "learning_rate": 9.847121017288185e-06,
      "loss": 0.1523,
      "step": 35530
    },
    {
      "epoch": 7.616802400342906,
      "grad_norm": 0.2873865067958832,
      "learning_rate": 9.844263466209459e-06,
      "loss": 0.3046,
      "step": 35540
    },
    {
      "epoch": 7.61894556365195,
      "grad_norm": 17.8861026763916,
      "learning_rate": 9.841405915130734e-06,
      "loss": 0.587,
      "step": 35550
    },
    {
      "epoch": 7.621088726960995,
      "grad_norm": 0.08596563339233398,
      "learning_rate": 9.838548364052008e-06,
      "loss": 0.3133,
      "step": 35560
    },
    {
      "epoch": 7.623231890270039,
      "grad_norm": 0.6298311352729797,
      "learning_rate": 9.835690812973284e-06,
      "loss": 0.3067,
      "step": 35570
    },
    {
      "epoch": 7.6253750535790825,
      "grad_norm": 0.1406574547290802,
      "learning_rate": 9.832833261894558e-06,
      "loss": 0.2787,
      "step": 35580
    },
    {
      "epoch": 7.627518216888127,
      "grad_norm": 0.051328953355550766,
      "learning_rate": 9.829975710815832e-06,
      "loss": 0.415,
      "step": 35590
    },
    {
      "epoch": 7.629661380197171,
      "grad_norm": 0.11970868706703186,
      "learning_rate": 9.827118159737106e-06,
      "loss": 0.1406,
      "step": 35600
    },
    {
      "epoch": 7.631804543506215,
      "grad_norm": 23.374053955078125,
      "learning_rate": 9.82426060865838e-06,
      "loss": 0.2911,
      "step": 35610
    },
    {
      "epoch": 7.63394770681526,
      "grad_norm": 0.13019344210624695,
      "learning_rate": 9.821403057579655e-06,
      "loss": 0.1538,
      "step": 35620
    },
    {
      "epoch": 7.6360908701243035,
      "grad_norm": 0.7316949367523193,
      "learning_rate": 9.818545506500929e-06,
      "loss": 0.3304,
      "step": 35630
    },
    {
      "epoch": 7.638234033433347,
      "grad_norm": 0.06929817795753479,
      "learning_rate": 9.815687955422205e-06,
      "loss": 0.5997,
      "step": 35640
    },
    {
      "epoch": 7.640377196742392,
      "grad_norm": 0.2602878212928772,
      "learning_rate": 9.812830404343479e-06,
      "loss": 0.3395,
      "step": 35650
    },
    {
      "epoch": 7.642520360051436,
      "grad_norm": 21.71327781677246,
      "learning_rate": 9.809972853264753e-06,
      "loss": 0.8637,
      "step": 35660
    },
    {
      "epoch": 7.64466352336048,
      "grad_norm": 0.28929421305656433,
      "learning_rate": 9.807115302186028e-06,
      "loss": 0.4593,
      "step": 35670
    },
    {
      "epoch": 7.646806686669525,
      "grad_norm": 0.4133674204349518,
      "learning_rate": 9.804257751107302e-06,
      "loss": 0.829,
      "step": 35680
    },
    {
      "epoch": 7.648949849978568,
      "grad_norm": 0.051955677568912506,
      "learning_rate": 9.801400200028576e-06,
      "loss": 0.5789,
      "step": 35690
    },
    {
      "epoch": 7.651093013287612,
      "grad_norm": 22.67428970336914,
      "learning_rate": 9.79854264894985e-06,
      "loss": 0.1777,
      "step": 35700
    },
    {
      "epoch": 7.653236176596657,
      "grad_norm": 1.286994218826294,
      "learning_rate": 9.795685097871126e-06,
      "loss": 0.5355,
      "step": 35710
    },
    {
      "epoch": 7.655379339905701,
      "grad_norm": 15.564370155334473,
      "learning_rate": 9.7928275467924e-06,
      "loss": 0.4927,
      "step": 35720
    },
    {
      "epoch": 7.657522503214745,
      "grad_norm": 15.822606086730957,
      "learning_rate": 9.789969995713674e-06,
      "loss": 0.7166,
      "step": 35730
    },
    {
      "epoch": 7.659665666523789,
      "grad_norm": 0.29745733737945557,
      "learning_rate": 9.787112444634949e-06,
      "loss": 0.5572,
      "step": 35740
    },
    {
      "epoch": 7.661808829832833,
      "grad_norm": 0.5143421292304993,
      "learning_rate": 9.784254893556223e-06,
      "loss": 0.4983,
      "step": 35750
    },
    {
      "epoch": 7.663951993141877,
      "grad_norm": 16.73369789123535,
      "learning_rate": 9.781397342477497e-06,
      "loss": 0.37,
      "step": 35760
    },
    {
      "epoch": 7.666095156450922,
      "grad_norm": 0.0249912291765213,
      "learning_rate": 9.778539791398773e-06,
      "loss": 0.1337,
      "step": 35770
    },
    {
      "epoch": 7.668238319759966,
      "grad_norm": 0.01886487752199173,
      "learning_rate": 9.775682240320047e-06,
      "loss": 0.5425,
      "step": 35780
    },
    {
      "epoch": 7.67038148306901,
      "grad_norm": 0.3924854099750519,
      "learning_rate": 9.772824689241322e-06,
      "loss": 0.3616,
      "step": 35790
    },
    {
      "epoch": 7.672524646378054,
      "grad_norm": 20.175579071044922,
      "learning_rate": 9.769967138162596e-06,
      "loss": 0.3236,
      "step": 35800
    },
    {
      "epoch": 7.674667809687098,
      "grad_norm": 0.20954333245754242,
      "learning_rate": 9.76710958708387e-06,
      "loss": 0.0062,
      "step": 35810
    },
    {
      "epoch": 7.676810972996142,
      "grad_norm": 0.012260888703167439,
      "learning_rate": 9.764252036005144e-06,
      "loss": 0.175,
      "step": 35820
    },
    {
      "epoch": 7.678954136305187,
      "grad_norm": 0.01469254307448864,
      "learning_rate": 9.761394484926418e-06,
      "loss": 0.0055,
      "step": 35830
    },
    {
      "epoch": 7.681097299614231,
      "grad_norm": 0.023071253672242165,
      "learning_rate": 9.758536933847693e-06,
      "loss": 1.1402,
      "step": 35840
    },
    {
      "epoch": 7.6832404629232744,
      "grad_norm": 0.8942857980728149,
      "learning_rate": 9.755679382768967e-06,
      "loss": 0.004,
      "step": 35850
    },
    {
      "epoch": 7.685383626232319,
      "grad_norm": 0.07652880996465683,
      "learning_rate": 9.752821831690243e-06,
      "loss": 0.1502,
      "step": 35860
    },
    {
      "epoch": 7.687526789541363,
      "grad_norm": 0.07243718951940536,
      "learning_rate": 9.749964280611517e-06,
      "loss": 0.8785,
      "step": 35870
    },
    {
      "epoch": 7.689669952850407,
      "grad_norm": 1.0533493757247925,
      "learning_rate": 9.747106729532791e-06,
      "loss": 0.4811,
      "step": 35880
    },
    {
      "epoch": 7.691813116159452,
      "grad_norm": 0.0644664466381073,
      "learning_rate": 9.744249178454067e-06,
      "loss": 0.4618,
      "step": 35890
    },
    {
      "epoch": 7.6939562794684955,
      "grad_norm": 0.05567526817321777,
      "learning_rate": 9.74139162737534e-06,
      "loss": 0.2192,
      "step": 35900
    },
    {
      "epoch": 7.696099442777539,
      "grad_norm": 0.2122427523136139,
      "learning_rate": 9.738534076296614e-06,
      "loss": 0.0823,
      "step": 35910
    },
    {
      "epoch": 7.698242606086584,
      "grad_norm": 19.828250885009766,
      "learning_rate": 9.735676525217888e-06,
      "loss": 0.3386,
      "step": 35920
    },
    {
      "epoch": 7.700385769395628,
      "grad_norm": 0.029200835153460503,
      "learning_rate": 9.732818974139164e-06,
      "loss": 0.1847,
      "step": 35930
    },
    {
      "epoch": 7.702528932704672,
      "grad_norm": 0.5538058280944824,
      "learning_rate": 9.729961423060438e-06,
      "loss": 0.1356,
      "step": 35940
    },
    {
      "epoch": 7.7046720960137165,
      "grad_norm": 0.33298617601394653,
      "learning_rate": 9.727103871981712e-06,
      "loss": 0.3106,
      "step": 35950
    },
    {
      "epoch": 7.70681525932276,
      "grad_norm": 0.026637928560376167,
      "learning_rate": 9.724246320902987e-06,
      "loss": 0.7388,
      "step": 35960
    },
    {
      "epoch": 7.708958422631804,
      "grad_norm": 0.061939410865306854,
      "learning_rate": 9.721388769824261e-06,
      "loss": 0.3251,
      "step": 35970
    },
    {
      "epoch": 7.711101585940849,
      "grad_norm": 0.10223386436700821,
      "learning_rate": 9.718531218745535e-06,
      "loss": 0.1769,
      "step": 35980
    },
    {
      "epoch": 7.713244749249893,
      "grad_norm": 0.06779774278402328,
      "learning_rate": 9.715673667666811e-06,
      "loss": 0.1531,
      "step": 35990
    },
    {
      "epoch": 7.715387912558937,
      "grad_norm": 19.410137176513672,
      "learning_rate": 9.712816116588085e-06,
      "loss": 0.7125,
      "step": 36000
    },
    {
      "epoch": 7.717531075867981,
      "grad_norm": 0.1541748195886612,
      "learning_rate": 9.70995856550936e-06,
      "loss": 0.3086,
      "step": 36010
    },
    {
      "epoch": 7.719674239177025,
      "grad_norm": 0.07827668637037277,
      "learning_rate": 9.707101014430633e-06,
      "loss": 0.4889,
      "step": 36020
    },
    {
      "epoch": 7.721817402486069,
      "grad_norm": 0.3822689950466156,
      "learning_rate": 9.704243463351908e-06,
      "loss": 0.627,
      "step": 36030
    },
    {
      "epoch": 7.723960565795114,
      "grad_norm": 303.72357177734375,
      "learning_rate": 9.701385912273182e-06,
      "loss": 0.543,
      "step": 36040
    },
    {
      "epoch": 7.726103729104158,
      "grad_norm": 0.49758800864219666,
      "learning_rate": 9.698528361194456e-06,
      "loss": 0.1472,
      "step": 36050
    },
    {
      "epoch": 7.7282468924132015,
      "grad_norm": 0.029243366792798042,
      "learning_rate": 9.695670810115732e-06,
      "loss": 0.0306,
      "step": 36060
    },
    {
      "epoch": 7.730390055722246,
      "grad_norm": 77.82889556884766,
      "learning_rate": 9.692813259037006e-06,
      "loss": 0.8006,
      "step": 36070
    },
    {
      "epoch": 7.73253321903129,
      "grad_norm": 0.05749889835715294,
      "learning_rate": 9.689955707958281e-06,
      "loss": 0.298,
      "step": 36080
    },
    {
      "epoch": 7.734676382340334,
      "grad_norm": 16.953323364257812,
      "learning_rate": 9.687098156879555e-06,
      "loss": 0.4358,
      "step": 36090
    },
    {
      "epoch": 7.736819545649379,
      "grad_norm": 0.2777635157108307,
      "learning_rate": 9.68424060580083e-06,
      "loss": 0.3079,
      "step": 36100
    },
    {
      "epoch": 7.7389627089584225,
      "grad_norm": 0.11537815630435944,
      "learning_rate": 9.681383054722105e-06,
      "loss": 0.1771,
      "step": 36110
    },
    {
      "epoch": 7.741105872267466,
      "grad_norm": 0.6634148955345154,
      "learning_rate": 9.678525503643379e-06,
      "loss": 0.7,
      "step": 36120
    },
    {
      "epoch": 7.743249035576511,
      "grad_norm": 0.05113697797060013,
      "learning_rate": 9.675667952564653e-06,
      "loss": 0.0043,
      "step": 36130
    },
    {
      "epoch": 7.745392198885555,
      "grad_norm": 0.08684100210666656,
      "learning_rate": 9.672810401485927e-06,
      "loss": 0.413,
      "step": 36140
    },
    {
      "epoch": 7.747535362194599,
      "grad_norm": 0.13779272139072418,
      "learning_rate": 9.669952850407202e-06,
      "loss": 0.4098,
      "step": 36150
    },
    {
      "epoch": 7.7496785255036436,
      "grad_norm": 18.269290924072266,
      "learning_rate": 9.667095299328476e-06,
      "loss": 0.8189,
      "step": 36160
    },
    {
      "epoch": 7.751821688812687,
      "grad_norm": 0.40138521790504456,
      "learning_rate": 9.66423774824975e-06,
      "loss": 0.1313,
      "step": 36170
    },
    {
      "epoch": 7.753964852121731,
      "grad_norm": 0.24250070750713348,
      "learning_rate": 9.661380197171026e-06,
      "loss": 0.2391,
      "step": 36180
    },
    {
      "epoch": 7.756108015430776,
      "grad_norm": 0.22543978691101074,
      "learning_rate": 9.6585226460923e-06,
      "loss": 0.0051,
      "step": 36190
    },
    {
      "epoch": 7.75825117873982,
      "grad_norm": 0.052480608224868774,
      "learning_rate": 9.655665095013575e-06,
      "loss": 0.6515,
      "step": 36200
    },
    {
      "epoch": 7.760394342048864,
      "grad_norm": 0.11804576963186264,
      "learning_rate": 9.65280754393485e-06,
      "loss": 0.3532,
      "step": 36210
    },
    {
      "epoch": 7.762537505357908,
      "grad_norm": 0.1729024052619934,
      "learning_rate": 9.649949992856123e-06,
      "loss": 0.0062,
      "step": 36220
    },
    {
      "epoch": 7.764680668666952,
      "grad_norm": 17.64396095275879,
      "learning_rate": 9.647092441777397e-06,
      "loss": 1.1625,
      "step": 36230
    },
    {
      "epoch": 7.766823831975996,
      "grad_norm": 0.9732678532600403,
      "learning_rate": 9.644234890698671e-06,
      "loss": 0.5662,
      "step": 36240
    },
    {
      "epoch": 7.768966995285041,
      "grad_norm": 0.3872458338737488,
      "learning_rate": 9.641377339619947e-06,
      "loss": 0.0265,
      "step": 36250
    },
    {
      "epoch": 7.771110158594085,
      "grad_norm": 0.23342224955558777,
      "learning_rate": 9.63851978854122e-06,
      "loss": 0.3329,
      "step": 36260
    },
    {
      "epoch": 7.773253321903129,
      "grad_norm": 0.1650952696800232,
      "learning_rate": 9.635662237462496e-06,
      "loss": 0.1885,
      "step": 36270
    },
    {
      "epoch": 7.775396485212173,
      "grad_norm": 0.05489439517259598,
      "learning_rate": 9.63280468638377e-06,
      "loss": 0.1673,
      "step": 36280
    },
    {
      "epoch": 7.777539648521217,
      "grad_norm": 0.023706482723355293,
      "learning_rate": 9.629947135305044e-06,
      "loss": 0.4426,
      "step": 36290
    },
    {
      "epoch": 7.779682811830261,
      "grad_norm": 0.23248976469039917,
      "learning_rate": 9.62708958422632e-06,
      "loss": 0.2581,
      "step": 36300
    },
    {
      "epoch": 7.781825975139306,
      "grad_norm": 0.07699967920780182,
      "learning_rate": 9.624232033147594e-06,
      "loss": 0.416,
      "step": 36310
    },
    {
      "epoch": 7.78396913844835,
      "grad_norm": 23.22112274169922,
      "learning_rate": 9.621374482068867e-06,
      "loss": 0.5141,
      "step": 36320
    },
    {
      "epoch": 7.786112301757393,
      "grad_norm": 16.86641502380371,
      "learning_rate": 9.618516930990143e-06,
      "loss": 0.166,
      "step": 36330
    },
    {
      "epoch": 7.788255465066438,
      "grad_norm": 0.2900411784648895,
      "learning_rate": 9.615659379911417e-06,
      "loss": 1.1519,
      "step": 36340
    },
    {
      "epoch": 7.790398628375482,
      "grad_norm": 0.3518134355545044,
      "learning_rate": 9.612801828832691e-06,
      "loss": 0.1635,
      "step": 36350
    },
    {
      "epoch": 7.792541791684526,
      "grad_norm": 0.2732352912425995,
      "learning_rate": 9.609944277753965e-06,
      "loss": 0.3213,
      "step": 36360
    },
    {
      "epoch": 7.794684954993571,
      "grad_norm": 0.14449289441108704,
      "learning_rate": 9.60708672667524e-06,
      "loss": 0.0038,
      "step": 36370
    },
    {
      "epoch": 7.7968281183026145,
      "grad_norm": 16.82931900024414,
      "learning_rate": 9.604229175596514e-06,
      "loss": 0.3516,
      "step": 36380
    },
    {
      "epoch": 7.798971281611658,
      "grad_norm": 0.10495281964540482,
      "learning_rate": 9.601371624517788e-06,
      "loss": 0.2138,
      "step": 36390
    },
    {
      "epoch": 7.801114444920703,
      "grad_norm": 0.18440335988998413,
      "learning_rate": 9.598514073439064e-06,
      "loss": 0.1712,
      "step": 36400
    },
    {
      "epoch": 7.803257608229747,
      "grad_norm": 0.06419728696346283,
      "learning_rate": 9.595656522360338e-06,
      "loss": 0.0439,
      "step": 36410
    },
    {
      "epoch": 7.805400771538792,
      "grad_norm": 0.3761388957500458,
      "learning_rate": 9.592798971281614e-06,
      "loss": 0.3815,
      "step": 36420
    },
    {
      "epoch": 7.8075439348478355,
      "grad_norm": 19.147632598876953,
      "learning_rate": 9.589941420202887e-06,
      "loss": 0.9865,
      "step": 36430
    },
    {
      "epoch": 7.809687098156879,
      "grad_norm": 75.1592025756836,
      "learning_rate": 9.587083869124161e-06,
      "loss": 0.5664,
      "step": 36440
    },
    {
      "epoch": 7.811830261465924,
      "grad_norm": 0.3901748061180115,
      "learning_rate": 9.584226318045435e-06,
      "loss": 0.1656,
      "step": 36450
    },
    {
      "epoch": 7.813973424774968,
      "grad_norm": 0.16337476670742035,
      "learning_rate": 9.58136876696671e-06,
      "loss": 0.5074,
      "step": 36460
    },
    {
      "epoch": 7.816116588084012,
      "grad_norm": 0.10651177912950516,
      "learning_rate": 9.578511215887985e-06,
      "loss": 0.1525,
      "step": 36470
    },
    {
      "epoch": 7.8182597513930565,
      "grad_norm": 0.20958741009235382,
      "learning_rate": 9.575653664809259e-06,
      "loss": 0.486,
      "step": 36480
    },
    {
      "epoch": 7.8204029147021,
      "grad_norm": 0.1832873672246933,
      "learning_rate": 9.572796113730534e-06,
      "loss": 0.2516,
      "step": 36490
    },
    {
      "epoch": 7.822546078011144,
      "grad_norm": 0.29011350870132446,
      "learning_rate": 9.569938562651808e-06,
      "loss": 0.522,
      "step": 36500
    },
    {
      "epoch": 7.824689241320189,
      "grad_norm": 0.6207288503646851,
      "learning_rate": 9.567081011573082e-06,
      "loss": 0.265,
      "step": 36510
    },
    {
      "epoch": 7.826832404629233,
      "grad_norm": 328.22491455078125,
      "learning_rate": 9.564223460494358e-06,
      "loss": 0.0228,
      "step": 36520
    },
    {
      "epoch": 7.828975567938277,
      "grad_norm": 224.10145568847656,
      "learning_rate": 9.561365909415632e-06,
      "loss": 0.4171,
      "step": 36530
    },
    {
      "epoch": 7.831118731247321,
      "grad_norm": 21.582162857055664,
      "learning_rate": 9.558508358336906e-06,
      "loss": 0.1425,
      "step": 36540
    },
    {
      "epoch": 7.833261894556365,
      "grad_norm": 0.33394816517829895,
      "learning_rate": 9.555650807258181e-06,
      "loss": 0.1067,
      "step": 36550
    },
    {
      "epoch": 7.835405057865409,
      "grad_norm": 0.022006792947649956,
      "learning_rate": 9.552793256179455e-06,
      "loss": 0.2737,
      "step": 36560
    },
    {
      "epoch": 7.837548221174454,
      "grad_norm": 0.02157072350382805,
      "learning_rate": 9.54993570510073e-06,
      "loss": 0.2086,
      "step": 36570
    },
    {
      "epoch": 7.839691384483498,
      "grad_norm": 0.02711731195449829,
      "learning_rate": 9.547078154022003e-06,
      "loss": 0.775,
      "step": 36580
    },
    {
      "epoch": 7.8418345477925415,
      "grad_norm": 0.21632488071918488,
      "learning_rate": 9.544220602943279e-06,
      "loss": 0.1371,
      "step": 36590
    },
    {
      "epoch": 7.843977711101586,
      "grad_norm": 0.2548312842845917,
      "learning_rate": 9.541363051864553e-06,
      "loss": 0.1876,
      "step": 36600
    },
    {
      "epoch": 7.84612087441063,
      "grad_norm": 22.88974952697754,
      "learning_rate": 9.538505500785827e-06,
      "loss": 0.3842,
      "step": 36610
    },
    {
      "epoch": 7.848264037719674,
      "grad_norm": 0.15049849450588226,
      "learning_rate": 9.535647949707102e-06,
      "loss": 0.3246,
      "step": 36620
    },
    {
      "epoch": 7.850407201028719,
      "grad_norm": 0.06636478006839752,
      "learning_rate": 9.532790398628376e-06,
      "loss": 0.3264,
      "step": 36630
    },
    {
      "epoch": 7.8525503643377625,
      "grad_norm": 16.552051544189453,
      "learning_rate": 9.529932847549652e-06,
      "loss": 0.2664,
      "step": 36640
    },
    {
      "epoch": 7.854693527646806,
      "grad_norm": 0.1170211061835289,
      "learning_rate": 9.527075296470926e-06,
      "loss": 0.4364,
      "step": 36650
    },
    {
      "epoch": 7.856836690955851,
      "grad_norm": 24.205995559692383,
      "learning_rate": 9.5242177453922e-06,
      "loss": 0.1436,
      "step": 36660
    },
    {
      "epoch": 7.858979854264895,
      "grad_norm": 0.05218149721622467,
      "learning_rate": 9.521360194313474e-06,
      "loss": 0.1727,
      "step": 36670
    },
    {
      "epoch": 7.861123017573939,
      "grad_norm": 0.2583732008934021,
      "learning_rate": 9.518502643234748e-06,
      "loss": 0.3485,
      "step": 36680
    },
    {
      "epoch": 7.863266180882984,
      "grad_norm": 0.05867204815149307,
      "learning_rate": 9.515645092156023e-06,
      "loss": 0.891,
      "step": 36690
    },
    {
      "epoch": 7.865409344192027,
      "grad_norm": 0.049700602889060974,
      "learning_rate": 9.512787541077297e-06,
      "loss": 0.5013,
      "step": 36700
    },
    {
      "epoch": 7.867552507501071,
      "grad_norm": 3.1962666511535645,
      "learning_rate": 9.509929989998573e-06,
      "loss": 0.6131,
      "step": 36710
    },
    {
      "epoch": 7.869695670810116,
      "grad_norm": 20.56723976135254,
      "learning_rate": 9.507072438919847e-06,
      "loss": 0.521,
      "step": 36720
    },
    {
      "epoch": 7.87183883411916,
      "grad_norm": 1321.062744140625,
      "learning_rate": 9.50421488784112e-06,
      "loss": 0.6694,
      "step": 36730
    },
    {
      "epoch": 7.873981997428204,
      "grad_norm": 0.9112843871116638,
      "learning_rate": 9.501357336762396e-06,
      "loss": 0.2405,
      "step": 36740
    },
    {
      "epoch": 7.876125160737248,
      "grad_norm": 17.513635635375977,
      "learning_rate": 9.49849978568367e-06,
      "loss": 0.7557,
      "step": 36750
    },
    {
      "epoch": 7.878268324046292,
      "grad_norm": 0.2314174920320511,
      "learning_rate": 9.495642234604944e-06,
      "loss": 0.1226,
      "step": 36760
    },
    {
      "epoch": 7.880411487355336,
      "grad_norm": 46.83075714111328,
      "learning_rate": 9.492784683526218e-06,
      "loss": 0.6043,
      "step": 36770
    },
    {
      "epoch": 7.882554650664381,
      "grad_norm": 0.10835856199264526,
      "learning_rate": 9.489927132447494e-06,
      "loss": 0.0048,
      "step": 36780
    },
    {
      "epoch": 7.884697813973425,
      "grad_norm": 21.709218978881836,
      "learning_rate": 9.487069581368768e-06,
      "loss": 0.4558,
      "step": 36790
    },
    {
      "epoch": 7.886840977282469,
      "grad_norm": 0.5911003351211548,
      "learning_rate": 9.484212030290041e-06,
      "loss": 0.2787,
      "step": 36800
    },
    {
      "epoch": 7.888984140591513,
      "grad_norm": 0.18119056522846222,
      "learning_rate": 9.481354479211317e-06,
      "loss": 0.5099,
      "step": 36810
    },
    {
      "epoch": 7.891127303900557,
      "grad_norm": 28.332477569580078,
      "learning_rate": 9.478496928132591e-06,
      "loss": 0.4027,
      "step": 36820
    },
    {
      "epoch": 7.893270467209601,
      "grad_norm": 0.02989460900425911,
      "learning_rate": 9.475639377053865e-06,
      "loss": 0.2945,
      "step": 36830
    },
    {
      "epoch": 7.895413630518646,
      "grad_norm": 0.4329921305179596,
      "learning_rate": 9.47278182597514e-06,
      "loss": 0.2697,
      "step": 36840
    },
    {
      "epoch": 7.89755679382769,
      "grad_norm": 0.06789080053567886,
      "learning_rate": 9.469924274896414e-06,
      "loss": 0.5138,
      "step": 36850
    },
    {
      "epoch": 7.8996999571367335,
      "grad_norm": 0.31356221437454224,
      "learning_rate": 9.46706672381769e-06,
      "loss": 0.0023,
      "step": 36860
    },
    {
      "epoch": 7.901843120445778,
      "grad_norm": 0.9308601021766663,
      "learning_rate": 9.464209172738964e-06,
      "loss": 0.6171,
      "step": 36870
    },
    {
      "epoch": 7.903986283754822,
      "grad_norm": 0.24628761410713196,
      "learning_rate": 9.461351621660238e-06,
      "loss": 0.2405,
      "step": 36880
    },
    {
      "epoch": 7.906129447063866,
      "grad_norm": 17.55177879333496,
      "learning_rate": 9.458494070581512e-06,
      "loss": 0.4425,
      "step": 36890
    },
    {
      "epoch": 7.908272610372911,
      "grad_norm": 17.810014724731445,
      "learning_rate": 9.455636519502786e-06,
      "loss": 0.4981,
      "step": 36900
    },
    {
      "epoch": 7.9104157736819545,
      "grad_norm": 0.2727172374725342,
      "learning_rate": 9.452778968424061e-06,
      "loss": 0.2808,
      "step": 36910
    },
    {
      "epoch": 7.912558936990998,
      "grad_norm": 0.5075958371162415,
      "learning_rate": 9.449921417345335e-06,
      "loss": 0.1284,
      "step": 36920
    },
    {
      "epoch": 7.914702100300043,
      "grad_norm": 0.07408440113067627,
      "learning_rate": 9.447063866266611e-06,
      "loss": 0.4214,
      "step": 36930
    },
    {
      "epoch": 7.916845263609087,
      "grad_norm": 0.09105591475963593,
      "learning_rate": 9.444206315187885e-06,
      "loss": 0.2942,
      "step": 36940
    },
    {
      "epoch": 7.918988426918132,
      "grad_norm": 19.634601593017578,
      "learning_rate": 9.441348764109159e-06,
      "loss": 0.8421,
      "step": 36950
    },
    {
      "epoch": 7.9211315902271755,
      "grad_norm": 0.2327715903520584,
      "learning_rate": 9.438491213030434e-06,
      "loss": 0.4598,
      "step": 36960
    },
    {
      "epoch": 7.923274753536219,
      "grad_norm": 0.15566833317279816,
      "learning_rate": 9.435633661951708e-06,
      "loss": 0.3237,
      "step": 36970
    },
    {
      "epoch": 7.925417916845264,
      "grad_norm": 0.19963939487934113,
      "learning_rate": 9.432776110872984e-06,
      "loss": 0.1353,
      "step": 36980
    },
    {
      "epoch": 7.927561080154308,
      "grad_norm": 0.6209357976913452,
      "learning_rate": 9.429918559794256e-06,
      "loss": 0.5974,
      "step": 36990
    },
    {
      "epoch": 7.929704243463352,
      "grad_norm": 0.2951395809650421,
      "learning_rate": 9.427061008715532e-06,
      "loss": 0.5047,
      "step": 37000
    },
    {
      "epoch": 7.9318474067723965,
      "grad_norm": 0.28483620285987854,
      "learning_rate": 9.424203457636806e-06,
      "loss": 0.1344,
      "step": 37010
    },
    {
      "epoch": 7.93399057008144,
      "grad_norm": 0.2594546973705292,
      "learning_rate": 9.42134590655808e-06,
      "loss": 0.4706,
      "step": 37020
    },
    {
      "epoch": 7.936133733390484,
      "grad_norm": 0.13952495157718658,
      "learning_rate": 9.418488355479355e-06,
      "loss": 0.5144,
      "step": 37030
    },
    {
      "epoch": 7.938276896699529,
      "grad_norm": 16.892873764038086,
      "learning_rate": 9.41563080440063e-06,
      "loss": 0.826,
      "step": 37040
    },
    {
      "epoch": 7.940420060008573,
      "grad_norm": 0.20250526070594788,
      "learning_rate": 9.412773253321905e-06,
      "loss": 0.242,
      "step": 37050
    },
    {
      "epoch": 7.942563223317617,
      "grad_norm": 0.23045901954174042,
      "learning_rate": 9.409915702243179e-06,
      "loss": 0.3187,
      "step": 37060
    },
    {
      "epoch": 7.944706386626661,
      "grad_norm": 0.02525247633457184,
      "learning_rate": 9.407058151164453e-06,
      "loss": 0.5931,
      "step": 37070
    },
    {
      "epoch": 7.946849549935705,
      "grad_norm": 17.097469329833984,
      "learning_rate": 9.404200600085728e-06,
      "loss": 0.1918,
      "step": 37080
    },
    {
      "epoch": 7.948992713244749,
      "grad_norm": 0.13371926546096802,
      "learning_rate": 9.401343049007e-06,
      "loss": 0.2508,
      "step": 37090
    },
    {
      "epoch": 7.951135876553794,
      "grad_norm": 0.16021879017353058,
      "learning_rate": 9.398485497928276e-06,
      "loss": 0.4026,
      "step": 37100
    },
    {
      "epoch": 7.953279039862838,
      "grad_norm": 0.06124856323003769,
      "learning_rate": 9.39562794684955e-06,
      "loss": 0.1166,
      "step": 37110
    },
    {
      "epoch": 7.9554222031718815,
      "grad_norm": 0.37885189056396484,
      "learning_rate": 9.392770395770826e-06,
      "loss": 0.8635,
      "step": 37120
    },
    {
      "epoch": 7.957565366480926,
      "grad_norm": 15.635056495666504,
      "learning_rate": 9.3899128446921e-06,
      "loss": 0.3609,
      "step": 37130
    },
    {
      "epoch": 7.95970852978997,
      "grad_norm": 17.125350952148438,
      "learning_rate": 9.387055293613374e-06,
      "loss": 0.5547,
      "step": 37140
    },
    {
      "epoch": 7.961851693099014,
      "grad_norm": 0.1899830549955368,
      "learning_rate": 9.38419774253465e-06,
      "loss": 0.3354,
      "step": 37150
    },
    {
      "epoch": 7.963994856408059,
      "grad_norm": 0.271851122379303,
      "learning_rate": 9.381340191455923e-06,
      "loss": 0.4228,
      "step": 37160
    },
    {
      "epoch": 7.966138019717103,
      "grad_norm": 0.2381017953157425,
      "learning_rate": 9.378482640377197e-06,
      "loss": 0.0074,
      "step": 37170
    },
    {
      "epoch": 7.968281183026146,
      "grad_norm": 0.23051418364048004,
      "learning_rate": 9.375625089298473e-06,
      "loss": 0.1391,
      "step": 37180
    },
    {
      "epoch": 7.970424346335191,
      "grad_norm": 0.523790180683136,
      "learning_rate": 9.372767538219747e-06,
      "loss": 0.4406,
      "step": 37190
    },
    {
      "epoch": 7.972567509644235,
      "grad_norm": 17.660921096801758,
      "learning_rate": 9.36990998714102e-06,
      "loss": 0.7748,
      "step": 37200
    },
    {
      "epoch": 7.974710672953279,
      "grad_norm": 0.25886455178260803,
      "learning_rate": 9.367052436062295e-06,
      "loss": 0.2787,
      "step": 37210
    },
    {
      "epoch": 7.976853836262324,
      "grad_norm": 0.29107150435447693,
      "learning_rate": 9.36419488498357e-06,
      "loss": 0.2102,
      "step": 37220
    },
    {
      "epoch": 7.978996999571367,
      "grad_norm": 0.21579597890377045,
      "learning_rate": 9.361337333904844e-06,
      "loss": 0.3257,
      "step": 37230
    },
    {
      "epoch": 7.981140162880411,
      "grad_norm": 0.1662481129169464,
      "learning_rate": 9.358479782826118e-06,
      "loss": 0.4417,
      "step": 37240
    },
    {
      "epoch": 7.983283326189456,
      "grad_norm": 19.16556167602539,
      "learning_rate": 9.355622231747394e-06,
      "loss": 0.3263,
      "step": 37250
    },
    {
      "epoch": 7.9854264894985,
      "grad_norm": 0.4211586117744446,
      "learning_rate": 9.352764680668668e-06,
      "loss": 0.2885,
      "step": 37260
    },
    {
      "epoch": 7.987569652807544,
      "grad_norm": 0.26952797174453735,
      "learning_rate": 9.349907129589943e-06,
      "loss": 0.1276,
      "step": 37270
    },
    {
      "epoch": 7.9897128161165885,
      "grad_norm": 0.04081115499138832,
      "learning_rate": 9.347049578511217e-06,
      "loss": 0.1545,
      "step": 37280
    },
    {
      "epoch": 7.991855979425632,
      "grad_norm": 17.67025375366211,
      "learning_rate": 9.344192027432491e-06,
      "loss": 0.4382,
      "step": 37290
    },
    {
      "epoch": 7.993999142734676,
      "grad_norm": 295.2655029296875,
      "learning_rate": 9.341334476353767e-06,
      "loss": 0.4489,
      "step": 37300
    },
    {
      "epoch": 7.996142306043721,
      "grad_norm": 0.041038189083337784,
      "learning_rate": 9.338476925275039e-06,
      "loss": 0.1618,
      "step": 37310
    },
    {
      "epoch": 7.998285469352765,
      "grad_norm": 0.277993381023407,
      "learning_rate": 9.335619374196315e-06,
      "loss": 0.4092,
      "step": 37320
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.926,
      "eval_f1": 0.6763848396501456,
      "eval_loss": 0.33619070053100586,
      "eval_precision": 0.755700325732899,
      "eval_recall": 0.6121372031662269,
      "eval_runtime": 703.2121,
      "eval_samples_per_second": 4.266,
      "eval_steps_per_second": 1.422,
      "step": 37328
    },
    {
      "epoch": 8.000428632661809,
      "grad_norm": 0.26362428069114685,
      "learning_rate": 9.332761823117588e-06,
      "loss": 0.4842,
      "step": 37330
    },
    {
      "epoch": 8.002571795970853,
      "grad_norm": 18.733903884887695,
      "learning_rate": 9.329904272038864e-06,
      "loss": 0.5621,
      "step": 37340
    },
    {
      "epoch": 8.004714959279896,
      "grad_norm": 0.14891093969345093,
      "learning_rate": 9.327046720960138e-06,
      "loss": 0.1546,
      "step": 37350
    },
    {
      "epoch": 8.006858122588941,
      "grad_norm": 27.086387634277344,
      "learning_rate": 9.324189169881412e-06,
      "loss": 0.3528,
      "step": 37360
    },
    {
      "epoch": 8.009001285897986,
      "grad_norm": 0.18462395668029785,
      "learning_rate": 9.321331618802688e-06,
      "loss": 0.0096,
      "step": 37370
    },
    {
      "epoch": 8.011144449207029,
      "grad_norm": 0.12279760837554932,
      "learning_rate": 9.318474067723961e-06,
      "loss": 0.1484,
      "step": 37380
    },
    {
      "epoch": 8.013287612516073,
      "grad_norm": 0.43042612075805664,
      "learning_rate": 9.315616516645235e-06,
      "loss": 0.0024,
      "step": 37390
    },
    {
      "epoch": 8.015430775825118,
      "grad_norm": 0.005588439293205738,
      "learning_rate": 9.312758965566511e-06,
      "loss": 0.0014,
      "step": 37400
    },
    {
      "epoch": 8.017573939134161,
      "grad_norm": 0.4771803319454193,
      "learning_rate": 9.309901414487785e-06,
      "loss": 0.1538,
      "step": 37410
    },
    {
      "epoch": 8.019717102443206,
      "grad_norm": 18.495065689086914,
      "learning_rate": 9.307043863409059e-06,
      "loss": 0.4505,
      "step": 37420
    },
    {
      "epoch": 8.02186026575225,
      "grad_norm": 0.008527846075594425,
      "learning_rate": 9.304186312330333e-06,
      "loss": 0.3956,
      "step": 37430
    },
    {
      "epoch": 8.024003429061294,
      "grad_norm": 0.1518862545490265,
      "learning_rate": 9.301328761251608e-06,
      "loss": 0.3571,
      "step": 37440
    },
    {
      "epoch": 8.026146592370338,
      "grad_norm": 0.26161304116249084,
      "learning_rate": 9.298471210172882e-06,
      "loss": 0.5771,
      "step": 37450
    },
    {
      "epoch": 8.028289755679383,
      "grad_norm": 0.06315294653177261,
      "learning_rate": 9.295613659094156e-06,
      "loss": 0.2458,
      "step": 37460
    },
    {
      "epoch": 8.030432918988426,
      "grad_norm": 23.643470764160156,
      "learning_rate": 9.292756108015432e-06,
      "loss": 0.3793,
      "step": 37470
    },
    {
      "epoch": 8.03257608229747,
      "grad_norm": 0.5015146732330322,
      "learning_rate": 9.289898556936706e-06,
      "loss": 0.0269,
      "step": 37480
    },
    {
      "epoch": 8.034719245606516,
      "grad_norm": 0.11347366869449615,
      "learning_rate": 9.287041005857981e-06,
      "loss": 0.0012,
      "step": 37490
    },
    {
      "epoch": 8.03686240891556,
      "grad_norm": 0.011344539001584053,
      "learning_rate": 9.284183454779255e-06,
      "loss": 0.433,
      "step": 37500
    },
    {
      "epoch": 8.039005572224603,
      "grad_norm": 18.604005813598633,
      "learning_rate": 9.28132590370053e-06,
      "loss": 0.3001,
      "step": 37510
    },
    {
      "epoch": 8.041148735533648,
      "grad_norm": 0.08749028295278549,
      "learning_rate": 9.278468352621803e-06,
      "loss": 0.4494,
      "step": 37520
    },
    {
      "epoch": 8.043291898842693,
      "grad_norm": 0.09186047315597534,
      "learning_rate": 9.275610801543077e-06,
      "loss": 0.2742,
      "step": 37530
    },
    {
      "epoch": 8.045435062151736,
      "grad_norm": 0.05145241320133209,
      "learning_rate": 9.272753250464353e-06,
      "loss": 0.2226,
      "step": 37540
    },
    {
      "epoch": 8.04757822546078,
      "grad_norm": 0.28293564915657043,
      "learning_rate": 9.269895699385627e-06,
      "loss": 0.1324,
      "step": 37550
    },
    {
      "epoch": 8.049721388769825,
      "grad_norm": 0.2210751622915268,
      "learning_rate": 9.267038148306902e-06,
      "loss": 0.2805,
      "step": 37560
    },
    {
      "epoch": 8.051864552078868,
      "grad_norm": 0.07594548910856247,
      "learning_rate": 9.264180597228176e-06,
      "loss": 1.0711,
      "step": 37570
    },
    {
      "epoch": 8.054007715387913,
      "grad_norm": 0.0729091614484787,
      "learning_rate": 9.26132304614945e-06,
      "loss": 0.0958,
      "step": 37580
    },
    {
      "epoch": 8.056150878696958,
      "grad_norm": 0.1019202321767807,
      "learning_rate": 9.258465495070726e-06,
      "loss": 0.6448,
      "step": 37590
    },
    {
      "epoch": 8.058294042006,
      "grad_norm": 0.44849610328674316,
      "learning_rate": 9.255607943992e-06,
      "loss": 0.2254,
      "step": 37600
    },
    {
      "epoch": 8.060437205315045,
      "grad_norm": 18.611114501953125,
      "learning_rate": 9.252750392913274e-06,
      "loss": 0.7154,
      "step": 37610
    },
    {
      "epoch": 8.06258036862409,
      "grad_norm": 7.8142876625061035,
      "learning_rate": 9.24989284183455e-06,
      "loss": 0.3999,
      "step": 37620
    },
    {
      "epoch": 8.064723531933133,
      "grad_norm": 18.01580810546875,
      "learning_rate": 9.247035290755823e-06,
      "loss": 0.3268,
      "step": 37630
    },
    {
      "epoch": 8.066866695242178,
      "grad_norm": 0.3558329939842224,
      "learning_rate": 9.244177739677097e-06,
      "loss": 0.0061,
      "step": 37640
    },
    {
      "epoch": 8.069009858551222,
      "grad_norm": 20.8928165435791,
      "learning_rate": 9.241320188598371e-06,
      "loss": 0.3566,
      "step": 37650
    },
    {
      "epoch": 8.071153021860265,
      "grad_norm": 0.27735453844070435,
      "learning_rate": 9.238462637519647e-06,
      "loss": 0.0049,
      "step": 37660
    },
    {
      "epoch": 8.07329618516931,
      "grad_norm": 0.03418377786874771,
      "learning_rate": 9.23560508644092e-06,
      "loss": 0.1537,
      "step": 37670
    },
    {
      "epoch": 8.075439348478355,
      "grad_norm": 0.16328221559524536,
      "learning_rate": 9.232747535362195e-06,
      "loss": 0.1973,
      "step": 37680
    },
    {
      "epoch": 8.077582511787398,
      "grad_norm": 0.06658745557069778,
      "learning_rate": 9.22988998428347e-06,
      "loss": 0.2011,
      "step": 37690
    },
    {
      "epoch": 8.079725675096443,
      "grad_norm": 0.1571984440088272,
      "learning_rate": 9.227032433204744e-06,
      "loss": 0.3003,
      "step": 37700
    },
    {
      "epoch": 8.081868838405487,
      "grad_norm": 0.0970391258597374,
      "learning_rate": 9.22417488212602e-06,
      "loss": 0.4348,
      "step": 37710
    },
    {
      "epoch": 8.08401200171453,
      "grad_norm": 0.042690545320510864,
      "learning_rate": 9.221317331047294e-06,
      "loss": 0.4546,
      "step": 37720
    },
    {
      "epoch": 8.086155165023575,
      "grad_norm": 0.07056441903114319,
      "learning_rate": 9.218459779968568e-06,
      "loss": 0.1542,
      "step": 37730
    },
    {
      "epoch": 8.08829832833262,
      "grad_norm": 0.11483786255121231,
      "learning_rate": 9.215602228889842e-06,
      "loss": 0.4821,
      "step": 37740
    },
    {
      "epoch": 8.090441491641663,
      "grad_norm": 0.1615382432937622,
      "learning_rate": 9.212744677811115e-06,
      "loss": 0.2257,
      "step": 37750
    },
    {
      "epoch": 8.092584654950707,
      "grad_norm": 0.2868771553039551,
      "learning_rate": 9.209887126732391e-06,
      "loss": 0.1687,
      "step": 37760
    },
    {
      "epoch": 8.094727818259752,
      "grad_norm": 0.23595589399337769,
      "learning_rate": 9.207029575653665e-06,
      "loss": 0.1718,
      "step": 37770
    },
    {
      "epoch": 8.096870981568795,
      "grad_norm": 0.17433972656726837,
      "learning_rate": 9.20417202457494e-06,
      "loss": 0.1328,
      "step": 37780
    },
    {
      "epoch": 8.09901414487784,
      "grad_norm": 0.14993590116500854,
      "learning_rate": 9.201314473496215e-06,
      "loss": 0.277,
      "step": 37790
    },
    {
      "epoch": 8.101157308186885,
      "grad_norm": 0.39527323842048645,
      "learning_rate": 9.198456922417488e-06,
      "loss": 0.1763,
      "step": 37800
    },
    {
      "epoch": 8.103300471495928,
      "grad_norm": 0.06794026494026184,
      "learning_rate": 9.195599371338764e-06,
      "loss": 0.6994,
      "step": 37810
    },
    {
      "epoch": 8.105443634804972,
      "grad_norm": 0.11524356156587601,
      "learning_rate": 9.192741820260038e-06,
      "loss": 0.6958,
      "step": 37820
    },
    {
      "epoch": 8.107586798114017,
      "grad_norm": 0.038426972925662994,
      "learning_rate": 9.189884269181314e-06,
      "loss": 0.1473,
      "step": 37830
    },
    {
      "epoch": 8.10972996142306,
      "grad_norm": 0.03660404309630394,
      "learning_rate": 9.187026718102586e-06,
      "loss": 0.8433,
      "step": 37840
    },
    {
      "epoch": 8.111873124732105,
      "grad_norm": 0.07973217964172363,
      "learning_rate": 9.184169167023862e-06,
      "loss": 0.0056,
      "step": 37850
    },
    {
      "epoch": 8.11401628804115,
      "grad_norm": 0.3185647428035736,
      "learning_rate": 9.181311615945135e-06,
      "loss": 0.3283,
      "step": 37860
    },
    {
      "epoch": 8.116159451350192,
      "grad_norm": 0.20434418320655823,
      "learning_rate": 9.17845406486641e-06,
      "loss": 0.1616,
      "step": 37870
    },
    {
      "epoch": 8.118302614659237,
      "grad_norm": 0.03300870209932327,
      "learning_rate": 9.175596513787685e-06,
      "loss": 0.0024,
      "step": 37880
    },
    {
      "epoch": 8.120445777968282,
      "grad_norm": 0.03168175369501114,
      "learning_rate": 9.172738962708959e-06,
      "loss": 0.002,
      "step": 37890
    },
    {
      "epoch": 8.122588941277325,
      "grad_norm": 0.07044185698032379,
      "learning_rate": 9.169881411630235e-06,
      "loss": 0.4033,
      "step": 37900
    },
    {
      "epoch": 8.12473210458637,
      "grad_norm": 0.3033865988254547,
      "learning_rate": 9.167023860551508e-06,
      "loss": 0.4915,
      "step": 37910
    },
    {
      "epoch": 8.126875267895414,
      "grad_norm": 0.1774490922689438,
      "learning_rate": 9.164166309472782e-06,
      "loss": 0.1539,
      "step": 37920
    },
    {
      "epoch": 8.129018431204457,
      "grad_norm": 20.191162109375,
      "learning_rate": 9.161308758394058e-06,
      "loss": 0.7229,
      "step": 37930
    },
    {
      "epoch": 8.131161594513502,
      "grad_norm": 0.07020553201436996,
      "learning_rate": 9.158451207315332e-06,
      "loss": 0.3682,
      "step": 37940
    },
    {
      "epoch": 8.133304757822547,
      "grad_norm": 20.041982650756836,
      "learning_rate": 9.155593656236606e-06,
      "loss": 0.252,
      "step": 37950
    },
    {
      "epoch": 8.13544792113159,
      "grad_norm": 0.0369853712618351,
      "learning_rate": 9.15273610515788e-06,
      "loss": 0.0018,
      "step": 37960
    },
    {
      "epoch": 8.137591084440635,
      "grad_norm": 0.47954627871513367,
      "learning_rate": 9.149878554079155e-06,
      "loss": 0.5632,
      "step": 37970
    },
    {
      "epoch": 8.13973424774968,
      "grad_norm": 0.24801863729953766,
      "learning_rate": 9.14702100300043e-06,
      "loss": 0.1761,
      "step": 37980
    },
    {
      "epoch": 8.141877411058722,
      "grad_norm": 23.65763282775879,
      "learning_rate": 9.144163451921703e-06,
      "loss": 0.5236,
      "step": 37990
    },
    {
      "epoch": 8.144020574367767,
      "grad_norm": 0.31679415702819824,
      "learning_rate": 9.141305900842979e-06,
      "loss": 0.0041,
      "step": 38000
    },
    {
      "epoch": 8.146163737676812,
      "grad_norm": 17.488601684570312,
      "learning_rate": 9.138448349764253e-06,
      "loss": 0.2907,
      "step": 38010
    },
    {
      "epoch": 8.148306900985855,
      "grad_norm": 0.03755500167608261,
      "learning_rate": 9.135590798685527e-06,
      "loss": 0.3165,
      "step": 38020
    },
    {
      "epoch": 8.1504500642949,
      "grad_norm": 0.03426654264330864,
      "learning_rate": 9.132733247606802e-06,
      "loss": 0.0944,
      "step": 38030
    },
    {
      "epoch": 8.152593227603944,
      "grad_norm": 16.0960750579834,
      "learning_rate": 9.129875696528076e-06,
      "loss": 0.5694,
      "step": 38040
    },
    {
      "epoch": 8.154736390912987,
      "grad_norm": 18.004169464111328,
      "learning_rate": 9.127018145449352e-06,
      "loss": 0.4968,
      "step": 38050
    },
    {
      "epoch": 8.156879554222032,
      "grad_norm": 0.25275370478630066,
      "learning_rate": 9.124160594370624e-06,
      "loss": 0.0287,
      "step": 38060
    },
    {
      "epoch": 8.159022717531077,
      "grad_norm": 0.11036662757396698,
      "learning_rate": 9.1213030432919e-06,
      "loss": 0.2178,
      "step": 38070
    },
    {
      "epoch": 8.16116588084012,
      "grad_norm": 0.024481842294335365,
      "learning_rate": 9.118445492213174e-06,
      "loss": 0.3963,
      "step": 38080
    },
    {
      "epoch": 8.163309044149164,
      "grad_norm": 0.0653798058629036,
      "learning_rate": 9.115587941134448e-06,
      "loss": 0.0683,
      "step": 38090
    },
    {
      "epoch": 8.165452207458209,
      "grad_norm": 0.13777635991573334,
      "learning_rate": 9.112730390055723e-06,
      "loss": 0.2052,
      "step": 38100
    },
    {
      "epoch": 8.167595370767252,
      "grad_norm": 0.1095847636461258,
      "learning_rate": 9.109872838976997e-06,
      "loss": 0.1418,
      "step": 38110
    },
    {
      "epoch": 8.169738534076297,
      "grad_norm": 6.838067054748535,
      "learning_rate": 9.107015287898273e-06,
      "loss": 0.5001,
      "step": 38120
    },
    {
      "epoch": 8.171881697385341,
      "grad_norm": 0.19611871242523193,
      "learning_rate": 9.104157736819547e-06,
      "loss": 0.3586,
      "step": 38130
    },
    {
      "epoch": 8.174024860694384,
      "grad_norm": 0.18821588158607483,
      "learning_rate": 9.10130018574082e-06,
      "loss": 0.0041,
      "step": 38140
    },
    {
      "epoch": 8.17616802400343,
      "grad_norm": 0.26402533054351807,
      "learning_rate": 9.098442634662096e-06,
      "loss": 0.3678,
      "step": 38150
    },
    {
      "epoch": 8.178311187312474,
      "grad_norm": 0.02329573594033718,
      "learning_rate": 9.09558508358337e-06,
      "loss": 0.0738,
      "step": 38160
    },
    {
      "epoch": 8.180454350621517,
      "grad_norm": 0.01002686470746994,
      "learning_rate": 9.092727532504644e-06,
      "loss": 0.2067,
      "step": 38170
    },
    {
      "epoch": 8.182597513930562,
      "grad_norm": 19.120159149169922,
      "learning_rate": 9.089869981425918e-06,
      "loss": 0.5647,
      "step": 38180
    },
    {
      "epoch": 8.184740677239606,
      "grad_norm": 0.13306958973407745,
      "learning_rate": 9.087012430347194e-06,
      "loss": 0.1179,
      "step": 38190
    },
    {
      "epoch": 8.18688384054865,
      "grad_norm": 0.05405165255069733,
      "learning_rate": 9.084154879268468e-06,
      "loss": 0.0007,
      "step": 38200
    },
    {
      "epoch": 8.189027003857694,
      "grad_norm": 20.675569534301758,
      "learning_rate": 9.081297328189742e-06,
      "loss": 0.2031,
      "step": 38210
    },
    {
      "epoch": 8.191170167166739,
      "grad_norm": 0.05156225338578224,
      "learning_rate": 9.078439777111017e-06,
      "loss": 0.5721,
      "step": 38220
    },
    {
      "epoch": 8.193313330475782,
      "grad_norm": 21.57841682434082,
      "learning_rate": 9.075582226032291e-06,
      "loss": 0.3423,
      "step": 38230
    },
    {
      "epoch": 8.195456493784826,
      "grad_norm": 8.778407096862793,
      "learning_rate": 9.072724674953565e-06,
      "loss": 0.4228,
      "step": 38240
    },
    {
      "epoch": 8.197599657093871,
      "grad_norm": 0.12581206858158112,
      "learning_rate": 9.06986712387484e-06,
      "loss": 0.3324,
      "step": 38250
    },
    {
      "epoch": 8.199742820402914,
      "grad_norm": 0.2334832400083542,
      "learning_rate": 9.067009572796115e-06,
      "loss": 0.0072,
      "step": 38260
    },
    {
      "epoch": 8.201885983711959,
      "grad_norm": 0.0386238731443882,
      "learning_rate": 9.064152021717389e-06,
      "loss": 0.0373,
      "step": 38270
    },
    {
      "epoch": 8.204029147021004,
      "grad_norm": 0.0439206063747406,
      "learning_rate": 9.061294470638662e-06,
      "loss": 0.0015,
      "step": 38280
    },
    {
      "epoch": 8.206172310330047,
      "grad_norm": 20.731735229492188,
      "learning_rate": 9.058436919559938e-06,
      "loss": 0.7583,
      "step": 38290
    },
    {
      "epoch": 8.208315473639091,
      "grad_norm": 19.263185501098633,
      "learning_rate": 9.055579368481212e-06,
      "loss": 0.3226,
      "step": 38300
    },
    {
      "epoch": 8.210458636948136,
      "grad_norm": 38.64298629760742,
      "learning_rate": 9.052721817402486e-06,
      "loss": 0.1841,
      "step": 38310
    },
    {
      "epoch": 8.212601800257179,
      "grad_norm": 0.035654809325933456,
      "learning_rate": 9.049864266323762e-06,
      "loss": 0.4355,
      "step": 38320
    },
    {
      "epoch": 8.214744963566224,
      "grad_norm": 0.009945598430931568,
      "learning_rate": 9.047006715245035e-06,
      "loss": 0.5336,
      "step": 38330
    },
    {
      "epoch": 8.216888126875268,
      "grad_norm": 0.09706229716539383,
      "learning_rate": 9.044149164166311e-06,
      "loss": 0.0059,
      "step": 38340
    },
    {
      "epoch": 8.219031290184311,
      "grad_norm": 0.020253079012036324,
      "learning_rate": 9.041291613087585e-06,
      "loss": 0.6722,
      "step": 38350
    },
    {
      "epoch": 8.221174453493356,
      "grad_norm": 20.3734130859375,
      "learning_rate": 9.038434062008859e-06,
      "loss": 0.693,
      "step": 38360
    },
    {
      "epoch": 8.223317616802401,
      "grad_norm": 20.953401565551758,
      "learning_rate": 9.035576510930135e-06,
      "loss": 0.3063,
      "step": 38370
    },
    {
      "epoch": 8.225460780111444,
      "grad_norm": 0.10255113989114761,
      "learning_rate": 9.032718959851407e-06,
      "loss": 0.2352,
      "step": 38380
    },
    {
      "epoch": 8.227603943420489,
      "grad_norm": 21.592378616333008,
      "learning_rate": 9.029861408772682e-06,
      "loss": 0.3077,
      "step": 38390
    },
    {
      "epoch": 8.229747106729533,
      "grad_norm": 19.115293502807617,
      "learning_rate": 9.027003857693956e-06,
      "loss": 0.1278,
      "step": 38400
    },
    {
      "epoch": 8.231890270038576,
      "grad_norm": 0.16366268694400787,
      "learning_rate": 9.024146306615232e-06,
      "loss": 0.0025,
      "step": 38410
    },
    {
      "epoch": 8.234033433347621,
      "grad_norm": 0.0063541787676513195,
      "learning_rate": 9.021288755536506e-06,
      "loss": 0.1879,
      "step": 38420
    },
    {
      "epoch": 8.236176596656666,
      "grad_norm": 0.00791300181299448,
      "learning_rate": 9.01843120445778e-06,
      "loss": 0.1835,
      "step": 38430
    },
    {
      "epoch": 8.238319759965709,
      "grad_norm": 19.845294952392578,
      "learning_rate": 9.015573653379055e-06,
      "loss": 0.6773,
      "step": 38440
    },
    {
      "epoch": 8.240462923274753,
      "grad_norm": 0.12445420771837234,
      "learning_rate": 9.01271610230033e-06,
      "loss": 0.6413,
      "step": 38450
    },
    {
      "epoch": 8.242606086583798,
      "grad_norm": 0.15730218589305878,
      "learning_rate": 9.009858551221603e-06,
      "loss": 0.1646,
      "step": 38460
    },
    {
      "epoch": 8.244749249892841,
      "grad_norm": 0.02286142110824585,
      "learning_rate": 9.007001000142879e-06,
      "loss": 0.1416,
      "step": 38470
    },
    {
      "epoch": 8.246892413201886,
      "grad_norm": 0.11778301745653152,
      "learning_rate": 9.004143449064153e-06,
      "loss": 0.4227,
      "step": 38480
    },
    {
      "epoch": 8.24903557651093,
      "grad_norm": 0.04256089776754379,
      "learning_rate": 9.001285897985427e-06,
      "loss": 0.1328,
      "step": 38490
    },
    {
      "epoch": 8.251178739819974,
      "grad_norm": 0.36882731318473816,
      "learning_rate": 8.9984283469067e-06,
      "loss": 0.4423,
      "step": 38500
    },
    {
      "epoch": 8.253321903129018,
      "grad_norm": 0.36391839385032654,
      "learning_rate": 8.995570795827976e-06,
      "loss": 0.3855,
      "step": 38510
    },
    {
      "epoch": 8.255465066438063,
      "grad_norm": 0.21392948925495148,
      "learning_rate": 8.99271324474925e-06,
      "loss": 0.2018,
      "step": 38520
    },
    {
      "epoch": 8.257608229747106,
      "grad_norm": 0.06436441838741302,
      "learning_rate": 8.989855693670524e-06,
      "loss": 0.3121,
      "step": 38530
    },
    {
      "epoch": 8.25975139305615,
      "grad_norm": 0.014531411230564117,
      "learning_rate": 8.9869981425918e-06,
      "loss": 0.0032,
      "step": 38540
    },
    {
      "epoch": 8.261894556365196,
      "grad_norm": 18.212284088134766,
      "learning_rate": 8.984140591513074e-06,
      "loss": 0.3601,
      "step": 38550
    },
    {
      "epoch": 8.264037719674239,
      "grad_norm": 0.022210286930203438,
      "learning_rate": 8.98128304043435e-06,
      "loss": 0.3802,
      "step": 38560
    },
    {
      "epoch": 8.266180882983283,
      "grad_norm": 0.2683594822883606,
      "learning_rate": 8.978425489355623e-06,
      "loss": 0.1055,
      "step": 38570
    },
    {
      "epoch": 8.268324046292328,
      "grad_norm": 0.010529093444347382,
      "learning_rate": 8.975567938276897e-06,
      "loss": 0.1436,
      "step": 38580
    },
    {
      "epoch": 8.270467209601371,
      "grad_norm": 0.005241819657385349,
      "learning_rate": 8.972710387198173e-06,
      "loss": 0.3847,
      "step": 38590
    },
    {
      "epoch": 8.272610372910416,
      "grad_norm": 0.06857892125844955,
      "learning_rate": 8.969852836119445e-06,
      "loss": 0.5416,
      "step": 38600
    },
    {
      "epoch": 8.27475353621946,
      "grad_norm": 0.3691715598106384,
      "learning_rate": 8.96699528504072e-06,
      "loss": 0.1917,
      "step": 38610
    },
    {
      "epoch": 8.276896699528503,
      "grad_norm": 0.5887055993080139,
      "learning_rate": 8.964137733961995e-06,
      "loss": 0.1597,
      "step": 38620
    },
    {
      "epoch": 8.279039862837548,
      "grad_norm": 0.004998599644750357,
      "learning_rate": 8.96128018288327e-06,
      "loss": 0.3214,
      "step": 38630
    },
    {
      "epoch": 8.281183026146593,
      "grad_norm": 53.95744323730469,
      "learning_rate": 8.958422631804544e-06,
      "loss": 0.2094,
      "step": 38640
    },
    {
      "epoch": 8.283326189455636,
      "grad_norm": 0.7212697267532349,
      "learning_rate": 8.955565080725818e-06,
      "loss": 0.3652,
      "step": 38650
    },
    {
      "epoch": 8.28546935276468,
      "grad_norm": 0.013624307699501514,
      "learning_rate": 8.952707529647094e-06,
      "loss": 0.243,
      "step": 38660
    },
    {
      "epoch": 8.287612516073725,
      "grad_norm": 0.2928704023361206,
      "learning_rate": 8.949849978568368e-06,
      "loss": 0.1222,
      "step": 38670
    },
    {
      "epoch": 8.289755679382768,
      "grad_norm": 0.006087981164455414,
      "learning_rate": 8.946992427489643e-06,
      "loss": 0.2164,
      "step": 38680
    },
    {
      "epoch": 8.291898842691813,
      "grad_norm": 0.27028584480285645,
      "learning_rate": 8.944134876410917e-06,
      "loss": 0.8235,
      "step": 38690
    },
    {
      "epoch": 8.294042006000858,
      "grad_norm": 0.052506428211927414,
      "learning_rate": 8.941277325332191e-06,
      "loss": 0.0016,
      "step": 38700
    },
    {
      "epoch": 8.2961851693099,
      "grad_norm": 18.873523712158203,
      "learning_rate": 8.938419774253465e-06,
      "loss": 0.3149,
      "step": 38710
    },
    {
      "epoch": 8.298328332618945,
      "grad_norm": 16.487619400024414,
      "learning_rate": 8.935562223174739e-06,
      "loss": 0.4661,
      "step": 38720
    },
    {
      "epoch": 8.30047149592799,
      "grad_norm": 34.67375183105469,
      "learning_rate": 8.932704672096015e-06,
      "loss": 0.6153,
      "step": 38730
    },
    {
      "epoch": 8.302614659237033,
      "grad_norm": 0.5382943153381348,
      "learning_rate": 8.929847121017289e-06,
      "loss": 0.0046,
      "step": 38740
    },
    {
      "epoch": 8.304757822546078,
      "grad_norm": 0.0061425757594406605,
      "learning_rate": 8.926989569938564e-06,
      "loss": 0.0026,
      "step": 38750
    },
    {
      "epoch": 8.306900985855123,
      "grad_norm": 0.012685495428740978,
      "learning_rate": 8.924132018859838e-06,
      "loss": 0.3214,
      "step": 38760
    },
    {
      "epoch": 8.309044149164166,
      "grad_norm": 22.176841735839844,
      "learning_rate": 8.921274467781112e-06,
      "loss": 0.4539,
      "step": 38770
    },
    {
      "epoch": 8.31118731247321,
      "grad_norm": 0.021237866953015327,
      "learning_rate": 8.918416916702388e-06,
      "loss": 0.0006,
      "step": 38780
    },
    {
      "epoch": 8.313330475782255,
      "grad_norm": 0.03701446205377579,
      "learning_rate": 8.915559365623662e-06,
      "loss": 0.2231,
      "step": 38790
    },
    {
      "epoch": 8.315473639091298,
      "grad_norm": 0.01720251515507698,
      "learning_rate": 8.912701814544936e-06,
      "loss": 0.0011,
      "step": 38800
    },
    {
      "epoch": 8.317616802400343,
      "grad_norm": 0.20318633317947388,
      "learning_rate": 8.90984426346621e-06,
      "loss": 0.2036,
      "step": 38810
    },
    {
      "epoch": 8.319759965709387,
      "grad_norm": 0.08790042251348495,
      "learning_rate": 8.906986712387485e-06,
      "loss": 0.0009,
      "step": 38820
    },
    {
      "epoch": 8.32190312901843,
      "grad_norm": 1.0379457473754883,
      "learning_rate": 8.904129161308759e-06,
      "loss": 0.2485,
      "step": 38830
    },
    {
      "epoch": 8.324046292327475,
      "grad_norm": 0.01020073052495718,
      "learning_rate": 8.901271610230033e-06,
      "loss": 0.3391,
      "step": 38840
    },
    {
      "epoch": 8.32618945563652,
      "grad_norm": 0.0070912656374275684,
      "learning_rate": 8.898414059151309e-06,
      "loss": 0.0021,
      "step": 38850
    },
    {
      "epoch": 8.328332618945563,
      "grad_norm": 0.0070229689590632915,
      "learning_rate": 8.895556508072582e-06,
      "loss": 0.5482,
      "step": 38860
    },
    {
      "epoch": 8.330475782254608,
      "grad_norm": 0.017617104575037956,
      "learning_rate": 8.892698956993856e-06,
      "loss": 0.2404,
      "step": 38870
    },
    {
      "epoch": 8.332618945563652,
      "grad_norm": 25.670305252075195,
      "learning_rate": 8.889841405915132e-06,
      "loss": 0.4467,
      "step": 38880
    },
    {
      "epoch": 8.334762108872695,
      "grad_norm": 19.780385971069336,
      "learning_rate": 8.886983854836406e-06,
      "loss": 0.2602,
      "step": 38890
    },
    {
      "epoch": 8.33690527218174,
      "grad_norm": 0.12656114995479584,
      "learning_rate": 8.884126303757682e-06,
      "loss": 0.0032,
      "step": 38900
    },
    {
      "epoch": 8.339048435490785,
      "grad_norm": 0.1308605968952179,
      "learning_rate": 8.881268752678955e-06,
      "loss": 0.1582,
      "step": 38910
    },
    {
      "epoch": 8.341191598799828,
      "grad_norm": 0.03515182062983513,
      "learning_rate": 8.87841120160023e-06,
      "loss": 0.1765,
      "step": 38920
    },
    {
      "epoch": 8.343334762108872,
      "grad_norm": 0.018411975353956223,
      "learning_rate": 8.875553650521503e-06,
      "loss": 0.2479,
      "step": 38930
    },
    {
      "epoch": 8.345477925417917,
      "grad_norm": 15.814654350280762,
      "learning_rate": 8.872696099442777e-06,
      "loss": 0.2735,
      "step": 38940
    },
    {
      "epoch": 8.34762108872696,
      "grad_norm": 0.0641586109995842,
      "learning_rate": 8.869838548364053e-06,
      "loss": 0.2623,
      "step": 38950
    },
    {
      "epoch": 8.349764252036005,
      "grad_norm": 0.021026529371738434,
      "learning_rate": 8.866980997285327e-06,
      "loss": 0.0038,
      "step": 38960
    },
    {
      "epoch": 8.35190741534505,
      "grad_norm": 0.13816097378730774,
      "learning_rate": 8.864123446206602e-06,
      "loss": 0.1613,
      "step": 38970
    },
    {
      "epoch": 8.354050578654093,
      "grad_norm": 0.00528201088309288,
      "learning_rate": 8.861265895127876e-06,
      "loss": 0.0013,
      "step": 38980
    },
    {
      "epoch": 8.356193741963137,
      "grad_norm": 0.009927294217050076,
      "learning_rate": 8.85840834404915e-06,
      "loss": 0.7388,
      "step": 38990
    },
    {
      "epoch": 8.358336905272182,
      "grad_norm": 0.017569858580827713,
      "learning_rate": 8.855550792970426e-06,
      "loss": 0.1218,
      "step": 39000
    },
    {
      "epoch": 8.360480068581225,
      "grad_norm": 1.3511207103729248,
      "learning_rate": 8.8526932418917e-06,
      "loss": 0.1748,
      "step": 39010
    },
    {
      "epoch": 8.36262323189027,
      "grad_norm": 0.037755176424980164,
      "learning_rate": 8.849835690812974e-06,
      "loss": 0.1546,
      "step": 39020
    },
    {
      "epoch": 8.364766395199315,
      "grad_norm": 0.03115702047944069,
      "learning_rate": 8.846978139734248e-06,
      "loss": 0.3614,
      "step": 39030
    },
    {
      "epoch": 8.366909558508357,
      "grad_norm": 0.02151758037507534,
      "learning_rate": 8.844120588655523e-06,
      "loss": 0.2274,
      "step": 39040
    },
    {
      "epoch": 8.369052721817402,
      "grad_norm": 16.798540115356445,
      "learning_rate": 8.841263037576797e-06,
      "loss": 0.2984,
      "step": 39050
    },
    {
      "epoch": 8.371195885126447,
      "grad_norm": 0.03016381710767746,
      "learning_rate": 8.838405486498071e-06,
      "loss": 0.236,
      "step": 39060
    },
    {
      "epoch": 8.37333904843549,
      "grad_norm": 0.08314655721187592,
      "learning_rate": 8.835547935419347e-06,
      "loss": 0.2445,
      "step": 39070
    },
    {
      "epoch": 8.375482211744535,
      "grad_norm": 0.1890222579240799,
      "learning_rate": 8.83269038434062e-06,
      "loss": 0.8743,
      "step": 39080
    },
    {
      "epoch": 8.37762537505358,
      "grad_norm": 0.11066463589668274,
      "learning_rate": 8.829832833261895e-06,
      "loss": 0.1437,
      "step": 39090
    },
    {
      "epoch": 8.379768538362622,
      "grad_norm": 0.06464175134897232,
      "learning_rate": 8.82697528218317e-06,
      "loss": 0.3317,
      "step": 39100
    },
    {
      "epoch": 8.381911701671667,
      "grad_norm": 0.1978749930858612,
      "learning_rate": 8.824117731104444e-06,
      "loss": 0.6944,
      "step": 39110
    },
    {
      "epoch": 8.384054864980712,
      "grad_norm": 0.13576991856098175,
      "learning_rate": 8.82126018002572e-06,
      "loss": 0.1542,
      "step": 39120
    },
    {
      "epoch": 8.386198028289755,
      "grad_norm": 0.33098968863487244,
      "learning_rate": 8.818402628946992e-06,
      "loss": 0.1159,
      "step": 39130
    },
    {
      "epoch": 8.3883411915988,
      "grad_norm": 25.9561824798584,
      "learning_rate": 8.815545077868268e-06,
      "loss": 0.7742,
      "step": 39140
    },
    {
      "epoch": 8.390484354907844,
      "grad_norm": 0.05116211622953415,
      "learning_rate": 8.812687526789542e-06,
      "loss": 0.2618,
      "step": 39150
    },
    {
      "epoch": 8.392627518216887,
      "grad_norm": 15.629012107849121,
      "learning_rate": 8.809829975710816e-06,
      "loss": 0.5132,
      "step": 39160
    },
    {
      "epoch": 8.394770681525932,
      "grad_norm": 2.209343671798706,
      "learning_rate": 8.806972424632091e-06,
      "loss": 0.9162,
      "step": 39170
    },
    {
      "epoch": 8.396913844834977,
      "grad_norm": 0.038827188313007355,
      "learning_rate": 8.804114873553365e-06,
      "loss": 0.0041,
      "step": 39180
    },
    {
      "epoch": 8.39905700814402,
      "grad_norm": 0.23936980962753296,
      "learning_rate": 8.80125732247464e-06,
      "loss": 0.2186,
      "step": 39190
    },
    {
      "epoch": 8.401200171453064,
      "grad_norm": 126.38056182861328,
      "learning_rate": 8.798399771395915e-06,
      "loss": 1.0537,
      "step": 39200
    },
    {
      "epoch": 8.40334333476211,
      "grad_norm": 4.634196758270264,
      "learning_rate": 8.795542220317189e-06,
      "loss": 0.4508,
      "step": 39210
    },
    {
      "epoch": 8.405486498071152,
      "grad_norm": 0.042747970670461655,
      "learning_rate": 8.792684669238464e-06,
      "loss": 0.1098,
      "step": 39220
    },
    {
      "epoch": 8.407629661380197,
      "grad_norm": 0.05980224162340164,
      "learning_rate": 8.789827118159738e-06,
      "loss": 0.2376,
      "step": 39230
    },
    {
      "epoch": 8.409772824689242,
      "grad_norm": 0.04406655952334404,
      "learning_rate": 8.786969567081012e-06,
      "loss": 0.0022,
      "step": 39240
    },
    {
      "epoch": 8.411915987998286,
      "grad_norm": 0.17886540293693542,
      "learning_rate": 8.784112016002286e-06,
      "loss": 0.0018,
      "step": 39250
    },
    {
      "epoch": 8.41405915130733,
      "grad_norm": 0.027538785710930824,
      "learning_rate": 8.781254464923562e-06,
      "loss": 0.4065,
      "step": 39260
    },
    {
      "epoch": 8.416202314616374,
      "grad_norm": 0.06550075113773346,
      "learning_rate": 8.778396913844836e-06,
      "loss": 0.7524,
      "step": 39270
    },
    {
      "epoch": 8.418345477925419,
      "grad_norm": 0.11979684233665466,
      "learning_rate": 8.77553936276611e-06,
      "loss": 0.2046,
      "step": 39280
    },
    {
      "epoch": 8.420488641234462,
      "grad_norm": 0.07527866959571838,
      "learning_rate": 8.772681811687385e-06,
      "loss": 0.0022,
      "step": 39290
    },
    {
      "epoch": 8.422631804543506,
      "grad_norm": 17.51732063293457,
      "learning_rate": 8.769824260608659e-06,
      "loss": 0.3534,
      "step": 39300
    },
    {
      "epoch": 8.424774967852551,
      "grad_norm": 0.21841281652450562,
      "learning_rate": 8.766966709529933e-06,
      "loss": 0.4221,
      "step": 39310
    },
    {
      "epoch": 8.426918131161594,
      "grad_norm": 2.2004568576812744,
      "learning_rate": 8.764109158451209e-06,
      "loss": 0.9002,
      "step": 39320
    },
    {
      "epoch": 8.429061294470639,
      "grad_norm": 87.2771987915039,
      "learning_rate": 8.761251607372483e-06,
      "loss": 0.3012,
      "step": 39330
    },
    {
      "epoch": 8.431204457779684,
      "grad_norm": 0.12234771251678467,
      "learning_rate": 8.758394056293758e-06,
      "loss": 0.4453,
      "step": 39340
    },
    {
      "epoch": 8.433347621088727,
      "grad_norm": 0.07311074435710907,
      "learning_rate": 8.75553650521503e-06,
      "loss": 0.0041,
      "step": 39350
    },
    {
      "epoch": 8.435490784397771,
      "grad_norm": 0.09338151663541794,
      "learning_rate": 8.752678954136306e-06,
      "loss": 0.087,
      "step": 39360
    },
    {
      "epoch": 8.437633947706816,
      "grad_norm": 0.24145643413066864,
      "learning_rate": 8.74982140305758e-06,
      "loss": 0.3627,
      "step": 39370
    },
    {
      "epoch": 8.439777111015859,
      "grad_norm": 0.3133002519607544,
      "learning_rate": 8.746963851978854e-06,
      "loss": 0.1733,
      "step": 39380
    },
    {
      "epoch": 8.441920274324904,
      "grad_norm": 0.01434401050209999,
      "learning_rate": 8.74410630090013e-06,
      "loss": 0.0036,
      "step": 39390
    },
    {
      "epoch": 8.444063437633949,
      "grad_norm": 0.07524047046899796,
      "learning_rate": 8.741248749821403e-06,
      "loss": 0.7721,
      "step": 39400
    },
    {
      "epoch": 8.446206600942991,
      "grad_norm": 51.390438079833984,
      "learning_rate": 8.738391198742679e-06,
      "loss": 0.309,
      "step": 39410
    },
    {
      "epoch": 8.448349764252036,
      "grad_norm": 0.5068479180335999,
      "learning_rate": 8.735533647663953e-06,
      "loss": 0.0038,
      "step": 39420
    },
    {
      "epoch": 8.450492927561081,
      "grad_norm": 0.17859089374542236,
      "learning_rate": 8.732676096585227e-06,
      "loss": 0.5155,
      "step": 39430
    },
    {
      "epoch": 8.452636090870124,
      "grad_norm": 46.64434814453125,
      "learning_rate": 8.729818545506502e-06,
      "loss": 0.2036,
      "step": 39440
    },
    {
      "epoch": 8.454779254179169,
      "grad_norm": 0.07019928097724915,
      "learning_rate": 8.726960994427775e-06,
      "loss": 0.49,
      "step": 39450
    },
    {
      "epoch": 8.456922417488213,
      "grad_norm": 0.08732324838638306,
      "learning_rate": 8.72410344334905e-06,
      "loss": 0.1549,
      "step": 39460
    },
    {
      "epoch": 8.459065580797256,
      "grad_norm": 20.246946334838867,
      "learning_rate": 8.721245892270324e-06,
      "loss": 0.3981,
      "step": 39470
    },
    {
      "epoch": 8.461208744106301,
      "grad_norm": 0.046957287937402725,
      "learning_rate": 8.7183883411916e-06,
      "loss": 0.1597,
      "step": 39480
    },
    {
      "epoch": 8.463351907415346,
      "grad_norm": 0.04556944593787193,
      "learning_rate": 8.715530790112874e-06,
      "loss": 0.5602,
      "step": 39490
    },
    {
      "epoch": 8.465495070724389,
      "grad_norm": 0.033446259796619415,
      "learning_rate": 8.712673239034148e-06,
      "loss": 0.1502,
      "step": 39500
    },
    {
      "epoch": 8.467638234033434,
      "grad_norm": 0.05399983376264572,
      "learning_rate": 8.709815687955423e-06,
      "loss": 0.1285,
      "step": 39510
    },
    {
      "epoch": 8.469781397342478,
      "grad_norm": 0.10366232693195343,
      "learning_rate": 8.706958136876697e-06,
      "loss": 0.589,
      "step": 39520
    },
    {
      "epoch": 8.471924560651521,
      "grad_norm": 33.75567626953125,
      "learning_rate": 8.704100585797973e-06,
      "loss": 0.1223,
      "step": 39530
    },
    {
      "epoch": 8.474067723960566,
      "grad_norm": 0.94941246509552,
      "learning_rate": 8.701243034719247e-06,
      "loss": 0.0037,
      "step": 39540
    },
    {
      "epoch": 8.47621088726961,
      "grad_norm": 0.05581687390804291,
      "learning_rate": 8.69838548364052e-06,
      "loss": 0.1753,
      "step": 39550
    },
    {
      "epoch": 8.478354050578654,
      "grad_norm": 0.011375701986253262,
      "learning_rate": 8.695527932561795e-06,
      "loss": 0.1651,
      "step": 39560
    },
    {
      "epoch": 8.480497213887698,
      "grad_norm": 0.573118269443512,
      "learning_rate": 8.692670381483069e-06,
      "loss": 0.1462,
      "step": 39570
    },
    {
      "epoch": 8.482640377196743,
      "grad_norm": 0.03822394460439682,
      "learning_rate": 8.689812830404344e-06,
      "loss": 0.3973,
      "step": 39580
    },
    {
      "epoch": 8.484783540505786,
      "grad_norm": 0.08746055513620377,
      "learning_rate": 8.686955279325618e-06,
      "loss": 0.1771,
      "step": 39590
    },
    {
      "epoch": 8.48692670381483,
      "grad_norm": 1.3145766258239746,
      "learning_rate": 8.684097728246894e-06,
      "loss": 0.2816,
      "step": 39600
    },
    {
      "epoch": 8.489069867123876,
      "grad_norm": 0.05189583823084831,
      "learning_rate": 8.681240177168168e-06,
      "loss": 0.003,
      "step": 39610
    },
    {
      "epoch": 8.491213030432919,
      "grad_norm": 0.03902150318026543,
      "learning_rate": 8.678382626089442e-06,
      "loss": 0.2164,
      "step": 39620
    },
    {
      "epoch": 8.493356193741963,
      "grad_norm": 0.03190385550260544,
      "learning_rate": 8.675525075010717e-06,
      "loss": 0.001,
      "step": 39630
    },
    {
      "epoch": 8.495499357051008,
      "grad_norm": 0.01670898124575615,
      "learning_rate": 8.672667523931991e-06,
      "loss": 0.0587,
      "step": 39640
    },
    {
      "epoch": 8.497642520360051,
      "grad_norm": 16.16501235961914,
      "learning_rate": 8.669809972853265e-06,
      "loss": 1.0623,
      "step": 39650
    },
    {
      "epoch": 8.499785683669096,
      "grad_norm": 0.014188634231686592,
      "learning_rate": 8.66695242177454e-06,
      "loss": 0.0028,
      "step": 39660
    },
    {
      "epoch": 8.50192884697814,
      "grad_norm": 0.008936072699725628,
      "learning_rate": 8.664094870695815e-06,
      "loss": 0.1403,
      "step": 39670
    },
    {
      "epoch": 8.504072010287183,
      "grad_norm": 17.57122039794922,
      "learning_rate": 8.661237319617089e-06,
      "loss": 0.3186,
      "step": 39680
    },
    {
      "epoch": 8.506215173596228,
      "grad_norm": 0.4502403736114502,
      "learning_rate": 8.658379768538363e-06,
      "loss": 0.0138,
      "step": 39690
    },
    {
      "epoch": 8.508358336905273,
      "grad_norm": 0.05493023991584778,
      "learning_rate": 8.655522217459638e-06,
      "loss": 0.5356,
      "step": 39700
    },
    {
      "epoch": 8.510501500214316,
      "grad_norm": 0.0077501703053712845,
      "learning_rate": 8.652664666380912e-06,
      "loss": 0.4706,
      "step": 39710
    },
    {
      "epoch": 8.51264466352336,
      "grad_norm": 0.7142720222473145,
      "learning_rate": 8.649807115302186e-06,
      "loss": 0.2726,
      "step": 39720
    },
    {
      "epoch": 8.514787826832405,
      "grad_norm": 0.08354644477367401,
      "learning_rate": 8.646949564223462e-06,
      "loss": 0.6398,
      "step": 39730
    },
    {
      "epoch": 8.516930990141448,
      "grad_norm": 0.11869587749242783,
      "learning_rate": 8.644092013144736e-06,
      "loss": 0.0584,
      "step": 39740
    },
    {
      "epoch": 8.519074153450493,
      "grad_norm": 21.50315284729004,
      "learning_rate": 8.641234462066011e-06,
      "loss": 0.6033,
      "step": 39750
    },
    {
      "epoch": 8.521217316759538,
      "grad_norm": 17.20130157470703,
      "learning_rate": 8.638376910987285e-06,
      "loss": 0.7824,
      "step": 39760
    },
    {
      "epoch": 8.52336048006858,
      "grad_norm": 15.804489135742188,
      "learning_rate": 8.635519359908559e-06,
      "loss": 0.5978,
      "step": 39770
    },
    {
      "epoch": 8.525503643377625,
      "grad_norm": 0.331064909696579,
      "learning_rate": 8.632661808829833e-06,
      "loss": 0.136,
      "step": 39780
    },
    {
      "epoch": 8.52764680668667,
      "grad_norm": 0.31775346398353577,
      "learning_rate": 8.629804257751107e-06,
      "loss": 0.1981,
      "step": 39790
    },
    {
      "epoch": 8.529789969995713,
      "grad_norm": 0.9822714924812317,
      "learning_rate": 8.626946706672383e-06,
      "loss": 0.2405,
      "step": 39800
    },
    {
      "epoch": 8.531933133304758,
      "grad_norm": 32.672760009765625,
      "learning_rate": 8.624089155593656e-06,
      "loss": 0.6039,
      "step": 39810
    },
    {
      "epoch": 8.534076296613803,
      "grad_norm": 0.03337041661143303,
      "learning_rate": 8.621231604514932e-06,
      "loss": 0.2023,
      "step": 39820
    },
    {
      "epoch": 8.536219459922846,
      "grad_norm": 27.62262725830078,
      "learning_rate": 8.618374053436206e-06,
      "loss": 0.3228,
      "step": 39830
    },
    {
      "epoch": 8.53836262323189,
      "grad_norm": 0.019543690606951714,
      "learning_rate": 8.61551650235748e-06,
      "loss": 0.2291,
      "step": 39840
    },
    {
      "epoch": 8.540505786540935,
      "grad_norm": 38.096534729003906,
      "learning_rate": 8.612658951278756e-06,
      "loss": 0.6069,
      "step": 39850
    },
    {
      "epoch": 8.542648949849978,
      "grad_norm": 21.015682220458984,
      "learning_rate": 8.60980140020003e-06,
      "loss": 0.1809,
      "step": 39860
    },
    {
      "epoch": 8.544792113159023,
      "grad_norm": 0.2912042737007141,
      "learning_rate": 8.606943849121303e-06,
      "loss": 0.0194,
      "step": 39870
    },
    {
      "epoch": 8.546935276468067,
      "grad_norm": 0.02654777094721794,
      "learning_rate": 8.604086298042577e-06,
      "loss": 0.0032,
      "step": 39880
    },
    {
      "epoch": 8.54907843977711,
      "grad_norm": 0.015383685007691383,
      "learning_rate": 8.601228746963853e-06,
      "loss": 0.3636,
      "step": 39890
    },
    {
      "epoch": 8.551221603086155,
      "grad_norm": 22.913406372070312,
      "learning_rate": 8.598371195885127e-06,
      "loss": 0.4867,
      "step": 39900
    },
    {
      "epoch": 8.5533647663952,
      "grad_norm": 0.07218282669782639,
      "learning_rate": 8.5955136448064e-06,
      "loss": 0.364,
      "step": 39910
    },
    {
      "epoch": 8.555507929704243,
      "grad_norm": 0.02861911803483963,
      "learning_rate": 8.592656093727676e-06,
      "loss": 0.1993,
      "step": 39920
    },
    {
      "epoch": 8.557651093013288,
      "grad_norm": 0.018661584705114365,
      "learning_rate": 8.58979854264895e-06,
      "loss": 0.1628,
      "step": 39930
    },
    {
      "epoch": 8.559794256322332,
      "grad_norm": 42.30048751831055,
      "learning_rate": 8.586940991570224e-06,
      "loss": 0.1198,
      "step": 39940
    },
    {
      "epoch": 8.561937419631375,
      "grad_norm": 0.09687718749046326,
      "learning_rate": 8.5840834404915e-06,
      "loss": 0.2192,
      "step": 39950
    },
    {
      "epoch": 8.56408058294042,
      "grad_norm": 0.902744472026825,
      "learning_rate": 8.581225889412774e-06,
      "loss": 0.2294,
      "step": 39960
    },
    {
      "epoch": 8.566223746249465,
      "grad_norm": 13.821211814880371,
      "learning_rate": 8.57836833833405e-06,
      "loss": 0.3677,
      "step": 39970
    },
    {
      "epoch": 8.568366909558508,
      "grad_norm": 0.11578591912984848,
      "learning_rate": 8.575510787255323e-06,
      "loss": 0.1228,
      "step": 39980
    },
    {
      "epoch": 8.570510072867553,
      "grad_norm": 0.014788069762289524,
      "learning_rate": 8.572653236176597e-06,
      "loss": 0.5225,
      "step": 39990
    },
    {
      "epoch": 8.572653236176597,
      "grad_norm": 0.09626694768667221,
      "learning_rate": 8.569795685097871e-06,
      "loss": 0.7023,
      "step": 40000
    },
    {
      "epoch": 8.57479639948564,
      "grad_norm": 0.13653779029846191,
      "learning_rate": 8.566938134019145e-06,
      "loss": 0.4331,
      "step": 40010
    },
    {
      "epoch": 8.576939562794685,
      "grad_norm": 0.4130242168903351,
      "learning_rate": 8.56408058294042e-06,
      "loss": 0.3433,
      "step": 40020
    },
    {
      "epoch": 8.57908272610373,
      "grad_norm": 0.38570597767829895,
      "learning_rate": 8.561223031861695e-06,
      "loss": 0.0078,
      "step": 40030
    },
    {
      "epoch": 8.581225889412773,
      "grad_norm": 0.10234250128269196,
      "learning_rate": 8.55836548078297e-06,
      "loss": 0.1394,
      "step": 40040
    },
    {
      "epoch": 8.583369052721817,
      "grad_norm": 0.007264575455337763,
      "learning_rate": 8.555507929704244e-06,
      "loss": 0.4393,
      "step": 40050
    },
    {
      "epoch": 8.585512216030862,
      "grad_norm": 0.007843109779059887,
      "learning_rate": 8.552650378625518e-06,
      "loss": 0.0019,
      "step": 40060
    },
    {
      "epoch": 8.587655379339905,
      "grad_norm": 0.017589349299669266,
      "learning_rate": 8.549792827546794e-06,
      "loss": 0.2944,
      "step": 40070
    },
    {
      "epoch": 8.58979854264895,
      "grad_norm": 0.16413933038711548,
      "learning_rate": 8.546935276468068e-06,
      "loss": 0.1487,
      "step": 40080
    },
    {
      "epoch": 8.591941705957995,
      "grad_norm": 0.12417595833539963,
      "learning_rate": 8.544077725389342e-06,
      "loss": 0.3416,
      "step": 40090
    },
    {
      "epoch": 8.594084869267038,
      "grad_norm": 54.262027740478516,
      "learning_rate": 8.541220174310616e-06,
      "loss": 0.376,
      "step": 40100
    },
    {
      "epoch": 8.596228032576082,
      "grad_norm": 0.1907828003168106,
      "learning_rate": 8.538362623231891e-06,
      "loss": 0.0116,
      "step": 40110
    },
    {
      "epoch": 8.598371195885127,
      "grad_norm": 0.08305463194847107,
      "learning_rate": 8.535505072153165e-06,
      "loss": 0.002,
      "step": 40120
    },
    {
      "epoch": 8.60051435919417,
      "grad_norm": 0.010043548420071602,
      "learning_rate": 8.532647521074439e-06,
      "loss": 0.1861,
      "step": 40130
    },
    {
      "epoch": 8.602657522503215,
      "grad_norm": 23.136314392089844,
      "learning_rate": 8.529789969995715e-06,
      "loss": 1.0844,
      "step": 40140
    },
    {
      "epoch": 8.60480068581226,
      "grad_norm": 0.030356867238879204,
      "learning_rate": 8.526932418916989e-06,
      "loss": 0.1349,
      "step": 40150
    },
    {
      "epoch": 8.606943849121302,
      "grad_norm": 0.04419887810945511,
      "learning_rate": 8.524074867838263e-06,
      "loss": 0.0996,
      "step": 40160
    },
    {
      "epoch": 8.609087012430347,
      "grad_norm": 0.023287614807486534,
      "learning_rate": 8.521217316759538e-06,
      "loss": 0.3954,
      "step": 40170
    },
    {
      "epoch": 8.611230175739392,
      "grad_norm": 0.033586349338293076,
      "learning_rate": 8.518359765680812e-06,
      "loss": 0.2276,
      "step": 40180
    },
    {
      "epoch": 8.613373339048435,
      "grad_norm": 0.018904484808444977,
      "learning_rate": 8.515502214602088e-06,
      "loss": 0.3167,
      "step": 40190
    },
    {
      "epoch": 8.61551650235748,
      "grad_norm": 703.2086181640625,
      "learning_rate": 8.512644663523362e-06,
      "loss": 0.3506,
      "step": 40200
    },
    {
      "epoch": 8.617659665666524,
      "grad_norm": 28.42487144470215,
      "learning_rate": 8.509787112444636e-06,
      "loss": 0.6383,
      "step": 40210
    },
    {
      "epoch": 8.619802828975567,
      "grad_norm": 20.202011108398438,
      "learning_rate": 8.50692956136591e-06,
      "loss": 0.2806,
      "step": 40220
    },
    {
      "epoch": 8.621945992284612,
      "grad_norm": 0.12196455150842667,
      "learning_rate": 8.504072010287183e-06,
      "loss": 0.3813,
      "step": 40230
    },
    {
      "epoch": 8.624089155593657,
      "grad_norm": 27.198169708251953,
      "learning_rate": 8.501214459208459e-06,
      "loss": 0.2486,
      "step": 40240
    },
    {
      "epoch": 8.6262323189027,
      "grad_norm": 0.05236569792032242,
      "learning_rate": 8.498356908129733e-06,
      "loss": 0.272,
      "step": 40250
    },
    {
      "epoch": 8.628375482211744,
      "grad_norm": 0.0938735380768776,
      "learning_rate": 8.495499357051009e-06,
      "loss": 0.3988,
      "step": 40260
    },
    {
      "epoch": 8.63051864552079,
      "grad_norm": 0.4728054404258728,
      "learning_rate": 8.492641805972283e-06,
      "loss": 0.0034,
      "step": 40270
    },
    {
      "epoch": 8.632661808829832,
      "grad_norm": 0.01925349235534668,
      "learning_rate": 8.489784254893557e-06,
      "loss": 0.393,
      "step": 40280
    },
    {
      "epoch": 8.634804972138877,
      "grad_norm": 0.10058274120092392,
      "learning_rate": 8.486926703814832e-06,
      "loss": 0.4127,
      "step": 40290
    },
    {
      "epoch": 8.636948135447922,
      "grad_norm": 0.047966040670871735,
      "learning_rate": 8.484069152736106e-06,
      "loss": 0.195,
      "step": 40300
    },
    {
      "epoch": 8.639091298756965,
      "grad_norm": 2.265253782272339,
      "learning_rate": 8.48121160165738e-06,
      "loss": 0.1537,
      "step": 40310
    },
    {
      "epoch": 8.64123446206601,
      "grad_norm": 40.98175811767578,
      "learning_rate": 8.478354050578654e-06,
      "loss": 0.5984,
      "step": 40320
    },
    {
      "epoch": 8.643377625375054,
      "grad_norm": 49.125858306884766,
      "learning_rate": 8.47549649949993e-06,
      "loss": 0.5645,
      "step": 40330
    },
    {
      "epoch": 8.645520788684097,
      "grad_norm": 17.67384910583496,
      "learning_rate": 8.472638948421203e-06,
      "loss": 0.7818,
      "step": 40340
    },
    {
      "epoch": 8.647663951993142,
      "grad_norm": 0.38930976390838623,
      "learning_rate": 8.469781397342477e-06,
      "loss": 0.3002,
      "step": 40350
    },
    {
      "epoch": 8.649807115302186,
      "grad_norm": 0.2144957035779953,
      "learning_rate": 8.466923846263753e-06,
      "loss": 0.4553,
      "step": 40360
    },
    {
      "epoch": 8.65195027861123,
      "grad_norm": 29.00233268737793,
      "learning_rate": 8.464066295185027e-06,
      "loss": 0.538,
      "step": 40370
    },
    {
      "epoch": 8.654093441920274,
      "grad_norm": 32.10455322265625,
      "learning_rate": 8.461208744106303e-06,
      "loss": 0.562,
      "step": 40380
    },
    {
      "epoch": 8.656236605229319,
      "grad_norm": 18.518234252929688,
      "learning_rate": 8.458351193027577e-06,
      "loss": 0.1026,
      "step": 40390
    },
    {
      "epoch": 8.658379768538362,
      "grad_norm": 0.13564717769622803,
      "learning_rate": 8.45549364194885e-06,
      "loss": 0.4037,
      "step": 40400
    },
    {
      "epoch": 8.660522931847407,
      "grad_norm": 18.174840927124023,
      "learning_rate": 8.452636090870126e-06,
      "loss": 0.8095,
      "step": 40410
    },
    {
      "epoch": 8.662666095156451,
      "grad_norm": 0.09925446659326553,
      "learning_rate": 8.449778539791398e-06,
      "loss": 0.1525,
      "step": 40420
    },
    {
      "epoch": 8.664809258465494,
      "grad_norm": 0.22911594808101654,
      "learning_rate": 8.446920988712674e-06,
      "loss": 0.1672,
      "step": 40430
    },
    {
      "epoch": 8.666952421774539,
      "grad_norm": 0.10798268765211105,
      "learning_rate": 8.444063437633948e-06,
      "loss": 0.0044,
      "step": 40440
    },
    {
      "epoch": 8.669095585083584,
      "grad_norm": 0.009319995529949665,
      "learning_rate": 8.441205886555223e-06,
      "loss": 0.13,
      "step": 40450
    },
    {
      "epoch": 8.671238748392627,
      "grad_norm": 0.02674884721636772,
      "learning_rate": 8.438348335476497e-06,
      "loss": 0.5424,
      "step": 40460
    },
    {
      "epoch": 8.673381911701671,
      "grad_norm": 0.12511077523231506,
      "learning_rate": 8.435490784397771e-06,
      "loss": 0.1897,
      "step": 40470
    },
    {
      "epoch": 8.675525075010716,
      "grad_norm": 0.022578248754143715,
      "learning_rate": 8.432633233319047e-06,
      "loss": 0.0013,
      "step": 40480
    },
    {
      "epoch": 8.67766823831976,
      "grad_norm": 0.050143465399742126,
      "learning_rate": 8.429775682240321e-06,
      "loss": 0.3455,
      "step": 40490
    },
    {
      "epoch": 8.679811401628804,
      "grad_norm": 0.012122150510549545,
      "learning_rate": 8.426918131161595e-06,
      "loss": 0.0007,
      "step": 40500
    },
    {
      "epoch": 8.681954564937849,
      "grad_norm": 0.039140745997428894,
      "learning_rate": 8.42406058008287e-06,
      "loss": 0.5692,
      "step": 40510
    },
    {
      "epoch": 8.684097728246892,
      "grad_norm": 41.09217834472656,
      "learning_rate": 8.421203029004144e-06,
      "loss": 0.2469,
      "step": 40520
    },
    {
      "epoch": 8.686240891555936,
      "grad_norm": 7.514996528625488,
      "learning_rate": 8.418345477925418e-06,
      "loss": 0.2694,
      "step": 40530
    },
    {
      "epoch": 8.688384054864981,
      "grad_norm": 0.17594929039478302,
      "learning_rate": 8.415487926846692e-06,
      "loss": 0.2262,
      "step": 40540
    },
    {
      "epoch": 8.690527218174024,
      "grad_norm": 0.24163499474525452,
      "learning_rate": 8.412630375767968e-06,
      "loss": 0.3162,
      "step": 40550
    },
    {
      "epoch": 8.692670381483069,
      "grad_norm": 0.13535922765731812,
      "learning_rate": 8.409772824689242e-06,
      "loss": 0.4729,
      "step": 40560
    },
    {
      "epoch": 8.694813544792114,
      "grad_norm": 0.056443579494953156,
      "learning_rate": 8.406915273610516e-06,
      "loss": 0.0028,
      "step": 40570
    },
    {
      "epoch": 8.696956708101157,
      "grad_norm": 0.00824487954378128,
      "learning_rate": 8.404057722531791e-06,
      "loss": 0.1016,
      "step": 40580
    },
    {
      "epoch": 8.699099871410201,
      "grad_norm": 0.10306967794895172,
      "learning_rate": 8.401200171453065e-06,
      "loss": 0.3987,
      "step": 40590
    },
    {
      "epoch": 8.701243034719246,
      "grad_norm": 1.7305139303207397,
      "learning_rate": 8.398342620374341e-06,
      "loss": 0.0397,
      "step": 40600
    },
    {
      "epoch": 8.703386198028289,
      "grad_norm": 0.1243576854467392,
      "learning_rate": 8.395485069295615e-06,
      "loss": 0.1752,
      "step": 40610
    },
    {
      "epoch": 8.705529361337334,
      "grad_norm": 21.63780975341797,
      "learning_rate": 8.392627518216889e-06,
      "loss": 0.3654,
      "step": 40620
    },
    {
      "epoch": 8.707672524646378,
      "grad_norm": 0.3721025288105011,
      "learning_rate": 8.389769967138164e-06,
      "loss": 0.2565,
      "step": 40630
    },
    {
      "epoch": 8.709815687955421,
      "grad_norm": 1.906679630279541,
      "learning_rate": 8.386912416059437e-06,
      "loss": 0.1604,
      "step": 40640
    },
    {
      "epoch": 8.711958851264466,
      "grad_norm": 0.5361832976341248,
      "learning_rate": 8.384054864980712e-06,
      "loss": 0.1375,
      "step": 40650
    },
    {
      "epoch": 8.71410201457351,
      "grad_norm": 0.006336581893265247,
      "learning_rate": 8.381197313901986e-06,
      "loss": 0.4393,
      "step": 40660
    },
    {
      "epoch": 8.716245177882554,
      "grad_norm": 0.05626823008060455,
      "learning_rate": 8.378339762823262e-06,
      "loss": 0.1698,
      "step": 40670
    },
    {
      "epoch": 8.718388341191599,
      "grad_norm": 0.029206177219748497,
      "learning_rate": 8.375482211744536e-06,
      "loss": 0.2396,
      "step": 40680
    },
    {
      "epoch": 8.720531504500643,
      "grad_norm": 1.2489975690841675,
      "learning_rate": 8.37262466066581e-06,
      "loss": 0.0839,
      "step": 40690
    },
    {
      "epoch": 8.722674667809688,
      "grad_norm": 0.0032608488108962774,
      "learning_rate": 8.369767109587085e-06,
      "loss": 0.2584,
      "step": 40700
    },
    {
      "epoch": 8.724817831118731,
      "grad_norm": 0.07331976294517517,
      "learning_rate": 8.366909558508359e-06,
      "loss": 0.3292,
      "step": 40710
    },
    {
      "epoch": 8.726960994427776,
      "grad_norm": 0.01696767285466194,
      "learning_rate": 8.364052007429633e-06,
      "loss": 0.0726,
      "step": 40720
    },
    {
      "epoch": 8.72910415773682,
      "grad_norm": 30.409286499023438,
      "learning_rate": 8.361194456350909e-06,
      "loss": 0.7582,
      "step": 40730
    },
    {
      "epoch": 8.731247321045863,
      "grad_norm": 7.91563081741333,
      "learning_rate": 8.358336905272183e-06,
      "loss": 0.3271,
      "step": 40740
    },
    {
      "epoch": 8.733390484354908,
      "grad_norm": 0.015028773806989193,
      "learning_rate": 8.355479354193457e-06,
      "loss": 0.62,
      "step": 40750
    },
    {
      "epoch": 8.735533647663953,
      "grad_norm": 5.324295997619629,
      "learning_rate": 8.35262180311473e-06,
      "loss": 0.2322,
      "step": 40760
    },
    {
      "epoch": 8.737676810972996,
      "grad_norm": 0.04587697610259056,
      "learning_rate": 8.349764252036006e-06,
      "loss": 0.0103,
      "step": 40770
    },
    {
      "epoch": 8.73981997428204,
      "grad_norm": 0.023518361151218414,
      "learning_rate": 8.34690670095728e-06,
      "loss": 0.5804,
      "step": 40780
    },
    {
      "epoch": 8.741963137591085,
      "grad_norm": 0.10897752642631531,
      "learning_rate": 8.344049149878554e-06,
      "loss": 0.3164,
      "step": 40790
    },
    {
      "epoch": 8.744106300900128,
      "grad_norm": 0.2407878041267395,
      "learning_rate": 8.34119159879983e-06,
      "loss": 0.4945,
      "step": 40800
    },
    {
      "epoch": 8.746249464209173,
      "grad_norm": 0.30710485577583313,
      "learning_rate": 8.338334047721104e-06,
      "loss": 0.0702,
      "step": 40810
    },
    {
      "epoch": 8.748392627518218,
      "grad_norm": 1.0778815746307373,
      "learning_rate": 8.335476496642379e-06,
      "loss": 0.1264,
      "step": 40820
    },
    {
      "epoch": 8.75053579082726,
      "grad_norm": 0.052261047065258026,
      "learning_rate": 8.332618945563653e-06,
      "loss": 0.1326,
      "step": 40830
    },
    {
      "epoch": 8.752678954136305,
      "grad_norm": 0.49135127663612366,
      "learning_rate": 8.329761394484927e-06,
      "loss": 0.1484,
      "step": 40840
    },
    {
      "epoch": 8.75482211744535,
      "grad_norm": 0.008410667069256306,
      "learning_rate": 8.326903843406201e-06,
      "loss": 0.2683,
      "step": 40850
    },
    {
      "epoch": 8.756965280754393,
      "grad_norm": 0.055980633944272995,
      "learning_rate": 8.324046292327475e-06,
      "loss": 0.3611,
      "step": 40860
    },
    {
      "epoch": 8.759108444063438,
      "grad_norm": 17.231245040893555,
      "learning_rate": 8.32118874124875e-06,
      "loss": 0.1122,
      "step": 40870
    },
    {
      "epoch": 8.761251607372483,
      "grad_norm": 39.809226989746094,
      "learning_rate": 8.318331190170024e-06,
      "loss": 0.3472,
      "step": 40880
    },
    {
      "epoch": 8.763394770681526,
      "grad_norm": 0.3188387453556061,
      "learning_rate": 8.3154736390913e-06,
      "loss": 0.0101,
      "step": 40890
    },
    {
      "epoch": 8.76553793399057,
      "grad_norm": 0.11842668056488037,
      "learning_rate": 8.312616088012574e-06,
      "loss": 0.2937,
      "step": 40900
    },
    {
      "epoch": 8.767681097299615,
      "grad_norm": 0.14996519684791565,
      "learning_rate": 8.309758536933848e-06,
      "loss": 0.2317,
      "step": 40910
    },
    {
      "epoch": 8.769824260608658,
      "grad_norm": 0.0859404057264328,
      "learning_rate": 8.306900985855123e-06,
      "loss": 0.1232,
      "step": 40920
    },
    {
      "epoch": 8.771967423917703,
      "grad_norm": 0.08981167525053024,
      "learning_rate": 8.304043434776397e-06,
      "loss": 0.0226,
      "step": 40930
    },
    {
      "epoch": 8.774110587226748,
      "grad_norm": 0.010759871453046799,
      "learning_rate": 8.301185883697671e-06,
      "loss": 0.1236,
      "step": 40940
    },
    {
      "epoch": 8.77625375053579,
      "grad_norm": 0.059101227670907974,
      "learning_rate": 8.298328332618947e-06,
      "loss": 0.3828,
      "step": 40950
    },
    {
      "epoch": 8.778396913844835,
      "grad_norm": 1.194520354270935,
      "learning_rate": 8.295470781540221e-06,
      "loss": 0.1956,
      "step": 40960
    },
    {
      "epoch": 8.78054007715388,
      "grad_norm": 0.007746861781924963,
      "learning_rate": 8.292613230461495e-06,
      "loss": 0.0006,
      "step": 40970
    },
    {
      "epoch": 8.782683240462923,
      "grad_norm": 0.01089682150632143,
      "learning_rate": 8.289755679382769e-06,
      "loss": 0.1894,
      "step": 40980
    },
    {
      "epoch": 8.784826403771968,
      "grad_norm": 0.005460540298372507,
      "learning_rate": 8.286898128304044e-06,
      "loss": 0.0466,
      "step": 40990
    },
    {
      "epoch": 8.786969567081012,
      "grad_norm": 0.008974514901638031,
      "learning_rate": 8.284040577225318e-06,
      "loss": 0.2373,
      "step": 41000
    },
    {
      "epoch": 8.789112730390055,
      "grad_norm": 88.43278503417969,
      "learning_rate": 8.281183026146592e-06,
      "loss": 0.2965,
      "step": 41010
    },
    {
      "epoch": 8.7912558936991,
      "grad_norm": 0.012782921083271503,
      "learning_rate": 8.278325475067868e-06,
      "loss": 0.1014,
      "step": 41020
    },
    {
      "epoch": 8.793399057008145,
      "grad_norm": 0.009061631746590137,
      "learning_rate": 8.275467923989142e-06,
      "loss": 0.3293,
      "step": 41030
    },
    {
      "epoch": 8.795542220317188,
      "grad_norm": 0.47235873341560364,
      "learning_rate": 8.272610372910417e-06,
      "loss": 0.1507,
      "step": 41040
    },
    {
      "epoch": 8.797685383626233,
      "grad_norm": 1.892969012260437,
      "learning_rate": 8.269752821831691e-06,
      "loss": 0.7213,
      "step": 41050
    },
    {
      "epoch": 8.799828546935277,
      "grad_norm": 0.2799134850502014,
      "learning_rate": 8.266895270752965e-06,
      "loss": 0.5504,
      "step": 41060
    },
    {
      "epoch": 8.80197171024432,
      "grad_norm": 0.20523793995380402,
      "learning_rate": 8.26403771967424e-06,
      "loss": 0.1449,
      "step": 41070
    },
    {
      "epoch": 8.804114873553365,
      "grad_norm": 25.76927947998047,
      "learning_rate": 8.261180168595513e-06,
      "loss": 0.2468,
      "step": 41080
    },
    {
      "epoch": 8.80625803686241,
      "grad_norm": 0.08058832585811615,
      "learning_rate": 8.258322617516789e-06,
      "loss": 0.0542,
      "step": 41090
    },
    {
      "epoch": 8.808401200171453,
      "grad_norm": 0.11311914026737213,
      "learning_rate": 8.255465066438063e-06,
      "loss": 0.796,
      "step": 41100
    },
    {
      "epoch": 8.810544363480497,
      "grad_norm": 0.20648443698883057,
      "learning_rate": 8.252607515359338e-06,
      "loss": 0.3014,
      "step": 41110
    },
    {
      "epoch": 8.812687526789542,
      "grad_norm": 0.15628568828105927,
      "learning_rate": 8.249749964280612e-06,
      "loss": 0.965,
      "step": 41120
    },
    {
      "epoch": 8.814830690098585,
      "grad_norm": 0.29966169595718384,
      "learning_rate": 8.246892413201886e-06,
      "loss": 0.1635,
      "step": 41130
    },
    {
      "epoch": 8.81697385340763,
      "grad_norm": 0.055263832211494446,
      "learning_rate": 8.244034862123162e-06,
      "loss": 0.2047,
      "step": 41140
    },
    {
      "epoch": 8.819117016716675,
      "grad_norm": 0.47952038049697876,
      "learning_rate": 8.241177311044436e-06,
      "loss": 0.0383,
      "step": 41150
    },
    {
      "epoch": 8.821260180025718,
      "grad_norm": 26.030860900878906,
      "learning_rate": 8.238319759965711e-06,
      "loss": 0.1901,
      "step": 41160
    },
    {
      "epoch": 8.823403343334762,
      "grad_norm": 0.21777139604091644,
      "learning_rate": 8.235462208886984e-06,
      "loss": 0.0482,
      "step": 41170
    },
    {
      "epoch": 8.825546506643807,
      "grad_norm": 0.05106349661946297,
      "learning_rate": 8.23260465780826e-06,
      "loss": 0.098,
      "step": 41180
    },
    {
      "epoch": 8.82768966995285,
      "grad_norm": 0.020455079153180122,
      "learning_rate": 8.229747106729533e-06,
      "loss": 0.0505,
      "step": 41190
    },
    {
      "epoch": 8.829832833261895,
      "grad_norm": 0.06013278290629387,
      "learning_rate": 8.226889555650807e-06,
      "loss": 0.2197,
      "step": 41200
    },
    {
      "epoch": 8.83197599657094,
      "grad_norm": 0.050956498831510544,
      "learning_rate": 8.224032004572083e-06,
      "loss": 0.3455,
      "step": 41210
    },
    {
      "epoch": 8.834119159879982,
      "grad_norm": 0.06705440580844879,
      "learning_rate": 8.221174453493357e-06,
      "loss": 0.1095,
      "step": 41220
    },
    {
      "epoch": 8.836262323189027,
      "grad_norm": 54.71967697143555,
      "learning_rate": 8.218316902414632e-06,
      "loss": 0.7785,
      "step": 41230
    },
    {
      "epoch": 8.838405486498072,
      "grad_norm": 0.06525268405675888,
      "learning_rate": 8.215459351335906e-06,
      "loss": 0.2908,
      "step": 41240
    },
    {
      "epoch": 8.840548649807115,
      "grad_norm": 18.30530548095703,
      "learning_rate": 8.21260180025718e-06,
      "loss": 0.6798,
      "step": 41250
    },
    {
      "epoch": 8.84269181311616,
      "grad_norm": 0.10898032784461975,
      "learning_rate": 8.209744249178456e-06,
      "loss": 0.3154,
      "step": 41260
    },
    {
      "epoch": 8.844834976425204,
      "grad_norm": 0.2019375115633011,
      "learning_rate": 8.20688669809973e-06,
      "loss": 0.8454,
      "step": 41270
    },
    {
      "epoch": 8.846978139734247,
      "grad_norm": 0.07823731750249863,
      "learning_rate": 8.204029147021004e-06,
      "loss": 0.2144,
      "step": 41280
    },
    {
      "epoch": 8.849121303043292,
      "grad_norm": 38.702022552490234,
      "learning_rate": 8.201171595942277e-06,
      "loss": 0.274,
      "step": 41290
    },
    {
      "epoch": 8.851264466352337,
      "grad_norm": 0.5468428730964661,
      "learning_rate": 8.198314044863553e-06,
      "loss": 0.4515,
      "step": 41300
    },
    {
      "epoch": 8.85340762966138,
      "grad_norm": 0.19282229244709015,
      "learning_rate": 8.195456493784827e-06,
      "loss": 0.0361,
      "step": 41310
    },
    {
      "epoch": 8.855550792970424,
      "grad_norm": 0.20014426112174988,
      "learning_rate": 8.192598942706101e-06,
      "loss": 0.2058,
      "step": 41320
    },
    {
      "epoch": 8.85769395627947,
      "grad_norm": 135.38430786132812,
      "learning_rate": 8.189741391627377e-06,
      "loss": 0.5373,
      "step": 41330
    },
    {
      "epoch": 8.859837119588512,
      "grad_norm": 0.042150117456912994,
      "learning_rate": 8.18688384054865e-06,
      "loss": 0.3435,
      "step": 41340
    },
    {
      "epoch": 8.861980282897557,
      "grad_norm": 0.03274379298090935,
      "learning_rate": 8.184026289469924e-06,
      "loss": 0.0016,
      "step": 41350
    },
    {
      "epoch": 8.864123446206602,
      "grad_norm": 0.09479028731584549,
      "learning_rate": 8.1811687383912e-06,
      "loss": 0.2102,
      "step": 41360
    },
    {
      "epoch": 8.866266609515645,
      "grad_norm": 0.24852177500724792,
      "learning_rate": 8.178311187312474e-06,
      "loss": 0.0015,
      "step": 41370
    },
    {
      "epoch": 8.86840977282469,
      "grad_norm": 0.08099158853292465,
      "learning_rate": 8.17545363623375e-06,
      "loss": 0.7199,
      "step": 41380
    },
    {
      "epoch": 8.870552936133734,
      "grad_norm": 0.02134418860077858,
      "learning_rate": 8.172596085155022e-06,
      "loss": 0.2705,
      "step": 41390
    },
    {
      "epoch": 8.872696099442777,
      "grad_norm": 0.0370614193379879,
      "learning_rate": 8.169738534076297e-06,
      "loss": 0.0035,
      "step": 41400
    },
    {
      "epoch": 8.874839262751822,
      "grad_norm": 0.07731101661920547,
      "learning_rate": 8.166880982997571e-06,
      "loss": 0.3606,
      "step": 41410
    },
    {
      "epoch": 8.876982426060867,
      "grad_norm": 22.018781661987305,
      "learning_rate": 8.164023431918845e-06,
      "loss": 0.5509,
      "step": 41420
    },
    {
      "epoch": 8.87912558936991,
      "grad_norm": 0.11311212927103043,
      "learning_rate": 8.161165880840121e-06,
      "loss": 0.0015,
      "step": 41430
    },
    {
      "epoch": 8.881268752678954,
      "grad_norm": 0.08703859895467758,
      "learning_rate": 8.158308329761395e-06,
      "loss": 0.7031,
      "step": 41440
    },
    {
      "epoch": 8.883411915987999,
      "grad_norm": 1.0575758218765259,
      "learning_rate": 8.15545077868267e-06,
      "loss": 0.1819,
      "step": 41450
    },
    {
      "epoch": 8.885555079297042,
      "grad_norm": 0.07740556448698044,
      "learning_rate": 8.152593227603944e-06,
      "loss": 0.0574,
      "step": 41460
    },
    {
      "epoch": 8.887698242606087,
      "grad_norm": 0.1162346601486206,
      "learning_rate": 8.149735676525218e-06,
      "loss": 0.3532,
      "step": 41470
    },
    {
      "epoch": 8.889841405915131,
      "grad_norm": 0.018865253776311874,
      "learning_rate": 8.146878125446494e-06,
      "loss": 0.1832,
      "step": 41480
    },
    {
      "epoch": 8.891984569224174,
      "grad_norm": 0.02274537831544876,
      "learning_rate": 8.144020574367766e-06,
      "loss": 0.2722,
      "step": 41490
    },
    {
      "epoch": 8.894127732533219,
      "grad_norm": 27.042497634887695,
      "learning_rate": 8.141163023289042e-06,
      "loss": 0.3482,
      "step": 41500
    },
    {
      "epoch": 8.896270895842264,
      "grad_norm": 59.34853744506836,
      "learning_rate": 8.138305472210316e-06,
      "loss": 0.4356,
      "step": 41510
    },
    {
      "epoch": 8.898414059151307,
      "grad_norm": 48.13654708862305,
      "learning_rate": 8.135447921131591e-06,
      "loss": 0.232,
      "step": 41520
    },
    {
      "epoch": 8.900557222460352,
      "grad_norm": 0.2786320745944977,
      "learning_rate": 8.132590370052865e-06,
      "loss": 0.351,
      "step": 41530
    },
    {
      "epoch": 8.902700385769396,
      "grad_norm": 2.0528464317321777,
      "learning_rate": 8.12973281897414e-06,
      "loss": 0.1713,
      "step": 41540
    },
    {
      "epoch": 8.90484354907844,
      "grad_norm": 0.981072187423706,
      "learning_rate": 8.126875267895415e-06,
      "loss": 0.0836,
      "step": 41550
    },
    {
      "epoch": 8.906986712387484,
      "grad_norm": 0.003944098483771086,
      "learning_rate": 8.124017716816689e-06,
      "loss": 0.3974,
      "step": 41560
    },
    {
      "epoch": 8.909129875696529,
      "grad_norm": 0.052463728934526443,
      "learning_rate": 8.121160165737963e-06,
      "loss": 0.462,
      "step": 41570
    },
    {
      "epoch": 8.911273039005572,
      "grad_norm": 45.952430725097656,
      "learning_rate": 8.118302614659238e-06,
      "loss": 0.4799,
      "step": 41580
    },
    {
      "epoch": 8.913416202314616,
      "grad_norm": 23.20116424560547,
      "learning_rate": 8.115445063580512e-06,
      "loss": 0.7963,
      "step": 41590
    },
    {
      "epoch": 8.915559365623661,
      "grad_norm": 0.27977707982063293,
      "learning_rate": 8.112587512501786e-06,
      "loss": 0.1916,
      "step": 41600
    },
    {
      "epoch": 8.917702528932704,
      "grad_norm": 0.1997196078300476,
      "learning_rate": 8.10972996142306e-06,
      "loss": 0.058,
      "step": 41610
    },
    {
      "epoch": 8.919845692241749,
      "grad_norm": 0.01686088927090168,
      "learning_rate": 8.106872410344336e-06,
      "loss": 0.1568,
      "step": 41620
    },
    {
      "epoch": 8.921988855550794,
      "grad_norm": 51.76325225830078,
      "learning_rate": 8.10401485926561e-06,
      "loss": 0.4298,
      "step": 41630
    },
    {
      "epoch": 8.924132018859837,
      "grad_norm": 5.492142677307129,
      "learning_rate": 8.101157308186884e-06,
      "loss": 0.2169,
      "step": 41640
    },
    {
      "epoch": 8.926275182168881,
      "grad_norm": 0.00551666971296072,
      "learning_rate": 8.09829975710816e-06,
      "loss": 0.1648,
      "step": 41650
    },
    {
      "epoch": 8.928418345477926,
      "grad_norm": 0.04636535048484802,
      "learning_rate": 8.095442206029433e-06,
      "loss": 0.3138,
      "step": 41660
    },
    {
      "epoch": 8.930561508786969,
      "grad_norm": 0.09310845285654068,
      "learning_rate": 8.092584654950709e-06,
      "loss": 0.1365,
      "step": 41670
    },
    {
      "epoch": 8.932704672096014,
      "grad_norm": 0.0519995279610157,
      "learning_rate": 8.089727103871983e-06,
      "loss": 0.1553,
      "step": 41680
    },
    {
      "epoch": 8.934847835405058,
      "grad_norm": 19.347570419311523,
      "learning_rate": 8.086869552793257e-06,
      "loss": 0.459,
      "step": 41690
    },
    {
      "epoch": 8.936990998714101,
      "grad_norm": 39.34239959716797,
      "learning_rate": 8.084012001714532e-06,
      "loss": 0.4005,
      "step": 41700
    },
    {
      "epoch": 8.939134162023146,
      "grad_norm": 0.3594045341014862,
      "learning_rate": 8.081154450635804e-06,
      "loss": 0.0909,
      "step": 41710
    },
    {
      "epoch": 8.94127732533219,
      "grad_norm": 0.06847720593214035,
      "learning_rate": 8.07829689955708e-06,
      "loss": 0.1533,
      "step": 41720
    },
    {
      "epoch": 8.943420488641234,
      "grad_norm": 0.13639874756336212,
      "learning_rate": 8.075439348478354e-06,
      "loss": 0.0024,
      "step": 41730
    },
    {
      "epoch": 8.945563651950279,
      "grad_norm": 0.08816790580749512,
      "learning_rate": 8.07258179739963e-06,
      "loss": 0.6958,
      "step": 41740
    },
    {
      "epoch": 8.947706815259323,
      "grad_norm": 28.789783477783203,
      "learning_rate": 8.069724246320904e-06,
      "loss": 0.4432,
      "step": 41750
    },
    {
      "epoch": 8.949849978568366,
      "grad_norm": 16.40318489074707,
      "learning_rate": 8.066866695242178e-06,
      "loss": 0.3061,
      "step": 41760
    },
    {
      "epoch": 8.951993141877411,
      "grad_norm": 0.07799800485372543,
      "learning_rate": 8.064009144163453e-06,
      "loss": 0.0211,
      "step": 41770
    },
    {
      "epoch": 8.954136305186456,
      "grad_norm": 85.6784896850586,
      "learning_rate": 8.061151593084727e-06,
      "loss": 0.0812,
      "step": 41780
    },
    {
      "epoch": 8.956279468495499,
      "grad_norm": 0.03067641519010067,
      "learning_rate": 8.058294042006003e-06,
      "loss": 0.2845,
      "step": 41790
    },
    {
      "epoch": 8.958422631804543,
      "grad_norm": 1.3759732246398926,
      "learning_rate": 8.055436490927277e-06,
      "loss": 0.2213,
      "step": 41800
    },
    {
      "epoch": 8.960565795113588,
      "grad_norm": 43.58488464355469,
      "learning_rate": 8.05257893984855e-06,
      "loss": 0.1428,
      "step": 41810
    },
    {
      "epoch": 8.962708958422631,
      "grad_norm": 0.010356555692851543,
      "learning_rate": 8.049721388769824e-06,
      "loss": 0.0008,
      "step": 41820
    },
    {
      "epoch": 8.964852121731676,
      "grad_norm": 0.0067286062985658646,
      "learning_rate": 8.046863837691098e-06,
      "loss": 0.3043,
      "step": 41830
    },
    {
      "epoch": 8.96699528504072,
      "grad_norm": 0.013498344458639622,
      "learning_rate": 8.044006286612374e-06,
      "loss": 0.4418,
      "step": 41840
    },
    {
      "epoch": 8.969138448349764,
      "grad_norm": 0.07263857126235962,
      "learning_rate": 8.041148735533648e-06,
      "loss": 0.1928,
      "step": 41850
    },
    {
      "epoch": 8.971281611658808,
      "grad_norm": 0.5479820966720581,
      "learning_rate": 8.038291184454924e-06,
      "loss": 0.0026,
      "step": 41860
    },
    {
      "epoch": 8.973424774967853,
      "grad_norm": 0.0375824049115181,
      "learning_rate": 8.035433633376198e-06,
      "loss": 0.0423,
      "step": 41870
    },
    {
      "epoch": 8.975567938276896,
      "grad_norm": 0.0548296757042408,
      "learning_rate": 8.032576082297471e-06,
      "loss": 0.0918,
      "step": 41880
    },
    {
      "epoch": 8.97771110158594,
      "grad_norm": 31.369949340820312,
      "learning_rate": 8.029718531218747e-06,
      "loss": 0.4541,
      "step": 41890
    },
    {
      "epoch": 8.979854264894986,
      "grad_norm": 0.04280821233987808,
      "learning_rate": 8.026860980140021e-06,
      "loss": 0.001,
      "step": 41900
    },
    {
      "epoch": 8.981997428204028,
      "grad_norm": 0.08248744159936905,
      "learning_rate": 8.024003429061295e-06,
      "loss": 0.4188,
      "step": 41910
    },
    {
      "epoch": 8.984140591513073,
      "grad_norm": 0.006157916039228439,
      "learning_rate": 8.021145877982569e-06,
      "loss": 0.057,
      "step": 41920
    },
    {
      "epoch": 8.986283754822118,
      "grad_norm": 20.222896575927734,
      "learning_rate": 8.018288326903844e-06,
      "loss": 0.6008,
      "step": 41930
    },
    {
      "epoch": 8.988426918131161,
      "grad_norm": 0.048943012952804565,
      "learning_rate": 8.015430775825118e-06,
      "loss": 0.2173,
      "step": 41940
    },
    {
      "epoch": 8.990570081440206,
      "grad_norm": 0.38406792283058167,
      "learning_rate": 8.012573224746392e-06,
      "loss": 0.2028,
      "step": 41950
    },
    {
      "epoch": 8.99271324474925,
      "grad_norm": 0.0723458081483841,
      "learning_rate": 8.009715673667668e-06,
      "loss": 0.3007,
      "step": 41960
    },
    {
      "epoch": 8.994856408058293,
      "grad_norm": 0.13921667635440826,
      "learning_rate": 8.006858122588942e-06,
      "loss": 0.4367,
      "step": 41970
    },
    {
      "epoch": 8.996999571367338,
      "grad_norm": 0.1718788743019104,
      "learning_rate": 8.004000571510216e-06,
      "loss": 0.1233,
      "step": 41980
    },
    {
      "epoch": 8.999142734676383,
      "grad_norm": 0.07655080407857895,
      "learning_rate": 8.001143020431491e-06,
      "loss": 0.4164,
      "step": 41990
    },
    {
      "epoch": 9.0,
      "eval_accuracy": 0.9466666666666667,
      "eval_f1": 0.7720797720797722,
      "eval_loss": 0.25788623094558716,
      "eval_precision": 0.8390092879256966,
      "eval_recall": 0.7150395778364116,
      "eval_runtime": 724.5716,
      "eval_samples_per_second": 4.14,
      "eval_steps_per_second": 1.38,
      "step": 41994
    },
    {
      "epoch": 9.001285897985426,
      "grad_norm": 0.12041644006967545,
      "learning_rate": 7.998285469352765e-06,
      "loss": 0.4375,
      "step": 42000
    },
    {
      "epoch": 9.00342906129447,
      "grad_norm": 27.815441131591797,
      "learning_rate": 7.995427918274041e-06,
      "loss": 0.2804,
      "step": 42010
    },
    {
      "epoch": 9.005572224603515,
      "grad_norm": 24.549165725708008,
      "learning_rate": 7.992570367195315e-06,
      "loss": 0.5627,
      "step": 42020
    },
    {
      "epoch": 9.007715387912558,
      "grad_norm": 0.1319691389799118,
      "learning_rate": 7.989712816116589e-06,
      "loss": 0.0021,
      "step": 42030
    },
    {
      "epoch": 9.009858551221603,
      "grad_norm": 0.034609563648700714,
      "learning_rate": 7.986855265037863e-06,
      "loss": 0.2556,
      "step": 42040
    },
    {
      "epoch": 9.012001714530648,
      "grad_norm": 0.0867999792098999,
      "learning_rate": 7.983997713959137e-06,
      "loss": 0.0733,
      "step": 42050
    },
    {
      "epoch": 9.01414487783969,
      "grad_norm": 0.049839578568935394,
      "learning_rate": 7.981140162880412e-06,
      "loss": 0.0019,
      "step": 42060
    },
    {
      "epoch": 9.016288041148735,
      "grad_norm": 0.03801633417606354,
      "learning_rate": 7.978282611801686e-06,
      "loss": 0.0009,
      "step": 42070
    },
    {
      "epoch": 9.01843120445778,
      "grad_norm": 0.005296322982758284,
      "learning_rate": 7.975425060722962e-06,
      "loss": 0.2665,
      "step": 42080
    },
    {
      "epoch": 9.020574367766823,
      "grad_norm": 0.025535641238093376,
      "learning_rate": 7.972567509644236e-06,
      "loss": 0.3965,
      "step": 42090
    },
    {
      "epoch": 9.022717531075868,
      "grad_norm": 0.0636071041226387,
      "learning_rate": 7.96970995856551e-06,
      "loss": 0.6365,
      "step": 42100
    },
    {
      "epoch": 9.024860694384913,
      "grad_norm": 0.28892335295677185,
      "learning_rate": 7.966852407486785e-06,
      "loss": 0.0176,
      "step": 42110
    },
    {
      "epoch": 9.027003857693956,
      "grad_norm": 0.15501198172569275,
      "learning_rate": 7.96399485640806e-06,
      "loss": 0.3603,
      "step": 42120
    },
    {
      "epoch": 9.029147021003,
      "grad_norm": 0.11874839663505554,
      "learning_rate": 7.961137305329333e-06,
      "loss": 0.4389,
      "step": 42130
    },
    {
      "epoch": 9.031290184312045,
      "grad_norm": 66.78632354736328,
      "learning_rate": 7.958279754250607e-06,
      "loss": 0.5035,
      "step": 42140
    },
    {
      "epoch": 9.033433347621088,
      "grad_norm": 0.021458987146615982,
      "learning_rate": 7.955422203171883e-06,
      "loss": 0.0073,
      "step": 42150
    },
    {
      "epoch": 9.035576510930133,
      "grad_norm": 0.3438866138458252,
      "learning_rate": 7.952564652093157e-06,
      "loss": 0.2507,
      "step": 42160
    },
    {
      "epoch": 9.037719674239177,
      "grad_norm": 0.08245792239904404,
      "learning_rate": 7.94970710101443e-06,
      "loss": 0.246,
      "step": 42170
    },
    {
      "epoch": 9.03986283754822,
      "grad_norm": 18.331951141357422,
      "learning_rate": 7.946849549935706e-06,
      "loss": 0.242,
      "step": 42180
    },
    {
      "epoch": 9.042006000857265,
      "grad_norm": 0.47997167706489563,
      "learning_rate": 7.94399199885698e-06,
      "loss": 0.3009,
      "step": 42190
    },
    {
      "epoch": 9.04414916416631,
      "grad_norm": 0.11925119906663895,
      "learning_rate": 7.941134447778254e-06,
      "loss": 0.0678,
      "step": 42200
    },
    {
      "epoch": 9.046292327475353,
      "grad_norm": 0.006239802576601505,
      "learning_rate": 7.93827689669953e-06,
      "loss": 0.1626,
      "step": 42210
    },
    {
      "epoch": 9.048435490784398,
      "grad_norm": 0.017594514414668083,
      "learning_rate": 7.935419345620804e-06,
      "loss": 0.2198,
      "step": 42220
    },
    {
      "epoch": 9.050578654093442,
      "grad_norm": 24.487258911132812,
      "learning_rate": 7.93256179454208e-06,
      "loss": 0.1276,
      "step": 42230
    },
    {
      "epoch": 9.052721817402485,
      "grad_norm": 0.03112511709332466,
      "learning_rate": 7.929704243463353e-06,
      "loss": 0.002,
      "step": 42240
    },
    {
      "epoch": 9.05486498071153,
      "grad_norm": 0.006045312620699406,
      "learning_rate": 7.926846692384627e-06,
      "loss": 0.0708,
      "step": 42250
    },
    {
      "epoch": 9.057008144020575,
      "grad_norm": 0.15397490561008453,
      "learning_rate": 7.923989141305901e-06,
      "loss": 0.4146,
      "step": 42260
    },
    {
      "epoch": 9.059151307329618,
      "grad_norm": 0.00477638840675354,
      "learning_rate": 7.921131590227175e-06,
      "loss": 0.6396,
      "step": 42270
    },
    {
      "epoch": 9.061294470638662,
      "grad_norm": 19.957368850708008,
      "learning_rate": 7.91827403914845e-06,
      "loss": 0.3382,
      "step": 42280
    },
    {
      "epoch": 9.063437633947707,
      "grad_norm": 0.038824357092380524,
      "learning_rate": 7.915416488069725e-06,
      "loss": 0.1409,
      "step": 42290
    },
    {
      "epoch": 9.06558079725675,
      "grad_norm": 0.1506733000278473,
      "learning_rate": 7.912558936991e-06,
      "loss": 0.2976,
      "step": 42300
    },
    {
      "epoch": 9.067723960565795,
      "grad_norm": 18.745269775390625,
      "learning_rate": 7.909701385912274e-06,
      "loss": 0.2753,
      "step": 42310
    },
    {
      "epoch": 9.06986712387484,
      "grad_norm": 0.05139261484146118,
      "learning_rate": 7.906843834833548e-06,
      "loss": 0.002,
      "step": 42320
    },
    {
      "epoch": 9.072010287183883,
      "grad_norm": 0.16855475306510925,
      "learning_rate": 7.903986283754824e-06,
      "loss": 0.0045,
      "step": 42330
    },
    {
      "epoch": 9.074153450492927,
      "grad_norm": 0.04412003606557846,
      "learning_rate": 7.901128732676098e-06,
      "loss": 0.2902,
      "step": 42340
    },
    {
      "epoch": 9.076296613801972,
      "grad_norm": 0.11697997897863388,
      "learning_rate": 7.898271181597371e-06,
      "loss": 0.0026,
      "step": 42350
    },
    {
      "epoch": 9.078439777111015,
      "grad_norm": 0.06994567811489105,
      "learning_rate": 7.895413630518645e-06,
      "loss": 0.2194,
      "step": 42360
    },
    {
      "epoch": 9.08058294042006,
      "grad_norm": 0.04155993089079857,
      "learning_rate": 7.892556079439921e-06,
      "loss": 0.1657,
      "step": 42370
    },
    {
      "epoch": 9.082726103729104,
      "grad_norm": 0.07636982202529907,
      "learning_rate": 7.889698528361195e-06,
      "loss": 0.0008,
      "step": 42380
    },
    {
      "epoch": 9.084869267038147,
      "grad_norm": 29.51714515686035,
      "learning_rate": 7.886840977282469e-06,
      "loss": 0.7416,
      "step": 42390
    },
    {
      "epoch": 9.087012430347192,
      "grad_norm": 0.04106367751955986,
      "learning_rate": 7.883983426203745e-06,
      "loss": 0.0723,
      "step": 42400
    },
    {
      "epoch": 9.089155593656237,
      "grad_norm": 0.0268825963139534,
      "learning_rate": 7.881125875125018e-06,
      "loss": 0.5205,
      "step": 42410
    },
    {
      "epoch": 9.09129875696528,
      "grad_norm": 0.07855416089296341,
      "learning_rate": 7.878268324046292e-06,
      "loss": 0.341,
      "step": 42420
    },
    {
      "epoch": 9.093441920274325,
      "grad_norm": 0.04871681332588196,
      "learning_rate": 7.875410772967568e-06,
      "loss": 0.5624,
      "step": 42430
    },
    {
      "epoch": 9.09558508358337,
      "grad_norm": 0.06297961622476578,
      "learning_rate": 7.872553221888842e-06,
      "loss": 0.3427,
      "step": 42440
    },
    {
      "epoch": 9.097728246892412,
      "grad_norm": 0.027014916762709618,
      "learning_rate": 7.869695670810118e-06,
      "loss": 0.0033,
      "step": 42450
    },
    {
      "epoch": 9.099871410201457,
      "grad_norm": 0.01459139958024025,
      "learning_rate": 7.86683811973139e-06,
      "loss": 0.204,
      "step": 42460
    },
    {
      "epoch": 9.102014573510502,
      "grad_norm": 28.807098388671875,
      "learning_rate": 7.863980568652665e-06,
      "loss": 0.3937,
      "step": 42470
    },
    {
      "epoch": 9.104157736819545,
      "grad_norm": 0.13979928195476532,
      "learning_rate": 7.86112301757394e-06,
      "loss": 0.189,
      "step": 42480
    },
    {
      "epoch": 9.10630090012859,
      "grad_norm": 66.98359680175781,
      "learning_rate": 7.858265466495213e-06,
      "loss": 0.2858,
      "step": 42490
    },
    {
      "epoch": 9.108444063437634,
      "grad_norm": 0.07063718140125275,
      "learning_rate": 7.855407915416489e-06,
      "loss": 0.0026,
      "step": 42500
    },
    {
      "epoch": 9.110587226746677,
      "grad_norm": 0.07500474154949188,
      "learning_rate": 7.852550364337763e-06,
      "loss": 0.2209,
      "step": 42510
    },
    {
      "epoch": 9.112730390055722,
      "grad_norm": 0.05720749869942665,
      "learning_rate": 7.849692813259038e-06,
      "loss": 0.1408,
      "step": 42520
    },
    {
      "epoch": 9.114873553364767,
      "grad_norm": 0.5601009726524353,
      "learning_rate": 7.846835262180312e-06,
      "loss": 0.0017,
      "step": 42530
    },
    {
      "epoch": 9.117016716673811,
      "grad_norm": 52.502525329589844,
      "learning_rate": 7.843977711101586e-06,
      "loss": 0.2476,
      "step": 42540
    },
    {
      "epoch": 9.119159879982854,
      "grad_norm": 0.024432796984910965,
      "learning_rate": 7.841120160022862e-06,
      "loss": 0.2435,
      "step": 42550
    },
    {
      "epoch": 9.1213030432919,
      "grad_norm": 0.1881939172744751,
      "learning_rate": 7.838262608944136e-06,
      "loss": 0.3461,
      "step": 42560
    },
    {
      "epoch": 9.123446206600944,
      "grad_norm": 0.04607277736067772,
      "learning_rate": 7.83540505786541e-06,
      "loss": 0.5068,
      "step": 42570
    },
    {
      "epoch": 9.125589369909987,
      "grad_norm": 0.15820680558681488,
      "learning_rate": 7.832547506786684e-06,
      "loss": 0.6118,
      "step": 42580
    },
    {
      "epoch": 9.127732533219032,
      "grad_norm": 0.08119749277830124,
      "learning_rate": 7.82968995570796e-06,
      "loss": 0.164,
      "step": 42590
    },
    {
      "epoch": 9.129875696528076,
      "grad_norm": 0.30974432826042175,
      "learning_rate": 7.826832404629233e-06,
      "loss": 0.3901,
      "step": 42600
    },
    {
      "epoch": 9.13201885983712,
      "grad_norm": 0.20048892498016357,
      "learning_rate": 7.823974853550507e-06,
      "loss": 0.7822,
      "step": 42610
    },
    {
      "epoch": 9.134162023146164,
      "grad_norm": 0.24737873673439026,
      "learning_rate": 7.821117302471783e-06,
      "loss": 0.0253,
      "step": 42620
    },
    {
      "epoch": 9.136305186455209,
      "grad_norm": 0.28515714406967163,
      "learning_rate": 7.818259751393057e-06,
      "loss": 0.0025,
      "step": 42630
    },
    {
      "epoch": 9.138448349764252,
      "grad_norm": 0.10402840375900269,
      "learning_rate": 7.815402200314332e-06,
      "loss": 0.0029,
      "step": 42640
    },
    {
      "epoch": 9.140591513073296,
      "grad_norm": 85.18228912353516,
      "learning_rate": 7.812544649235606e-06,
      "loss": 0.2273,
      "step": 42650
    },
    {
      "epoch": 9.142734676382341,
      "grad_norm": 0.01662563718855381,
      "learning_rate": 7.80968709815688e-06,
      "loss": 0.1697,
      "step": 42660
    },
    {
      "epoch": 9.144877839691384,
      "grad_norm": 0.003308498300611973,
      "learning_rate": 7.806829547078156e-06,
      "loss": 0.2893,
      "step": 42670
    },
    {
      "epoch": 9.147021003000429,
      "grad_norm": 0.002448639366775751,
      "learning_rate": 7.803971995999428e-06,
      "loss": 0.1695,
      "step": 42680
    },
    {
      "epoch": 9.149164166309474,
      "grad_norm": 0.0708777979016304,
      "learning_rate": 7.801114444920704e-06,
      "loss": 0.0004,
      "step": 42690
    },
    {
      "epoch": 9.151307329618517,
      "grad_norm": 0.0063099670223891735,
      "learning_rate": 7.798256893841978e-06,
      "loss": 0.1655,
      "step": 42700
    },
    {
      "epoch": 9.153450492927561,
      "grad_norm": 0.008271616883575916,
      "learning_rate": 7.795399342763253e-06,
      "loss": 0.3281,
      "step": 42710
    },
    {
      "epoch": 9.155593656236606,
      "grad_norm": 16.437286376953125,
      "learning_rate": 7.792541791684527e-06,
      "loss": 0.4472,
      "step": 42720
    },
    {
      "epoch": 9.157736819545649,
      "grad_norm": 19.829242706298828,
      "learning_rate": 7.789684240605801e-06,
      "loss": 0.1371,
      "step": 42730
    },
    {
      "epoch": 9.159879982854694,
      "grad_norm": 0.014167740009725094,
      "learning_rate": 7.786826689527077e-06,
      "loss": 0.0012,
      "step": 42740
    },
    {
      "epoch": 9.162023146163738,
      "grad_norm": 0.0248723067343235,
      "learning_rate": 7.78396913844835e-06,
      "loss": 0.1496,
      "step": 42750
    },
    {
      "epoch": 9.164166309472781,
      "grad_norm": 5.1487836837768555,
      "learning_rate": 7.781111587369625e-06,
      "loss": 0.1771,
      "step": 42760
    },
    {
      "epoch": 9.166309472781826,
      "grad_norm": 0.0019579543732106686,
      "learning_rate": 7.7782540362909e-06,
      "loss": 0.1634,
      "step": 42770
    },
    {
      "epoch": 9.168452636090871,
      "grad_norm": 0.5885647535324097,
      "learning_rate": 7.775396485212174e-06,
      "loss": 0.2372,
      "step": 42780
    },
    {
      "epoch": 9.170595799399914,
      "grad_norm": 0.3362009525299072,
      "learning_rate": 7.772538934133448e-06,
      "loss": 0.0929,
      "step": 42790
    },
    {
      "epoch": 9.172738962708959,
      "grad_norm": 0.05337297171354294,
      "learning_rate": 7.769681383054722e-06,
      "loss": 0.3872,
      "step": 42800
    },
    {
      "epoch": 9.174882126018003,
      "grad_norm": 0.0016099526546895504,
      "learning_rate": 7.766823831975998e-06,
      "loss": 0.3397,
      "step": 42810
    },
    {
      "epoch": 9.177025289327046,
      "grad_norm": 16.860443115234375,
      "learning_rate": 7.763966280897272e-06,
      "loss": 0.3921,
      "step": 42820
    },
    {
      "epoch": 9.179168452636091,
      "grad_norm": 0.0887572169303894,
      "learning_rate": 7.761108729818545e-06,
      "loss": 0.2137,
      "step": 42830
    },
    {
      "epoch": 9.181311615945136,
      "grad_norm": 0.0863737091422081,
      "learning_rate": 7.758251178739821e-06,
      "loss": 0.0018,
      "step": 42840
    },
    {
      "epoch": 9.183454779254179,
      "grad_norm": 23.4195613861084,
      "learning_rate": 7.755393627661095e-06,
      "loss": 0.4018,
      "step": 42850
    },
    {
      "epoch": 9.185597942563223,
      "grad_norm": 0.7338249683380127,
      "learning_rate": 7.75253607658237e-06,
      "loss": 0.1298,
      "step": 42860
    },
    {
      "epoch": 9.187741105872268,
      "grad_norm": 0.16460023820400238,
      "learning_rate": 7.749678525503645e-06,
      "loss": 0.2772,
      "step": 42870
    },
    {
      "epoch": 9.189884269181311,
      "grad_norm": 0.09646199643611908,
      "learning_rate": 7.746820974424918e-06,
      "loss": 0.0041,
      "step": 42880
    },
    {
      "epoch": 9.192027432490356,
      "grad_norm": 0.01919708587229252,
      "learning_rate": 7.743963423346192e-06,
      "loss": 0.0894,
      "step": 42890
    },
    {
      "epoch": 9.1941705957994,
      "grad_norm": 0.00231262412853539,
      "learning_rate": 7.741105872267466e-06,
      "loss": 0.0008,
      "step": 42900
    },
    {
      "epoch": 9.196313759108444,
      "grad_norm": 0.1677105873823166,
      "learning_rate": 7.738248321188742e-06,
      "loss": 0.0244,
      "step": 42910
    },
    {
      "epoch": 9.198456922417488,
      "grad_norm": 0.14990293979644775,
      "learning_rate": 7.735390770110016e-06,
      "loss": 0.1221,
      "step": 42920
    },
    {
      "epoch": 9.200600085726533,
      "grad_norm": 0.03807621821761131,
      "learning_rate": 7.732533219031292e-06,
      "loss": 0.2454,
      "step": 42930
    },
    {
      "epoch": 9.202743249035576,
      "grad_norm": 21.903018951416016,
      "learning_rate": 7.729675667952565e-06,
      "loss": 0.5156,
      "step": 42940
    },
    {
      "epoch": 9.20488641234462,
      "grad_norm": 0.028652958571910858,
      "learning_rate": 7.72681811687384e-06,
      "loss": 0.0011,
      "step": 42950
    },
    {
      "epoch": 9.207029575653666,
      "grad_norm": 22.645244598388672,
      "learning_rate": 7.723960565795115e-06,
      "loss": 0.4636,
      "step": 42960
    },
    {
      "epoch": 9.209172738962708,
      "grad_norm": 0.07727005332708359,
      "learning_rate": 7.721103014716389e-06,
      "loss": 0.0011,
      "step": 42970
    },
    {
      "epoch": 9.211315902271753,
      "grad_norm": 0.002482164651155472,
      "learning_rate": 7.718245463637663e-06,
      "loss": 0.2437,
      "step": 42980
    },
    {
      "epoch": 9.213459065580798,
      "grad_norm": 0.09626820683479309,
      "learning_rate": 7.715387912558938e-06,
      "loss": 0.1713,
      "step": 42990
    },
    {
      "epoch": 9.215602228889841,
      "grad_norm": 0.2747611701488495,
      "learning_rate": 7.712530361480212e-06,
      "loss": 0.2162,
      "step": 43000
    },
    {
      "epoch": 9.217745392198886,
      "grad_norm": 30.91707420349121,
      "learning_rate": 7.709672810401486e-06,
      "loss": 0.5885,
      "step": 43010
    },
    {
      "epoch": 9.21988855550793,
      "grad_norm": 32.45783233642578,
      "learning_rate": 7.70681525932276e-06,
      "loss": 0.2655,
      "step": 43020
    },
    {
      "epoch": 9.222031718816973,
      "grad_norm": 17.531814575195312,
      "learning_rate": 7.703957708244036e-06,
      "loss": 0.2192,
      "step": 43030
    },
    {
      "epoch": 9.224174882126018,
      "grad_norm": 0.2190360128879547,
      "learning_rate": 7.70110015716531e-06,
      "loss": 0.1614,
      "step": 43040
    },
    {
      "epoch": 9.226318045435063,
      "grad_norm": 0.032055072486400604,
      "learning_rate": 7.698242606086584e-06,
      "loss": 0.2438,
      "step": 43050
    },
    {
      "epoch": 9.228461208744106,
      "grad_norm": 33.36330795288086,
      "learning_rate": 7.69538505500786e-06,
      "loss": 0.2798,
      "step": 43060
    },
    {
      "epoch": 9.23060437205315,
      "grad_norm": 0.08615178614854813,
      "learning_rate": 7.692527503929133e-06,
      "loss": 0.4407,
      "step": 43070
    },
    {
      "epoch": 9.232747535362195,
      "grad_norm": 0.0017795158782973886,
      "learning_rate": 7.689669952850409e-06,
      "loss": 0.1939,
      "step": 43080
    },
    {
      "epoch": 9.234890698671238,
      "grad_norm": 0.007441072259098291,
      "learning_rate": 7.686812401771683e-06,
      "loss": 0.0039,
      "step": 43090
    },
    {
      "epoch": 9.237033861980283,
      "grad_norm": 0.7530040740966797,
      "learning_rate": 7.683954850692957e-06,
      "loss": 0.0513,
      "step": 43100
    },
    {
      "epoch": 9.239177025289328,
      "grad_norm": 0.00178746716119349,
      "learning_rate": 7.68109729961423e-06,
      "loss": 0.0002,
      "step": 43110
    },
    {
      "epoch": 9.24132018859837,
      "grad_norm": 0.0073237549513578415,
      "learning_rate": 7.678239748535505e-06,
      "loss": 0.3339,
      "step": 43120
    },
    {
      "epoch": 9.243463351907415,
      "grad_norm": 0.16444678604602814,
      "learning_rate": 7.67538219745678e-06,
      "loss": 0.3457,
      "step": 43130
    },
    {
      "epoch": 9.24560651521646,
      "grad_norm": 20.827617645263672,
      "learning_rate": 7.672524646378054e-06,
      "loss": 0.4571,
      "step": 43140
    },
    {
      "epoch": 9.247749678525503,
      "grad_norm": 0.1226474940776825,
      "learning_rate": 7.66966709529933e-06,
      "loss": 0.3862,
      "step": 43150
    },
    {
      "epoch": 9.249892841834548,
      "grad_norm": 0.3769390285015106,
      "learning_rate": 7.666809544220604e-06,
      "loss": 0.3448,
      "step": 43160
    },
    {
      "epoch": 9.252036005143593,
      "grad_norm": 0.19621317088603973,
      "learning_rate": 7.663951993141878e-06,
      "loss": 0.0057,
      "step": 43170
    },
    {
      "epoch": 9.254179168452636,
      "grad_norm": 0.015222416259348392,
      "learning_rate": 7.661094442063153e-06,
      "loss": 0.1565,
      "step": 43180
    },
    {
      "epoch": 9.25632233176168,
      "grad_norm": 0.14233586192131042,
      "learning_rate": 7.658236890984427e-06,
      "loss": 0.0031,
      "step": 43190
    },
    {
      "epoch": 9.258465495070725,
      "grad_norm": 0.05654890835285187,
      "learning_rate": 7.655379339905701e-06,
      "loss": 0.4296,
      "step": 43200
    },
    {
      "epoch": 9.260608658379768,
      "grad_norm": 0.049536749720573425,
      "learning_rate": 7.652521788826975e-06,
      "loss": 0.0009,
      "step": 43210
    },
    {
      "epoch": 9.262751821688813,
      "grad_norm": 0.0023751682601869106,
      "learning_rate": 7.64966423774825e-06,
      "loss": 0.2316,
      "step": 43220
    },
    {
      "epoch": 9.264894984997857,
      "grad_norm": 0.377742737531662,
      "learning_rate": 7.646806686669525e-06,
      "loss": 0.0589,
      "step": 43230
    },
    {
      "epoch": 9.2670381483069,
      "grad_norm": 0.0096090417355299,
      "learning_rate": 7.643949135590799e-06,
      "loss": 0.0015,
      "step": 43240
    },
    {
      "epoch": 9.269181311615945,
      "grad_norm": 0.021316349506378174,
      "learning_rate": 7.641091584512074e-06,
      "loss": 0.0005,
      "step": 43250
    },
    {
      "epoch": 9.27132447492499,
      "grad_norm": 0.02479143813252449,
      "learning_rate": 7.638234033433348e-06,
      "loss": 0.1713,
      "step": 43260
    },
    {
      "epoch": 9.273467638234033,
      "grad_norm": 0.03842715173959732,
      "learning_rate": 7.635376482354622e-06,
      "loss": 0.0009,
      "step": 43270
    },
    {
      "epoch": 9.275610801543078,
      "grad_norm": 0.011935491114854813,
      "learning_rate": 7.632518931275898e-06,
      "loss": 0.383,
      "step": 43280
    },
    {
      "epoch": 9.277753964852122,
      "grad_norm": 0.02574104815721512,
      "learning_rate": 7.629661380197172e-06,
      "loss": 0.2453,
      "step": 43290
    },
    {
      "epoch": 9.279897128161165,
      "grad_norm": 18.390302658081055,
      "learning_rate": 7.626803829118446e-06,
      "loss": 0.1967,
      "step": 43300
    },
    {
      "epoch": 9.28204029147021,
      "grad_norm": 0.05368516221642494,
      "learning_rate": 7.623946278039721e-06,
      "loss": 0.2827,
      "step": 43310
    },
    {
      "epoch": 9.284183454779255,
      "grad_norm": 0.001277419039979577,
      "learning_rate": 7.621088726960994e-06,
      "loss": 0.0014,
      "step": 43320
    },
    {
      "epoch": 9.286326618088298,
      "grad_norm": 0.0017272091936320066,
      "learning_rate": 7.618231175882269e-06,
      "loss": 0.0494,
      "step": 43330
    },
    {
      "epoch": 9.288469781397342,
      "grad_norm": 0.0012229951098561287,
      "learning_rate": 7.615373624803544e-06,
      "loss": 0.0629,
      "step": 43340
    },
    {
      "epoch": 9.290612944706387,
      "grad_norm": 0.13389793038368225,
      "learning_rate": 7.6125160737248185e-06,
      "loss": 0.1006,
      "step": 43350
    },
    {
      "epoch": 9.29275610801543,
      "grad_norm": 0.08635633438825607,
      "learning_rate": 7.6096585226460924e-06,
      "loss": 0.3897,
      "step": 43360
    },
    {
      "epoch": 9.294899271324475,
      "grad_norm": 0.022316966205835342,
      "learning_rate": 7.606800971567367e-06,
      "loss": 0.0185,
      "step": 43370
    },
    {
      "epoch": 9.29704243463352,
      "grad_norm": 0.049940746277570724,
      "learning_rate": 7.603943420488642e-06,
      "loss": 0.6101,
      "step": 43380
    },
    {
      "epoch": 9.299185597942563,
      "grad_norm": 17.722959518432617,
      "learning_rate": 7.601085869409917e-06,
      "loss": 0.2159,
      "step": 43390
    },
    {
      "epoch": 9.301328761251607,
      "grad_norm": 0.1459004431962967,
      "learning_rate": 7.5982283183311915e-06,
      "loss": 0.002,
      "step": 43400
    },
    {
      "epoch": 9.303471924560652,
      "grad_norm": 0.0029788841493427753,
      "learning_rate": 7.5953707672524655e-06,
      "loss": 0.0507,
      "step": 43410
    },
    {
      "epoch": 9.305615087869695,
      "grad_norm": 0.040976982563734055,
      "learning_rate": 7.59251321617374e-06,
      "loss": 0.0403,
      "step": 43420
    },
    {
      "epoch": 9.30775825117874,
      "grad_norm": 0.002552824793383479,
      "learning_rate": 7.589655665095013e-06,
      "loss": 0.1917,
      "step": 43430
    },
    {
      "epoch": 9.309901414487785,
      "grad_norm": 0.002218359149992466,
      "learning_rate": 7.586798114016288e-06,
      "loss": 0.1226,
      "step": 43440
    },
    {
      "epoch": 9.312044577796827,
      "grad_norm": 0.04021772742271423,
      "learning_rate": 7.583940562937563e-06,
      "loss": 0.0007,
      "step": 43450
    },
    {
      "epoch": 9.314187741105872,
      "grad_norm": 0.0030413686763495207,
      "learning_rate": 7.581083011858838e-06,
      "loss": 0.0013,
      "step": 43460
    },
    {
      "epoch": 9.316330904414917,
      "grad_norm": 0.023670798167586327,
      "learning_rate": 7.5782254607801124e-06,
      "loss": 0.1566,
      "step": 43470
    },
    {
      "epoch": 9.31847406772396,
      "grad_norm": 0.023177871480584145,
      "learning_rate": 7.575367909701386e-06,
      "loss": 0.0003,
      "step": 43480
    },
    {
      "epoch": 9.320617231033005,
      "grad_norm": 0.18672651052474976,
      "learning_rate": 7.572510358622661e-06,
      "loss": 0.3926,
      "step": 43490
    },
    {
      "epoch": 9.32276039434205,
      "grad_norm": 0.0024440940469503403,
      "learning_rate": 7.569652807543936e-06,
      "loss": 0.2725,
      "step": 43500
    },
    {
      "epoch": 9.324903557651092,
      "grad_norm": 0.05302030220627785,
      "learning_rate": 7.566795256465211e-06,
      "loss": 0.2714,
      "step": 43510
    },
    {
      "epoch": 9.327046720960137,
      "grad_norm": 0.16011540591716766,
      "learning_rate": 7.563937705386485e-06,
      "loss": 0.1902,
      "step": 43520
    },
    {
      "epoch": 9.329189884269182,
      "grad_norm": 19.62297248840332,
      "learning_rate": 7.5610801543077585e-06,
      "loss": 0.3921,
      "step": 43530
    },
    {
      "epoch": 9.331333047578225,
      "grad_norm": 0.06672583520412445,
      "learning_rate": 7.558222603229033e-06,
      "loss": 0.213,
      "step": 43540
    },
    {
      "epoch": 9.33347621088727,
      "grad_norm": 0.17095476388931274,
      "learning_rate": 7.555365052150307e-06,
      "loss": 0.0695,
      "step": 43550
    },
    {
      "epoch": 9.335619374196314,
      "grad_norm": 0.07657592743635178,
      "learning_rate": 7.552507501071582e-06,
      "loss": 0.0021,
      "step": 43560
    },
    {
      "epoch": 9.337762537505357,
      "grad_norm": 103.8792953491211,
      "learning_rate": 7.549649949992857e-06,
      "loss": 0.1357,
      "step": 43570
    },
    {
      "epoch": 9.339905700814402,
      "grad_norm": 0.0027493739034980536,
      "learning_rate": 7.5467923989141316e-06,
      "loss": 0.2248,
      "step": 43580
    },
    {
      "epoch": 9.342048864123447,
      "grad_norm": 0.01287199929356575,
      "learning_rate": 7.5439348478354055e-06,
      "loss": 0.3033,
      "step": 43590
    },
    {
      "epoch": 9.34419202743249,
      "grad_norm": 0.004284940659999847,
      "learning_rate": 7.54107729675668e-06,
      "loss": 0.0011,
      "step": 43600
    },
    {
      "epoch": 9.346335190741534,
      "grad_norm": 0.002082715043798089,
      "learning_rate": 7.538219745677955e-06,
      "loss": 0.1463,
      "step": 43610
    },
    {
      "epoch": 9.34847835405058,
      "grad_norm": 0.12544797360897064,
      "learning_rate": 7.53536219459923e-06,
      "loss": 0.7685,
      "step": 43620
    },
    {
      "epoch": 9.350621517359622,
      "grad_norm": 0.005598755553364754,
      "learning_rate": 7.532504643520504e-06,
      "loss": 0.0052,
      "step": 43630
    },
    {
      "epoch": 9.352764680668667,
      "grad_norm": 0.0574025958776474,
      "learning_rate": 7.529647092441778e-06,
      "loss": 0.0017,
      "step": 43640
    },
    {
      "epoch": 9.354907843977712,
      "grad_norm": 0.08370242267847061,
      "learning_rate": 7.5267895413630525e-06,
      "loss": 0.022,
      "step": 43650
    },
    {
      "epoch": 9.357051007286755,
      "grad_norm": 0.09795327484607697,
      "learning_rate": 7.523931990284326e-06,
      "loss": 0.0029,
      "step": 43660
    },
    {
      "epoch": 9.3591941705958,
      "grad_norm": 0.03436938673257828,
      "learning_rate": 7.521074439205601e-06,
      "loss": 0.0182,
      "step": 43670
    },
    {
      "epoch": 9.361337333904844,
      "grad_norm": 0.05049659311771393,
      "learning_rate": 7.518216888126876e-06,
      "loss": 0.0004,
      "step": 43680
    },
    {
      "epoch": 9.363480497213887,
      "grad_norm": 0.02493804693222046,
      "learning_rate": 7.515359337048151e-06,
      "loss": 0.4929,
      "step": 43690
    },
    {
      "epoch": 9.365623660522932,
      "grad_norm": 0.0313691608607769,
      "learning_rate": 7.512501785969425e-06,
      "loss": 0.0005,
      "step": 43700
    },
    {
      "epoch": 9.367766823831976,
      "grad_norm": 0.08107767254114151,
      "learning_rate": 7.509644234890699e-06,
      "loss": 0.0008,
      "step": 43710
    },
    {
      "epoch": 9.36990998714102,
      "grad_norm": 17.642446517944336,
      "learning_rate": 7.506786683811974e-06,
      "loss": 0.5954,
      "step": 43720
    },
    {
      "epoch": 9.372053150450064,
      "grad_norm": 0.07190348207950592,
      "learning_rate": 7.503929132733249e-06,
      "loss": 0.4231,
      "step": 43730
    },
    {
      "epoch": 9.374196313759109,
      "grad_norm": 0.1073869839310646,
      "learning_rate": 7.501071581654523e-06,
      "loss": 0.0008,
      "step": 43740
    },
    {
      "epoch": 9.376339477068152,
      "grad_norm": 0.02136586233973503,
      "learning_rate": 7.498214030575797e-06,
      "loss": 0.5476,
      "step": 43750
    },
    {
      "epoch": 9.378482640377197,
      "grad_norm": 0.07004690170288086,
      "learning_rate": 7.495356479497072e-06,
      "loss": 0.002,
      "step": 43760
    },
    {
      "epoch": 9.380625803686241,
      "grad_norm": 0.03300253301858902,
      "learning_rate": 7.4924989284183455e-06,
      "loss": 0.1683,
      "step": 43770
    },
    {
      "epoch": 9.382768966995284,
      "grad_norm": 0.00796444434672594,
      "learning_rate": 7.48964137733962e-06,
      "loss": 0.1416,
      "step": 43780
    },
    {
      "epoch": 9.384912130304329,
      "grad_norm": 0.38159847259521484,
      "learning_rate": 7.486783826260895e-06,
      "loss": 0.0017,
      "step": 43790
    },
    {
      "epoch": 9.387055293613374,
      "grad_norm": 0.017727719619870186,
      "learning_rate": 7.48392627518217e-06,
      "loss": 0.0034,
      "step": 43800
    },
    {
      "epoch": 9.389198456922417,
      "grad_norm": 0.0029671217780560255,
      "learning_rate": 7.481068724103444e-06,
      "loss": 0.0007,
      "step": 43810
    },
    {
      "epoch": 9.391341620231461,
      "grad_norm": 0.050119973719120026,
      "learning_rate": 7.4782111730247186e-06,
      "loss": 0.1964,
      "step": 43820
    },
    {
      "epoch": 9.393484783540506,
      "grad_norm": 0.0066679674200713634,
      "learning_rate": 7.475353621945993e-06,
      "loss": 0.0005,
      "step": 43830
    },
    {
      "epoch": 9.39562794684955,
      "grad_norm": 0.002325613982975483,
      "learning_rate": 7.472496070867268e-06,
      "loss": 0.2358,
      "step": 43840
    },
    {
      "epoch": 9.397771110158594,
      "grad_norm": 1080.4708251953125,
      "learning_rate": 7.469638519788542e-06,
      "loss": 0.2157,
      "step": 43850
    },
    {
      "epoch": 9.399914273467639,
      "grad_norm": 0.17864423990249634,
      "learning_rate": 7.466780968709816e-06,
      "loss": 0.181,
      "step": 43860
    },
    {
      "epoch": 9.402057436776682,
      "grad_norm": 0.34841737151145935,
      "learning_rate": 7.463923417631091e-06,
      "loss": 0.0939,
      "step": 43870
    },
    {
      "epoch": 9.404200600085726,
      "grad_norm": 0.0313669815659523,
      "learning_rate": 7.461065866552365e-06,
      "loss": 0.3383,
      "step": 43880
    },
    {
      "epoch": 9.406343763394771,
      "grad_norm": 0.0032304420601576567,
      "learning_rate": 7.4582083154736394e-06,
      "loss": 0.4525,
      "step": 43890
    },
    {
      "epoch": 9.408486926703814,
      "grad_norm": 0.12719544768333435,
      "learning_rate": 7.455350764394914e-06,
      "loss": 0.2554,
      "step": 43900
    },
    {
      "epoch": 9.410630090012859,
      "grad_norm": 0.20898917317390442,
      "learning_rate": 7.452493213316189e-06,
      "loss": 0.2012,
      "step": 43910
    },
    {
      "epoch": 9.412773253321904,
      "grad_norm": 0.13584868609905243,
      "learning_rate": 7.449635662237463e-06,
      "loss": 0.0493,
      "step": 43920
    },
    {
      "epoch": 9.414916416630946,
      "grad_norm": 0.004251453559845686,
      "learning_rate": 7.446778111158738e-06,
      "loss": 0.4889,
      "step": 43930
    },
    {
      "epoch": 9.417059579939991,
      "grad_norm": 0.00405462272465229,
      "learning_rate": 7.4439205600800125e-06,
      "loss": 0.1473,
      "step": 43940
    },
    {
      "epoch": 9.419202743249036,
      "grad_norm": 0.004912513308227062,
      "learning_rate": 7.441063009001287e-06,
      "loss": 0.0011,
      "step": 43950
    },
    {
      "epoch": 9.421345906558079,
      "grad_norm": 0.014962173998355865,
      "learning_rate": 7.43820545792256e-06,
      "loss": 0.2367,
      "step": 43960
    },
    {
      "epoch": 9.423489069867124,
      "grad_norm": 0.21348412334918976,
      "learning_rate": 7.435347906843835e-06,
      "loss": 0.2644,
      "step": 43970
    },
    {
      "epoch": 9.425632233176168,
      "grad_norm": 0.0426965095102787,
      "learning_rate": 7.43249035576511e-06,
      "loss": 0.2026,
      "step": 43980
    },
    {
      "epoch": 9.427775396485211,
      "grad_norm": 0.006661816965788603,
      "learning_rate": 7.429632804686384e-06,
      "loss": 0.2542,
      "step": 43990
    },
    {
      "epoch": 9.429918559794256,
      "grad_norm": 0.034442681819200516,
      "learning_rate": 7.426775253607659e-06,
      "loss": 0.1705,
      "step": 44000
    },
    {
      "epoch": 9.4320617231033,
      "grad_norm": 0.046709977090358734,
      "learning_rate": 7.423917702528933e-06,
      "loss": 0.777,
      "step": 44010
    },
    {
      "epoch": 9.434204886412344,
      "grad_norm": 15.76237678527832,
      "learning_rate": 7.421060151450208e-06,
      "loss": 0.5262,
      "step": 44020
    },
    {
      "epoch": 9.436348049721389,
      "grad_norm": 0.1412057876586914,
      "learning_rate": 7.418202600371482e-06,
      "loss": 0.0039,
      "step": 44030
    },
    {
      "epoch": 9.438491213030433,
      "grad_norm": 0.17578189074993134,
      "learning_rate": 7.415345049292757e-06,
      "loss": 0.0235,
      "step": 44040
    },
    {
      "epoch": 9.440634376339476,
      "grad_norm": 0.17911407351493835,
      "learning_rate": 7.412487498214032e-06,
      "loss": 0.1973,
      "step": 44050
    },
    {
      "epoch": 9.442777539648521,
      "grad_norm": 0.11264217644929886,
      "learning_rate": 7.409629947135306e-06,
      "loss": 0.1896,
      "step": 44060
    },
    {
      "epoch": 9.444920702957566,
      "grad_norm": 0.08513081818819046,
      "learning_rate": 7.4067723960565795e-06,
      "loss": 0.2098,
      "step": 44070
    },
    {
      "epoch": 9.447063866266609,
      "grad_norm": 0.06339765340089798,
      "learning_rate": 7.403914844977854e-06,
      "loss": 0.0015,
      "step": 44080
    },
    {
      "epoch": 9.449207029575653,
      "grad_norm": 0.03696896508336067,
      "learning_rate": 7.401057293899129e-06,
      "loss": 0.0266,
      "step": 44090
    },
    {
      "epoch": 9.451350192884698,
      "grad_norm": 0.00493220891803503,
      "learning_rate": 7.398199742820403e-06,
      "loss": 0.0003,
      "step": 44100
    },
    {
      "epoch": 9.453493356193743,
      "grad_norm": 0.00551300635561347,
      "learning_rate": 7.395342191741678e-06,
      "loss": 0.1579,
      "step": 44110
    },
    {
      "epoch": 9.455636519502786,
      "grad_norm": 0.0022730575874447823,
      "learning_rate": 7.3924846406629525e-06,
      "loss": 0.0839,
      "step": 44120
    },
    {
      "epoch": 9.45777968281183,
      "grad_norm": 0.05695946142077446,
      "learning_rate": 7.389627089584227e-06,
      "loss": 0.6711,
      "step": 44130
    },
    {
      "epoch": 9.459922846120875,
      "grad_norm": 0.0457911342382431,
      "learning_rate": 7.386769538505501e-06,
      "loss": 0.0007,
      "step": 44140
    },
    {
      "epoch": 9.462066009429918,
      "grad_norm": 0.004101752303540707,
      "learning_rate": 7.383911987426776e-06,
      "loss": 0.0004,
      "step": 44150
    },
    {
      "epoch": 9.464209172738963,
      "grad_norm": 0.3748878836631775,
      "learning_rate": 7.381054436348051e-06,
      "loss": 0.1623,
      "step": 44160
    },
    {
      "epoch": 9.466352336048008,
      "grad_norm": 34.09120559692383,
      "learning_rate": 7.3781968852693255e-06,
      "loss": 0.4245,
      "step": 44170
    },
    {
      "epoch": 9.46849549935705,
      "grad_norm": 0.06048451364040375,
      "learning_rate": 7.375339334190599e-06,
      "loss": 0.2788,
      "step": 44180
    },
    {
      "epoch": 9.470638662666095,
      "grad_norm": 50.17930603027344,
      "learning_rate": 7.372481783111873e-06,
      "loss": 0.3384,
      "step": 44190
    },
    {
      "epoch": 9.47278182597514,
      "grad_norm": 0.11653189361095428,
      "learning_rate": 7.369624232033148e-06,
      "loss": 0.3152,
      "step": 44200
    },
    {
      "epoch": 9.474924989284183,
      "grad_norm": 0.05533141270279884,
      "learning_rate": 7.366766680954422e-06,
      "loss": 0.1046,
      "step": 44210
    },
    {
      "epoch": 9.477068152593228,
      "grad_norm": 0.11189527809619904,
      "learning_rate": 7.363909129875697e-06,
      "loss": 0.1315,
      "step": 44220
    },
    {
      "epoch": 9.479211315902273,
      "grad_norm": 0.030857013538479805,
      "learning_rate": 7.361051578796972e-06,
      "loss": 0.001,
      "step": 44230
    },
    {
      "epoch": 9.481354479211316,
      "grad_norm": 0.07069959491491318,
      "learning_rate": 7.358194027718246e-06,
      "loss": 0.2764,
      "step": 44240
    },
    {
      "epoch": 9.48349764252036,
      "grad_norm": 0.06451516598463058,
      "learning_rate": 7.355336476639521e-06,
      "loss": 0.2097,
      "step": 44250
    },
    {
      "epoch": 9.485640805829405,
      "grad_norm": 0.023667989298701286,
      "learning_rate": 7.352478925560795e-06,
      "loss": 0.0011,
      "step": 44260
    },
    {
      "epoch": 9.487783969138448,
      "grad_norm": 0.005691132042557001,
      "learning_rate": 7.34962137448207e-06,
      "loss": 0.0158,
      "step": 44270
    },
    {
      "epoch": 9.489927132447493,
      "grad_norm": 0.003179218154400587,
      "learning_rate": 7.346763823403345e-06,
      "loss": 0.0102,
      "step": 44280
    },
    {
      "epoch": 9.492070295756537,
      "grad_norm": 21.06938934326172,
      "learning_rate": 7.343906272324618e-06,
      "loss": 0.3894,
      "step": 44290
    },
    {
      "epoch": 9.49421345906558,
      "grad_norm": 0.024295015260577202,
      "learning_rate": 7.3410487212458925e-06,
      "loss": 0.8359,
      "step": 44300
    },
    {
      "epoch": 9.496356622374625,
      "grad_norm": 0.05457431450486183,
      "learning_rate": 7.338191170167167e-06,
      "loss": 0.1899,
      "step": 44310
    },
    {
      "epoch": 9.49849978568367,
      "grad_norm": 0.047074589878320694,
      "learning_rate": 7.335333619088442e-06,
      "loss": 0.5635,
      "step": 44320
    },
    {
      "epoch": 9.500642948992713,
      "grad_norm": 0.19798880815505981,
      "learning_rate": 7.332476068009716e-06,
      "loss": 0.4105,
      "step": 44330
    },
    {
      "epoch": 9.502786112301758,
      "grad_norm": 0.1710294485092163,
      "learning_rate": 7.329618516930991e-06,
      "loss": 0.3009,
      "step": 44340
    },
    {
      "epoch": 9.504929275610802,
      "grad_norm": 0.37398597598075867,
      "learning_rate": 7.3267609658522655e-06,
      "loss": 0.245,
      "step": 44350
    },
    {
      "epoch": 9.507072438919845,
      "grad_norm": 0.29821014404296875,
      "learning_rate": 7.32390341477354e-06,
      "loss": 0.1354,
      "step": 44360
    },
    {
      "epoch": 9.50921560222889,
      "grad_norm": 0.09837495535612106,
      "learning_rate": 7.321045863694814e-06,
      "loss": 0.1201,
      "step": 44370
    },
    {
      "epoch": 9.511358765537935,
      "grad_norm": 0.07125640660524368,
      "learning_rate": 7.318188312616089e-06,
      "loss": 0.1591,
      "step": 44380
    },
    {
      "epoch": 9.513501928846978,
      "grad_norm": 0.009847023524343967,
      "learning_rate": 7.315330761537363e-06,
      "loss": 0.103,
      "step": 44390
    },
    {
      "epoch": 9.515645092156022,
      "grad_norm": 0.044710054993629456,
      "learning_rate": 7.312473210458637e-06,
      "loss": 0.4146,
      "step": 44400
    },
    {
      "epoch": 9.517788255465067,
      "grad_norm": 26.047788619995117,
      "learning_rate": 7.309615659379912e-06,
      "loss": 0.4174,
      "step": 44410
    },
    {
      "epoch": 9.51993141877411,
      "grad_norm": 0.06055096536874771,
      "learning_rate": 7.3067581083011864e-06,
      "loss": 0.1938,
      "step": 44420
    },
    {
      "epoch": 9.522074582083155,
      "grad_norm": 0.20798243582248688,
      "learning_rate": 7.303900557222461e-06,
      "loss": 0.5147,
      "step": 44430
    },
    {
      "epoch": 9.5242177453922,
      "grad_norm": 0.14572088420391083,
      "learning_rate": 7.301043006143735e-06,
      "loss": 0.2253,
      "step": 44440
    },
    {
      "epoch": 9.526360908701243,
      "grad_norm": 0.05486202985048294,
      "learning_rate": 7.29818545506501e-06,
      "loss": 0.0021,
      "step": 44450
    },
    {
      "epoch": 9.528504072010287,
      "grad_norm": 21.66480827331543,
      "learning_rate": 7.295327903986285e-06,
      "loss": 0.1821,
      "step": 44460
    },
    {
      "epoch": 9.530647235319332,
      "grad_norm": 0.10354360938072205,
      "learning_rate": 7.2924703529075595e-06,
      "loss": 0.178,
      "step": 44470
    },
    {
      "epoch": 9.532790398628375,
      "grad_norm": 0.07902916520833969,
      "learning_rate": 7.289612801828833e-06,
      "loss": 0.2106,
      "step": 44480
    },
    {
      "epoch": 9.53493356193742,
      "grad_norm": 0.03385141119360924,
      "learning_rate": 7.286755250750108e-06,
      "loss": 0.1743,
      "step": 44490
    },
    {
      "epoch": 9.537076725246465,
      "grad_norm": 0.0025534078013151884,
      "learning_rate": 7.283897699671382e-06,
      "loss": 0.1839,
      "step": 44500
    },
    {
      "epoch": 9.539219888555508,
      "grad_norm": 0.0019462751224637032,
      "learning_rate": 7.281040148592656e-06,
      "loss": 0.0125,
      "step": 44510
    },
    {
      "epoch": 9.541363051864552,
      "grad_norm": 0.019295090809464455,
      "learning_rate": 7.278182597513931e-06,
      "loss": 0.0028,
      "step": 44520
    },
    {
      "epoch": 9.543506215173597,
      "grad_norm": 0.0051229484379291534,
      "learning_rate": 7.2753250464352056e-06,
      "loss": 0.7242,
      "step": 44530
    },
    {
      "epoch": 9.54564937848264,
      "grad_norm": 0.02683340013027191,
      "learning_rate": 7.27246749535648e-06,
      "loss": 0.2555,
      "step": 44540
    },
    {
      "epoch": 9.547792541791685,
      "grad_norm": 0.13877663016319275,
      "learning_rate": 7.269609944277754e-06,
      "loss": 0.0016,
      "step": 44550
    },
    {
      "epoch": 9.54993570510073,
      "grad_norm": 0.0533534474670887,
      "learning_rate": 7.266752393199029e-06,
      "loss": 0.1529,
      "step": 44560
    },
    {
      "epoch": 9.552078868409772,
      "grad_norm": 27.113815307617188,
      "learning_rate": 7.263894842120304e-06,
      "loss": 0.2384,
      "step": 44570
    },
    {
      "epoch": 9.554222031718817,
      "grad_norm": 0.10003947466611862,
      "learning_rate": 7.261037291041579e-06,
      "loss": 0.0011,
      "step": 44580
    },
    {
      "epoch": 9.556365195027862,
      "grad_norm": 0.082669697701931,
      "learning_rate": 7.2581797399628525e-06,
      "loss": 0.0012,
      "step": 44590
    },
    {
      "epoch": 9.558508358336905,
      "grad_norm": 0.15000097453594208,
      "learning_rate": 7.255322188884127e-06,
      "loss": 0.1516,
      "step": 44600
    },
    {
      "epoch": 9.56065152164595,
      "grad_norm": 0.22816146910190582,
      "learning_rate": 7.252464637805401e-06,
      "loss": 0.2055,
      "step": 44610
    },
    {
      "epoch": 9.562794684954994,
      "grad_norm": 0.09683100879192352,
      "learning_rate": 7.249607086726675e-06,
      "loss": 0.3069,
      "step": 44620
    },
    {
      "epoch": 9.564937848264037,
      "grad_norm": 0.012780374847352505,
      "learning_rate": 7.24674953564795e-06,
      "loss": 0.1916,
      "step": 44630
    },
    {
      "epoch": 9.567081011573082,
      "grad_norm": 1.3623790740966797,
      "learning_rate": 7.243891984569225e-06,
      "loss": 0.2823,
      "step": 44640
    },
    {
      "epoch": 9.569224174882127,
      "grad_norm": 23.459875106811523,
      "learning_rate": 7.2410344334904995e-06,
      "loss": 0.4247,
      "step": 44650
    },
    {
      "epoch": 9.57136733819117,
      "grad_norm": 0.03161558881402016,
      "learning_rate": 7.238176882411773e-06,
      "loss": 0.3751,
      "step": 44660
    },
    {
      "epoch": 9.573510501500214,
      "grad_norm": 0.11747206747531891,
      "learning_rate": 7.235319331333048e-06,
      "loss": 0.3289,
      "step": 44670
    },
    {
      "epoch": 9.57565366480926,
      "grad_norm": 0.0018640462076291442,
      "learning_rate": 7.232461780254323e-06,
      "loss": 0.2482,
      "step": 44680
    },
    {
      "epoch": 9.577796828118302,
      "grad_norm": 0.001264429185539484,
      "learning_rate": 7.229604229175598e-06,
      "loss": 0.0013,
      "step": 44690
    },
    {
      "epoch": 9.579939991427347,
      "grad_norm": 0.04134086146950722,
      "learning_rate": 7.226746678096872e-06,
      "loss": 0.0016,
      "step": 44700
    },
    {
      "epoch": 9.582083154736392,
      "grad_norm": 0.0013804140035063028,
      "learning_rate": 7.223889127018146e-06,
      "loss": 0.1099,
      "step": 44710
    },
    {
      "epoch": 9.584226318045435,
      "grad_norm": 0.09324225038290024,
      "learning_rate": 7.22103157593942e-06,
      "loss": 0.1916,
      "step": 44720
    },
    {
      "epoch": 9.58636948135448,
      "grad_norm": 0.14742407202720642,
      "learning_rate": 7.218174024860694e-06,
      "loss": 0.0987,
      "step": 44730
    },
    {
      "epoch": 9.588512644663524,
      "grad_norm": 1.1869779825210571,
      "learning_rate": 7.215316473781969e-06,
      "loss": 0.002,
      "step": 44740
    },
    {
      "epoch": 9.590655807972567,
      "grad_norm": 81.0752944946289,
      "learning_rate": 7.212458922703244e-06,
      "loss": 0.3888,
      "step": 44750
    },
    {
      "epoch": 9.592798971281612,
      "grad_norm": 157.32704162597656,
      "learning_rate": 7.209601371624519e-06,
      "loss": 0.0684,
      "step": 44760
    },
    {
      "epoch": 9.594942134590656,
      "grad_norm": 0.0333390049636364,
      "learning_rate": 7.2067438205457926e-06,
      "loss": 0.5361,
      "step": 44770
    },
    {
      "epoch": 9.5970852978997,
      "grad_norm": 0.1154797226190567,
      "learning_rate": 7.203886269467067e-06,
      "loss": 0.0011,
      "step": 44780
    },
    {
      "epoch": 9.599228461208744,
      "grad_norm": 0.007388430181890726,
      "learning_rate": 7.201028718388342e-06,
      "loss": 0.0008,
      "step": 44790
    },
    {
      "epoch": 9.601371624517789,
      "grad_norm": 0.009602248668670654,
      "learning_rate": 7.198171167309617e-06,
      "loss": 0.6554,
      "step": 44800
    },
    {
      "epoch": 9.603514787826832,
      "grad_norm": 0.014456432312726974,
      "learning_rate": 7.195313616230891e-06,
      "loss": 0.5579,
      "step": 44810
    },
    {
      "epoch": 9.605657951135877,
      "grad_norm": 0.34774115681648254,
      "learning_rate": 7.192456065152165e-06,
      "loss": 0.2619,
      "step": 44820
    },
    {
      "epoch": 9.607801114444921,
      "grad_norm": 0.11894597113132477,
      "learning_rate": 7.1895985140734395e-06,
      "loss": 0.1691,
      "step": 44830
    },
    {
      "epoch": 9.609944277753964,
      "grad_norm": 0.007367943413555622,
      "learning_rate": 7.1867409629947134e-06,
      "loss": 0.3266,
      "step": 44840
    },
    {
      "epoch": 9.612087441063009,
      "grad_norm": 0.27875080704689026,
      "learning_rate": 7.183883411915988e-06,
      "loss": 0.0021,
      "step": 44850
    },
    {
      "epoch": 9.614230604372054,
      "grad_norm": 0.20857153832912445,
      "learning_rate": 7.181025860837263e-06,
      "loss": 0.2713,
      "step": 44860
    },
    {
      "epoch": 9.616373767681097,
      "grad_norm": 0.23227983713150024,
      "learning_rate": 7.178168309758538e-06,
      "loss": 0.21,
      "step": 44870
    },
    {
      "epoch": 9.618516930990141,
      "grad_norm": 0.08803154528141022,
      "learning_rate": 7.175310758679812e-06,
      "loss": 0.003,
      "step": 44880
    },
    {
      "epoch": 9.620660094299186,
      "grad_norm": 0.006737608462572098,
      "learning_rate": 7.1724532076010865e-06,
      "loss": 0.174,
      "step": 44890
    },
    {
      "epoch": 9.62280325760823,
      "grad_norm": 1.9490123987197876,
      "learning_rate": 7.169595656522361e-06,
      "loss": 0.1518,
      "step": 44900
    },
    {
      "epoch": 9.624946420917274,
      "grad_norm": 0.0605892539024353,
      "learning_rate": 7.166738105443636e-06,
      "loss": 0.0007,
      "step": 44910
    },
    {
      "epoch": 9.627089584226319,
      "grad_norm": 0.11426353454589844,
      "learning_rate": 7.16388055436491e-06,
      "loss": 0.1328,
      "step": 44920
    },
    {
      "epoch": 9.629232747535362,
      "grad_norm": 0.0032302848994731903,
      "learning_rate": 7.161023003286184e-06,
      "loss": 0.3048,
      "step": 44930
    },
    {
      "epoch": 9.631375910844406,
      "grad_norm": 0.10133528709411621,
      "learning_rate": 7.158165452207459e-06,
      "loss": 0.2459,
      "step": 44940
    },
    {
      "epoch": 9.633519074153451,
      "grad_norm": 1.1995447874069214,
      "learning_rate": 7.155307901128733e-06,
      "loss": 0.1276,
      "step": 44950
    },
    {
      "epoch": 9.635662237462494,
      "grad_norm": 0.053518809378147125,
      "learning_rate": 7.152450350050007e-06,
      "loss": 0.1373,
      "step": 44960
    },
    {
      "epoch": 9.637805400771539,
      "grad_norm": 0.11936712265014648,
      "learning_rate": 7.149592798971282e-06,
      "loss": 0.7931,
      "step": 44970
    },
    {
      "epoch": 9.639948564080584,
      "grad_norm": 0.13551579415798187,
      "learning_rate": 7.146735247892557e-06,
      "loss": 0.1786,
      "step": 44980
    },
    {
      "epoch": 9.642091727389626,
      "grad_norm": 0.24134515225887299,
      "learning_rate": 7.143877696813831e-06,
      "loss": 0.1204,
      "step": 44990
    },
    {
      "epoch": 9.644234890698671,
      "grad_norm": 1.8505960702896118,
      "learning_rate": 7.141020145735106e-06,
      "loss": 0.2521,
      "step": 45000
    },
    {
      "epoch": 9.646378054007716,
      "grad_norm": 0.0020886738784611225,
      "learning_rate": 7.13816259465638e-06,
      "loss": 0.1812,
      "step": 45010
    },
    {
      "epoch": 9.648521217316759,
      "grad_norm": 0.02340950071811676,
      "learning_rate": 7.135305043577655e-06,
      "loss": 0.1077,
      "step": 45020
    },
    {
      "epoch": 9.650664380625804,
      "grad_norm": 0.13827000558376312,
      "learning_rate": 7.13244749249893e-06,
      "loss": 0.931,
      "step": 45030
    },
    {
      "epoch": 9.652807543934848,
      "grad_norm": 0.053056035190820694,
      "learning_rate": 7.129589941420203e-06,
      "loss": 0.2279,
      "step": 45040
    },
    {
      "epoch": 9.654950707243891,
      "grad_norm": 0.12844914197921753,
      "learning_rate": 7.126732390341478e-06,
      "loss": 0.1426,
      "step": 45050
    },
    {
      "epoch": 9.657093870552936,
      "grad_norm": 0.10168086737394333,
      "learning_rate": 7.123874839262752e-06,
      "loss": 0.1764,
      "step": 45060
    },
    {
      "epoch": 9.65923703386198,
      "grad_norm": 24.513629913330078,
      "learning_rate": 7.1210172881840265e-06,
      "loss": 0.1224,
      "step": 45070
    },
    {
      "epoch": 9.661380197171024,
      "grad_norm": 0.0067296102643013,
      "learning_rate": 7.118159737105301e-06,
      "loss": 0.1882,
      "step": 45080
    },
    {
      "epoch": 9.663523360480069,
      "grad_norm": 0.053312186151742935,
      "learning_rate": 7.115302186026576e-06,
      "loss": 0.2601,
      "step": 45090
    },
    {
      "epoch": 9.665666523789113,
      "grad_norm": 18.795387268066406,
      "learning_rate": 7.112444634947851e-06,
      "loss": 0.3538,
      "step": 45100
    },
    {
      "epoch": 9.667809687098156,
      "grad_norm": 0.9988741278648376,
      "learning_rate": 7.109587083869125e-06,
      "loss": 0.1299,
      "step": 45110
    },
    {
      "epoch": 9.669952850407201,
      "grad_norm": 0.07842031866312027,
      "learning_rate": 7.1067295327903995e-06,
      "loss": 0.6169,
      "step": 45120
    },
    {
      "epoch": 9.672096013716246,
      "grad_norm": 0.07869745045900345,
      "learning_rate": 7.103871981711674e-06,
      "loss": 0.1536,
      "step": 45130
    },
    {
      "epoch": 9.674239177025289,
      "grad_norm": 0.12546075880527496,
      "learning_rate": 7.101014430632947e-06,
      "loss": 0.0034,
      "step": 45140
    },
    {
      "epoch": 9.676382340334333,
      "grad_norm": 0.030565746128559113,
      "learning_rate": 7.098156879554222e-06,
      "loss": 0.001,
      "step": 45150
    },
    {
      "epoch": 9.678525503643378,
      "grad_norm": 0.050333987921476364,
      "learning_rate": 7.095299328475497e-06,
      "loss": 0.0009,
      "step": 45160
    },
    {
      "epoch": 9.680668666952421,
      "grad_norm": 0.13598032295703888,
      "learning_rate": 7.092441777396772e-06,
      "loss": 0.1742,
      "step": 45170
    },
    {
      "epoch": 9.682811830261466,
      "grad_norm": 0.0029037182684987783,
      "learning_rate": 7.089584226318046e-06,
      "loss": 0.0014,
      "step": 45180
    },
    {
      "epoch": 9.68495499357051,
      "grad_norm": 0.058214325457811356,
      "learning_rate": 7.08672667523932e-06,
      "loss": 0.1748,
      "step": 45190
    },
    {
      "epoch": 9.687098156879554,
      "grad_norm": 0.002984920283779502,
      "learning_rate": 7.083869124160595e-06,
      "loss": 0.1558,
      "step": 45200
    },
    {
      "epoch": 9.689241320188598,
      "grad_norm": 20.551607131958008,
      "learning_rate": 7.08101157308187e-06,
      "loss": 0.371,
      "step": 45210
    },
    {
      "epoch": 9.691384483497643,
      "grad_norm": 0.0026516991201788187,
      "learning_rate": 7.078154022003144e-06,
      "loss": 0.0015,
      "step": 45220
    },
    {
      "epoch": 9.693527646806686,
      "grad_norm": 0.04584897309541702,
      "learning_rate": 7.075296470924419e-06,
      "loss": 0.0014,
      "step": 45230
    },
    {
      "epoch": 9.69567081011573,
      "grad_norm": 0.05409471318125725,
      "learning_rate": 7.0724389198456934e-06,
      "loss": 0.2577,
      "step": 45240
    },
    {
      "epoch": 9.697813973424775,
      "grad_norm": 0.07133716344833374,
      "learning_rate": 7.0695813687669665e-06,
      "loss": 0.0036,
      "step": 45250
    },
    {
      "epoch": 9.699957136733818,
      "grad_norm": 0.20976832509040833,
      "learning_rate": 7.066723817688241e-06,
      "loss": 0.2143,
      "step": 45260
    },
    {
      "epoch": 9.702100300042863,
      "grad_norm": 0.0021365948487073183,
      "learning_rate": 7.063866266609516e-06,
      "loss": 0.0014,
      "step": 45270
    },
    {
      "epoch": 9.704243463351908,
      "grad_norm": 18.871986389160156,
      "learning_rate": 7.061008715530791e-06,
      "loss": 0.3508,
      "step": 45280
    },
    {
      "epoch": 9.70638662666095,
      "grad_norm": 0.07861214131116867,
      "learning_rate": 7.058151164452065e-06,
      "loss": 0.0013,
      "step": 45290
    },
    {
      "epoch": 9.708529789969996,
      "grad_norm": 0.09168267995119095,
      "learning_rate": 7.0552936133733396e-06,
      "loss": 0.2211,
      "step": 45300
    },
    {
      "epoch": 9.71067295327904,
      "grad_norm": 0.027233755216002464,
      "learning_rate": 7.052436062294614e-06,
      "loss": 0.5408,
      "step": 45310
    },
    {
      "epoch": 9.712816116588083,
      "grad_norm": 0.20489874482154846,
      "learning_rate": 7.049578511215889e-06,
      "loss": 0.1399,
      "step": 45320
    },
    {
      "epoch": 9.714959279897128,
      "grad_norm": 0.03639446943998337,
      "learning_rate": 7.046720960137163e-06,
      "loss": 0.002,
      "step": 45330
    },
    {
      "epoch": 9.717102443206173,
      "grad_norm": 0.03999953716993332,
      "learning_rate": 7.043863409058438e-06,
      "loss": 0.0019,
      "step": 45340
    },
    {
      "epoch": 9.719245606515216,
      "grad_norm": 0.13352219760417938,
      "learning_rate": 7.041005857979713e-06,
      "loss": 0.286,
      "step": 45350
    },
    {
      "epoch": 9.72138876982426,
      "grad_norm": 0.025868503376841545,
      "learning_rate": 7.038148306900986e-06,
      "loss": 0.406,
      "step": 45360
    },
    {
      "epoch": 9.723531933133305,
      "grad_norm": 0.06817431002855301,
      "learning_rate": 7.0352907558222604e-06,
      "loss": 0.3919,
      "step": 45370
    },
    {
      "epoch": 9.725675096442348,
      "grad_norm": 1.404000997543335,
      "learning_rate": 7.032433204743535e-06,
      "loss": 0.3043,
      "step": 45380
    },
    {
      "epoch": 9.727818259751393,
      "grad_norm": 0.11436934769153595,
      "learning_rate": 7.02957565366481e-06,
      "loss": 0.005,
      "step": 45390
    },
    {
      "epoch": 9.729961423060438,
      "grad_norm": 0.008805532939732075,
      "learning_rate": 7.026718102586084e-06,
      "loss": 0.4223,
      "step": 45400
    },
    {
      "epoch": 9.73210458636948,
      "grad_norm": 0.003735182574018836,
      "learning_rate": 7.023860551507359e-06,
      "loss": 0.1994,
      "step": 45410
    },
    {
      "epoch": 9.734247749678525,
      "grad_norm": 0.10197550803422928,
      "learning_rate": 7.0210030004286335e-06,
      "loss": 0.3332,
      "step": 45420
    },
    {
      "epoch": 9.73639091298757,
      "grad_norm": 0.0032886676490306854,
      "learning_rate": 7.018145449349908e-06,
      "loss": 0.185,
      "step": 45430
    },
    {
      "epoch": 9.738534076296613,
      "grad_norm": 0.14123007655143738,
      "learning_rate": 7.015287898271182e-06,
      "loss": 0.0028,
      "step": 45440
    },
    {
      "epoch": 9.740677239605658,
      "grad_norm": 0.1434439867734909,
      "learning_rate": 7.012430347192457e-06,
      "loss": 0.0017,
      "step": 45450
    },
    {
      "epoch": 9.742820402914703,
      "grad_norm": 0.06185910850763321,
      "learning_rate": 7.009572796113732e-06,
      "loss": 0.1973,
      "step": 45460
    },
    {
      "epoch": 9.744963566223745,
      "grad_norm": 0.059437211602926254,
      "learning_rate": 7.006715245035005e-06,
      "loss": 0.001,
      "step": 45470
    },
    {
      "epoch": 9.74710672953279,
      "grad_norm": 0.001975641120225191,
      "learning_rate": 7.00385769395628e-06,
      "loss": 0.1559,
      "step": 45480
    },
    {
      "epoch": 9.749249892841835,
      "grad_norm": 0.009508977644145489,
      "learning_rate": 7.001000142877554e-06,
      "loss": 0.1903,
      "step": 45490
    },
    {
      "epoch": 9.751393056150878,
      "grad_norm": 0.011524678207933903,
      "learning_rate": 6.998142591798829e-06,
      "loss": 0.3285,
      "step": 45500
    },
    {
      "epoch": 9.753536219459923,
      "grad_norm": 0.0041219801642000675,
      "learning_rate": 6.995285040720103e-06,
      "loss": 0.0009,
      "step": 45510
    },
    {
      "epoch": 9.755679382768967,
      "grad_norm": 0.023860931396484375,
      "learning_rate": 6.992427489641378e-06,
      "loss": 0.003,
      "step": 45520
    },
    {
      "epoch": 9.75782254607801,
      "grad_norm": 17.764097213745117,
      "learning_rate": 6.989569938562653e-06,
      "loss": 0.1911,
      "step": 45530
    },
    {
      "epoch": 9.759965709387055,
      "grad_norm": 0.1098448857665062,
      "learning_rate": 6.986712387483927e-06,
      "loss": 0.1572,
      "step": 45540
    },
    {
      "epoch": 9.7621088726961,
      "grad_norm": 0.11520583182573318,
      "learning_rate": 6.983854836405201e-06,
      "loss": 0.1621,
      "step": 45550
    },
    {
      "epoch": 9.764252036005143,
      "grad_norm": 0.012562839314341545,
      "learning_rate": 6.980997285326476e-06,
      "loss": 0.1586,
      "step": 45560
    },
    {
      "epoch": 9.766395199314188,
      "grad_norm": 0.06835134327411652,
      "learning_rate": 6.97813973424775e-06,
      "loss": 0.0009,
      "step": 45570
    },
    {
      "epoch": 9.768538362623232,
      "grad_norm": 0.11809132248163223,
      "learning_rate": 6.975282183169024e-06,
      "loss": 0.1686,
      "step": 45580
    },
    {
      "epoch": 9.770681525932275,
      "grad_norm": 0.07784094661474228,
      "learning_rate": 6.972424632090299e-06,
      "loss": 0.001,
      "step": 45590
    },
    {
      "epoch": 9.77282468924132,
      "grad_norm": 0.004395571071654558,
      "learning_rate": 6.9695670810115735e-06,
      "loss": 0.7259,
      "step": 45600
    },
    {
      "epoch": 9.774967852550365,
      "grad_norm": 21.641437530517578,
      "learning_rate": 6.966709529932848e-06,
      "loss": 0.1269,
      "step": 45610
    },
    {
      "epoch": 9.777111015859408,
      "grad_norm": 0.04546723514795303,
      "learning_rate": 6.963851978854122e-06,
      "loss": 0.3637,
      "step": 45620
    },
    {
      "epoch": 9.779254179168452,
      "grad_norm": 0.3007816970348358,
      "learning_rate": 6.960994427775397e-06,
      "loss": 0.0919,
      "step": 45630
    },
    {
      "epoch": 9.781397342477497,
      "grad_norm": 0.20216819643974304,
      "learning_rate": 6.958136876696672e-06,
      "loss": 0.0019,
      "step": 45640
    },
    {
      "epoch": 9.78354050578654,
      "grad_norm": 1.018884539604187,
      "learning_rate": 6.9552793256179465e-06,
      "loss": 0.0051,
      "step": 45650
    },
    {
      "epoch": 9.785683669095585,
      "grad_norm": 0.21610184013843536,
      "learning_rate": 6.9524217745392205e-06,
      "loss": 0.1921,
      "step": 45660
    },
    {
      "epoch": 9.78782683240463,
      "grad_norm": 0.0249757282435894,
      "learning_rate": 6.949564223460495e-06,
      "loss": 0.1087,
      "step": 45670
    },
    {
      "epoch": 9.789969995713673,
      "grad_norm": 0.10259053111076355,
      "learning_rate": 6.946706672381769e-06,
      "loss": 0.1583,
      "step": 45680
    },
    {
      "epoch": 9.792113159022717,
      "grad_norm": 0.177775576710701,
      "learning_rate": 6.943849121303043e-06,
      "loss": 0.0016,
      "step": 45690
    },
    {
      "epoch": 9.794256322331762,
      "grad_norm": 0.052511148154735565,
      "learning_rate": 6.940991570224318e-06,
      "loss": 0.0009,
      "step": 45700
    },
    {
      "epoch": 9.796399485640805,
      "grad_norm": 0.00191316322889179,
      "learning_rate": 6.938134019145593e-06,
      "loss": 0.4004,
      "step": 45710
    },
    {
      "epoch": 9.79854264894985,
      "grad_norm": 0.207218199968338,
      "learning_rate": 6.935276468066867e-06,
      "loss": 0.1847,
      "step": 45720
    },
    {
      "epoch": 9.800685812258894,
      "grad_norm": 0.002534053986892104,
      "learning_rate": 6.932418916988141e-06,
      "loss": 0.0008,
      "step": 45730
    },
    {
      "epoch": 9.802828975567937,
      "grad_norm": 0.0360456146299839,
      "learning_rate": 6.929561365909416e-06,
      "loss": 0.3179,
      "step": 45740
    },
    {
      "epoch": 9.804972138876982,
      "grad_norm": 17.826013565063477,
      "learning_rate": 6.926703814830691e-06,
      "loss": 0.2055,
      "step": 45750
    },
    {
      "epoch": 9.807115302186027,
      "grad_norm": 0.08898770064115524,
      "learning_rate": 6.923846263751966e-06,
      "loss": 0.0007,
      "step": 45760
    },
    {
      "epoch": 9.80925846549507,
      "grad_norm": 0.0022699618712067604,
      "learning_rate": 6.92098871267324e-06,
      "loss": 0.3839,
      "step": 45770
    },
    {
      "epoch": 9.811401628804115,
      "grad_norm": 0.10573536902666092,
      "learning_rate": 6.918131161594514e-06,
      "loss": 0.0018,
      "step": 45780
    },
    {
      "epoch": 9.81354479211316,
      "grad_norm": 0.11378315091133118,
      "learning_rate": 6.915273610515788e-06,
      "loss": 0.4953,
      "step": 45790
    },
    {
      "epoch": 9.815687955422202,
      "grad_norm": 0.03226056322455406,
      "learning_rate": 6.912416059437062e-06,
      "loss": 0.0013,
      "step": 45800
    },
    {
      "epoch": 9.817831118731247,
      "grad_norm": 0.0016496008029207587,
      "learning_rate": 6.909558508358337e-06,
      "loss": 0.0004,
      "step": 45810
    },
    {
      "epoch": 9.819974282040292,
      "grad_norm": 0.022617585957050323,
      "learning_rate": 6.906700957279612e-06,
      "loss": 0.001,
      "step": 45820
    },
    {
      "epoch": 9.822117445349335,
      "grad_norm": 0.12754862010478973,
      "learning_rate": 6.9038434062008866e-06,
      "loss": 0.5042,
      "step": 45830
    },
    {
      "epoch": 9.82426060865838,
      "grad_norm": 0.009588079527020454,
      "learning_rate": 6.9009858551221605e-06,
      "loss": 0.3835,
      "step": 45840
    },
    {
      "epoch": 9.826403771967424,
      "grad_norm": 0.10294003784656525,
      "learning_rate": 6.898128304043435e-06,
      "loss": 0.001,
      "step": 45850
    },
    {
      "epoch": 9.828546935276467,
      "grad_norm": 0.07950524240732193,
      "learning_rate": 6.89527075296471e-06,
      "loss": 0.0013,
      "step": 45860
    },
    {
      "epoch": 9.830690098585512,
      "grad_norm": 0.016008850187063217,
      "learning_rate": 6.892413201885985e-06,
      "loss": 0.0005,
      "step": 45870
    },
    {
      "epoch": 9.832833261894557,
      "grad_norm": 0.03949408233165741,
      "learning_rate": 6.88955565080726e-06,
      "loss": 0.6485,
      "step": 45880
    },
    {
      "epoch": 9.8349764252036,
      "grad_norm": 0.050924960523843765,
      "learning_rate": 6.8866980997285335e-06,
      "loss": 0.118,
      "step": 45890
    },
    {
      "epoch": 9.837119588512644,
      "grad_norm": 0.003373118583112955,
      "learning_rate": 6.8838405486498074e-06,
      "loss": 0.0005,
      "step": 45900
    },
    {
      "epoch": 9.839262751821689,
      "grad_norm": 26.501235961914062,
      "learning_rate": 6.880982997571081e-06,
      "loss": 0.6377,
      "step": 45910
    },
    {
      "epoch": 9.841405915130732,
      "grad_norm": 1.0909576416015625,
      "learning_rate": 6.878125446492356e-06,
      "loss": 0.0048,
      "step": 45920
    },
    {
      "epoch": 9.843549078439777,
      "grad_norm": 0.3110530972480774,
      "learning_rate": 6.875267895413631e-06,
      "loss": 0.0029,
      "step": 45930
    },
    {
      "epoch": 9.845692241748822,
      "grad_norm": 0.019471023231744766,
      "learning_rate": 6.872410344334906e-06,
      "loss": 0.1718,
      "step": 45940
    },
    {
      "epoch": 9.847835405057864,
      "grad_norm": 0.0012547337682917714,
      "learning_rate": 6.8695527932561805e-06,
      "loss": 0.2173,
      "step": 45950
    },
    {
      "epoch": 9.84997856836691,
      "grad_norm": 0.050172414630651474,
      "learning_rate": 6.866695242177454e-06,
      "loss": 0.1805,
      "step": 45960
    },
    {
      "epoch": 9.852121731675954,
      "grad_norm": 0.003932999912649393,
      "learning_rate": 6.863837691098729e-06,
      "loss": 0.1672,
      "step": 45970
    },
    {
      "epoch": 9.854264894984997,
      "grad_norm": 0.0013435911387205124,
      "learning_rate": 6.860980140020004e-06,
      "loss": 0.1503,
      "step": 45980
    },
    {
      "epoch": 9.856408058294042,
      "grad_norm": 0.001692014280706644,
      "learning_rate": 6.858122588941279e-06,
      "loss": 0.0011,
      "step": 45990
    },
    {
      "epoch": 9.858551221603086,
      "grad_norm": 0.013238764367997646,
      "learning_rate": 6.855265037862552e-06,
      "loss": 0.1751,
      "step": 46000
    },
    {
      "epoch": 9.860694384912131,
      "grad_norm": 0.0023553771898150444,
      "learning_rate": 6.852407486783827e-06,
      "loss": 0.1791,
      "step": 46010
    },
    {
      "epoch": 9.862837548221174,
      "grad_norm": 0.0011812194716185331,
      "learning_rate": 6.849549935705101e-06,
      "loss": 0.2085,
      "step": 46020
    },
    {
      "epoch": 9.864980711530219,
      "grad_norm": 0.03545883670449257,
      "learning_rate": 6.846692384626375e-06,
      "loss": 0.0018,
      "step": 46030
    },
    {
      "epoch": 9.867123874839264,
      "grad_norm": 0.0026130520273,
      "learning_rate": 6.84383483354765e-06,
      "loss": 0.0006,
      "step": 46040
    },
    {
      "epoch": 9.869267038148307,
      "grad_norm": 0.0020902312826365232,
      "learning_rate": 6.840977282468925e-06,
      "loss": 0.2703,
      "step": 46050
    },
    {
      "epoch": 9.871410201457351,
      "grad_norm": 0.14238184690475464,
      "learning_rate": 6.8381197313902e-06,
      "loss": 0.1734,
      "step": 46060
    },
    {
      "epoch": 9.873553364766396,
      "grad_norm": 0.11013250797986984,
      "learning_rate": 6.8352621803114735e-06,
      "loss": 0.3103,
      "step": 46070
    },
    {
      "epoch": 9.875696528075439,
      "grad_norm": 0.09471665322780609,
      "learning_rate": 6.832404629232748e-06,
      "loss": 0.2911,
      "step": 46080
    },
    {
      "epoch": 9.877839691384484,
      "grad_norm": 0.11273451894521713,
      "learning_rate": 6.829547078154023e-06,
      "loss": 0.5207,
      "step": 46090
    },
    {
      "epoch": 9.879982854693528,
      "grad_norm": 0.08376651257276535,
      "learning_rate": 6.826689527075298e-06,
      "loss": 0.3864,
      "step": 46100
    },
    {
      "epoch": 9.882126018002571,
      "grad_norm": 0.06852047890424728,
      "learning_rate": 6.823831975996571e-06,
      "loss": 0.0021,
      "step": 46110
    },
    {
      "epoch": 9.884269181311616,
      "grad_norm": 0.09673037379980087,
      "learning_rate": 6.820974424917846e-06,
      "loss": 0.1063,
      "step": 46120
    },
    {
      "epoch": 9.88641234462066,
      "grad_norm": 21.757339477539062,
      "learning_rate": 6.8181168738391205e-06,
      "loss": 0.3279,
      "step": 46130
    },
    {
      "epoch": 9.888555507929704,
      "grad_norm": 19.45977783203125,
      "learning_rate": 6.8152593227603944e-06,
      "loss": 0.277,
      "step": 46140
    },
    {
      "epoch": 9.890698671238749,
      "grad_norm": 0.05065716803073883,
      "learning_rate": 6.812401771681669e-06,
      "loss": 0.3934,
      "step": 46150
    },
    {
      "epoch": 9.892841834547793,
      "grad_norm": 0.03345506638288498,
      "learning_rate": 6.809544220602944e-06,
      "loss": 0.0881,
      "step": 46160
    },
    {
      "epoch": 9.894984997856836,
      "grad_norm": 0.02834705449640751,
      "learning_rate": 6.806686669524219e-06,
      "loss": 0.0356,
      "step": 46170
    },
    {
      "epoch": 9.897128161165881,
      "grad_norm": 0.0010782129829749465,
      "learning_rate": 6.803829118445493e-06,
      "loss": 0.0026,
      "step": 46180
    },
    {
      "epoch": 9.899271324474926,
      "grad_norm": 0.08132975548505783,
      "learning_rate": 6.8009715673667675e-06,
      "loss": 0.2823,
      "step": 46190
    },
    {
      "epoch": 9.901414487783969,
      "grad_norm": 0.33468300104141235,
      "learning_rate": 6.798114016288042e-06,
      "loss": 0.45,
      "step": 46200
    },
    {
      "epoch": 9.903557651093013,
      "grad_norm": 0.0727572962641716,
      "learning_rate": 6.795256465209317e-06,
      "loss": 0.3857,
      "step": 46210
    },
    {
      "epoch": 9.905700814402058,
      "grad_norm": 0.025624915957450867,
      "learning_rate": 6.79239891413059e-06,
      "loss": 0.0009,
      "step": 46220
    },
    {
      "epoch": 9.907843977711101,
      "grad_norm": 0.002858829451724887,
      "learning_rate": 6.789541363051865e-06,
      "loss": 0.0007,
      "step": 46230
    },
    {
      "epoch": 9.909987141020146,
      "grad_norm": 0.0034084594808518887,
      "learning_rate": 6.78668381197314e-06,
      "loss": 0.0005,
      "step": 46240
    },
    {
      "epoch": 9.91213030432919,
      "grad_norm": 0.1457967907190323,
      "learning_rate": 6.7838262608944136e-06,
      "loss": 0.3204,
      "step": 46250
    },
    {
      "epoch": 9.914273467638234,
      "grad_norm": 0.0025092982687056065,
      "learning_rate": 6.780968709815688e-06,
      "loss": 0.0016,
      "step": 46260
    },
    {
      "epoch": 9.916416630947278,
      "grad_norm": 10.302703857421875,
      "learning_rate": 6.778111158736963e-06,
      "loss": 0.6813,
      "step": 46270
    },
    {
      "epoch": 9.918559794256323,
      "grad_norm": 0.11716658622026443,
      "learning_rate": 6.775253607658238e-06,
      "loss": 0.0015,
      "step": 46280
    },
    {
      "epoch": 9.920702957565366,
      "grad_norm": 0.29693499207496643,
      "learning_rate": 6.772396056579512e-06,
      "loss": 0.1965,
      "step": 46290
    },
    {
      "epoch": 9.92284612087441,
      "grad_norm": 0.004298306070268154,
      "learning_rate": 6.769538505500787e-06,
      "loss": 0.1983,
      "step": 46300
    },
    {
      "epoch": 9.924989284183455,
      "grad_norm": 0.15053750574588776,
      "learning_rate": 6.766680954422061e-06,
      "loss": 0.1566,
      "step": 46310
    },
    {
      "epoch": 9.927132447492498,
      "grad_norm": 0.05174091085791588,
      "learning_rate": 6.763823403343336e-06,
      "loss": 0.1898,
      "step": 46320
    },
    {
      "epoch": 9.929275610801543,
      "grad_norm": 0.0035724740009754896,
      "learning_rate": 6.760965852264609e-06,
      "loss": 0.3242,
      "step": 46330
    },
    {
      "epoch": 9.931418774110588,
      "grad_norm": 0.0617000013589859,
      "learning_rate": 6.758108301185884e-06,
      "loss": 0.1165,
      "step": 46340
    },
    {
      "epoch": 9.933561937419631,
      "grad_norm": 0.039913568645715714,
      "learning_rate": 6.755250750107159e-06,
      "loss": 0.0605,
      "step": 46350
    },
    {
      "epoch": 9.935705100728676,
      "grad_norm": 0.0893511101603508,
      "learning_rate": 6.752393199028433e-06,
      "loss": 0.0012,
      "step": 46360
    },
    {
      "epoch": 9.93784826403772,
      "grad_norm": 0.002566003007814288,
      "learning_rate": 6.7495356479497075e-06,
      "loss": 0.3904,
      "step": 46370
    },
    {
      "epoch": 9.939991427346763,
      "grad_norm": 0.10404325276613235,
      "learning_rate": 6.746678096870982e-06,
      "loss": 0.0006,
      "step": 46380
    },
    {
      "epoch": 9.942134590655808,
      "grad_norm": 0.07466576248407364,
      "learning_rate": 6.743820545792257e-06,
      "loss": 0.3954,
      "step": 46390
    },
    {
      "epoch": 9.944277753964853,
      "grad_norm": 0.014273175969719887,
      "learning_rate": 6.740962994713531e-06,
      "loss": 0.0021,
      "step": 46400
    },
    {
      "epoch": 9.946420917273896,
      "grad_norm": 0.06995736062526703,
      "learning_rate": 6.738105443634806e-06,
      "loss": 0.2068,
      "step": 46410
    },
    {
      "epoch": 9.94856408058294,
      "grad_norm": 0.06277712434530258,
      "learning_rate": 6.7352478925560805e-06,
      "loss": 0.0017,
      "step": 46420
    },
    {
      "epoch": 9.950707243891985,
      "grad_norm": 0.07412324845790863,
      "learning_rate": 6.732390341477354e-06,
      "loss": 0.1918,
      "step": 46430
    },
    {
      "epoch": 9.952850407201028,
      "grad_norm": 0.004446110688149929,
      "learning_rate": 6.729532790398628e-06,
      "loss": 0.0013,
      "step": 46440
    },
    {
      "epoch": 9.954993570510073,
      "grad_norm": 0.0023282638285309076,
      "learning_rate": 6.726675239319903e-06,
      "loss": 0.0005,
      "step": 46450
    },
    {
      "epoch": 9.957136733819118,
      "grad_norm": 0.002012035110965371,
      "learning_rate": 6.723817688241178e-06,
      "loss": 0.0009,
      "step": 46460
    },
    {
      "epoch": 9.95927989712816,
      "grad_norm": 0.07693086564540863,
      "learning_rate": 6.720960137162452e-06,
      "loss": 0.179,
      "step": 46470
    },
    {
      "epoch": 9.961423060437205,
      "grad_norm": 0.01857168972492218,
      "learning_rate": 6.718102586083727e-06,
      "loss": 0.5179,
      "step": 46480
    },
    {
      "epoch": 9.96356622374625,
      "grad_norm": 0.04749194532632828,
      "learning_rate": 6.715245035005001e-06,
      "loss": 0.1668,
      "step": 46490
    },
    {
      "epoch": 9.965709387055293,
      "grad_norm": 0.07468998432159424,
      "learning_rate": 6.712387483926276e-06,
      "loss": 0.1787,
      "step": 46500
    },
    {
      "epoch": 9.967852550364338,
      "grad_norm": 0.05730929970741272,
      "learning_rate": 6.70952993284755e-06,
      "loss": 0.3956,
      "step": 46510
    },
    {
      "epoch": 9.969995713673383,
      "grad_norm": 23.87452507019043,
      "learning_rate": 6.706672381768825e-06,
      "loss": 0.1613,
      "step": 46520
    },
    {
      "epoch": 9.972138876982426,
      "grad_norm": 17.480276107788086,
      "learning_rate": 6.7038148306901e-06,
      "loss": 0.189,
      "step": 46530
    },
    {
      "epoch": 9.97428204029147,
      "grad_norm": 0.08407876640558243,
      "learning_rate": 6.700957279611373e-06,
      "loss": 0.5289,
      "step": 46540
    },
    {
      "epoch": 9.976425203600515,
      "grad_norm": 0.01688767410814762,
      "learning_rate": 6.6980997285326475e-06,
      "loss": 0.3478,
      "step": 46550
    },
    {
      "epoch": 9.978568366909558,
      "grad_norm": 0.07849688827991486,
      "learning_rate": 6.695242177453922e-06,
      "loss": 0.0015,
      "step": 46560
    },
    {
      "epoch": 9.980711530218603,
      "grad_norm": 0.013006195425987244,
      "learning_rate": 6.692384626375197e-06,
      "loss": 0.2506,
      "step": 46570
    },
    {
      "epoch": 9.982854693527647,
      "grad_norm": 0.13147646188735962,
      "learning_rate": 6.689527075296471e-06,
      "loss": 0.0008,
      "step": 46580
    },
    {
      "epoch": 9.98499785683669,
      "grad_norm": 0.008919062092900276,
      "learning_rate": 6.686669524217746e-06,
      "loss": 0.1702,
      "step": 46590
    },
    {
      "epoch": 9.987141020145735,
      "grad_norm": 0.0015340180834755301,
      "learning_rate": 6.6838119731390205e-06,
      "loss": 0.2185,
      "step": 46600
    },
    {
      "epoch": 9.98928418345478,
      "grad_norm": 0.1253848522901535,
      "learning_rate": 6.680954422060295e-06,
      "loss": 0.0059,
      "step": 46610
    },
    {
      "epoch": 9.991427346763823,
      "grad_norm": 53.36408233642578,
      "learning_rate": 6.67809687098157e-06,
      "loss": 0.2837,
      "step": 46620
    },
    {
      "epoch": 9.993570510072868,
      "grad_norm": 0.2395484447479248,
      "learning_rate": 6.675239319902844e-06,
      "loss": 0.1499,
      "step": 46630
    },
    {
      "epoch": 9.995713673381912,
      "grad_norm": 0.004437865223735571,
      "learning_rate": 6.672381768824119e-06,
      "loss": 0.3299,
      "step": 46640
    },
    {
      "epoch": 9.997856836690955,
      "grad_norm": 0.02300027199089527,
      "learning_rate": 6.669524217745392e-06,
      "loss": 0.5431,
      "step": 46650
    },
    {
      "epoch": 10.0,
      "grad_norm": 17.946880340576172,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.2818,
      "step": 46660
    },
    {
      "epoch": 10.0,
      "eval_accuracy": 0.9533333333333334,
      "eval_f1": 0.8138297872340425,
      "eval_loss": 0.2586711347103119,
      "eval_precision": 0.8203753351206434,
      "eval_recall": 0.8073878627968337,
      "eval_runtime": 703.9139,
      "eval_samples_per_second": 4.262,
      "eval_steps_per_second": 1.421,
      "step": 46660
    },
    {
      "epoch": 10.002143163309045,
      "grad_norm": 25.712148666381836,
      "learning_rate": 6.663809115587941e-06,
      "loss": 0.3668,
      "step": 46670
    },
    {
      "epoch": 10.004286326618088,
      "grad_norm": 0.243878573179245,
      "learning_rate": 6.660951564509216e-06,
      "loss": 0.0023,
      "step": 46680
    },
    {
      "epoch": 10.006429489927132,
      "grad_norm": 0.034591712057590485,
      "learning_rate": 6.658094013430491e-06,
      "loss": 0.0016,
      "step": 46690
    },
    {
      "epoch": 10.008572653236177,
      "grad_norm": 0.04944775998592377,
      "learning_rate": 6.655236462351765e-06,
      "loss": 0.1178,
      "step": 46700
    },
    {
      "epoch": 10.01071581654522,
      "grad_norm": 0.001574033871293068,
      "learning_rate": 6.65237891127304e-06,
      "loss": 0.0019,
      "step": 46710
    },
    {
      "epoch": 10.012858979854265,
      "grad_norm": 0.0009806612506508827,
      "learning_rate": 6.6495213601943145e-06,
      "loss": 0.1593,
      "step": 46720
    },
    {
      "epoch": 10.01500214316331,
      "grad_norm": 0.001418661093339324,
      "learning_rate": 6.646663809115589e-06,
      "loss": 0.0016,
      "step": 46730
    },
    {
      "epoch": 10.017145306472353,
      "grad_norm": 0.06194910779595375,
      "learning_rate": 6.643806258036863e-06,
      "loss": 0.0008,
      "step": 46740
    },
    {
      "epoch": 10.019288469781397,
      "grad_norm": 0.0015193953877314925,
      "learning_rate": 6.640948706958137e-06,
      "loss": 0.0002,
      "step": 46750
    },
    {
      "epoch": 10.021431633090442,
      "grad_norm": 91.93638610839844,
      "learning_rate": 6.638091155879412e-06,
      "loss": 0.4048,
      "step": 46760
    },
    {
      "epoch": 10.023574796399485,
      "grad_norm": 0.06854776293039322,
      "learning_rate": 6.635233604800686e-06,
      "loss": 0.517,
      "step": 46770
    },
    {
      "epoch": 10.02571795970853,
      "grad_norm": 0.09538639336824417,
      "learning_rate": 6.6323760537219606e-06,
      "loss": 0.1768,
      "step": 46780
    },
    {
      "epoch": 10.027861123017574,
      "grad_norm": 0.09167743474245071,
      "learning_rate": 6.629518502643235e-06,
      "loss": 0.0023,
      "step": 46790
    },
    {
      "epoch": 10.030004286326617,
      "grad_norm": 0.06551612913608551,
      "learning_rate": 6.62666095156451e-06,
      "loss": 0.001,
      "step": 46800
    },
    {
      "epoch": 10.032147449635662,
      "grad_norm": 0.019257843494415283,
      "learning_rate": 6.623803400485784e-06,
      "loss": 0.4482,
      "step": 46810
    },
    {
      "epoch": 10.034290612944707,
      "grad_norm": 0.059078507125377655,
      "learning_rate": 6.620945849407059e-06,
      "loss": 0.2082,
      "step": 46820
    },
    {
      "epoch": 10.03643377625375,
      "grad_norm": 25.60329246520996,
      "learning_rate": 6.618088298328334e-06,
      "loss": 0.1468,
      "step": 46830
    },
    {
      "epoch": 10.038576939562795,
      "grad_norm": 46.34754180908203,
      "learning_rate": 6.615230747249608e-06,
      "loss": 0.1153,
      "step": 46840
    },
    {
      "epoch": 10.04072010287184,
      "grad_norm": 0.09499961882829666,
      "learning_rate": 6.612373196170882e-06,
      "loss": 0.1837,
      "step": 46850
    },
    {
      "epoch": 10.042863266180882,
      "grad_norm": 0.030842838808894157,
      "learning_rate": 6.609515645092156e-06,
      "loss": 0.0012,
      "step": 46860
    },
    {
      "epoch": 10.045006429489927,
      "grad_norm": 0.002979902783408761,
      "learning_rate": 6.606658094013431e-06,
      "loss": 0.0017,
      "step": 46870
    },
    {
      "epoch": 10.047149592798972,
      "grad_norm": 26.719968795776367,
      "learning_rate": 6.603800542934705e-06,
      "loss": 0.6282,
      "step": 46880
    },
    {
      "epoch": 10.049292756108015,
      "grad_norm": 0.0026497424114495516,
      "learning_rate": 6.60094299185598e-06,
      "loss": 0.1768,
      "step": 46890
    },
    {
      "epoch": 10.05143591941706,
      "grad_norm": 0.11566229909658432,
      "learning_rate": 6.5980854407772545e-06,
      "loss": 0.1722,
      "step": 46900
    },
    {
      "epoch": 10.053579082726104,
      "grad_norm": 0.011427439749240875,
      "learning_rate": 6.595227889698529e-06,
      "loss": 0.1126,
      "step": 46910
    },
    {
      "epoch": 10.055722246035147,
      "grad_norm": 0.01718313619494438,
      "learning_rate": 6.592370338619803e-06,
      "loss": 0.263,
      "step": 46920
    },
    {
      "epoch": 10.057865409344192,
      "grad_norm": 0.03252217173576355,
      "learning_rate": 6.589512787541078e-06,
      "loss": 0.3494,
      "step": 46930
    },
    {
      "epoch": 10.060008572653237,
      "grad_norm": 0.225408673286438,
      "learning_rate": 6.586655236462353e-06,
      "loss": 0.2735,
      "step": 46940
    },
    {
      "epoch": 10.06215173596228,
      "grad_norm": 0.007698705419898033,
      "learning_rate": 6.5837976853836275e-06,
      "loss": 0.1365,
      "step": 46950
    },
    {
      "epoch": 10.064294899271324,
      "grad_norm": 1.0855697393417358,
      "learning_rate": 6.5809401343049014e-06,
      "loss": 0.0047,
      "step": 46960
    },
    {
      "epoch": 10.066438062580369,
      "grad_norm": 0.12173442542552948,
      "learning_rate": 6.578082583226175e-06,
      "loss": 0.0015,
      "step": 46970
    },
    {
      "epoch": 10.068581225889412,
      "grad_norm": 0.04636843875050545,
      "learning_rate": 6.57522503214745e-06,
      "loss": 0.309,
      "step": 46980
    },
    {
      "epoch": 10.070724389198457,
      "grad_norm": 0.13666513562202454,
      "learning_rate": 6.572367481068724e-06,
      "loss": 0.1712,
      "step": 46990
    },
    {
      "epoch": 10.072867552507502,
      "grad_norm": 1.5194419622421265,
      "learning_rate": 6.569509929989999e-06,
      "loss": 0.0025,
      "step": 47000
    },
    {
      "epoch": 10.075010715816545,
      "grad_norm": 0.0014798743650317192,
      "learning_rate": 6.566652378911274e-06,
      "loss": 0.0034,
      "step": 47010
    },
    {
      "epoch": 10.07715387912559,
      "grad_norm": 0.011377478018403053,
      "learning_rate": 6.563794827832548e-06,
      "loss": 0.3532,
      "step": 47020
    },
    {
      "epoch": 10.079297042434634,
      "grad_norm": 51.73723602294922,
      "learning_rate": 6.560937276753822e-06,
      "loss": 0.556,
      "step": 47030
    },
    {
      "epoch": 10.081440205743677,
      "grad_norm": 0.017946192994713783,
      "learning_rate": 6.558079725675097e-06,
      "loss": 0.0011,
      "step": 47040
    },
    {
      "epoch": 10.083583369052722,
      "grad_norm": 0.01694716513156891,
      "learning_rate": 6.555222174596372e-06,
      "loss": 0.0009,
      "step": 47050
    },
    {
      "epoch": 10.085726532361766,
      "grad_norm": 0.001755574718117714,
      "learning_rate": 6.552364623517647e-06,
      "loss": 0.0007,
      "step": 47060
    },
    {
      "epoch": 10.08786969567081,
      "grad_norm": 0.011714930646121502,
      "learning_rate": 6.5495070724389206e-06,
      "loss": 0.0008,
      "step": 47070
    },
    {
      "epoch": 10.090012858979854,
      "grad_norm": 0.014047421514987946,
      "learning_rate": 6.5466495213601945e-06,
      "loss": 0.1839,
      "step": 47080
    },
    {
      "epoch": 10.092156022288899,
      "grad_norm": 0.0035088020376861095,
      "learning_rate": 6.543791970281469e-06,
      "loss": 0.0007,
      "step": 47090
    },
    {
      "epoch": 10.094299185597942,
      "grad_norm": 0.0019065766828134656,
      "learning_rate": 6.540934419202743e-06,
      "loss": 0.0008,
      "step": 47100
    },
    {
      "epoch": 10.096442348906987,
      "grad_norm": 0.002455082954838872,
      "learning_rate": 6.538076868124018e-06,
      "loss": 0.1708,
      "step": 47110
    },
    {
      "epoch": 10.098585512216031,
      "grad_norm": 0.002000480890274048,
      "learning_rate": 6.535219317045293e-06,
      "loss": 0.1783,
      "step": 47120
    },
    {
      "epoch": 10.100728675525074,
      "grad_norm": 0.0015027531189844012,
      "learning_rate": 6.5323617659665675e-06,
      "loss": 0.1447,
      "step": 47130
    },
    {
      "epoch": 10.102871838834119,
      "grad_norm": 0.030875643715262413,
      "learning_rate": 6.5295042148878415e-06,
      "loss": 0.0009,
      "step": 47140
    },
    {
      "epoch": 10.105015002143164,
      "grad_norm": 0.13297297060489655,
      "learning_rate": 6.526646663809116e-06,
      "loss": 0.0004,
      "step": 47150
    },
    {
      "epoch": 10.107158165452207,
      "grad_norm": 0.0011696509318426251,
      "learning_rate": 6.523789112730391e-06,
      "loss": 0.0006,
      "step": 47160
    },
    {
      "epoch": 10.109301328761251,
      "grad_norm": 0.0018559374148026109,
      "learning_rate": 6.520931561651666e-06,
      "loss": 0.2142,
      "step": 47170
    },
    {
      "epoch": 10.111444492070296,
      "grad_norm": 0.07570041716098785,
      "learning_rate": 6.518074010572939e-06,
      "loss": 0.3797,
      "step": 47180
    },
    {
      "epoch": 10.11358765537934,
      "grad_norm": 1.069040298461914,
      "learning_rate": 6.515216459494214e-06,
      "loss": 0.0018,
      "step": 47190
    },
    {
      "epoch": 10.115730818688384,
      "grad_norm": 16.41971778869629,
      "learning_rate": 6.512358908415488e-06,
      "loss": 0.1789,
      "step": 47200
    },
    {
      "epoch": 10.117873981997429,
      "grad_norm": 23.575149536132812,
      "learning_rate": 6.509501357336762e-06,
      "loss": 0.3388,
      "step": 47210
    },
    {
      "epoch": 10.120017145306472,
      "grad_norm": 0.17690332233905792,
      "learning_rate": 6.506643806258037e-06,
      "loss": 0.1808,
      "step": 47220
    },
    {
      "epoch": 10.122160308615516,
      "grad_norm": 0.036460548639297485,
      "learning_rate": 6.503786255179312e-06,
      "loss": 0.1431,
      "step": 47230
    },
    {
      "epoch": 10.124303471924561,
      "grad_norm": 0.055998507887125015,
      "learning_rate": 6.500928704100587e-06,
      "loss": 0.002,
      "step": 47240
    },
    {
      "epoch": 10.126446635233604,
      "grad_norm": 0.16639560461044312,
      "learning_rate": 6.498071153021861e-06,
      "loss": 0.1554,
      "step": 47250
    },
    {
      "epoch": 10.128589798542649,
      "grad_norm": 16.674850463867188,
      "learning_rate": 6.495213601943135e-06,
      "loss": 0.3173,
      "step": 47260
    },
    {
      "epoch": 10.130732961851693,
      "grad_norm": 0.08945708721876144,
      "learning_rate": 6.49235605086441e-06,
      "loss": 0.0011,
      "step": 47270
    },
    {
      "epoch": 10.132876125160736,
      "grad_norm": 0.027644600719213486,
      "learning_rate": 6.489498499785685e-06,
      "loss": 0.3137,
      "step": 47280
    },
    {
      "epoch": 10.135019288469781,
      "grad_norm": 0.001033158041536808,
      "learning_rate": 6.486640948706958e-06,
      "loss": 0.0009,
      "step": 47290
    },
    {
      "epoch": 10.137162451778826,
      "grad_norm": 0.11479291319847107,
      "learning_rate": 6.483783397628233e-06,
      "loss": 0.3344,
      "step": 47300
    },
    {
      "epoch": 10.139305615087869,
      "grad_norm": 0.11028926819562912,
      "learning_rate": 6.4809258465495076e-06,
      "loss": 0.5622,
      "step": 47310
    },
    {
      "epoch": 10.141448778396914,
      "grad_norm": 0.033301301300525665,
      "learning_rate": 6.4780682954707815e-06,
      "loss": 0.0009,
      "step": 47320
    },
    {
      "epoch": 10.143591941705958,
      "grad_norm": 0.015578407794237137,
      "learning_rate": 6.475210744392056e-06,
      "loss": 0.3477,
      "step": 47330
    },
    {
      "epoch": 10.145735105015001,
      "grad_norm": 18.475950241088867,
      "learning_rate": 6.472353193313331e-06,
      "loss": 0.5304,
      "step": 47340
    },
    {
      "epoch": 10.147878268324046,
      "grad_norm": 19.97397232055664,
      "learning_rate": 6.469495642234606e-06,
      "loss": 0.4559,
      "step": 47350
    },
    {
      "epoch": 10.15002143163309,
      "grad_norm": 0.07331319153308868,
      "learning_rate": 6.46663809115588e-06,
      "loss": 0.0026,
      "step": 47360
    },
    {
      "epoch": 10.152164594942134,
      "grad_norm": 0.021167561411857605,
      "learning_rate": 6.4637805400771545e-06,
      "loss": 0.0014,
      "step": 47370
    },
    {
      "epoch": 10.154307758251178,
      "grad_norm": 0.1813291609287262,
      "learning_rate": 6.460922988998429e-06,
      "loss": 0.0021,
      "step": 47380
    },
    {
      "epoch": 10.156450921560223,
      "grad_norm": 0.14224030077457428,
      "learning_rate": 6.458065437919704e-06,
      "loss": 0.0011,
      "step": 47390
    },
    {
      "epoch": 10.158594084869266,
      "grad_norm": 0.006694825366139412,
      "learning_rate": 6.455207886840977e-06,
      "loss": 0.0008,
      "step": 47400
    },
    {
      "epoch": 10.160737248178311,
      "grad_norm": 0.1166893020272255,
      "learning_rate": 6.452350335762252e-06,
      "loss": 0.0015,
      "step": 47410
    },
    {
      "epoch": 10.162880411487356,
      "grad_norm": 0.008210144005715847,
      "learning_rate": 6.449492784683527e-06,
      "loss": 0.381,
      "step": 47420
    },
    {
      "epoch": 10.165023574796399,
      "grad_norm": 0.03667392209172249,
      "learning_rate": 6.446635233604801e-06,
      "loss": 0.0244,
      "step": 47430
    },
    {
      "epoch": 10.167166738105443,
      "grad_norm": 0.1389235109090805,
      "learning_rate": 6.443777682526075e-06,
      "loss": 0.159,
      "step": 47440
    },
    {
      "epoch": 10.169309901414488,
      "grad_norm": 0.17116665840148926,
      "learning_rate": 6.44092013144735e-06,
      "loss": 0.4001,
      "step": 47450
    },
    {
      "epoch": 10.171453064723533,
      "grad_norm": 0.024147769436240196,
      "learning_rate": 6.438062580368625e-06,
      "loss": 0.0016,
      "step": 47460
    },
    {
      "epoch": 10.173596228032576,
      "grad_norm": 0.0014405065448954701,
      "learning_rate": 6.4352050292899e-06,
      "loss": 0.0013,
      "step": 47470
    },
    {
      "epoch": 10.17573939134162,
      "grad_norm": 0.0009989365935325623,
      "learning_rate": 6.432347478211174e-06,
      "loss": 0.2862,
      "step": 47480
    },
    {
      "epoch": 10.177882554650665,
      "grad_norm": 0.11078063398599625,
      "learning_rate": 6.4294899271324484e-06,
      "loss": 0.322,
      "step": 47490
    },
    {
      "epoch": 10.180025717959708,
      "grad_norm": 0.004994944203644991,
      "learning_rate": 6.426632376053723e-06,
      "loss": 0.1354,
      "step": 47500
    },
    {
      "epoch": 10.182168881268753,
      "grad_norm": 0.14000719785690308,
      "learning_rate": 6.423774824974996e-06,
      "loss": 0.0024,
      "step": 47510
    },
    {
      "epoch": 10.184312044577798,
      "grad_norm": 0.09075918793678284,
      "learning_rate": 6.420917273896271e-06,
      "loss": 0.1421,
      "step": 47520
    },
    {
      "epoch": 10.18645520788684,
      "grad_norm": 0.14057813584804535,
      "learning_rate": 6.418059722817546e-06,
      "loss": 0.0012,
      "step": 47530
    },
    {
      "epoch": 10.188598371195885,
      "grad_norm": 0.00329726398922503,
      "learning_rate": 6.415202171738821e-06,
      "loss": 0.0006,
      "step": 47540
    },
    {
      "epoch": 10.19074153450493,
      "grad_norm": 0.05768107250332832,
      "learning_rate": 6.4123446206600945e-06,
      "loss": 0.0018,
      "step": 47550
    },
    {
      "epoch": 10.192884697813973,
      "grad_norm": 0.0038339239545166492,
      "learning_rate": 6.409487069581369e-06,
      "loss": 0.4546,
      "step": 47560
    },
    {
      "epoch": 10.195027861123018,
      "grad_norm": 0.004124925471842289,
      "learning_rate": 6.406629518502644e-06,
      "loss": 0.6746,
      "step": 47570
    },
    {
      "epoch": 10.197171024432063,
      "grad_norm": 30.974674224853516,
      "learning_rate": 6.403771967423919e-06,
      "loss": 0.4622,
      "step": 47580
    },
    {
      "epoch": 10.199314187741106,
      "grad_norm": 0.04157986491918564,
      "learning_rate": 6.400914416345193e-06,
      "loss": 0.1496,
      "step": 47590
    },
    {
      "epoch": 10.20145735105015,
      "grad_norm": 0.026183608919382095,
      "learning_rate": 6.3980568652664676e-06,
      "loss": 0.1701,
      "step": 47600
    },
    {
      "epoch": 10.203600514359195,
      "grad_norm": 0.23780903220176697,
      "learning_rate": 6.3951993141877415e-06,
      "loss": 0.0016,
      "step": 47610
    },
    {
      "epoch": 10.205743677668238,
      "grad_norm": 0.29296019673347473,
      "learning_rate": 6.3923417631090154e-06,
      "loss": 0.2211,
      "step": 47620
    },
    {
      "epoch": 10.207886840977283,
      "grad_norm": 0.022978218272328377,
      "learning_rate": 6.38948421203029e-06,
      "loss": 0.0007,
      "step": 47630
    },
    {
      "epoch": 10.210030004286327,
      "grad_norm": 0.30838194489479065,
      "learning_rate": 6.386626660951565e-06,
      "loss": 0.1367,
      "step": 47640
    },
    {
      "epoch": 10.21217316759537,
      "grad_norm": 0.017812862992286682,
      "learning_rate": 6.38376910987284e-06,
      "loss": 0.1876,
      "step": 47650
    },
    {
      "epoch": 10.214316330904415,
      "grad_norm": 0.0752348005771637,
      "learning_rate": 6.380911558794114e-06,
      "loss": 0.0011,
      "step": 47660
    },
    {
      "epoch": 10.21645949421346,
      "grad_norm": 0.11325110495090485,
      "learning_rate": 6.3780540077153885e-06,
      "loss": 0.4213,
      "step": 47670
    },
    {
      "epoch": 10.218602657522503,
      "grad_norm": 0.005401929374784231,
      "learning_rate": 6.375196456636663e-06,
      "loss": 0.0026,
      "step": 47680
    },
    {
      "epoch": 10.220745820831548,
      "grad_norm": 0.06968802958726883,
      "learning_rate": 6.372338905557938e-06,
      "loss": 0.0075,
      "step": 47690
    },
    {
      "epoch": 10.222888984140592,
      "grad_norm": 0.016730891540646553,
      "learning_rate": 6.369481354479212e-06,
      "loss": 0.2508,
      "step": 47700
    },
    {
      "epoch": 10.225032147449635,
      "grad_norm": 0.016207877546548843,
      "learning_rate": 6.366623803400487e-06,
      "loss": 0.0009,
      "step": 47710
    },
    {
      "epoch": 10.22717531075868,
      "grad_norm": 0.03961733728647232,
      "learning_rate": 6.363766252321761e-06,
      "loss": 0.1949,
      "step": 47720
    },
    {
      "epoch": 10.229318474067725,
      "grad_norm": 0.03765641525387764,
      "learning_rate": 6.3609087012430346e-06,
      "loss": 0.0009,
      "step": 47730
    },
    {
      "epoch": 10.231461637376768,
      "grad_norm": 0.17652040719985962,
      "learning_rate": 6.358051150164309e-06,
      "loss": 0.001,
      "step": 47740
    },
    {
      "epoch": 10.233604800685812,
      "grad_norm": 0.024989765137434006,
      "learning_rate": 6.355193599085584e-06,
      "loss": 0.1998,
      "step": 47750
    },
    {
      "epoch": 10.235747963994857,
      "grad_norm": 22.361915588378906,
      "learning_rate": 6.352336048006859e-06,
      "loss": 0.3471,
      "step": 47760
    },
    {
      "epoch": 10.2378911273039,
      "grad_norm": 0.09268621355295181,
      "learning_rate": 6.349478496928133e-06,
      "loss": 0.331,
      "step": 47770
    },
    {
      "epoch": 10.240034290612945,
      "grad_norm": 0.09752737730741501,
      "learning_rate": 6.346620945849408e-06,
      "loss": 0.0016,
      "step": 47780
    },
    {
      "epoch": 10.24217745392199,
      "grad_norm": 0.006306179333478212,
      "learning_rate": 6.343763394770682e-06,
      "loss": 0.0011,
      "step": 47790
    },
    {
      "epoch": 10.244320617231033,
      "grad_norm": 18.521095275878906,
      "learning_rate": 6.340905843691957e-06,
      "loss": 0.3324,
      "step": 47800
    },
    {
      "epoch": 10.246463780540077,
      "grad_norm": 0.027032913640141487,
      "learning_rate": 6.338048292613231e-06,
      "loss": 0.0015,
      "step": 47810
    },
    {
      "epoch": 10.248606943849122,
      "grad_norm": 0.06610174477100372,
      "learning_rate": 6.335190741534506e-06,
      "loss": 0.0019,
      "step": 47820
    },
    {
      "epoch": 10.250750107158165,
      "grad_norm": 0.07252268493175507,
      "learning_rate": 6.33233319045578e-06,
      "loss": 0.4293,
      "step": 47830
    },
    {
      "epoch": 10.25289327046721,
      "grad_norm": 0.053040627390146255,
      "learning_rate": 6.329475639377054e-06,
      "loss": 0.0007,
      "step": 47840
    },
    {
      "epoch": 10.255036433776255,
      "grad_norm": 0.005673127248883247,
      "learning_rate": 6.3266180882983285e-06,
      "loss": 0.0007,
      "step": 47850
    },
    {
      "epoch": 10.257179597085297,
      "grad_norm": 0.15051349997520447,
      "learning_rate": 6.323760537219603e-06,
      "loss": 0.1402,
      "step": 47860
    },
    {
      "epoch": 10.259322760394342,
      "grad_norm": 0.03282010927796364,
      "learning_rate": 6.320902986140878e-06,
      "loss": 0.3901,
      "step": 47870
    },
    {
      "epoch": 10.261465923703387,
      "grad_norm": 0.19043882191181183,
      "learning_rate": 6.318045435062152e-06,
      "loss": 0.259,
      "step": 47880
    },
    {
      "epoch": 10.26360908701243,
      "grad_norm": 0.43094322085380554,
      "learning_rate": 6.315187883983427e-06,
      "loss": 0.0983,
      "step": 47890
    },
    {
      "epoch": 10.265752250321475,
      "grad_norm": 0.042200107127428055,
      "learning_rate": 6.3123303329047015e-06,
      "loss": 0.3524,
      "step": 47900
    },
    {
      "epoch": 10.26789541363052,
      "grad_norm": 1.4723634719848633,
      "learning_rate": 6.309472781825976e-06,
      "loss": 0.1103,
      "step": 47910
    },
    {
      "epoch": 10.270038576939562,
      "grad_norm": 0.0037180848885327578,
      "learning_rate": 6.30661523074725e-06,
      "loss": 0.2909,
      "step": 47920
    },
    {
      "epoch": 10.272181740248607,
      "grad_norm": 1.2508891820907593,
      "learning_rate": 6.303757679668525e-06,
      "loss": 0.0019,
      "step": 47930
    },
    {
      "epoch": 10.274324903557652,
      "grad_norm": 0.3856596350669861,
      "learning_rate": 6.300900128589799e-06,
      "loss": 0.1037,
      "step": 47940
    },
    {
      "epoch": 10.276468066866695,
      "grad_norm": 0.18170538544654846,
      "learning_rate": 6.298042577511073e-06,
      "loss": 0.0011,
      "step": 47950
    },
    {
      "epoch": 10.27861123017574,
      "grad_norm": 0.9492823481559753,
      "learning_rate": 6.295185026432348e-06,
      "loss": 0.3105,
      "step": 47960
    },
    {
      "epoch": 10.280754393484784,
      "grad_norm": 30.945981979370117,
      "learning_rate": 6.292327475353622e-06,
      "loss": 0.1273,
      "step": 47970
    },
    {
      "epoch": 10.282897556793827,
      "grad_norm": 0.0025164559483528137,
      "learning_rate": 6.289469924274897e-06,
      "loss": 0.0002,
      "step": 47980
    },
    {
      "epoch": 10.285040720102872,
      "grad_norm": 0.014930411241948605,
      "learning_rate": 6.286612373196171e-06,
      "loss": 0.0003,
      "step": 47990
    },
    {
      "epoch": 10.287183883411917,
      "grad_norm": 0.0032145476434379816,
      "learning_rate": 6.283754822117446e-06,
      "loss": 0.1905,
      "step": 48000
    },
    {
      "epoch": 10.28932704672096,
      "grad_norm": 0.01741153374314308,
      "learning_rate": 6.280897271038721e-06,
      "loss": 0.0003,
      "step": 48010
    },
    {
      "epoch": 10.291470210030004,
      "grad_norm": 0.04764767363667488,
      "learning_rate": 6.2780397199599954e-06,
      "loss": 0.0002,
      "step": 48020
    },
    {
      "epoch": 10.29361337333905,
      "grad_norm": 0.00178557971958071,
      "learning_rate": 6.275182168881269e-06,
      "loss": 0.1288,
      "step": 48030
    },
    {
      "epoch": 10.295756536648092,
      "grad_norm": 0.04638853669166565,
      "learning_rate": 6.272324617802543e-06,
      "loss": 0.3942,
      "step": 48040
    },
    {
      "epoch": 10.297899699957137,
      "grad_norm": 0.024149214848876,
      "learning_rate": 6.269467066723818e-06,
      "loss": 0.0004,
      "step": 48050
    },
    {
      "epoch": 10.300042863266182,
      "grad_norm": 0.34147775173187256,
      "learning_rate": 6.266609515645092e-06,
      "loss": 0.1634,
      "step": 48060
    },
    {
      "epoch": 10.302186026575225,
      "grad_norm": 46.957576751708984,
      "learning_rate": 6.263751964566367e-06,
      "loss": 0.3786,
      "step": 48070
    },
    {
      "epoch": 10.30432918988427,
      "grad_norm": 1.0686907768249512,
      "learning_rate": 6.2608944134876415e-06,
      "loss": 0.2326,
      "step": 48080
    },
    {
      "epoch": 10.306472353193314,
      "grad_norm": 0.017343692481517792,
      "learning_rate": 6.258036862408916e-06,
      "loss": 0.0024,
      "step": 48090
    },
    {
      "epoch": 10.308615516502357,
      "grad_norm": 0.05690714344382286,
      "learning_rate": 6.25517931133019e-06,
      "loss": 0.2473,
      "step": 48100
    },
    {
      "epoch": 10.310758679811402,
      "grad_norm": 79.41307067871094,
      "learning_rate": 6.252321760251465e-06,
      "loss": 0.0935,
      "step": 48110
    },
    {
      "epoch": 10.312901843120446,
      "grad_norm": 0.7342223525047302,
      "learning_rate": 6.24946420917274e-06,
      "loss": 0.0961,
      "step": 48120
    },
    {
      "epoch": 10.31504500642949,
      "grad_norm": 0.0016397733706980944,
      "learning_rate": 6.2466066580940146e-06,
      "loss": 0.0016,
      "step": 48130
    },
    {
      "epoch": 10.317188169738534,
      "grad_norm": 0.006749204825609922,
      "learning_rate": 6.2437491070152885e-06,
      "loss": 0.0009,
      "step": 48140
    },
    {
      "epoch": 10.319331333047579,
      "grad_norm": 0.05709407851099968,
      "learning_rate": 6.2408915559365624e-06,
      "loss": 0.1278,
      "step": 48150
    },
    {
      "epoch": 10.321474496356622,
      "grad_norm": 0.005917008500546217,
      "learning_rate": 6.238034004857837e-06,
      "loss": 0.4985,
      "step": 48160
    },
    {
      "epoch": 10.323617659665667,
      "grad_norm": 0.003793079871684313,
      "learning_rate": 6.235176453779111e-06,
      "loss": 0.0009,
      "step": 48170
    },
    {
      "epoch": 10.325760822974711,
      "grad_norm": 0.5699343085289001,
      "learning_rate": 6.232318902700386e-06,
      "loss": 0.109,
      "step": 48180
    },
    {
      "epoch": 10.327903986283754,
      "grad_norm": 0.009720878675580025,
      "learning_rate": 6.229461351621661e-06,
      "loss": 0.1669,
      "step": 48190
    },
    {
      "epoch": 10.330047149592799,
      "grad_norm": 0.0032836117316037416,
      "learning_rate": 6.2266038005429355e-06,
      "loss": 0.1058,
      "step": 48200
    },
    {
      "epoch": 10.332190312901844,
      "grad_norm": 0.1222531795501709,
      "learning_rate": 6.223746249464209e-06,
      "loss": 0.0004,
      "step": 48210
    },
    {
      "epoch": 10.334333476210887,
      "grad_norm": 0.005620164796710014,
      "learning_rate": 6.220888698385484e-06,
      "loss": 0.1273,
      "step": 48220
    },
    {
      "epoch": 10.336476639519931,
      "grad_norm": 0.004564985167235136,
      "learning_rate": 6.218031147306759e-06,
      "loss": 0.0009,
      "step": 48230
    },
    {
      "epoch": 10.338619802828976,
      "grad_norm": 23.562711715698242,
      "learning_rate": 6.215173596228034e-06,
      "loss": 0.2987,
      "step": 48240
    },
    {
      "epoch": 10.34076296613802,
      "grad_norm": 0.02345154993236065,
      "learning_rate": 6.2123160451493085e-06,
      "loss": 0.4204,
      "step": 48250
    },
    {
      "epoch": 10.342906129447064,
      "grad_norm": 0.6904876232147217,
      "learning_rate": 6.2094584940705816e-06,
      "loss": 0.2716,
      "step": 48260
    },
    {
      "epoch": 10.345049292756109,
      "grad_norm": 0.014881912618875504,
      "learning_rate": 6.206600942991856e-06,
      "loss": 0.0992,
      "step": 48270
    },
    {
      "epoch": 10.347192456065152,
      "grad_norm": 0.5043994784355164,
      "learning_rate": 6.20374339191313e-06,
      "loss": 0.0009,
      "step": 48280
    },
    {
      "epoch": 10.349335619374196,
      "grad_norm": 0.011557821184396744,
      "learning_rate": 6.200885840834405e-06,
      "loss": 0.001,
      "step": 48290
    },
    {
      "epoch": 10.351478782683241,
      "grad_norm": 0.0030329464934766293,
      "learning_rate": 6.19802828975568e-06,
      "loss": 0.2219,
      "step": 48300
    },
    {
      "epoch": 10.353621945992284,
      "grad_norm": 0.003873965935781598,
      "learning_rate": 6.195170738676955e-06,
      "loss": 0.0002,
      "step": 48310
    },
    {
      "epoch": 10.355765109301329,
      "grad_norm": 0.05604344606399536,
      "learning_rate": 6.192313187598229e-06,
      "loss": 0.0013,
      "step": 48320
    },
    {
      "epoch": 10.357908272610374,
      "grad_norm": 27.02155303955078,
      "learning_rate": 6.189455636519503e-06,
      "loss": 0.7792,
      "step": 48330
    },
    {
      "epoch": 10.360051435919416,
      "grad_norm": 0.037804506719112396,
      "learning_rate": 6.186598085440778e-06,
      "loss": 0.4805,
      "step": 48340
    },
    {
      "epoch": 10.362194599228461,
      "grad_norm": 6.144318103790283,
      "learning_rate": 6.183740534362053e-06,
      "loss": 0.004,
      "step": 48350
    },
    {
      "epoch": 10.364337762537506,
      "grad_norm": 0.0276956744492054,
      "learning_rate": 6.180882983283326e-06,
      "loss": 0.2312,
      "step": 48360
    },
    {
      "epoch": 10.366480925846549,
      "grad_norm": 0.047082457691431046,
      "learning_rate": 6.178025432204601e-06,
      "loss": 0.0011,
      "step": 48370
    },
    {
      "epoch": 10.368624089155594,
      "grad_norm": 0.3865435719490051,
      "learning_rate": 6.1751678811258755e-06,
      "loss": 0.001,
      "step": 48380
    },
    {
      "epoch": 10.370767252464638,
      "grad_norm": 0.0029840278439223766,
      "learning_rate": 6.17231033004715e-06,
      "loss": 0.1556,
      "step": 48390
    },
    {
      "epoch": 10.372910415773681,
      "grad_norm": 0.0035917814821004868,
      "learning_rate": 6.169452778968424e-06,
      "loss": 0.1704,
      "step": 48400
    },
    {
      "epoch": 10.375053579082726,
      "grad_norm": 0.01906769350171089,
      "learning_rate": 6.166595227889699e-06,
      "loss": 0.1907,
      "step": 48410
    },
    {
      "epoch": 10.37719674239177,
      "grad_norm": 0.03955472260713577,
      "learning_rate": 6.163737676810974e-06,
      "loss": 0.1477,
      "step": 48420
    },
    {
      "epoch": 10.379339905700814,
      "grad_norm": 34.06617736816406,
      "learning_rate": 6.1608801257322485e-06,
      "loss": 0.3628,
      "step": 48430
    },
    {
      "epoch": 10.381483069009859,
      "grad_norm": 345.71588134765625,
      "learning_rate": 6.1580225746535224e-06,
      "loss": 0.0298,
      "step": 48440
    },
    {
      "epoch": 10.383626232318903,
      "grad_norm": 0.6914671063423157,
      "learning_rate": 6.155165023574797e-06,
      "loss": 0.0026,
      "step": 48450
    },
    {
      "epoch": 10.385769395627946,
      "grad_norm": 0.025659961625933647,
      "learning_rate": 6.152307472496072e-06,
      "loss": 0.2244,
      "step": 48460
    },
    {
      "epoch": 10.387912558936991,
      "grad_norm": 0.037630822509527206,
      "learning_rate": 6.149449921417345e-06,
      "loss": 0.2198,
      "step": 48470
    },
    {
      "epoch": 10.390055722246036,
      "grad_norm": 0.007950942032039165,
      "learning_rate": 6.14659237033862e-06,
      "loss": 0.3313,
      "step": 48480
    },
    {
      "epoch": 10.392198885555079,
      "grad_norm": 0.003174984361976385,
      "learning_rate": 6.143734819259895e-06,
      "loss": 0.4186,
      "step": 48490
    },
    {
      "epoch": 10.394342048864123,
      "grad_norm": 25.33264923095703,
      "learning_rate": 6.140877268181169e-06,
      "loss": 0.2405,
      "step": 48500
    },
    {
      "epoch": 10.396485212173168,
      "grad_norm": 0.07116342335939407,
      "learning_rate": 6.138019717102443e-06,
      "loss": 0.3615,
      "step": 48510
    },
    {
      "epoch": 10.398628375482211,
      "grad_norm": 20.88184356689453,
      "learning_rate": 6.135162166023718e-06,
      "loss": 0.2932,
      "step": 48520
    },
    {
      "epoch": 10.400771538791256,
      "grad_norm": 0.002851929748430848,
      "learning_rate": 6.132304614944993e-06,
      "loss": 0.1218,
      "step": 48530
    },
    {
      "epoch": 10.4029147021003,
      "grad_norm": 0.0021510061342269182,
      "learning_rate": 6.129447063866268e-06,
      "loss": 0.1187,
      "step": 48540
    },
    {
      "epoch": 10.405057865409344,
      "grad_norm": 0.0017259469022974372,
      "learning_rate": 6.126589512787542e-06,
      "loss": 0.1857,
      "step": 48550
    },
    {
      "epoch": 10.407201028718388,
      "grad_norm": 0.5920785069465637,
      "learning_rate": 6.123731961708816e-06,
      "loss": 0.0025,
      "step": 48560
    },
    {
      "epoch": 10.409344192027433,
      "grad_norm": 0.6581560373306274,
      "learning_rate": 6.120874410630091e-06,
      "loss": 0.1863,
      "step": 48570
    },
    {
      "epoch": 10.411487355336476,
      "grad_norm": 0.0017356674652546644,
      "learning_rate": 6.118016859551364e-06,
      "loss": 0.0007,
      "step": 48580
    },
    {
      "epoch": 10.41363051864552,
      "grad_norm": 0.1416180431842804,
      "learning_rate": 6.115159308472639e-06,
      "loss": 0.8425,
      "step": 48590
    },
    {
      "epoch": 10.415773681954565,
      "grad_norm": 0.13843891024589539,
      "learning_rate": 6.112301757393914e-06,
      "loss": 0.1889,
      "step": 48600
    },
    {
      "epoch": 10.417916845263608,
      "grad_norm": 0.017087651416659355,
      "learning_rate": 6.1094442063151885e-06,
      "loss": 0.1735,
      "step": 48610
    },
    {
      "epoch": 10.420060008572653,
      "grad_norm": 0.02548551745712757,
      "learning_rate": 6.1065866552364625e-06,
      "loss": 0.0013,
      "step": 48620
    },
    {
      "epoch": 10.422203171881698,
      "grad_norm": 0.02272837795317173,
      "learning_rate": 6.103729104157737e-06,
      "loss": 0.0006,
      "step": 48630
    },
    {
      "epoch": 10.42434633519074,
      "grad_norm": 0.041429705917835236,
      "learning_rate": 6.100871553079012e-06,
      "loss": 0.1724,
      "step": 48640
    },
    {
      "epoch": 10.426489498499786,
      "grad_norm": 17.622453689575195,
      "learning_rate": 6.098014002000287e-06,
      "loss": 0.5276,
      "step": 48650
    },
    {
      "epoch": 10.42863266180883,
      "grad_norm": 0.0033771940506994724,
      "learning_rate": 6.095156450921561e-06,
      "loss": 0.4131,
      "step": 48660
    },
    {
      "epoch": 10.430775825117873,
      "grad_norm": 0.07944599539041519,
      "learning_rate": 6.0922988998428355e-06,
      "loss": 0.3341,
      "step": 48670
    },
    {
      "epoch": 10.432918988426918,
      "grad_norm": 0.21560348570346832,
      "learning_rate": 6.08944134876411e-06,
      "loss": 0.002,
      "step": 48680
    },
    {
      "epoch": 10.435062151735963,
      "grad_norm": 0.1532246172428131,
      "learning_rate": 6.086583797685383e-06,
      "loss": 0.0018,
      "step": 48690
    },
    {
      "epoch": 10.437205315045006,
      "grad_norm": 0.17931778728961945,
      "learning_rate": 6.083726246606658e-06,
      "loss": 0.1702,
      "step": 48700
    },
    {
      "epoch": 10.43934847835405,
      "grad_norm": 0.002035250887274742,
      "learning_rate": 6.080868695527933e-06,
      "loss": 0.2867,
      "step": 48710
    },
    {
      "epoch": 10.441491641663095,
      "grad_norm": 19.095661163330078,
      "learning_rate": 6.078011144449208e-06,
      "loss": 0.3301,
      "step": 48720
    },
    {
      "epoch": 10.443634804972138,
      "grad_norm": 0.0033064947929233313,
      "learning_rate": 6.075153593370482e-06,
      "loss": 0.0012,
      "step": 48730
    },
    {
      "epoch": 10.445777968281183,
      "grad_norm": 0.00222987262532115,
      "learning_rate": 6.072296042291756e-06,
      "loss": 0.1591,
      "step": 48740
    },
    {
      "epoch": 10.447921131590228,
      "grad_norm": 0.03064671903848648,
      "learning_rate": 6.069438491213031e-06,
      "loss": 0.001,
      "step": 48750
    },
    {
      "epoch": 10.45006429489927,
      "grad_norm": 0.36261430382728577,
      "learning_rate": 6.066580940134306e-06,
      "loss": 0.0025,
      "step": 48760
    },
    {
      "epoch": 10.452207458208315,
      "grad_norm": 0.03487922251224518,
      "learning_rate": 6.06372338905558e-06,
      "loss": 0.0009,
      "step": 48770
    },
    {
      "epoch": 10.45435062151736,
      "grad_norm": 0.0018883576849475503,
      "learning_rate": 6.060865837976855e-06,
      "loss": 0.0003,
      "step": 48780
    },
    {
      "epoch": 10.456493784826403,
      "grad_norm": 0.0034950152039527893,
      "learning_rate": 6.0580082868981286e-06,
      "loss": 0.0001,
      "step": 48790
    },
    {
      "epoch": 10.458636948135448,
      "grad_norm": 0.03254197910428047,
      "learning_rate": 6.0551507358194025e-06,
      "loss": 0.1622,
      "step": 48800
    },
    {
      "epoch": 10.460780111444492,
      "grad_norm": 18.99525260925293,
      "learning_rate": 6.052293184740677e-06,
      "loss": 0.4495,
      "step": 48810
    },
    {
      "epoch": 10.462923274753535,
      "grad_norm": 0.0026395937893539667,
      "learning_rate": 6.049435633661952e-06,
      "loss": 0.0005,
      "step": 48820
    },
    {
      "epoch": 10.46506643806258,
      "grad_norm": 0.04556219279766083,
      "learning_rate": 6.046578082583227e-06,
      "loss": 0.2628,
      "step": 48830
    },
    {
      "epoch": 10.467209601371625,
      "grad_norm": 68.65130615234375,
      "learning_rate": 6.043720531504501e-06,
      "loss": 0.3514,
      "step": 48840
    },
    {
      "epoch": 10.469352764680668,
      "grad_norm": 0.18615153431892395,
      "learning_rate": 6.0408629804257755e-06,
      "loss": 0.1298,
      "step": 48850
    },
    {
      "epoch": 10.471495927989713,
      "grad_norm": 0.053230296820402145,
      "learning_rate": 6.03800542934705e-06,
      "loss": 0.2258,
      "step": 48860
    },
    {
      "epoch": 10.473639091298757,
      "grad_norm": 28.999351501464844,
      "learning_rate": 6.035147878268325e-06,
      "loss": 0.1411,
      "step": 48870
    },
    {
      "epoch": 10.4757822546078,
      "grad_norm": 0.0023781261406838894,
      "learning_rate": 6.032290327189599e-06,
      "loss": 0.0003,
      "step": 48880
    },
    {
      "epoch": 10.477925417916845,
      "grad_norm": 0.0016447579255327582,
      "learning_rate": 6.029432776110874e-06,
      "loss": 0.2672,
      "step": 48890
    },
    {
      "epoch": 10.48006858122589,
      "grad_norm": 0.0016056797467172146,
      "learning_rate": 6.026575225032148e-06,
      "loss": 0.3136,
      "step": 48900
    },
    {
      "epoch": 10.482211744534933,
      "grad_norm": 0.0014866456622257829,
      "learning_rate": 6.023717673953422e-06,
      "loss": 0.0018,
      "step": 48910
    },
    {
      "epoch": 10.484354907843978,
      "grad_norm": 0.0017243020702153444,
      "learning_rate": 6.020860122874696e-06,
      "loss": 0.0017,
      "step": 48920
    },
    {
      "epoch": 10.486498071153022,
      "grad_norm": 0.07916194945573807,
      "learning_rate": 6.018002571795971e-06,
      "loss": 0.0005,
      "step": 48930
    },
    {
      "epoch": 10.488641234462065,
      "grad_norm": 0.34817492961883545,
      "learning_rate": 6.015145020717246e-06,
      "loss": 0.2067,
      "step": 48940
    },
    {
      "epoch": 10.49078439777111,
      "grad_norm": 0.3037826716899872,
      "learning_rate": 6.01228746963852e-06,
      "loss": 0.2166,
      "step": 48950
    },
    {
      "epoch": 10.492927561080155,
      "grad_norm": 0.0033566849306225777,
      "learning_rate": 6.009429918559795e-06,
      "loss": 0.1093,
      "step": 48960
    },
    {
      "epoch": 10.495070724389198,
      "grad_norm": 0.0022968347184360027,
      "learning_rate": 6.0065723674810694e-06,
      "loss": 0.3802,
      "step": 48970
    },
    {
      "epoch": 10.497213887698242,
      "grad_norm": 0.008974102325737476,
      "learning_rate": 6.003714816402344e-06,
      "loss": 0.1349,
      "step": 48980
    },
    {
      "epoch": 10.499357051007287,
      "grad_norm": 0.2870391011238098,
      "learning_rate": 6.000857265323618e-06,
      "loss": 0.1344,
      "step": 48990
    },
    {
      "epoch": 10.50150021431633,
      "grad_norm": 0.0008192052482627332,
      "learning_rate": 5.997999714244893e-06,
      "loss": 0.1826,
      "step": 49000
    },
    {
      "epoch": 10.503643377625375,
      "grad_norm": 0.07215095311403275,
      "learning_rate": 5.995142163166167e-06,
      "loss": 0.0007,
      "step": 49010
    },
    {
      "epoch": 10.50578654093442,
      "grad_norm": 0.0013731354847550392,
      "learning_rate": 5.992284612087441e-06,
      "loss": 0.1316,
      "step": 49020
    },
    {
      "epoch": 10.507929704243463,
      "grad_norm": 0.001443652668967843,
      "learning_rate": 5.9894270610087156e-06,
      "loss": 0.0001,
      "step": 49030
    },
    {
      "epoch": 10.510072867552507,
      "grad_norm": 42.292274475097656,
      "learning_rate": 5.98656950992999e-06,
      "loss": 0.4585,
      "step": 49040
    },
    {
      "epoch": 10.512216030861552,
      "grad_norm": 0.12595319747924805,
      "learning_rate": 5.983711958851265e-06,
      "loss": 0.3679,
      "step": 49050
    },
    {
      "epoch": 10.514359194170595,
      "grad_norm": 0.00879970844835043,
      "learning_rate": 5.980854407772539e-06,
      "loss": 0.3079,
      "step": 49060
    },
    {
      "epoch": 10.51650235747964,
      "grad_norm": 0.024180656298995018,
      "learning_rate": 5.977996856693814e-06,
      "loss": 0.0006,
      "step": 49070
    },
    {
      "epoch": 10.518645520788684,
      "grad_norm": 0.37044817209243774,
      "learning_rate": 5.975139305615089e-06,
      "loss": 0.0022,
      "step": 49080
    },
    {
      "epoch": 10.520788684097727,
      "grad_norm": 0.006162718869745731,
      "learning_rate": 5.972281754536363e-06,
      "loss": 0.1752,
      "step": 49090
    },
    {
      "epoch": 10.522931847406772,
      "grad_norm": 0.0034568365663290024,
      "learning_rate": 5.969424203457638e-06,
      "loss": 0.0012,
      "step": 49100
    },
    {
      "epoch": 10.525075010715817,
      "grad_norm": 0.0029206755571067333,
      "learning_rate": 5.966566652378912e-06,
      "loss": 0.0007,
      "step": 49110
    },
    {
      "epoch": 10.52721817402486,
      "grad_norm": 32.9863166809082,
      "learning_rate": 5.963709101300186e-06,
      "loss": 0.1949,
      "step": 49120
    },
    {
      "epoch": 10.529361337333905,
      "grad_norm": 0.11618717014789581,
      "learning_rate": 5.96085155022146e-06,
      "loss": 0.0012,
      "step": 49130
    },
    {
      "epoch": 10.53150450064295,
      "grad_norm": 0.003123903414234519,
      "learning_rate": 5.957993999142735e-06,
      "loss": 0.0009,
      "step": 49140
    },
    {
      "epoch": 10.533647663951992,
      "grad_norm": 0.004009762313216925,
      "learning_rate": 5.9551364480640095e-06,
      "loss": 0.001,
      "step": 49150
    },
    {
      "epoch": 10.535790827261037,
      "grad_norm": 0.006176946219056845,
      "learning_rate": 5.952278896985284e-06,
      "loss": 0.1615,
      "step": 49160
    },
    {
      "epoch": 10.537933990570082,
      "grad_norm": 0.0028123788069933653,
      "learning_rate": 5.949421345906559e-06,
      "loss": 0.1747,
      "step": 49170
    },
    {
      "epoch": 10.540077153879125,
      "grad_norm": 22.825597763061523,
      "learning_rate": 5.946563794827833e-06,
      "loss": 0.3034,
      "step": 49180
    },
    {
      "epoch": 10.54222031718817,
      "grad_norm": 0.003893905784934759,
      "learning_rate": 5.943706243749108e-06,
      "loss": 0.0008,
      "step": 49190
    },
    {
      "epoch": 10.544363480497214,
      "grad_norm": 0.010776021517813206,
      "learning_rate": 5.9408486926703825e-06,
      "loss": 0.0005,
      "step": 49200
    },
    {
      "epoch": 10.546506643806257,
      "grad_norm": 0.0028046933002769947,
      "learning_rate": 5.937991141591657e-06,
      "loss": 0.0003,
      "step": 49210
    },
    {
      "epoch": 10.548649807115302,
      "grad_norm": 0.015023460611701012,
      "learning_rate": 5.93513359051293e-06,
      "loss": 0.0007,
      "step": 49220
    },
    {
      "epoch": 10.550792970424347,
      "grad_norm": 47.36464309692383,
      "learning_rate": 5.932276039434205e-06,
      "loss": 0.2777,
      "step": 49230
    },
    {
      "epoch": 10.55293613373339,
      "grad_norm": 0.0050346157513558865,
      "learning_rate": 5.92941848835548e-06,
      "loss": 0.28,
      "step": 49240
    },
    {
      "epoch": 10.555079297042434,
      "grad_norm": 0.0764726996421814,
      "learning_rate": 5.926560937276754e-06,
      "loss": 0.0021,
      "step": 49250
    },
    {
      "epoch": 10.557222460351479,
      "grad_norm": 0.014807472936809063,
      "learning_rate": 5.923703386198029e-06,
      "loss": 0.2687,
      "step": 49260
    },
    {
      "epoch": 10.559365623660522,
      "grad_norm": 0.016021277755498886,
      "learning_rate": 5.920845835119303e-06,
      "loss": 0.2634,
      "step": 49270
    },
    {
      "epoch": 10.561508786969567,
      "grad_norm": 0.06302233785390854,
      "learning_rate": 5.917988284040578e-06,
      "loss": 0.0018,
      "step": 49280
    },
    {
      "epoch": 10.563651950278611,
      "grad_norm": 0.0017126197926700115,
      "learning_rate": 5.915130732961852e-06,
      "loss": 0.1748,
      "step": 49290
    },
    {
      "epoch": 10.565795113587654,
      "grad_norm": 0.04623893275856972,
      "learning_rate": 5.912273181883127e-06,
      "loss": 0.1608,
      "step": 49300
    },
    {
      "epoch": 10.5679382768967,
      "grad_norm": 23.283798217773438,
      "learning_rate": 5.909415630804402e-06,
      "loss": 0.1274,
      "step": 49310
    },
    {
      "epoch": 10.570081440205744,
      "grad_norm": 0.6845731139183044,
      "learning_rate": 5.906558079725676e-06,
      "loss": 0.0016,
      "step": 49320
    },
    {
      "epoch": 10.572224603514789,
      "grad_norm": 0.2948490083217621,
      "learning_rate": 5.9037005286469495e-06,
      "loss": 0.4453,
      "step": 49330
    },
    {
      "epoch": 10.574367766823832,
      "grad_norm": 0.019819999113678932,
      "learning_rate": 5.900842977568224e-06,
      "loss": 0.2108,
      "step": 49340
    },
    {
      "epoch": 10.576510930132876,
      "grad_norm": 0.15289156138896942,
      "learning_rate": 5.897985426489499e-06,
      "loss": 0.1717,
      "step": 49350
    },
    {
      "epoch": 10.578654093441921,
      "grad_norm": 0.014055597595870495,
      "learning_rate": 5.895127875410773e-06,
      "loss": 0.0009,
      "step": 49360
    },
    {
      "epoch": 10.580797256750964,
      "grad_norm": 0.001082449802197516,
      "learning_rate": 5.892270324332048e-06,
      "loss": 0.0005,
      "step": 49370
    },
    {
      "epoch": 10.582940420060009,
      "grad_norm": 0.002146820304915309,
      "learning_rate": 5.8894127732533225e-06,
      "loss": 0.5221,
      "step": 49380
    },
    {
      "epoch": 10.585083583369054,
      "grad_norm": 0.16043323278427124,
      "learning_rate": 5.886555222174597e-06,
      "loss": 0.1758,
      "step": 49390
    },
    {
      "epoch": 10.587226746678096,
      "grad_norm": 0.08682063966989517,
      "learning_rate": 5.883697671095871e-06,
      "loss": 0.0008,
      "step": 49400
    },
    {
      "epoch": 10.589369909987141,
      "grad_norm": 0.06431227177381516,
      "learning_rate": 5.880840120017146e-06,
      "loss": 0.1921,
      "step": 49410
    },
    {
      "epoch": 10.591513073296186,
      "grad_norm": 0.010348320007324219,
      "learning_rate": 5.877982568938421e-06,
      "loss": 0.6852,
      "step": 49420
    },
    {
      "epoch": 10.593656236605229,
      "grad_norm": 0.005467511713504791,
      "learning_rate": 5.8751250178596956e-06,
      "loss": 0.0019,
      "step": 49430
    },
    {
      "epoch": 10.595799399914274,
      "grad_norm": 0.06504396349191666,
      "learning_rate": 5.872267466780969e-06,
      "loss": 0.0008,
      "step": 49440
    },
    {
      "epoch": 10.597942563223318,
      "grad_norm": 0.0033816248178482056,
      "learning_rate": 5.869409915702243e-06,
      "loss": 0.0906,
      "step": 49450
    },
    {
      "epoch": 10.600085726532361,
      "grad_norm": 28.881141662597656,
      "learning_rate": 5.866552364623518e-06,
      "loss": 0.3053,
      "step": 49460
    },
    {
      "epoch": 10.602228889841406,
      "grad_norm": 0.00588396517559886,
      "learning_rate": 5.863694813544792e-06,
      "loss": 0.0003,
      "step": 49470
    },
    {
      "epoch": 10.60437205315045,
      "grad_norm": 0.0019248779863119125,
      "learning_rate": 5.860837262466067e-06,
      "loss": 0.3279,
      "step": 49480
    },
    {
      "epoch": 10.606515216459494,
      "grad_norm": 36.43517303466797,
      "learning_rate": 5.857979711387342e-06,
      "loss": 0.5697,
      "step": 49490
    },
    {
      "epoch": 10.608658379768539,
      "grad_norm": 0.10433666408061981,
      "learning_rate": 5.8551221603086164e-06,
      "loss": 0.3064,
      "step": 49500
    },
    {
      "epoch": 10.610801543077583,
      "grad_norm": 0.007380576804280281,
      "learning_rate": 5.85226460922989e-06,
      "loss": 0.342,
      "step": 49510
    },
    {
      "epoch": 10.612944706386626,
      "grad_norm": 0.3628985285758972,
      "learning_rate": 5.849407058151165e-06,
      "loss": 0.2439,
      "step": 49520
    },
    {
      "epoch": 10.615087869695671,
      "grad_norm": 15.305450439453125,
      "learning_rate": 5.84654950707244e-06,
      "loss": 0.0044,
      "step": 49530
    },
    {
      "epoch": 10.617231033004716,
      "grad_norm": 0.008827129378914833,
      "learning_rate": 5.843691955993715e-06,
      "loss": 0.0011,
      "step": 49540
    },
    {
      "epoch": 10.619374196313759,
      "grad_norm": 0.7442054152488708,
      "learning_rate": 5.840834404914988e-06,
      "loss": 0.0023,
      "step": 49550
    },
    {
      "epoch": 10.621517359622803,
      "grad_norm": 0.006626805756241083,
      "learning_rate": 5.8379768538362625e-06,
      "loss": 0.15,
      "step": 49560
    },
    {
      "epoch": 10.623660522931848,
      "grad_norm": 0.07836990058422089,
      "learning_rate": 5.835119302757537e-06,
      "loss": 0.0003,
      "step": 49570
    },
    {
      "epoch": 10.625803686240891,
      "grad_norm": 0.008386977948248386,
      "learning_rate": 5.832261751678811e-06,
      "loss": 0.4774,
      "step": 49580
    },
    {
      "epoch": 10.627946849549936,
      "grad_norm": 0.10537996888160706,
      "learning_rate": 5.829404200600086e-06,
      "loss": 0.0006,
      "step": 49590
    },
    {
      "epoch": 10.63009001285898,
      "grad_norm": 0.03619534522294998,
      "learning_rate": 5.826546649521361e-06,
      "loss": 0.0017,
      "step": 49600
    },
    {
      "epoch": 10.632233176168024,
      "grad_norm": 0.008418443612754345,
      "learning_rate": 5.823689098442636e-06,
      "loss": 0.3439,
      "step": 49610
    },
    {
      "epoch": 10.634376339477068,
      "grad_norm": 0.009105228818953037,
      "learning_rate": 5.8208315473639095e-06,
      "loss": 0.1075,
      "step": 49620
    },
    {
      "epoch": 10.636519502786113,
      "grad_norm": 0.04958846792578697,
      "learning_rate": 5.817973996285184e-06,
      "loss": 0.2215,
      "step": 49630
    },
    {
      "epoch": 10.638662666095156,
      "grad_norm": 0.005328831262886524,
      "learning_rate": 5.815116445206459e-06,
      "loss": 0.3274,
      "step": 49640
    },
    {
      "epoch": 10.6408058294042,
      "grad_norm": 0.06087511405348778,
      "learning_rate": 5.812258894127732e-06,
      "loss": 0.0016,
      "step": 49650
    },
    {
      "epoch": 10.642948992713245,
      "grad_norm": 1.0038113594055176,
      "learning_rate": 5.809401343049007e-06,
      "loss": 0.2205,
      "step": 49660
    },
    {
      "epoch": 10.645092156022288,
      "grad_norm": 0.00868697464466095,
      "learning_rate": 5.806543791970282e-06,
      "loss": 0.284,
      "step": 49670
    },
    {
      "epoch": 10.647235319331333,
      "grad_norm": 0.14117828011512756,
      "learning_rate": 5.8036862408915565e-06,
      "loss": 0.0018,
      "step": 49680
    },
    {
      "epoch": 10.649378482640378,
      "grad_norm": 0.0688837319612503,
      "learning_rate": 5.80082868981283e-06,
      "loss": 0.1655,
      "step": 49690
    },
    {
      "epoch": 10.65152164594942,
      "grad_norm": 0.2195781022310257,
      "learning_rate": 5.797971138734105e-06,
      "loss": 0.0021,
      "step": 49700
    },
    {
      "epoch": 10.653664809258466,
      "grad_norm": 2729.724609375,
      "learning_rate": 5.79511358765538e-06,
      "loss": 0.1538,
      "step": 49710
    },
    {
      "epoch": 10.65580797256751,
      "grad_norm": 0.004411912988871336,
      "learning_rate": 5.792256036576655e-06,
      "loss": 0.0013,
      "step": 49720
    },
    {
      "epoch": 10.657951135876553,
      "grad_norm": 0.041968442499637604,
      "learning_rate": 5.789398485497929e-06,
      "loss": 0.3009,
      "step": 49730
    },
    {
      "epoch": 10.660094299185598,
      "grad_norm": 0.0031491792760789394,
      "learning_rate": 5.786540934419203e-06,
      "loss": 0.0003,
      "step": 49740
    },
    {
      "epoch": 10.662237462494643,
      "grad_norm": 0.04767036810517311,
      "learning_rate": 5.783683383340478e-06,
      "loss": 0.2429,
      "step": 49750
    },
    {
      "epoch": 10.664380625803686,
      "grad_norm": 29.28851890563965,
      "learning_rate": 5.780825832261751e-06,
      "loss": 0.6425,
      "step": 49760
    },
    {
      "epoch": 10.66652378911273,
      "grad_norm": 18.507877349853516,
      "learning_rate": 5.777968281183026e-06,
      "loss": 0.1529,
      "step": 49770
    },
    {
      "epoch": 10.668666952421775,
      "grad_norm": 0.003641868708655238,
      "learning_rate": 5.775110730104301e-06,
      "loss": 0.3191,
      "step": 49780
    },
    {
      "epoch": 10.670810115730818,
      "grad_norm": 0.5405552983283997,
      "learning_rate": 5.772253179025576e-06,
      "loss": 0.1733,
      "step": 49790
    },
    {
      "epoch": 10.672953279039863,
      "grad_norm": 0.013633601367473602,
      "learning_rate": 5.7693956279468495e-06,
      "loss": 0.002,
      "step": 49800
    },
    {
      "epoch": 10.675096442348908,
      "grad_norm": 0.004168214276432991,
      "learning_rate": 5.766538076868124e-06,
      "loss": 0.4053,
      "step": 49810
    },
    {
      "epoch": 10.67723960565795,
      "grad_norm": 16.584909439086914,
      "learning_rate": 5.763680525789399e-06,
      "loss": 0.445,
      "step": 49820
    },
    {
      "epoch": 10.679382768966995,
      "grad_norm": 0.1861218363046646,
      "learning_rate": 5.760822974710674e-06,
      "loss": 0.0042,
      "step": 49830
    },
    {
      "epoch": 10.68152593227604,
      "grad_norm": 0.05709820240736008,
      "learning_rate": 5.757965423631949e-06,
      "loss": 0.0027,
      "step": 49840
    },
    {
      "epoch": 10.683669095585083,
      "grad_norm": 0.1403479129076004,
      "learning_rate": 5.7551078725532226e-06,
      "loss": 0.001,
      "step": 49850
    },
    {
      "epoch": 10.685812258894128,
      "grad_norm": 0.006418508943170309,
      "learning_rate": 5.752250321474497e-06,
      "loss": 0.0012,
      "step": 49860
    },
    {
      "epoch": 10.687955422203173,
      "grad_norm": 0.02267525903880596,
      "learning_rate": 5.74939277039577e-06,
      "loss": 0.1535,
      "step": 49870
    },
    {
      "epoch": 10.690098585512215,
      "grad_norm": 0.05090964585542679,
      "learning_rate": 5.746535219317045e-06,
      "loss": 0.1742,
      "step": 49880
    },
    {
      "epoch": 10.69224174882126,
      "grad_norm": 0.012360618449747562,
      "learning_rate": 5.74367766823832e-06,
      "loss": 0.1784,
      "step": 49890
    },
    {
      "epoch": 10.694384912130305,
      "grad_norm": 34.9688606262207,
      "learning_rate": 5.740820117159595e-06,
      "loss": 0.0916,
      "step": 49900
    },
    {
      "epoch": 10.696528075439348,
      "grad_norm": 0.0064076934941112995,
      "learning_rate": 5.7379625660808695e-06,
      "loss": 0.0005,
      "step": 49910
    },
    {
      "epoch": 10.698671238748393,
      "grad_norm": 19.54803466796875,
      "learning_rate": 5.7351050150021434e-06,
      "loss": 0.228,
      "step": 49920
    },
    {
      "epoch": 10.700814402057437,
      "grad_norm": 17.54738426208496,
      "learning_rate": 5.732247463923418e-06,
      "loss": 0.1422,
      "step": 49930
    },
    {
      "epoch": 10.70295756536648,
      "grad_norm": 0.19898028671741486,
      "learning_rate": 5.729389912844693e-06,
      "loss": 0.5968,
      "step": 49940
    },
    {
      "epoch": 10.705100728675525,
      "grad_norm": 0.00624631904065609,
      "learning_rate": 5.726532361765968e-06,
      "loss": 0.0042,
      "step": 49950
    },
    {
      "epoch": 10.70724389198457,
      "grad_norm": 0.028657203540205956,
      "learning_rate": 5.723674810687242e-06,
      "loss": 0.0012,
      "step": 49960
    },
    {
      "epoch": 10.709387055293613,
      "grad_norm": 0.023336006328463554,
      "learning_rate": 5.720817259608516e-06,
      "loss": 0.0007,
      "step": 49970
    },
    {
      "epoch": 10.711530218602658,
      "grad_norm": 7.190393924713135,
      "learning_rate": 5.71795970852979e-06,
      "loss": 0.1453,
      "step": 49980
    },
    {
      "epoch": 10.713673381911702,
      "grad_norm": 0.10902395844459534,
      "learning_rate": 5.715102157451064e-06,
      "loss": 0.1982,
      "step": 49990
    },
    {
      "epoch": 10.715816545220745,
      "grad_norm": 0.12288153916597366,
      "learning_rate": 5.712244606372339e-06,
      "loss": 0.1727,
      "step": 50000
    },
    {
      "epoch": 10.71795970852979,
      "grad_norm": 0.061240214854478836,
      "learning_rate": 5.709387055293614e-06,
      "loss": 0.2789,
      "step": 50010
    },
    {
      "epoch": 10.720102871838835,
      "grad_norm": 0.17127127945423126,
      "learning_rate": 5.706529504214889e-06,
      "loss": 0.0038,
      "step": 50020
    },
    {
      "epoch": 10.722246035147878,
      "grad_norm": 35.37010192871094,
      "learning_rate": 5.703671953136163e-06,
      "loss": 0.1084,
      "step": 50030
    },
    {
      "epoch": 10.724389198456922,
      "grad_norm": 0.004633346106857061,
      "learning_rate": 5.700814402057437e-06,
      "loss": 0.0499,
      "step": 50040
    },
    {
      "epoch": 10.726532361765967,
      "grad_norm": 69.7342529296875,
      "learning_rate": 5.697956850978712e-06,
      "loss": 0.4587,
      "step": 50050
    },
    {
      "epoch": 10.72867552507501,
      "grad_norm": 0.41317808628082275,
      "learning_rate": 5.695099299899987e-06,
      "loss": 0.0012,
      "step": 50060
    },
    {
      "epoch": 10.730818688384055,
      "grad_norm": 0.03360363841056824,
      "learning_rate": 5.692241748821261e-06,
      "loss": 0.1907,
      "step": 50070
    },
    {
      "epoch": 10.7329618516931,
      "grad_norm": 0.006720628123730421,
      "learning_rate": 5.689384197742535e-06,
      "loss": 0.0015,
      "step": 50080
    },
    {
      "epoch": 10.735105015002143,
      "grad_norm": 0.05175274610519409,
      "learning_rate": 5.6865266466638095e-06,
      "loss": 0.0244,
      "step": 50090
    },
    {
      "epoch": 10.737248178311187,
      "grad_norm": 0.00407924922183156,
      "learning_rate": 5.6836690955850835e-06,
      "loss": 0.034,
      "step": 50100
    },
    {
      "epoch": 10.739391341620232,
      "grad_norm": 20.387615203857422,
      "learning_rate": 5.680811544506358e-06,
      "loss": 0.0859,
      "step": 50110
    },
    {
      "epoch": 10.741534504929275,
      "grad_norm": 0.0013673831708729267,
      "learning_rate": 5.677953993427633e-06,
      "loss": 0.1658,
      "step": 50120
    },
    {
      "epoch": 10.74367766823832,
      "grad_norm": 0.0293127428740263,
      "learning_rate": 5.675096442348908e-06,
      "loss": 0.1671,
      "step": 50130
    },
    {
      "epoch": 10.745820831547364,
      "grad_norm": 0.003118464956060052,
      "learning_rate": 5.672238891270182e-06,
      "loss": 0.1018,
      "step": 50140
    },
    {
      "epoch": 10.747963994856407,
      "grad_norm": 0.0019668734166771173,
      "learning_rate": 5.6693813401914565e-06,
      "loss": 0.0004,
      "step": 50150
    },
    {
      "epoch": 10.750107158165452,
      "grad_norm": 0.024587571620941162,
      "learning_rate": 5.666523789112731e-06,
      "loss": 0.0005,
      "step": 50160
    },
    {
      "epoch": 10.752250321474497,
      "grad_norm": 0.0014613964594900608,
      "learning_rate": 5.663666238034006e-06,
      "loss": 0.0002,
      "step": 50170
    },
    {
      "epoch": 10.75439348478354,
      "grad_norm": 0.0021863719448447227,
      "learning_rate": 5.66080868695528e-06,
      "loss": 0.1391,
      "step": 50180
    },
    {
      "epoch": 10.756536648092585,
      "grad_norm": 23.443559646606445,
      "learning_rate": 5.657951135876554e-06,
      "loss": 0.3197,
      "step": 50190
    },
    {
      "epoch": 10.75867981140163,
      "grad_norm": 0.006259922403842211,
      "learning_rate": 5.655093584797829e-06,
      "loss": 0.0009,
      "step": 50200
    },
    {
      "epoch": 10.760822974710672,
      "grad_norm": 0.025812329724431038,
      "learning_rate": 5.652236033719103e-06,
      "loss": 0.5513,
      "step": 50210
    },
    {
      "epoch": 10.762966138019717,
      "grad_norm": 0.02184579335153103,
      "learning_rate": 5.649378482640377e-06,
      "loss": 0.4523,
      "step": 50220
    },
    {
      "epoch": 10.765109301328762,
      "grad_norm": 0.03070305846631527,
      "learning_rate": 5.646520931561652e-06,
      "loss": 0.3861,
      "step": 50230
    },
    {
      "epoch": 10.767252464637805,
      "grad_norm": 0.007091649807989597,
      "learning_rate": 5.643663380482927e-06,
      "loss": 0.2292,
      "step": 50240
    },
    {
      "epoch": 10.76939562794685,
      "grad_norm": 0.03636760264635086,
      "learning_rate": 5.640805829404201e-06,
      "loss": 0.4518,
      "step": 50250
    },
    {
      "epoch": 10.771538791255894,
      "grad_norm": 0.6089093089103699,
      "learning_rate": 5.637948278325476e-06,
      "loss": 0.0015,
      "step": 50260
    },
    {
      "epoch": 10.773681954564937,
      "grad_norm": 0.04381326586008072,
      "learning_rate": 5.63509072724675e-06,
      "loss": 0.6986,
      "step": 50270
    },
    {
      "epoch": 10.775825117873982,
      "grad_norm": 0.16073457896709442,
      "learning_rate": 5.632233176168025e-06,
      "loss": 0.5905,
      "step": 50280
    },
    {
      "epoch": 10.777968281183027,
      "grad_norm": 23.286035537719727,
      "learning_rate": 5.629375625089299e-06,
      "loss": 0.2412,
      "step": 50290
    },
    {
      "epoch": 10.78011144449207,
      "grad_norm": 0.02953553944826126,
      "learning_rate": 5.626518074010573e-06,
      "loss": 0.002,
      "step": 50300
    },
    {
      "epoch": 10.782254607801114,
      "grad_norm": 0.018952330574393272,
      "learning_rate": 5.623660522931848e-06,
      "loss": 0.2284,
      "step": 50310
    },
    {
      "epoch": 10.784397771110159,
      "grad_norm": 17.634916305541992,
      "learning_rate": 5.620802971853122e-06,
      "loss": 0.3851,
      "step": 50320
    },
    {
      "epoch": 10.786540934419202,
      "grad_norm": 18.764057159423828,
      "learning_rate": 5.6179454207743965e-06,
      "loss": 0.2494,
      "step": 50330
    },
    {
      "epoch": 10.788684097728247,
      "grad_norm": 145.974853515625,
      "learning_rate": 5.615087869695671e-06,
      "loss": 0.4064,
      "step": 50340
    },
    {
      "epoch": 10.790827261037292,
      "grad_norm": 0.2527451813220978,
      "learning_rate": 5.612230318616946e-06,
      "loss": 0.421,
      "step": 50350
    },
    {
      "epoch": 10.792970424346334,
      "grad_norm": 0.0564151331782341,
      "learning_rate": 5.60937276753822e-06,
      "loss": 0.2043,
      "step": 50360
    },
    {
      "epoch": 10.79511358765538,
      "grad_norm": 0.10065331310033798,
      "learning_rate": 5.606515216459495e-06,
      "loss": 0.5243,
      "step": 50370
    },
    {
      "epoch": 10.797256750964424,
      "grad_norm": 0.045719314366579056,
      "learning_rate": 5.6036576653807696e-06,
      "loss": 0.0024,
      "step": 50380
    },
    {
      "epoch": 10.799399914273467,
      "grad_norm": 0.04931149259209633,
      "learning_rate": 5.600800114302044e-06,
      "loss": 0.2935,
      "step": 50390
    },
    {
      "epoch": 10.801543077582512,
      "grad_norm": 0.1055828407406807,
      "learning_rate": 5.597942563223317e-06,
      "loss": 0.0044,
      "step": 50400
    },
    {
      "epoch": 10.803686240891556,
      "grad_norm": 0.025127669796347618,
      "learning_rate": 5.595085012144592e-06,
      "loss": 0.1307,
      "step": 50410
    },
    {
      "epoch": 10.8058294042006,
      "grad_norm": 0.005063049960881472,
      "learning_rate": 5.592227461065867e-06,
      "loss": 0.1549,
      "step": 50420
    },
    {
      "epoch": 10.807972567509644,
      "grad_norm": 0.007831763476133347,
      "learning_rate": 5.589369909987141e-06,
      "loss": 0.1926,
      "step": 50430
    },
    {
      "epoch": 10.810115730818689,
      "grad_norm": 38.02047348022461,
      "learning_rate": 5.586512358908416e-06,
      "loss": 0.1423,
      "step": 50440
    },
    {
      "epoch": 10.812258894127732,
      "grad_norm": 0.06707867980003357,
      "learning_rate": 5.5836548078296904e-06,
      "loss": 0.0019,
      "step": 50450
    },
    {
      "epoch": 10.814402057436777,
      "grad_norm": 131.26080322265625,
      "learning_rate": 5.580797256750965e-06,
      "loss": 0.5171,
      "step": 50460
    },
    {
      "epoch": 10.816545220745821,
      "grad_norm": 49.63138198852539,
      "learning_rate": 5.577939705672239e-06,
      "loss": 0.3383,
      "step": 50470
    },
    {
      "epoch": 10.818688384054864,
      "grad_norm": 0.010553702712059021,
      "learning_rate": 5.575082154593514e-06,
      "loss": 0.0014,
      "step": 50480
    },
    {
      "epoch": 10.820831547363909,
      "grad_norm": 0.007057979702949524,
      "learning_rate": 5.572224603514789e-06,
      "loss": 0.0005,
      "step": 50490
    },
    {
      "epoch": 10.822974710672954,
      "grad_norm": 0.0063917734660208225,
      "learning_rate": 5.5693670524360635e-06,
      "loss": 0.0418,
      "step": 50500
    },
    {
      "epoch": 10.825117873981997,
      "grad_norm": 0.0061931186355650425,
      "learning_rate": 5.5665095013573366e-06,
      "loss": 0.0039,
      "step": 50510
    },
    {
      "epoch": 10.827261037291041,
      "grad_norm": 0.0030564009211957455,
      "learning_rate": 5.563651950278611e-06,
      "loss": 0.1836,
      "step": 50520
    },
    {
      "epoch": 10.829404200600086,
      "grad_norm": 0.0040489742532372475,
      "learning_rate": 5.560794399199886e-06,
      "loss": 0.2088,
      "step": 50530
    },
    {
      "epoch": 10.831547363909129,
      "grad_norm": 0.03643042594194412,
      "learning_rate": 5.55793684812116e-06,
      "loss": 0.0006,
      "step": 50540
    },
    {
      "epoch": 10.833690527218174,
      "grad_norm": 0.006242800038307905,
      "learning_rate": 5.555079297042435e-06,
      "loss": 0.0003,
      "step": 50550
    },
    {
      "epoch": 10.835833690527219,
      "grad_norm": 0.039843712002038956,
      "learning_rate": 5.55222174596371e-06,
      "loss": 0.1439,
      "step": 50560
    },
    {
      "epoch": 10.837976853836262,
      "grad_norm": 0.004543929360806942,
      "learning_rate": 5.549364194884984e-06,
      "loss": 0.001,
      "step": 50570
    },
    {
      "epoch": 10.840120017145306,
      "grad_norm": 0.06760843098163605,
      "learning_rate": 5.546506643806258e-06,
      "loss": 0.422,
      "step": 50580
    },
    {
      "epoch": 10.842263180454351,
      "grad_norm": 0.022817617282271385,
      "learning_rate": 5.543649092727533e-06,
      "loss": 0.0004,
      "step": 50590
    },
    {
      "epoch": 10.844406343763394,
      "grad_norm": 0.001936638611368835,
      "learning_rate": 5.540791541648808e-06,
      "loss": 0.1357,
      "step": 50600
    },
    {
      "epoch": 10.846549507072439,
      "grad_norm": 0.0067183454521000385,
      "learning_rate": 5.537933990570083e-06,
      "loss": 0.1031,
      "step": 50610
    },
    {
      "epoch": 10.848692670381483,
      "grad_norm": 0.04333396255970001,
      "learning_rate": 5.535076439491356e-06,
      "loss": 0.2339,
      "step": 50620
    },
    {
      "epoch": 10.850835833690526,
      "grad_norm": 0.01151504646986723,
      "learning_rate": 5.5322188884126305e-06,
      "loss": 0.118,
      "step": 50630
    },
    {
      "epoch": 10.852978996999571,
      "grad_norm": 675.1954345703125,
      "learning_rate": 5.529361337333905e-06,
      "loss": 0.2165,
      "step": 50640
    },
    {
      "epoch": 10.855122160308616,
      "grad_norm": 0.0019100611098110676,
      "learning_rate": 5.526503786255179e-06,
      "loss": 0.1966,
      "step": 50650
    },
    {
      "epoch": 10.857265323617659,
      "grad_norm": 0.09261270612478256,
      "learning_rate": 5.523646235176454e-06,
      "loss": 0.026,
      "step": 50660
    },
    {
      "epoch": 10.859408486926704,
      "grad_norm": 0.001206491608172655,
      "learning_rate": 5.520788684097729e-06,
      "loss": 0.0284,
      "step": 50670
    },
    {
      "epoch": 10.861551650235748,
      "grad_norm": 41.00638198852539,
      "learning_rate": 5.5179311330190035e-06,
      "loss": 0.1996,
      "step": 50680
    },
    {
      "epoch": 10.863694813544793,
      "grad_norm": 0.045535579323768616,
      "learning_rate": 5.515073581940278e-06,
      "loss": 0.1458,
      "step": 50690
    },
    {
      "epoch": 10.865837976853836,
      "grad_norm": 0.001052571926265955,
      "learning_rate": 5.512216030861552e-06,
      "loss": 0.0006,
      "step": 50700
    },
    {
      "epoch": 10.86798114016288,
      "grad_norm": 26.384780883789062,
      "learning_rate": 5.509358479782827e-06,
      "loss": 0.1538,
      "step": 50710
    },
    {
      "epoch": 10.870124303471925,
      "grad_norm": 19.71722412109375,
      "learning_rate": 5.506500928704102e-06,
      "loss": 0.3218,
      "step": 50720
    },
    {
      "epoch": 10.872267466780968,
      "grad_norm": 0.36516332626342773,
      "learning_rate": 5.503643377625375e-06,
      "loss": 0.1642,
      "step": 50730
    },
    {
      "epoch": 10.874410630090013,
      "grad_norm": 0.11781220138072968,
      "learning_rate": 5.50078582654665e-06,
      "loss": 0.1994,
      "step": 50740
    },
    {
      "epoch": 10.876553793399058,
      "grad_norm": 0.35497114062309265,
      "learning_rate": 5.497928275467924e-06,
      "loss": 0.0078,
      "step": 50750
    },
    {
      "epoch": 10.8786969567081,
      "grad_norm": 0.0016626863507553935,
      "learning_rate": 5.495070724389199e-06,
      "loss": 0.0016,
      "step": 50760
    },
    {
      "epoch": 10.880840120017146,
      "grad_norm": 0.005397929344326258,
      "learning_rate": 5.492213173310473e-06,
      "loss": 0.0974,
      "step": 50770
    },
    {
      "epoch": 10.88298328332619,
      "grad_norm": 0.00856425054371357,
      "learning_rate": 5.489355622231748e-06,
      "loss": 0.0003,
      "step": 50780
    },
    {
      "epoch": 10.885126446635233,
      "grad_norm": 36.82328414916992,
      "learning_rate": 5.486498071153023e-06,
      "loss": 0.48,
      "step": 50790
    },
    {
      "epoch": 10.887269609944278,
      "grad_norm": 0.034133777022361755,
      "learning_rate": 5.483640520074297e-06,
      "loss": 0.0202,
      "step": 50800
    },
    {
      "epoch": 10.889412773253323,
      "grad_norm": 36.536888122558594,
      "learning_rate": 5.480782968995571e-06,
      "loss": 0.0939,
      "step": 50810
    },
    {
      "epoch": 10.891555936562366,
      "grad_norm": 21.840261459350586,
      "learning_rate": 5.477925417916846e-06,
      "loss": 0.5401,
      "step": 50820
    },
    {
      "epoch": 10.89369909987141,
      "grad_norm": 0.061269428580999374,
      "learning_rate": 5.47506786683812e-06,
      "loss": 0.0055,
      "step": 50830
    },
    {
      "epoch": 10.895842263180455,
      "grad_norm": 0.035644493997097015,
      "learning_rate": 5.472210315759394e-06,
      "loss": 0.1263,
      "step": 50840
    },
    {
      "epoch": 10.897985426489498,
      "grad_norm": 8.904890060424805,
      "learning_rate": 5.469352764680669e-06,
      "loss": 0.0035,
      "step": 50850
    },
    {
      "epoch": 10.900128589798543,
      "grad_norm": 0.0015534256817772985,
      "learning_rate": 5.4664952136019435e-06,
      "loss": 0.5005,
      "step": 50860
    },
    {
      "epoch": 10.902271753107588,
      "grad_norm": 0.009875278919935226,
      "learning_rate": 5.463637662523218e-06,
      "loss": 0.0017,
      "step": 50870
    },
    {
      "epoch": 10.90441491641663,
      "grad_norm": 0.007382796611636877,
      "learning_rate": 5.460780111444492e-06,
      "loss": 0.0003,
      "step": 50880
    },
    {
      "epoch": 10.906558079725675,
      "grad_norm": 0.010387817397713661,
      "learning_rate": 5.457922560365767e-06,
      "loss": 0.0494,
      "step": 50890
    },
    {
      "epoch": 10.90870124303472,
      "grad_norm": 0.0017140121199190617,
      "learning_rate": 5.455065009287042e-06,
      "loss": 0.1886,
      "step": 50900
    },
    {
      "epoch": 10.910844406343763,
      "grad_norm": 0.0024867800530046225,
      "learning_rate": 5.4522074582083166e-06,
      "loss": 0.1313,
      "step": 50910
    },
    {
      "epoch": 10.912987569652808,
      "grad_norm": 0.05605961009860039,
      "learning_rate": 5.4493499071295905e-06,
      "loss": 0.4365,
      "step": 50920
    },
    {
      "epoch": 10.915130732961853,
      "grad_norm": 20.539358139038086,
      "learning_rate": 5.446492356050865e-06,
      "loss": 0.3112,
      "step": 50930
    },
    {
      "epoch": 10.917273896270896,
      "grad_norm": 80.3593521118164,
      "learning_rate": 5.443634804972139e-06,
      "loss": 0.3528,
      "step": 50940
    },
    {
      "epoch": 10.91941705957994,
      "grad_norm": 0.04381100833415985,
      "learning_rate": 5.440777253893413e-06,
      "loss": 0.0007,
      "step": 50950
    },
    {
      "epoch": 10.921560222888985,
      "grad_norm": 0.03891107067465782,
      "learning_rate": 5.437919702814688e-06,
      "loss": 0.2278,
      "step": 50960
    },
    {
      "epoch": 10.923703386198028,
      "grad_norm": 0.009385429322719574,
      "learning_rate": 5.435062151735963e-06,
      "loss": 0.1504,
      "step": 50970
    },
    {
      "epoch": 10.925846549507073,
      "grad_norm": 0.5790009498596191,
      "learning_rate": 5.4322046006572374e-06,
      "loss": 0.0016,
      "step": 50980
    },
    {
      "epoch": 10.927989712816117,
      "grad_norm": 0.06412076950073242,
      "learning_rate": 5.429347049578511e-06,
      "loss": 0.0021,
      "step": 50990
    },
    {
      "epoch": 10.93013287612516,
      "grad_norm": 0.48246750235557556,
      "learning_rate": 5.426489498499786e-06,
      "loss": 0.161,
      "step": 51000
    },
    {
      "epoch": 10.932276039434205,
      "grad_norm": 0.09019005298614502,
      "learning_rate": 5.423631947421061e-06,
      "loss": 0.0009,
      "step": 51010
    },
    {
      "epoch": 10.93441920274325,
      "grad_norm": 0.005354174412786961,
      "learning_rate": 5.420774396342336e-06,
      "loss": 0.0471,
      "step": 51020
    },
    {
      "epoch": 10.936562366052293,
      "grad_norm": 0.04562009871006012,
      "learning_rate": 5.41791684526361e-06,
      "loss": 0.0015,
      "step": 51030
    },
    {
      "epoch": 10.938705529361338,
      "grad_norm": 0.020345890894532204,
      "learning_rate": 5.415059294184884e-06,
      "loss": 0.1563,
      "step": 51040
    },
    {
      "epoch": 10.940848692670382,
      "grad_norm": 0.0013976552290841937,
      "learning_rate": 5.412201743106158e-06,
      "loss": 0.2139,
      "step": 51050
    },
    {
      "epoch": 10.942991855979425,
      "grad_norm": 0.002445872873067856,
      "learning_rate": 5.409344192027432e-06,
      "loss": 0.1513,
      "step": 51060
    },
    {
      "epoch": 10.94513501928847,
      "grad_norm": 0.010752505622804165,
      "learning_rate": 5.406486640948707e-06,
      "loss": 0.0003,
      "step": 51070
    },
    {
      "epoch": 10.947278182597515,
      "grad_norm": 56.394065856933594,
      "learning_rate": 5.403629089869982e-06,
      "loss": 0.5652,
      "step": 51080
    },
    {
      "epoch": 10.949421345906558,
      "grad_norm": 0.11040748655796051,
      "learning_rate": 5.400771538791257e-06,
      "loss": 0.0008,
      "step": 51090
    },
    {
      "epoch": 10.951564509215602,
      "grad_norm": 0.06934613734483719,
      "learning_rate": 5.3979139877125305e-06,
      "loss": 0.7857,
      "step": 51100
    },
    {
      "epoch": 10.953707672524647,
      "grad_norm": 0.2125711888074875,
      "learning_rate": 5.395056436633805e-06,
      "loss": 0.0021,
      "step": 51110
    },
    {
      "epoch": 10.95585083583369,
      "grad_norm": 0.021774210035800934,
      "learning_rate": 5.39219888555508e-06,
      "loss": 0.0018,
      "step": 51120
    },
    {
      "epoch": 10.957993999142735,
      "grad_norm": 0.07176420837640762,
      "learning_rate": 5.389341334476355e-06,
      "loss": 0.1472,
      "step": 51130
    },
    {
      "epoch": 10.96013716245178,
      "grad_norm": 21.615337371826172,
      "learning_rate": 5.386483783397629e-06,
      "loss": 0.4595,
      "step": 51140
    },
    {
      "epoch": 10.962280325760823,
      "grad_norm": 0.012185828760266304,
      "learning_rate": 5.3836262323189035e-06,
      "loss": 0.1318,
      "step": 51150
    },
    {
      "epoch": 10.964423489069867,
      "grad_norm": 0.03383050113916397,
      "learning_rate": 5.3807686812401775e-06,
      "loss": 0.002,
      "step": 51160
    },
    {
      "epoch": 10.966566652378912,
      "grad_norm": 0.019400006160140038,
      "learning_rate": 5.377911130161451e-06,
      "loss": 0.0038,
      "step": 51170
    },
    {
      "epoch": 10.968709815687955,
      "grad_norm": 0.0006715171621181071,
      "learning_rate": 5.375053579082726e-06,
      "loss": 0.0185,
      "step": 51180
    },
    {
      "epoch": 10.970852978997,
      "grad_norm": 0.0012890632497146726,
      "learning_rate": 5.372196028004001e-06,
      "loss": 0.5345,
      "step": 51190
    },
    {
      "epoch": 10.972996142306044,
      "grad_norm": 0.07651351392269135,
      "learning_rate": 5.369338476925276e-06,
      "loss": 0.3194,
      "step": 51200
    },
    {
      "epoch": 10.975139305615087,
      "grad_norm": 0.002218609442934394,
      "learning_rate": 5.36648092584655e-06,
      "loss": 0.1383,
      "step": 51210
    },
    {
      "epoch": 10.977282468924132,
      "grad_norm": 0.08330011367797852,
      "learning_rate": 5.3636233747678244e-06,
      "loss": 0.2123,
      "step": 51220
    },
    {
      "epoch": 10.979425632233177,
      "grad_norm": 25.292789459228516,
      "learning_rate": 5.360765823689099e-06,
      "loss": 0.3549,
      "step": 51230
    },
    {
      "epoch": 10.98156879554222,
      "grad_norm": 0.004294723272323608,
      "learning_rate": 5.357908272610374e-06,
      "loss": 0.1655,
      "step": 51240
    },
    {
      "epoch": 10.983711958851265,
      "grad_norm": 0.4095415771007538,
      "learning_rate": 5.355050721531648e-06,
      "loss": 0.1301,
      "step": 51250
    },
    {
      "epoch": 10.98585512216031,
      "grad_norm": 0.276353120803833,
      "learning_rate": 5.352193170452922e-06,
      "loss": 0.0013,
      "step": 51260
    },
    {
      "epoch": 10.987998285469352,
      "grad_norm": 22.21834945678711,
      "learning_rate": 5.349335619374197e-06,
      "loss": 0.5111,
      "step": 51270
    },
    {
      "epoch": 10.990141448778397,
      "grad_norm": 0.5097258687019348,
      "learning_rate": 5.3464780682954705e-06,
      "loss": 0.1745,
      "step": 51280
    },
    {
      "epoch": 10.992284612087442,
      "grad_norm": 0.17909683287143707,
      "learning_rate": 5.343620517216745e-06,
      "loss": 0.2594,
      "step": 51290
    },
    {
      "epoch": 10.994427775396485,
      "grad_norm": 0.07592350989580154,
      "learning_rate": 5.34076296613802e-06,
      "loss": 0.3713,
      "step": 51300
    },
    {
      "epoch": 10.99657093870553,
      "grad_norm": 0.0027025945018976927,
      "learning_rate": 5.337905415059295e-06,
      "loss": 0.0015,
      "step": 51310
    },
    {
      "epoch": 10.998714102014574,
      "grad_norm": 0.4477957785129547,
      "learning_rate": 5.335047863980569e-06,
      "loss": 0.2711,
      "step": 51320
    },
    {
      "epoch": 11.0,
      "eval_accuracy": 0.9573333333333334,
      "eval_f1": 0.8251366120218578,
      "eval_loss": 0.2109002321958542,
      "eval_precision": 0.8555240793201133,
      "eval_recall": 0.7968337730870713,
      "eval_runtime": 703.2935,
      "eval_samples_per_second": 4.266,
      "eval_steps_per_second": 1.422,
      "step": 51326
    },
    {
      "epoch": 11.000857265323617,
      "grad_norm": 0.23959000408649445,
      "learning_rate": 5.3321903129018436e-06,
      "loss": 0.0322,
      "step": 51330
    },
    {
      "epoch": 11.003000428632662,
      "grad_norm": 0.017798179760575294,
      "learning_rate": 5.329332761823118e-06,
      "loss": 0.001,
      "step": 51340
    },
    {
      "epoch": 11.005143591941707,
      "grad_norm": 0.006427764892578125,
      "learning_rate": 5.326475210744393e-06,
      "loss": 0.3439,
      "step": 51350
    },
    {
      "epoch": 11.00728675525075,
      "grad_norm": 0.005048506427556276,
      "learning_rate": 5.323617659665667e-06,
      "loss": 0.0021,
      "step": 51360
    },
    {
      "epoch": 11.009429918559794,
      "grad_norm": 87.9131851196289,
      "learning_rate": 5.320760108586941e-06,
      "loss": 0.4874,
      "step": 51370
    },
    {
      "epoch": 11.011573081868839,
      "grad_norm": 0.012109123170375824,
      "learning_rate": 5.317902557508216e-06,
      "loss": 0.0006,
      "step": 51380
    },
    {
      "epoch": 11.013716245177882,
      "grad_norm": 0.0217012707144022,
      "learning_rate": 5.31504500642949e-06,
      "loss": 0.0032,
      "step": 51390
    },
    {
      "epoch": 11.015859408486927,
      "grad_norm": 0.014388770796358585,
      "learning_rate": 5.3121874553507645e-06,
      "loss": 0.0011,
      "step": 51400
    },
    {
      "epoch": 11.018002571795972,
      "grad_norm": 0.007927118800580502,
      "learning_rate": 5.309329904272039e-06,
      "loss": 0.0003,
      "step": 51410
    },
    {
      "epoch": 11.020145735105014,
      "grad_norm": 0.022220930084586143,
      "learning_rate": 5.306472353193314e-06,
      "loss": 0.1163,
      "step": 51420
    },
    {
      "epoch": 11.02228889841406,
      "grad_norm": 0.021968776360154152,
      "learning_rate": 5.303614802114588e-06,
      "loss": 0.0005,
      "step": 51430
    },
    {
      "epoch": 11.024432061723104,
      "grad_norm": 0.016956152394413948,
      "learning_rate": 5.300757251035863e-06,
      "loss": 0.0004,
      "step": 51440
    },
    {
      "epoch": 11.026575225032147,
      "grad_norm": 0.2293911576271057,
      "learning_rate": 5.2978996999571375e-06,
      "loss": 0.0025,
      "step": 51450
    },
    {
      "epoch": 11.028718388341192,
      "grad_norm": 0.022946331650018692,
      "learning_rate": 5.295042148878412e-06,
      "loss": 0.2647,
      "step": 51460
    },
    {
      "epoch": 11.030861551650236,
      "grad_norm": 0.29859620332717896,
      "learning_rate": 5.292184597799687e-06,
      "loss": 0.164,
      "step": 51470
    },
    {
      "epoch": 11.03300471495928,
      "grad_norm": 0.029642602428793907,
      "learning_rate": 5.28932704672096e-06,
      "loss": 0.0008,
      "step": 51480
    },
    {
      "epoch": 11.035147878268324,
      "grad_norm": 0.012005538679659367,
      "learning_rate": 5.286469495642235e-06,
      "loss": 0.0005,
      "step": 51490
    },
    {
      "epoch": 11.037291041577369,
      "grad_norm": 0.07617247849702835,
      "learning_rate": 5.283611944563509e-06,
      "loss": 0.3171,
      "step": 51500
    },
    {
      "epoch": 11.039434204886412,
      "grad_norm": 0.02574537880718708,
      "learning_rate": 5.280754393484784e-06,
      "loss": 0.2213,
      "step": 51510
    },
    {
      "epoch": 11.041577368195457,
      "grad_norm": 0.0436757393181324,
      "learning_rate": 5.277896842406058e-06,
      "loss": 0.0006,
      "step": 51520
    },
    {
      "epoch": 11.043720531504501,
      "grad_norm": 441.1175842285156,
      "learning_rate": 5.275039291327333e-06,
      "loss": 0.2586,
      "step": 51530
    },
    {
      "epoch": 11.045863694813544,
      "grad_norm": 0.010435225442051888,
      "learning_rate": 5.272181740248608e-06,
      "loss": 0.1797,
      "step": 51540
    },
    {
      "epoch": 11.048006858122589,
      "grad_norm": 0.008839861489832401,
      "learning_rate": 5.269324189169882e-06,
      "loss": 0.1525,
      "step": 51550
    },
    {
      "epoch": 11.050150021431634,
      "grad_norm": 0.04966927319765091,
      "learning_rate": 5.266466638091157e-06,
      "loss": 0.3098,
      "step": 51560
    },
    {
      "epoch": 11.052293184740677,
      "grad_norm": 0.03756370022892952,
      "learning_rate": 5.263609087012431e-06,
      "loss": 0.0005,
      "step": 51570
    },
    {
      "epoch": 11.054436348049721,
      "grad_norm": 0.00807972066104412,
      "learning_rate": 5.260751535933706e-06,
      "loss": 0.131,
      "step": 51580
    },
    {
      "epoch": 11.056579511358766,
      "grad_norm": 0.036827102303504944,
      "learning_rate": 5.257893984854979e-06,
      "loss": 0.0011,
      "step": 51590
    },
    {
      "epoch": 11.05872267466781,
      "grad_norm": 0.008239001035690308,
      "learning_rate": 5.255036433776254e-06,
      "loss": 0.2473,
      "step": 51600
    },
    {
      "epoch": 11.060865837976854,
      "grad_norm": 0.005098686087876558,
      "learning_rate": 5.252178882697529e-06,
      "loss": 0.1496,
      "step": 51610
    },
    {
      "epoch": 11.063009001285899,
      "grad_norm": 0.01165330596268177,
      "learning_rate": 5.249321331618803e-06,
      "loss": 0.4063,
      "step": 51620
    },
    {
      "epoch": 11.065152164594942,
      "grad_norm": 0.015663035213947296,
      "learning_rate": 5.2464637805400775e-06,
      "loss": 0.0011,
      "step": 51630
    },
    {
      "epoch": 11.067295327903986,
      "grad_norm": 0.010471200570464134,
      "learning_rate": 5.243606229461352e-06,
      "loss": 0.1384,
      "step": 51640
    },
    {
      "epoch": 11.069438491213031,
      "grad_norm": 0.006318971049040556,
      "learning_rate": 5.240748678382627e-06,
      "loss": 0.0002,
      "step": 51650
    },
    {
      "epoch": 11.071581654522074,
      "grad_norm": 0.03187538683414459,
      "learning_rate": 5.237891127303901e-06,
      "loss": 0.0013,
      "step": 51660
    },
    {
      "epoch": 11.073724817831119,
      "grad_norm": 0.011339327320456505,
      "learning_rate": 5.235033576225176e-06,
      "loss": 0.1327,
      "step": 51670
    },
    {
      "epoch": 11.075867981140163,
      "grad_norm": 0.5749906897544861,
      "learning_rate": 5.2321760251464505e-06,
      "loss": 0.0012,
      "step": 51680
    },
    {
      "epoch": 11.078011144449206,
      "grad_norm": 0.02301013469696045,
      "learning_rate": 5.229318474067724e-06,
      "loss": 0.0007,
      "step": 51690
    },
    {
      "epoch": 11.080154307758251,
      "grad_norm": 1.0807745456695557,
      "learning_rate": 5.226460922988998e-06,
      "loss": 0.0015,
      "step": 51700
    },
    {
      "epoch": 11.082297471067296,
      "grad_norm": 0.002322328509762883,
      "learning_rate": 5.223603371910273e-06,
      "loss": 0.0001,
      "step": 51710
    },
    {
      "epoch": 11.084440634376339,
      "grad_norm": 0.023768575862050056,
      "learning_rate": 5.220745820831548e-06,
      "loss": 0.2525,
      "step": 51720
    },
    {
      "epoch": 11.086583797685384,
      "grad_norm": 0.008867491967976093,
      "learning_rate": 5.217888269752822e-06,
      "loss": 0.4514,
      "step": 51730
    },
    {
      "epoch": 11.088726960994428,
      "grad_norm": 0.011569627560675144,
      "learning_rate": 5.215030718674097e-06,
      "loss": 0.0006,
      "step": 51740
    },
    {
      "epoch": 11.090870124303471,
      "grad_norm": 0.006182675715535879,
      "learning_rate": 5.2121731675953714e-06,
      "loss": 0.0002,
      "step": 51750
    },
    {
      "epoch": 11.093013287612516,
      "grad_norm": 0.009762176312506199,
      "learning_rate": 5.209315616516646e-06,
      "loss": 0.0009,
      "step": 51760
    },
    {
      "epoch": 11.09515645092156,
      "grad_norm": 0.15435823798179626,
      "learning_rate": 5.20645806543792e-06,
      "loss": 0.5438,
      "step": 51770
    },
    {
      "epoch": 11.097299614230604,
      "grad_norm": 0.04543430730700493,
      "learning_rate": 5.203600514359195e-06,
      "loss": 0.0004,
      "step": 51780
    },
    {
      "epoch": 11.099442777539648,
      "grad_norm": 0.031524330377578735,
      "learning_rate": 5.20074296328047e-06,
      "loss": 0.2707,
      "step": 51790
    },
    {
      "epoch": 11.101585940848693,
      "grad_norm": 0.0013639365788549185,
      "learning_rate": 5.197885412201743e-06,
      "loss": 0.005,
      "step": 51800
    },
    {
      "epoch": 11.103729104157736,
      "grad_norm": 0.06226956844329834,
      "learning_rate": 5.1950278611230175e-06,
      "loss": 0.0007,
      "step": 51810
    },
    {
      "epoch": 11.105872267466781,
      "grad_norm": 0.006626463960856199,
      "learning_rate": 5.192170310044292e-06,
      "loss": 0.2075,
      "step": 51820
    },
    {
      "epoch": 11.108015430775826,
      "grad_norm": 0.009700716473162174,
      "learning_rate": 5.189312758965567e-06,
      "loss": 0.4967,
      "step": 51830
    },
    {
      "epoch": 11.110158594084869,
      "grad_norm": 0.019479291513562202,
      "learning_rate": 5.186455207886841e-06,
      "loss": 0.0007,
      "step": 51840
    },
    {
      "epoch": 11.112301757393913,
      "grad_norm": 0.0024449483025819063,
      "learning_rate": 5.183597656808116e-06,
      "loss": 0.3414,
      "step": 51850
    },
    {
      "epoch": 11.114444920702958,
      "grad_norm": 0.007608440704643726,
      "learning_rate": 5.1807401057293906e-06,
      "loss": 0.0002,
      "step": 51860
    },
    {
      "epoch": 11.116588084012001,
      "grad_norm": 0.045307084918022156,
      "learning_rate": 5.177882554650665e-06,
      "loss": 0.4297,
      "step": 51870
    },
    {
      "epoch": 11.118731247321046,
      "grad_norm": 0.004230293445289135,
      "learning_rate": 5.175025003571939e-06,
      "loss": 0.5016,
      "step": 51880
    },
    {
      "epoch": 11.12087441063009,
      "grad_norm": 0.6686012148857117,
      "learning_rate": 5.172167452493214e-06,
      "loss": 0.1093,
      "step": 51890
    },
    {
      "epoch": 11.123017573939133,
      "grad_norm": 0.004652931354939938,
      "learning_rate": 5.169309901414489e-06,
      "loss": 0.0011,
      "step": 51900
    },
    {
      "epoch": 11.125160737248178,
      "grad_norm": 0.05068180710077286,
      "learning_rate": 5.166452350335762e-06,
      "loss": 0.1509,
      "step": 51910
    },
    {
      "epoch": 11.127303900557223,
      "grad_norm": 17.68301773071289,
      "learning_rate": 5.163594799257037e-06,
      "loss": 0.115,
      "step": 51920
    },
    {
      "epoch": 11.129447063866266,
      "grad_norm": 0.001227581175044179,
      "learning_rate": 5.1607372481783115e-06,
      "loss": 0.1173,
      "step": 51930
    },
    {
      "epoch": 11.13159022717531,
      "grad_norm": 0.002721392549574375,
      "learning_rate": 5.157879697099586e-06,
      "loss": 0.1147,
      "step": 51940
    },
    {
      "epoch": 11.133733390484355,
      "grad_norm": 0.004404120147228241,
      "learning_rate": 5.15502214602086e-06,
      "loss": 0.4281,
      "step": 51950
    },
    {
      "epoch": 11.135876553793398,
      "grad_norm": 0.01078415010124445,
      "learning_rate": 5.152164594942135e-06,
      "loss": 0.1459,
      "step": 51960
    },
    {
      "epoch": 11.138019717102443,
      "grad_norm": 0.005048240534961224,
      "learning_rate": 5.14930704386341e-06,
      "loss": 0.0012,
      "step": 51970
    },
    {
      "epoch": 11.140162880411488,
      "grad_norm": 0.03543831780552864,
      "learning_rate": 5.1464494927846845e-06,
      "loss": 0.224,
      "step": 51980
    },
    {
      "epoch": 11.14230604372053,
      "grad_norm": 0.004074434749782085,
      "learning_rate": 5.143591941705958e-06,
      "loss": 0.0756,
      "step": 51990
    },
    {
      "epoch": 11.144449207029576,
      "grad_norm": 21.248096466064453,
      "learning_rate": 5.140734390627233e-06,
      "loss": 0.349,
      "step": 52000
    },
    {
      "epoch": 11.14659237033862,
      "grad_norm": 0.01315614115446806,
      "learning_rate": 5.137876839548507e-06,
      "loss": 0.0029,
      "step": 52010
    },
    {
      "epoch": 11.148735533647663,
      "grad_norm": 0.01483775582164526,
      "learning_rate": 5.135019288469781e-06,
      "loss": 0.0024,
      "step": 52020
    },
    {
      "epoch": 11.150878696956708,
      "grad_norm": 26.046724319458008,
      "learning_rate": 5.132161737391056e-06,
      "loss": 0.3248,
      "step": 52030
    },
    {
      "epoch": 11.153021860265753,
      "grad_norm": 0.010039574466645718,
      "learning_rate": 5.129304186312331e-06,
      "loss": 0.0004,
      "step": 52040
    },
    {
      "epoch": 11.155165023574796,
      "grad_norm": 0.02083730325102806,
      "learning_rate": 5.126446635233605e-06,
      "loss": 0.0002,
      "step": 52050
    },
    {
      "epoch": 11.15730818688384,
      "grad_norm": 0.06833011656999588,
      "learning_rate": 5.123589084154879e-06,
      "loss": 0.0081,
      "step": 52060
    },
    {
      "epoch": 11.159451350192885,
      "grad_norm": 0.0033617429435253143,
      "learning_rate": 5.120731533076154e-06,
      "loss": 0.2376,
      "step": 52070
    },
    {
      "epoch": 11.161594513501928,
      "grad_norm": 0.00480880169197917,
      "learning_rate": 5.117873981997429e-06,
      "loss": 0.0015,
      "step": 52080
    },
    {
      "epoch": 11.163737676810973,
      "grad_norm": 0.021119264885783195,
      "learning_rate": 5.115016430918704e-06,
      "loss": 0.1045,
      "step": 52090
    },
    {
      "epoch": 11.165880840120018,
      "grad_norm": 0.01472731027752161,
      "learning_rate": 5.1121588798399775e-06,
      "loss": 0.1203,
      "step": 52100
    },
    {
      "epoch": 11.16802400342906,
      "grad_norm": 0.022380318492650986,
      "learning_rate": 5.109301328761252e-06,
      "loss": 0.2179,
      "step": 52110
    },
    {
      "epoch": 11.170167166738105,
      "grad_norm": 0.05758798494935036,
      "learning_rate": 5.106443777682526e-06,
      "loss": 0.6401,
      "step": 52120
    },
    {
      "epoch": 11.17231033004715,
      "grad_norm": 0.0037988293915987015,
      "learning_rate": 5.1035862266038e-06,
      "loss": 0.1308,
      "step": 52130
    },
    {
      "epoch": 11.174453493356193,
      "grad_norm": 0.003957974258810282,
      "learning_rate": 5.100728675525075e-06,
      "loss": 0.0007,
      "step": 52140
    },
    {
      "epoch": 11.176596656665238,
      "grad_norm": 0.02687607891857624,
      "learning_rate": 5.09787112444635e-06,
      "loss": 0.0027,
      "step": 52150
    },
    {
      "epoch": 11.178739819974282,
      "grad_norm": 0.028977764770388603,
      "learning_rate": 5.0950135733676245e-06,
      "loss": 0.0005,
      "step": 52160
    },
    {
      "epoch": 11.180882983283325,
      "grad_norm": 0.006157877389341593,
      "learning_rate": 5.0921560222888984e-06,
      "loss": 0.234,
      "step": 52170
    },
    {
      "epoch": 11.18302614659237,
      "grad_norm": 0.0024687277618795633,
      "learning_rate": 5.089298471210173e-06,
      "loss": 0.0004,
      "step": 52180
    },
    {
      "epoch": 11.185169309901415,
      "grad_norm": 0.0030768392607569695,
      "learning_rate": 5.086440920131448e-06,
      "loss": 0.0007,
      "step": 52190
    },
    {
      "epoch": 11.187312473210458,
      "grad_norm": 0.005691814236342907,
      "learning_rate": 5.083583369052723e-06,
      "loss": 0.1511,
      "step": 52200
    },
    {
      "epoch": 11.189455636519503,
      "grad_norm": 47.69424057006836,
      "learning_rate": 5.080725817973997e-06,
      "loss": 0.1245,
      "step": 52210
    },
    {
      "epoch": 11.191598799828547,
      "grad_norm": 0.07450217753648758,
      "learning_rate": 5.0778682668952715e-06,
      "loss": 0.0007,
      "step": 52220
    },
    {
      "epoch": 11.19374196313759,
      "grad_norm": 0.0017983163706958294,
      "learning_rate": 5.075010715816545e-06,
      "loss": 0.0004,
      "step": 52230
    },
    {
      "epoch": 11.195885126446635,
      "grad_norm": 0.004292706493288279,
      "learning_rate": 5.072153164737819e-06,
      "loss": 0.2531,
      "step": 52240
    },
    {
      "epoch": 11.19802828975568,
      "grad_norm": 0.016115156933665276,
      "learning_rate": 5.069295613659094e-06,
      "loss": 0.1224,
      "step": 52250
    },
    {
      "epoch": 11.200171453064723,
      "grad_norm": 401.7176818847656,
      "learning_rate": 5.066438062580369e-06,
      "loss": 0.1535,
      "step": 52260
    },
    {
      "epoch": 11.202314616373767,
      "grad_norm": 0.00274792592972517,
      "learning_rate": 5.063580511501644e-06,
      "loss": 0.0005,
      "step": 52270
    },
    {
      "epoch": 11.204457779682812,
      "grad_norm": 0.024863671511411667,
      "learning_rate": 5.0607229604229176e-06,
      "loss": 0.2183,
      "step": 52280
    },
    {
      "epoch": 11.206600942991855,
      "grad_norm": 0.05348048731684685,
      "learning_rate": 5.057865409344192e-06,
      "loss": 0.1657,
      "step": 52290
    },
    {
      "epoch": 11.2087441063009,
      "grad_norm": 0.14570650458335876,
      "learning_rate": 5.055007858265467e-06,
      "loss": 0.001,
      "step": 52300
    },
    {
      "epoch": 11.210887269609945,
      "grad_norm": 0.09028014540672302,
      "learning_rate": 5.052150307186742e-06,
      "loss": 0.2999,
      "step": 52310
    },
    {
      "epoch": 11.213030432918988,
      "grad_norm": 651.0805053710938,
      "learning_rate": 5.049292756108017e-06,
      "loss": 0.2198,
      "step": 52320
    },
    {
      "epoch": 11.215173596228032,
      "grad_norm": 0.26209914684295654,
      "learning_rate": 5.046435205029291e-06,
      "loss": 0.2534,
      "step": 52330
    },
    {
      "epoch": 11.217316759537077,
      "grad_norm": 0.04204476997256279,
      "learning_rate": 5.0435776539505645e-06,
      "loss": 0.3279,
      "step": 52340
    },
    {
      "epoch": 11.21945992284612,
      "grad_norm": 0.37287437915802,
      "learning_rate": 5.0407201028718385e-06,
      "loss": 0.0013,
      "step": 52350
    },
    {
      "epoch": 11.221603086155165,
      "grad_norm": 0.12066598236560822,
      "learning_rate": 5.037862551793113e-06,
      "loss": 0.0011,
      "step": 52360
    },
    {
      "epoch": 11.22374624946421,
      "grad_norm": 0.07533172518014908,
      "learning_rate": 5.035005000714388e-06,
      "loss": 0.0007,
      "step": 52370
    },
    {
      "epoch": 11.225889412773252,
      "grad_norm": 0.016338735818862915,
      "learning_rate": 5.032147449635663e-06,
      "loss": 0.0836,
      "step": 52380
    },
    {
      "epoch": 11.228032576082297,
      "grad_norm": 0.0013083325466141105,
      "learning_rate": 5.0292898985569376e-06,
      "loss": 0.0007,
      "step": 52390
    },
    {
      "epoch": 11.230175739391342,
      "grad_norm": 0.01346469484269619,
      "learning_rate": 5.0264323474782115e-06,
      "loss": 0.2361,
      "step": 52400
    },
    {
      "epoch": 11.232318902700385,
      "grad_norm": 0.02530655823647976,
      "learning_rate": 5.023574796399486e-06,
      "loss": 0.2096,
      "step": 52410
    },
    {
      "epoch": 11.23446206600943,
      "grad_norm": 0.014918407425284386,
      "learning_rate": 5.020717245320761e-06,
      "loss": 0.3392,
      "step": 52420
    },
    {
      "epoch": 11.236605229318474,
      "grad_norm": 0.03870745748281479,
      "learning_rate": 5.017859694242036e-06,
      "loss": 0.1435,
      "step": 52430
    },
    {
      "epoch": 11.238748392627517,
      "grad_norm": 0.1536935567855835,
      "learning_rate": 5.015002143163309e-06,
      "loss": 0.0006,
      "step": 52440
    },
    {
      "epoch": 11.240891555936562,
      "grad_norm": 0.0010888158576563,
      "learning_rate": 5.012144592084584e-06,
      "loss": 0.1131,
      "step": 52450
    },
    {
      "epoch": 11.243034719245607,
      "grad_norm": 0.060952458530664444,
      "learning_rate": 5.0092870410058584e-06,
      "loss": 0.2256,
      "step": 52460
    },
    {
      "epoch": 11.24517788255465,
      "grad_norm": 0.025849873200058937,
      "learning_rate": 5.006429489927132e-06,
      "loss": 0.0006,
      "step": 52470
    },
    {
      "epoch": 11.247321045863695,
      "grad_norm": 0.0010450647678226233,
      "learning_rate": 5.003571938848407e-06,
      "loss": 0.0004,
      "step": 52480
    },
    {
      "epoch": 11.24946420917274,
      "grad_norm": 0.0010539954528212547,
      "learning_rate": 5.000714387769682e-06,
      "loss": 0.2406,
      "step": 52490
    },
    {
      "epoch": 11.251607372481782,
      "grad_norm": 16.821388244628906,
      "learning_rate": 4.997856836690957e-06,
      "loss": 0.3503,
      "step": 52500
    },
    {
      "epoch": 11.253750535790827,
      "grad_norm": 0.18546709418296814,
      "learning_rate": 4.994999285612231e-06,
      "loss": 0.002,
      "step": 52510
    },
    {
      "epoch": 11.255893699099872,
      "grad_norm": 0.8048393130302429,
      "learning_rate": 4.9921417345335046e-06,
      "loss": 0.2249,
      "step": 52520
    },
    {
      "epoch": 11.258036862408915,
      "grad_norm": 0.0019239690154790878,
      "learning_rate": 4.989284183454779e-06,
      "loss": 0.2333,
      "step": 52530
    },
    {
      "epoch": 11.26018002571796,
      "grad_norm": 0.059561748057603836,
      "learning_rate": 4.986426632376054e-06,
      "loss": 0.0006,
      "step": 52540
    },
    {
      "epoch": 11.262323189027004,
      "grad_norm": 0.053306736052036285,
      "learning_rate": 4.983569081297329e-06,
      "loss": 0.001,
      "step": 52550
    },
    {
      "epoch": 11.264466352336047,
      "grad_norm": 0.012559797614812851,
      "learning_rate": 4.980711530218603e-06,
      "loss": 0.0002,
      "step": 52560
    },
    {
      "epoch": 11.266609515645092,
      "grad_norm": 0.2230725884437561,
      "learning_rate": 4.977853979139878e-06,
      "loss": 0.0008,
      "step": 52570
    },
    {
      "epoch": 11.268752678954137,
      "grad_norm": 0.07381363213062286,
      "learning_rate": 4.9749964280611515e-06,
      "loss": 0.1804,
      "step": 52580
    },
    {
      "epoch": 11.270895842263181,
      "grad_norm": 0.001637944602407515,
      "learning_rate": 4.972138876982426e-06,
      "loss": 0.1493,
      "step": 52590
    },
    {
      "epoch": 11.273039005572224,
      "grad_norm": 0.0014126194873824716,
      "learning_rate": 4.969281325903701e-06,
      "loss": 0.0003,
      "step": 52600
    },
    {
      "epoch": 11.275182168881269,
      "grad_norm": 0.0019301441498100758,
      "learning_rate": 4.966423774824976e-06,
      "loss": 0.4584,
      "step": 52610
    },
    {
      "epoch": 11.277325332190314,
      "grad_norm": 0.001102870679460466,
      "learning_rate": 4.96356622374625e-06,
      "loss": 0.2257,
      "step": 52620
    },
    {
      "epoch": 11.279468495499357,
      "grad_norm": 24.555397033691406,
      "learning_rate": 4.960708672667524e-06,
      "loss": 0.367,
      "step": 52630
    },
    {
      "epoch": 11.281611658808401,
      "grad_norm": 0.029459943994879723,
      "learning_rate": 4.9578511215887985e-06,
      "loss": 0.0012,
      "step": 52640
    },
    {
      "epoch": 11.283754822117446,
      "grad_norm": 0.019526969641447067,
      "learning_rate": 4.954993570510073e-06,
      "loss": 0.127,
      "step": 52650
    },
    {
      "epoch": 11.28589798542649,
      "grad_norm": 0.06516937166452408,
      "learning_rate": 4.952136019431348e-06,
      "loss": 0.2926,
      "step": 52660
    },
    {
      "epoch": 11.288041148735534,
      "grad_norm": 0.04272846877574921,
      "learning_rate": 4.949278468352623e-06,
      "loss": 0.0012,
      "step": 52670
    },
    {
      "epoch": 11.290184312044579,
      "grad_norm": 0.054163858294487,
      "learning_rate": 4.946420917273897e-06,
      "loss": 0.0156,
      "step": 52680
    },
    {
      "epoch": 11.292327475353622,
      "grad_norm": 0.055824194103479385,
      "learning_rate": 4.943563366195171e-06,
      "loss": 0.1538,
      "step": 52690
    },
    {
      "epoch": 11.294470638662666,
      "grad_norm": 0.041016001254320145,
      "learning_rate": 4.9407058151164454e-06,
      "loss": 0.0056,
      "step": 52700
    },
    {
      "epoch": 11.296613801971711,
      "grad_norm": 0.07352422177791595,
      "learning_rate": 4.93784826403772e-06,
      "loss": 0.0011,
      "step": 52710
    },
    {
      "epoch": 11.298756965280754,
      "grad_norm": 0.07202626764774323,
      "learning_rate": 4.934990712958995e-06,
      "loss": 0.1836,
      "step": 52720
    },
    {
      "epoch": 11.300900128589799,
      "grad_norm": 0.05579651892185211,
      "learning_rate": 4.932133161880269e-06,
      "loss": 0.3327,
      "step": 52730
    },
    {
      "epoch": 11.303043291898843,
      "grad_norm": 60.884029388427734,
      "learning_rate": 4.929275610801544e-06,
      "loss": 0.4004,
      "step": 52740
    },
    {
      "epoch": 11.305186455207886,
      "grad_norm": 0.0006967101944610476,
      "learning_rate": 4.926418059722818e-06,
      "loss": 0.0009,
      "step": 52750
    },
    {
      "epoch": 11.307329618516931,
      "grad_norm": 0.08190163224935532,
      "learning_rate": 4.923560508644092e-06,
      "loss": 0.0006,
      "step": 52760
    },
    {
      "epoch": 11.309472781825976,
      "grad_norm": 0.0028237563092261553,
      "learning_rate": 4.920702957565367e-06,
      "loss": 0.2441,
      "step": 52770
    },
    {
      "epoch": 11.311615945135019,
      "grad_norm": 0.10881560295820236,
      "learning_rate": 4.917845406486642e-06,
      "loss": 0.0003,
      "step": 52780
    },
    {
      "epoch": 11.313759108444064,
      "grad_norm": 502.2848205566406,
      "learning_rate": 4.914987855407916e-06,
      "loss": 0.0371,
      "step": 52790
    },
    {
      "epoch": 11.315902271753108,
      "grad_norm": 0.06410227715969086,
      "learning_rate": 4.91213030432919e-06,
      "loss": 0.19,
      "step": 52800
    },
    {
      "epoch": 11.318045435062151,
      "grad_norm": 0.0011528414906933904,
      "learning_rate": 4.9092727532504646e-06,
      "loss": 0.0004,
      "step": 52810
    },
    {
      "epoch": 11.320188598371196,
      "grad_norm": 0.049112845212221146,
      "learning_rate": 4.906415202171739e-06,
      "loss": 0.0009,
      "step": 52820
    },
    {
      "epoch": 11.32233176168024,
      "grad_norm": 0.04829617589712143,
      "learning_rate": 4.903557651093014e-06,
      "loss": 0.1285,
      "step": 52830
    },
    {
      "epoch": 11.324474924989284,
      "grad_norm": 0.0005226381472311914,
      "learning_rate": 4.900700100014288e-06,
      "loss": 0.0448,
      "step": 52840
    },
    {
      "epoch": 11.326618088298329,
      "grad_norm": 0.044436264783144,
      "learning_rate": 4.897842548935563e-06,
      "loss": 0.0008,
      "step": 52850
    },
    {
      "epoch": 11.328761251607373,
      "grad_norm": 0.4459041655063629,
      "learning_rate": 4.894984997856837e-06,
      "loss": 0.1553,
      "step": 52860
    },
    {
      "epoch": 11.330904414916416,
      "grad_norm": 0.05074460804462433,
      "learning_rate": 4.8921274467781115e-06,
      "loss": 0.0012,
      "step": 52870
    },
    {
      "epoch": 11.333047578225461,
      "grad_norm": 19.897247314453125,
      "learning_rate": 4.889269895699386e-06,
      "loss": 0.1453,
      "step": 52880
    },
    {
      "epoch": 11.335190741534506,
      "grad_norm": 0.2706218659877777,
      "learning_rate": 4.886412344620661e-06,
      "loss": 0.0018,
      "step": 52890
    },
    {
      "epoch": 11.337333904843549,
      "grad_norm": 1.308210849761963,
      "learning_rate": 4.883554793541935e-06,
      "loss": 0.0011,
      "step": 52900
    },
    {
      "epoch": 11.339477068152593,
      "grad_norm": 0.0011535651283338666,
      "learning_rate": 4.880697242463209e-06,
      "loss": 0.2393,
      "step": 52910
    },
    {
      "epoch": 11.341620231461638,
      "grad_norm": 0.006167924962937832,
      "learning_rate": 4.877839691384484e-06,
      "loss": 0.0023,
      "step": 52920
    },
    {
      "epoch": 11.343763394770681,
      "grad_norm": 0.03526251018047333,
      "learning_rate": 4.8749821403057585e-06,
      "loss": 0.0006,
      "step": 52930
    },
    {
      "epoch": 11.345906558079726,
      "grad_norm": 0.028662042692303658,
      "learning_rate": 4.872124589227033e-06,
      "loss": 0.0008,
      "step": 52940
    },
    {
      "epoch": 11.34804972138877,
      "grad_norm": 0.03024880215525627,
      "learning_rate": 4.869267038148307e-06,
      "loss": 0.1501,
      "step": 52950
    },
    {
      "epoch": 11.350192884697814,
      "grad_norm": 0.0010404043132439256,
      "learning_rate": 4.866409487069582e-06,
      "loss": 0.1368,
      "step": 52960
    },
    {
      "epoch": 11.352336048006858,
      "grad_norm": 0.031487029045820236,
      "learning_rate": 4.863551935990856e-06,
      "loss": 0.0003,
      "step": 52970
    },
    {
      "epoch": 11.354479211315903,
      "grad_norm": 0.000809068267699331,
      "learning_rate": 4.860694384912131e-06,
      "loss": 0.0004,
      "step": 52980
    },
    {
      "epoch": 11.356622374624946,
      "grad_norm": 0.02573751099407673,
      "learning_rate": 4.8578368338334054e-06,
      "loss": 0.0004,
      "step": 52990
    },
    {
      "epoch": 11.35876553793399,
      "grad_norm": 0.04192150756716728,
      "learning_rate": 4.85497928275468e-06,
      "loss": 0.0007,
      "step": 53000
    },
    {
      "epoch": 11.360908701243035,
      "grad_norm": 0.023782355710864067,
      "learning_rate": 4.852121731675954e-06,
      "loss": 0.0004,
      "step": 53010
    },
    {
      "epoch": 11.363051864552078,
      "grad_norm": 0.0030452492646872997,
      "learning_rate": 4.849264180597228e-06,
      "loss": 0.0006,
      "step": 53020
    },
    {
      "epoch": 11.365195027861123,
      "grad_norm": 0.019250843673944473,
      "learning_rate": 4.846406629518503e-06,
      "loss": 0.0002,
      "step": 53030
    },
    {
      "epoch": 11.367338191170168,
      "grad_norm": 0.00155108910985291,
      "learning_rate": 4.843549078439778e-06,
      "loss": 0.1649,
      "step": 53040
    },
    {
      "epoch": 11.36948135447921,
      "grad_norm": 0.088596872985363,
      "learning_rate": 4.840691527361052e-06,
      "loss": 0.1648,
      "step": 53050
    },
    {
      "epoch": 11.371624517788256,
      "grad_norm": 0.03816251829266548,
      "learning_rate": 4.837833976282326e-06,
      "loss": 0.0004,
      "step": 53060
    },
    {
      "epoch": 11.3737676810973,
      "grad_norm": 0.0023155726958066225,
      "learning_rate": 4.834976425203601e-06,
      "loss": 0.0008,
      "step": 53070
    },
    {
      "epoch": 11.375910844406343,
      "grad_norm": 0.030562138184905052,
      "learning_rate": 4.832118874124875e-06,
      "loss": 0.4902,
      "step": 53080
    },
    {
      "epoch": 11.378054007715388,
      "grad_norm": 0.01464579626917839,
      "learning_rate": 4.82926132304615e-06,
      "loss": 0.1617,
      "step": 53090
    },
    {
      "epoch": 11.380197171024433,
      "grad_norm": 0.0011968379840254784,
      "learning_rate": 4.826403771967425e-06,
      "loss": 0.0003,
      "step": 53100
    },
    {
      "epoch": 11.382340334333476,
      "grad_norm": 0.16574256122112274,
      "learning_rate": 4.8235462208886985e-06,
      "loss": 0.0014,
      "step": 53110
    },
    {
      "epoch": 11.38448349764252,
      "grad_norm": 0.004385397303849459,
      "learning_rate": 4.820688669809973e-06,
      "loss": 0.1603,
      "step": 53120
    },
    {
      "epoch": 11.386626660951565,
      "grad_norm": 0.17340940237045288,
      "learning_rate": 4.817831118731248e-06,
      "loss": 0.0004,
      "step": 53130
    },
    {
      "epoch": 11.388769824260608,
      "grad_norm": 0.00039439572719857097,
      "learning_rate": 4.814973567652522e-06,
      "loss": 0.2031,
      "step": 53140
    },
    {
      "epoch": 11.390912987569653,
      "grad_norm": 0.0009922492317855358,
      "learning_rate": 4.812116016573797e-06,
      "loss": 0.4441,
      "step": 53150
    },
    {
      "epoch": 11.393056150878698,
      "grad_norm": 0.03848836198449135,
      "learning_rate": 4.8092584654950715e-06,
      "loss": 0.0005,
      "step": 53160
    },
    {
      "epoch": 11.39519931418774,
      "grad_norm": 0.07463616877794266,
      "learning_rate": 4.8064009144163455e-06,
      "loss": 0.0018,
      "step": 53170
    },
    {
      "epoch": 11.397342477496785,
      "grad_norm": 0.3801592290401459,
      "learning_rate": 4.80354336333762e-06,
      "loss": 0.0011,
      "step": 53180
    },
    {
      "epoch": 11.39948564080583,
      "grad_norm": 0.002541744150221348,
      "learning_rate": 4.800685812258894e-06,
      "loss": 0.0004,
      "step": 53190
    },
    {
      "epoch": 11.401628804114873,
      "grad_norm": 0.0002474203356541693,
      "learning_rate": 4.797828261180169e-06,
      "loss": 0.0014,
      "step": 53200
    },
    {
      "epoch": 11.403771967423918,
      "grad_norm": 0.0016928678378462791,
      "learning_rate": 4.794970710101444e-06,
      "loss": 0.0002,
      "step": 53210
    },
    {
      "epoch": 11.405915130732962,
      "grad_norm": 0.002762342570349574,
      "learning_rate": 4.792113159022718e-06,
      "loss": 0.2713,
      "step": 53220
    },
    {
      "epoch": 11.408058294042005,
      "grad_norm": 0.033739231526851654,
      "learning_rate": 4.7892556079439924e-06,
      "loss": 0.5677,
      "step": 53230
    },
    {
      "epoch": 11.41020145735105,
      "grad_norm": 0.04446389153599739,
      "learning_rate": 4.786398056865267e-06,
      "loss": 0.001,
      "step": 53240
    },
    {
      "epoch": 11.412344620660095,
      "grad_norm": 0.03930749371647835,
      "learning_rate": 4.783540505786541e-06,
      "loss": 0.3484,
      "step": 53250
    },
    {
      "epoch": 11.414487783969138,
      "grad_norm": 0.0028068674728274345,
      "learning_rate": 4.780682954707816e-06,
      "loss": 0.156,
      "step": 53260
    },
    {
      "epoch": 11.416630947278183,
      "grad_norm": 253.9042510986328,
      "learning_rate": 4.777825403629091e-06,
      "loss": 0.7208,
      "step": 53270
    },
    {
      "epoch": 11.418774110587227,
      "grad_norm": 0.3206147849559784,
      "learning_rate": 4.774967852550365e-06,
      "loss": 0.0018,
      "step": 53280
    },
    {
      "epoch": 11.42091727389627,
      "grad_norm": 0.3604865074157715,
      "learning_rate": 4.772110301471639e-06,
      "loss": 0.1441,
      "step": 53290
    },
    {
      "epoch": 11.423060437205315,
      "grad_norm": 0.1385134905576706,
      "learning_rate": 4.769252750392913e-06,
      "loss": 0.0019,
      "step": 53300
    },
    {
      "epoch": 11.42520360051436,
      "grad_norm": 0.05839618667960167,
      "learning_rate": 4.766395199314188e-06,
      "loss": 0.3255,
      "step": 53310
    },
    {
      "epoch": 11.427346763823403,
      "grad_norm": 0.09582098573446274,
      "learning_rate": 4.763537648235463e-06,
      "loss": 0.0019,
      "step": 53320
    },
    {
      "epoch": 11.429489927132447,
      "grad_norm": 0.459598571062088,
      "learning_rate": 4.760680097156737e-06,
      "loss": 0.0021,
      "step": 53330
    },
    {
      "epoch": 11.431633090441492,
      "grad_norm": 0.6202705502510071,
      "learning_rate": 4.7578225460780116e-06,
      "loss": 0.1414,
      "step": 53340
    },
    {
      "epoch": 11.433776253750535,
      "grad_norm": 0.01672225631773472,
      "learning_rate": 4.754964994999286e-06,
      "loss": 0.0005,
      "step": 53350
    },
    {
      "epoch": 11.43591941705958,
      "grad_norm": 0.054269786924123764,
      "learning_rate": 4.75210744392056e-06,
      "loss": 0.0008,
      "step": 53360
    },
    {
      "epoch": 11.438062580368625,
      "grad_norm": 0.029157400131225586,
      "learning_rate": 4.749249892841835e-06,
      "loss": 0.1573,
      "step": 53370
    },
    {
      "epoch": 11.440205743677668,
      "grad_norm": 0.1391351819038391,
      "learning_rate": 4.746392341763109e-06,
      "loss": 0.1918,
      "step": 53380
    },
    {
      "epoch": 11.442348906986712,
      "grad_norm": 0.00025046811788342893,
      "learning_rate": 4.743534790684384e-06,
      "loss": 0.0004,
      "step": 53390
    },
    {
      "epoch": 11.444492070295757,
      "grad_norm": 0.15636292099952698,
      "learning_rate": 4.7406772396056585e-06,
      "loss": 0.0006,
      "step": 53400
    },
    {
      "epoch": 11.4466352336048,
      "grad_norm": 0.0029386875685304403,
      "learning_rate": 4.7378196885269325e-06,
      "loss": 0.1657,
      "step": 53410
    },
    {
      "epoch": 11.448778396913845,
      "grad_norm": 0.0010398689191788435,
      "learning_rate": 4.734962137448207e-06,
      "loss": 0.0011,
      "step": 53420
    },
    {
      "epoch": 11.45092156022289,
      "grad_norm": 0.001495553064160049,
      "learning_rate": 4.732104586369482e-06,
      "loss": 0.4479,
      "step": 53430
    },
    {
      "epoch": 11.453064723531933,
      "grad_norm": 0.036591168493032455,
      "learning_rate": 4.729247035290756e-06,
      "loss": 0.1644,
      "step": 53440
    },
    {
      "epoch": 11.455207886840977,
      "grad_norm": 0.008601617999374866,
      "learning_rate": 4.726389484212031e-06,
      "loss": 0.2103,
      "step": 53450
    },
    {
      "epoch": 11.457351050150022,
      "grad_norm": 0.19929863512516022,
      "learning_rate": 4.7235319331333055e-06,
      "loss": 0.001,
      "step": 53460
    },
    {
      "epoch": 11.459494213459065,
      "grad_norm": 0.04674691706895828,
      "learning_rate": 4.720674382054579e-06,
      "loss": 0.003,
      "step": 53470
    },
    {
      "epoch": 11.46163737676811,
      "grad_norm": 0.02636786177754402,
      "learning_rate": 4.717816830975854e-06,
      "loss": 0.0009,
      "step": 53480
    },
    {
      "epoch": 11.463780540077154,
      "grad_norm": 0.0008611672674305737,
      "learning_rate": 4.714959279897128e-06,
      "loss": 0.1607,
      "step": 53490
    },
    {
      "epoch": 11.465923703386197,
      "grad_norm": 0.02100246772170067,
      "learning_rate": 4.712101728818403e-06,
      "loss": 0.1729,
      "step": 53500
    },
    {
      "epoch": 11.468066866695242,
      "grad_norm": 0.0009195363381877542,
      "learning_rate": 4.709244177739678e-06,
      "loss": 0.0006,
      "step": 53510
    },
    {
      "epoch": 11.470210030004287,
      "grad_norm": 18.31732940673828,
      "learning_rate": 4.7063866266609524e-06,
      "loss": 0.1577,
      "step": 53520
    },
    {
      "epoch": 11.47235319331333,
      "grad_norm": 0.023775599896907806,
      "learning_rate": 4.703529075582226e-06,
      "loss": 0.4782,
      "step": 53530
    },
    {
      "epoch": 11.474496356622375,
      "grad_norm": 0.06357280910015106,
      "learning_rate": 4.7006715245035e-06,
      "loss": 0.0003,
      "step": 53540
    },
    {
      "epoch": 11.47663951993142,
      "grad_norm": 0.0013770483201369643,
      "learning_rate": 4.697813973424775e-06,
      "loss": 0.1478,
      "step": 53550
    },
    {
      "epoch": 11.478782683240462,
      "grad_norm": 0.007497391197830439,
      "learning_rate": 4.69495642234605e-06,
      "loss": 0.2967,
      "step": 53560
    },
    {
      "epoch": 11.480925846549507,
      "grad_norm": 0.01571785844862461,
      "learning_rate": 4.692098871267325e-06,
      "loss": 0.097,
      "step": 53570
    },
    {
      "epoch": 11.483069009858552,
      "grad_norm": 0.44292283058166504,
      "learning_rate": 4.6892413201885986e-06,
      "loss": 0.0074,
      "step": 53580
    },
    {
      "epoch": 11.485212173167595,
      "grad_norm": 0.0013732152292504907,
      "learning_rate": 4.686383769109873e-06,
      "loss": 0.0004,
      "step": 53590
    },
    {
      "epoch": 11.48735533647664,
      "grad_norm": 0.00023119528486859053,
      "learning_rate": 4.683526218031147e-06,
      "loss": 0.0023,
      "step": 53600
    },
    {
      "epoch": 11.489498499785684,
      "grad_norm": 0.0350121408700943,
      "learning_rate": 4.680668666952422e-06,
      "loss": 0.4638,
      "step": 53610
    },
    {
      "epoch": 11.491641663094727,
      "grad_norm": 0.027338463813066483,
      "learning_rate": 4.677811115873697e-06,
      "loss": 0.129,
      "step": 53620
    },
    {
      "epoch": 11.493784826403772,
      "grad_norm": 0.02863301895558834,
      "learning_rate": 4.674953564794972e-06,
      "loss": 0.1483,
      "step": 53630
    },
    {
      "epoch": 11.495927989712817,
      "grad_norm": 0.05874953791499138,
      "learning_rate": 4.6720960137162455e-06,
      "loss": 0.1254,
      "step": 53640
    },
    {
      "epoch": 11.49807115302186,
      "grad_norm": 0.004382743034511805,
      "learning_rate": 4.6692384626375194e-06,
      "loss": 0.0007,
      "step": 53650
    },
    {
      "epoch": 11.500214316330904,
      "grad_norm": 0.0056245592422783375,
      "learning_rate": 4.666380911558794e-06,
      "loss": 0.0018,
      "step": 53660
    },
    {
      "epoch": 11.502357479639949,
      "grad_norm": 0.0031362699810415506,
      "learning_rate": 4.663523360480069e-06,
      "loss": 0.001,
      "step": 53670
    },
    {
      "epoch": 11.504500642948992,
      "grad_norm": 0.0007604126003570855,
      "learning_rate": 4.660665809401344e-06,
      "loss": 0.0001,
      "step": 53680
    },
    {
      "epoch": 11.506643806258037,
      "grad_norm": 0.0010007999371737242,
      "learning_rate": 4.657808258322618e-06,
      "loss": 0.0002,
      "step": 53690
    },
    {
      "epoch": 11.508786969567081,
      "grad_norm": 0.15942683815956116,
      "learning_rate": 4.6549507072438925e-06,
      "loss": 0.3905,
      "step": 53700
    },
    {
      "epoch": 11.510930132876124,
      "grad_norm": 0.1340189278125763,
      "learning_rate": 4.652093156165166e-06,
      "loss": 0.1336,
      "step": 53710
    },
    {
      "epoch": 11.51307329618517,
      "grad_norm": 0.03214266151189804,
      "learning_rate": 4.649235605086441e-06,
      "loss": 0.1456,
      "step": 53720
    },
    {
      "epoch": 11.515216459494214,
      "grad_norm": 23.78784942626953,
      "learning_rate": 4.646378054007716e-06,
      "loss": 0.7691,
      "step": 53730
    },
    {
      "epoch": 11.517359622803257,
      "grad_norm": 0.018718693405389786,
      "learning_rate": 4.643520502928991e-06,
      "loss": 0.136,
      "step": 53740
    },
    {
      "epoch": 11.519502786112302,
      "grad_norm": 0.2447458952665329,
      "learning_rate": 4.640662951850265e-06,
      "loss": 0.1283,
      "step": 53750
    },
    {
      "epoch": 11.521645949421346,
      "grad_norm": 0.08621004968881607,
      "learning_rate": 4.637805400771539e-06,
      "loss": 0.1339,
      "step": 53760
    },
    {
      "epoch": 11.52378911273039,
      "grad_norm": 0.009950440376996994,
      "learning_rate": 4.634947849692813e-06,
      "loss": 0.1228,
      "step": 53770
    },
    {
      "epoch": 11.525932276039434,
      "grad_norm": 0.5354999899864197,
      "learning_rate": 4.632090298614088e-06,
      "loss": 0.1452,
      "step": 53780
    },
    {
      "epoch": 11.528075439348479,
      "grad_norm": 0.07704443484544754,
      "learning_rate": 4.629232747535363e-06,
      "loss": 0.0009,
      "step": 53790
    },
    {
      "epoch": 11.530218602657522,
      "grad_norm": 0.0013871631817892194,
      "learning_rate": 4.626375196456637e-06,
      "loss": 0.2087,
      "step": 53800
    },
    {
      "epoch": 11.532361765966566,
      "grad_norm": 0.0011047694133594632,
      "learning_rate": 4.623517645377912e-06,
      "loss": 0.1288,
      "step": 53810
    },
    {
      "epoch": 11.534504929275611,
      "grad_norm": 0.022178076207637787,
      "learning_rate": 4.6206600942991855e-06,
      "loss": 0.1309,
      "step": 53820
    },
    {
      "epoch": 11.536648092584654,
      "grad_norm": 0.10417681187391281,
      "learning_rate": 4.61780254322046e-06,
      "loss": 0.0006,
      "step": 53830
    },
    {
      "epoch": 11.538791255893699,
      "grad_norm": 0.0012034676037728786,
      "learning_rate": 4.614944992141735e-06,
      "loss": 0.0031,
      "step": 53840
    },
    {
      "epoch": 11.540934419202744,
      "grad_norm": 46.248504638671875,
      "learning_rate": 4.61208744106301e-06,
      "loss": 0.13,
      "step": 53850
    },
    {
      "epoch": 11.543077582511787,
      "grad_norm": 0.2934442460536957,
      "learning_rate": 4.609229889984284e-06,
      "loss": 0.0008,
      "step": 53860
    },
    {
      "epoch": 11.545220745820831,
      "grad_norm": 0.0011720703914761543,
      "learning_rate": 4.606372338905558e-06,
      "loss": 0.1529,
      "step": 53870
    },
    {
      "epoch": 11.547363909129876,
      "grad_norm": 0.04615914076566696,
      "learning_rate": 4.6035147878268325e-06,
      "loss": 0.1267,
      "step": 53880
    },
    {
      "epoch": 11.549507072438919,
      "grad_norm": 0.015949619933962822,
      "learning_rate": 4.600657236748107e-06,
      "loss": 0.1603,
      "step": 53890
    },
    {
      "epoch": 11.551650235747964,
      "grad_norm": 0.0009201218490488827,
      "learning_rate": 4.597799685669382e-06,
      "loss": 0.2189,
      "step": 53900
    },
    {
      "epoch": 11.553793399057009,
      "grad_norm": 0.25682511925697327,
      "learning_rate": 4.594942134590657e-06,
      "loss": 0.2084,
      "step": 53910
    },
    {
      "epoch": 11.555936562366051,
      "grad_norm": 0.059805721044540405,
      "learning_rate": 4.592084583511931e-06,
      "loss": 0.1689,
      "step": 53920
    },
    {
      "epoch": 11.558079725675096,
      "grad_norm": 0.19084054231643677,
      "learning_rate": 4.589227032433205e-06,
      "loss": 0.0014,
      "step": 53930
    },
    {
      "epoch": 11.560222888984141,
      "grad_norm": 0.15484656393527985,
      "learning_rate": 4.5863694813544795e-06,
      "loss": 0.232,
      "step": 53940
    },
    {
      "epoch": 11.562366052293184,
      "grad_norm": 0.0008662760374136269,
      "learning_rate": 4.583511930275754e-06,
      "loss": 0.0002,
      "step": 53950
    },
    {
      "epoch": 11.564509215602229,
      "grad_norm": 0.042404018342494965,
      "learning_rate": 4.580654379197029e-06,
      "loss": 0.1592,
      "step": 53960
    },
    {
      "epoch": 11.566652378911273,
      "grad_norm": 15.659562110900879,
      "learning_rate": 4.577796828118303e-06,
      "loss": 0.1394,
      "step": 53970
    },
    {
      "epoch": 11.568795542220316,
      "grad_norm": 0.25430718064308167,
      "learning_rate": 4.574939277039578e-06,
      "loss": 0.0013,
      "step": 53980
    },
    {
      "epoch": 11.570938705529361,
      "grad_norm": 0.15183834731578827,
      "learning_rate": 4.572081725960852e-06,
      "loss": 0.0009,
      "step": 53990
    },
    {
      "epoch": 11.573081868838406,
      "grad_norm": 0.03401407599449158,
      "learning_rate": 4.569224174882126e-06,
      "loss": 0.0006,
      "step": 54000
    },
    {
      "epoch": 11.57522503214745,
      "grad_norm": 0.0009567094966769218,
      "learning_rate": 4.566366623803401e-06,
      "loss": 0.001,
      "step": 54010
    },
    {
      "epoch": 11.577368195456494,
      "grad_norm": 20.992612838745117,
      "learning_rate": 4.563509072724676e-06,
      "loss": 0.3027,
      "step": 54020
    },
    {
      "epoch": 11.579511358765538,
      "grad_norm": 0.20918972790241241,
      "learning_rate": 4.56065152164595e-06,
      "loss": 0.001,
      "step": 54030
    },
    {
      "epoch": 11.581654522074583,
      "grad_norm": 0.13868848979473114,
      "learning_rate": 4.557793970567224e-06,
      "loss": 0.3746,
      "step": 54040
    },
    {
      "epoch": 11.583797685383626,
      "grad_norm": 0.14130768179893494,
      "learning_rate": 4.554936419488499e-06,
      "loss": 0.1234,
      "step": 54050
    },
    {
      "epoch": 11.58594084869267,
      "grad_norm": 0.01477964036166668,
      "learning_rate": 4.552078868409773e-06,
      "loss": 0.1854,
      "step": 54060
    },
    {
      "epoch": 11.588084012001715,
      "grad_norm": 0.00016035903536248952,
      "learning_rate": 4.549221317331048e-06,
      "loss": 0.192,
      "step": 54070
    },
    {
      "epoch": 11.590227175310758,
      "grad_norm": 0.016748478636145592,
      "learning_rate": 4.546363766252322e-06,
      "loss": 0.19,
      "step": 54080
    },
    {
      "epoch": 11.592370338619803,
      "grad_norm": 0.00030361191602423787,
      "learning_rate": 4.543506215173597e-06,
      "loss": 0.2945,
      "step": 54090
    },
    {
      "epoch": 11.594513501928848,
      "grad_norm": 0.00019360605801921338,
      "learning_rate": 4.540648664094871e-06,
      "loss": 0.2211,
      "step": 54100
    },
    {
      "epoch": 11.59665666523789,
      "grad_norm": 0.0007166349678300321,
      "learning_rate": 4.5377911130161456e-06,
      "loss": 0.4796,
      "step": 54110
    },
    {
      "epoch": 11.598799828546936,
      "grad_norm": 420.52691650390625,
      "learning_rate": 4.53493356193742e-06,
      "loss": 0.0875,
      "step": 54120
    },
    {
      "epoch": 11.60094299185598,
      "grad_norm": 0.02721826732158661,
      "learning_rate": 4.532076010858694e-06,
      "loss": 0.0965,
      "step": 54130
    },
    {
      "epoch": 11.603086155165023,
      "grad_norm": 0.0054338774643838406,
      "learning_rate": 4.529218459779969e-06,
      "loss": 0.1778,
      "step": 54140
    },
    {
      "epoch": 11.605229318474068,
      "grad_norm": 19.985239028930664,
      "learning_rate": 4.526360908701243e-06,
      "loss": 0.3227,
      "step": 54150
    },
    {
      "epoch": 11.607372481783113,
      "grad_norm": 0.07476354390382767,
      "learning_rate": 4.523503357622518e-06,
      "loss": 0.0007,
      "step": 54160
    },
    {
      "epoch": 11.609515645092156,
      "grad_norm": 0.002611194970086217,
      "learning_rate": 4.5206458065437925e-06,
      "loss": 0.2048,
      "step": 54170
    },
    {
      "epoch": 11.6116588084012,
      "grad_norm": 0.0020665451884269714,
      "learning_rate": 4.517788255465067e-06,
      "loss": 0.0023,
      "step": 54180
    },
    {
      "epoch": 11.613801971710245,
      "grad_norm": 0.000663739163428545,
      "learning_rate": 4.514930704386341e-06,
      "loss": 0.2084,
      "step": 54190
    },
    {
      "epoch": 11.615945135019288,
      "grad_norm": 0.0007650648476555943,
      "learning_rate": 4.512073153307616e-06,
      "loss": 0.0001,
      "step": 54200
    },
    {
      "epoch": 11.618088298328333,
      "grad_norm": 0.02704474702477455,
      "learning_rate": 4.50921560222889e-06,
      "loss": 0.1265,
      "step": 54210
    },
    {
      "epoch": 11.620231461637378,
      "grad_norm": 0.023570094257593155,
      "learning_rate": 4.506358051150165e-06,
      "loss": 0.0008,
      "step": 54220
    },
    {
      "epoch": 11.62237462494642,
      "grad_norm": 0.055098287761211395,
      "learning_rate": 4.5035005000714395e-06,
      "loss": 0.0008,
      "step": 54230
    },
    {
      "epoch": 11.624517788255465,
      "grad_norm": 0.2752643823623657,
      "learning_rate": 4.500642948992713e-06,
      "loss": 0.0012,
      "step": 54240
    },
    {
      "epoch": 11.62666095156451,
      "grad_norm": 0.045627426356077194,
      "learning_rate": 4.497785397913988e-06,
      "loss": 0.0005,
      "step": 54250
    },
    {
      "epoch": 11.628804114873553,
      "grad_norm": 0.02893473394215107,
      "learning_rate": 4.494927846835262e-06,
      "loss": 0.153,
      "step": 54260
    },
    {
      "epoch": 11.630947278182598,
      "grad_norm": 0.0017029504524543881,
      "learning_rate": 4.492070295756537e-06,
      "loss": 0.0009,
      "step": 54270
    },
    {
      "epoch": 11.633090441491643,
      "grad_norm": 0.002132946392521262,
      "learning_rate": 4.489212744677812e-06,
      "loss": 0.0012,
      "step": 54280
    },
    {
      "epoch": 11.635233604800685,
      "grad_norm": 0.5109345316886902,
      "learning_rate": 4.4863551935990864e-06,
      "loss": 0.2172,
      "step": 54290
    },
    {
      "epoch": 11.63737676810973,
      "grad_norm": 0.2577965259552002,
      "learning_rate": 4.48349764252036e-06,
      "loss": 0.3704,
      "step": 54300
    },
    {
      "epoch": 11.639519931418775,
      "grad_norm": 0.005370581056922674,
      "learning_rate": 4.480640091441635e-06,
      "loss": 0.5508,
      "step": 54310
    },
    {
      "epoch": 11.641663094727818,
      "grad_norm": 0.0750061422586441,
      "learning_rate": 4.477782540362909e-06,
      "loss": 0.1992,
      "step": 54320
    },
    {
      "epoch": 11.643806258036863,
      "grad_norm": 0.005121676251292229,
      "learning_rate": 4.474924989284184e-06,
      "loss": 0.1666,
      "step": 54330
    },
    {
      "epoch": 11.645949421345907,
      "grad_norm": 0.1337260752916336,
      "learning_rate": 4.472067438205459e-06,
      "loss": 0.1212,
      "step": 54340
    },
    {
      "epoch": 11.64809258465495,
      "grad_norm": 0.03477748483419418,
      "learning_rate": 4.4692098871267325e-06,
      "loss": 0.0953,
      "step": 54350
    },
    {
      "epoch": 11.650235747963995,
      "grad_norm": 0.019823092967271805,
      "learning_rate": 4.466352336048007e-06,
      "loss": 0.1497,
      "step": 54360
    },
    {
      "epoch": 11.65237891127304,
      "grad_norm": 0.0004985926207154989,
      "learning_rate": 4.463494784969282e-06,
      "loss": 0.0804,
      "step": 54370
    },
    {
      "epoch": 11.654522074582083,
      "grad_norm": 0.0358540415763855,
      "learning_rate": 4.460637233890556e-06,
      "loss": 0.0049,
      "step": 54380
    },
    {
      "epoch": 11.656665237891128,
      "grad_norm": 0.17046110332012177,
      "learning_rate": 4.457779682811831e-06,
      "loss": 0.0018,
      "step": 54390
    },
    {
      "epoch": 11.658808401200172,
      "grad_norm": 0.018881551921367645,
      "learning_rate": 4.454922131733105e-06,
      "loss": 0.0925,
      "step": 54400
    },
    {
      "epoch": 11.660951564509215,
      "grad_norm": 0.08053280413150787,
      "learning_rate": 4.4520645806543795e-06,
      "loss": 0.1432,
      "step": 54410
    },
    {
      "epoch": 11.66309472781826,
      "grad_norm": 0.0032811337150633335,
      "learning_rate": 4.449207029575654e-06,
      "loss": 0.0008,
      "step": 54420
    },
    {
      "epoch": 11.665237891127305,
      "grad_norm": 0.0020939174573868513,
      "learning_rate": 4.446349478496928e-06,
      "loss": 0.3076,
      "step": 54430
    },
    {
      "epoch": 11.667381054436348,
      "grad_norm": 0.00046831584768369794,
      "learning_rate": 4.443491927418203e-06,
      "loss": 0.001,
      "step": 54440
    },
    {
      "epoch": 11.669524217745392,
      "grad_norm": 0.035260409116744995,
      "learning_rate": 4.440634376339478e-06,
      "loss": 0.0005,
      "step": 54450
    },
    {
      "epoch": 11.671667381054437,
      "grad_norm": 0.010595252737402916,
      "learning_rate": 4.437776825260752e-06,
      "loss": 0.1726,
      "step": 54460
    },
    {
      "epoch": 11.67381054436348,
      "grad_norm": 0.0006262640818022192,
      "learning_rate": 4.4349192741820265e-06,
      "loss": 0.2461,
      "step": 54470
    },
    {
      "epoch": 11.675953707672525,
      "grad_norm": 0.04617147892713547,
      "learning_rate": 4.432061723103301e-06,
      "loss": 0.0017,
      "step": 54480
    },
    {
      "epoch": 11.67809687098157,
      "grad_norm": 0.041625577956438065,
      "learning_rate": 4.429204172024575e-06,
      "loss": 0.0002,
      "step": 54490
    },
    {
      "epoch": 11.680240034290613,
      "grad_norm": 0.019752977415919304,
      "learning_rate": 4.42634662094585e-06,
      "loss": 0.3992,
      "step": 54500
    },
    {
      "epoch": 11.682383197599657,
      "grad_norm": 0.000993631430901587,
      "learning_rate": 4.423489069867124e-06,
      "loss": 0.0006,
      "step": 54510
    },
    {
      "epoch": 11.684526360908702,
      "grad_norm": 0.03805907815694809,
      "learning_rate": 4.420631518788399e-06,
      "loss": 0.2315,
      "step": 54520
    },
    {
      "epoch": 11.686669524217745,
      "grad_norm": 0.0018579491879791021,
      "learning_rate": 4.417773967709673e-06,
      "loss": 0.0039,
      "step": 54530
    },
    {
      "epoch": 11.68881268752679,
      "grad_norm": 0.03706018999218941,
      "learning_rate": 4.414916416630947e-06,
      "loss": 0.001,
      "step": 54540
    },
    {
      "epoch": 11.690955850835834,
      "grad_norm": 0.002001965418457985,
      "learning_rate": 4.412058865552222e-06,
      "loss": 0.2009,
      "step": 54550
    },
    {
      "epoch": 11.693099014144877,
      "grad_norm": 0.014672120101749897,
      "learning_rate": 4.409201314473496e-06,
      "loss": 0.0001,
      "step": 54560
    },
    {
      "epoch": 11.695242177453922,
      "grad_norm": 0.014057803899049759,
      "learning_rate": 4.406343763394771e-06,
      "loss": 0.0048,
      "step": 54570
    },
    {
      "epoch": 11.697385340762967,
      "grad_norm": 0.006782601121813059,
      "learning_rate": 4.403486212316046e-06,
      "loss": 0.326,
      "step": 54580
    },
    {
      "epoch": 11.69952850407201,
      "grad_norm": 23.643945693969727,
      "learning_rate": 4.40062866123732e-06,
      "loss": 0.5629,
      "step": 54590
    },
    {
      "epoch": 11.701671667381055,
      "grad_norm": 0.004538156092166901,
      "learning_rate": 4.397771110158594e-06,
      "loss": 0.0006,
      "step": 54600
    },
    {
      "epoch": 11.7038148306901,
      "grad_norm": 4.414127826690674,
      "learning_rate": 4.394913559079869e-06,
      "loss": 0.3326,
      "step": 54610
    },
    {
      "epoch": 11.705957993999142,
      "grad_norm": 0.09298224002122879,
      "learning_rate": 4.392056008001143e-06,
      "loss": 0.0006,
      "step": 54620
    },
    {
      "epoch": 11.708101157308187,
      "grad_norm": 19.24544334411621,
      "learning_rate": 4.389198456922418e-06,
      "loss": 0.3923,
      "step": 54630
    },
    {
      "epoch": 11.710244320617232,
      "grad_norm": 0.017736244946718216,
      "learning_rate": 4.3863409058436925e-06,
      "loss": 0.0019,
      "step": 54640
    },
    {
      "epoch": 11.712387483926275,
      "grad_norm": 0.0008533012005500495,
      "learning_rate": 4.3834833547649665e-06,
      "loss": 0.0048,
      "step": 54650
    },
    {
      "epoch": 11.71453064723532,
      "grad_norm": 0.09059440344572067,
      "learning_rate": 4.380625803686241e-06,
      "loss": 0.263,
      "step": 54660
    },
    {
      "epoch": 11.716673810544364,
      "grad_norm": 0.009308901615440845,
      "learning_rate": 4.377768252607515e-06,
      "loss": 0.0008,
      "step": 54670
    },
    {
      "epoch": 11.718816973853407,
      "grad_norm": 0.0003615008608903736,
      "learning_rate": 4.37491070152879e-06,
      "loss": 0.0003,
      "step": 54680
    },
    {
      "epoch": 11.720960137162452,
      "grad_norm": 0.0002572814410086721,
      "learning_rate": 4.372053150450065e-06,
      "loss": 0.0012,
      "step": 54690
    },
    {
      "epoch": 11.723103300471497,
      "grad_norm": 50.038394927978516,
      "learning_rate": 4.3691955993713395e-06,
      "loss": 0.1381,
      "step": 54700
    },
    {
      "epoch": 11.72524646378054,
      "grad_norm": 0.8671150803565979,
      "learning_rate": 4.3663380482926134e-06,
      "loss": 0.1091,
      "step": 54710
    },
    {
      "epoch": 11.727389627089584,
      "grad_norm": 0.008237157948315144,
      "learning_rate": 4.363480497213887e-06,
      "loss": 0.0007,
      "step": 54720
    },
    {
      "epoch": 11.729532790398629,
      "grad_norm": 9.299543380737305,
      "learning_rate": 4.360622946135162e-06,
      "loss": 0.0017,
      "step": 54730
    },
    {
      "epoch": 11.731675953707672,
      "grad_norm": 0.001821752404794097,
      "learning_rate": 4.357765395056437e-06,
      "loss": 0.281,
      "step": 54740
    },
    {
      "epoch": 11.733819117016717,
      "grad_norm": 0.0010680134873837233,
      "learning_rate": 4.354907843977712e-06,
      "loss": 0.6662,
      "step": 54750
    },
    {
      "epoch": 11.735962280325761,
      "grad_norm": 0.002254763152450323,
      "learning_rate": 4.3520502928989865e-06,
      "loss": 0.0001,
      "step": 54760
    },
    {
      "epoch": 11.738105443634804,
      "grad_norm": 0.0005162739544175565,
      "learning_rate": 4.34919274182026e-06,
      "loss": 0.4509,
      "step": 54770
    },
    {
      "epoch": 11.74024860694385,
      "grad_norm": 0.0004911675932817161,
      "learning_rate": 4.346335190741534e-06,
      "loss": 0.001,
      "step": 54780
    },
    {
      "epoch": 11.742391770252894,
      "grad_norm": 28.584611892700195,
      "learning_rate": 4.343477639662809e-06,
      "loss": 0.1714,
      "step": 54790
    },
    {
      "epoch": 11.744534933561937,
      "grad_norm": 36.48455047607422,
      "learning_rate": 4.340620088584084e-06,
      "loss": 0.4298,
      "step": 54800
    },
    {
      "epoch": 11.746678096870982,
      "grad_norm": 0.00708690844476223,
      "learning_rate": 4.337762537505359e-06,
      "loss": 0.0037,
      "step": 54810
    },
    {
      "epoch": 11.748821260180026,
      "grad_norm": 0.02804628200829029,
      "learning_rate": 4.3349049864266326e-06,
      "loss": 0.0025,
      "step": 54820
    },
    {
      "epoch": 11.75096442348907,
      "grad_norm": 0.016535509377717972,
      "learning_rate": 4.332047435347907e-06,
      "loss": 0.0002,
      "step": 54830
    },
    {
      "epoch": 11.753107586798114,
      "grad_norm": 0.02298147603869438,
      "learning_rate": 4.329189884269181e-06,
      "loss": 0.3943,
      "step": 54840
    },
    {
      "epoch": 11.755250750107159,
      "grad_norm": 0.0007798179285600781,
      "learning_rate": 4.326332333190456e-06,
      "loss": 0.4704,
      "step": 54850
    },
    {
      "epoch": 11.757393913416202,
      "grad_norm": 0.2941083610057831,
      "learning_rate": 4.323474782111731e-06,
      "loss": 0.2577,
      "step": 54860
    },
    {
      "epoch": 11.759537076725247,
      "grad_norm": 22.69809913635254,
      "learning_rate": 4.320617231033006e-06,
      "loss": 0.1406,
      "step": 54870
    },
    {
      "epoch": 11.761680240034291,
      "grad_norm": 0.0017514342907816172,
      "learning_rate": 4.3177596799542795e-06,
      "loss": 0.1935,
      "step": 54880
    },
    {
      "epoch": 11.763823403343334,
      "grad_norm": 0.0016689181793481112,
      "learning_rate": 4.3149021288755535e-06,
      "loss": 0.246,
      "step": 54890
    },
    {
      "epoch": 11.765966566652379,
      "grad_norm": 0.002887421054765582,
      "learning_rate": 4.312044577796828e-06,
      "loss": 0.5515,
      "step": 54900
    },
    {
      "epoch": 11.768109729961424,
      "grad_norm": 0.0012044060276821256,
      "learning_rate": 4.309187026718103e-06,
      "loss": 0.1501,
      "step": 54910
    },
    {
      "epoch": 11.770252893270467,
      "grad_norm": 0.10018051415681839,
      "learning_rate": 4.306329475639378e-06,
      "loss": 0.0016,
      "step": 54920
    },
    {
      "epoch": 11.772396056579511,
      "grad_norm": 420.5429382324219,
      "learning_rate": 4.303471924560652e-06,
      "loss": 0.5965,
      "step": 54930
    },
    {
      "epoch": 11.774539219888556,
      "grad_norm": 0.028333958238363266,
      "learning_rate": 4.3006143734819265e-06,
      "loss": 0.0014,
      "step": 54940
    },
    {
      "epoch": 11.776682383197599,
      "grad_norm": 0.008931117132306099,
      "learning_rate": 4.2977568224032e-06,
      "loss": 0.0015,
      "step": 54950
    },
    {
      "epoch": 11.778825546506644,
      "grad_norm": 0.07238984107971191,
      "learning_rate": 4.294899271324475e-06,
      "loss": 0.0028,
      "step": 54960
    },
    {
      "epoch": 11.780968709815689,
      "grad_norm": 0.013805657625198364,
      "learning_rate": 4.29204172024575e-06,
      "loss": 0.0006,
      "step": 54970
    },
    {
      "epoch": 11.783111873124732,
      "grad_norm": 1147.2991943359375,
      "learning_rate": 4.289184169167025e-06,
      "loss": 0.0172,
      "step": 54980
    },
    {
      "epoch": 11.785255036433776,
      "grad_norm": 0.0064378222450613976,
      "learning_rate": 4.286326618088299e-06,
      "loss": 0.3802,
      "step": 54990
    },
    {
      "epoch": 11.787398199742821,
      "grad_norm": 0.0654120147228241,
      "learning_rate": 4.283469067009573e-06,
      "loss": 0.0393,
      "step": 55000
    },
    {
      "epoch": 11.789541363051864,
      "grad_norm": 21.264005661010742,
      "learning_rate": 4.280611515930847e-06,
      "loss": 0.1941,
      "step": 55010
    },
    {
      "epoch": 11.791684526360909,
      "grad_norm": 0.0841173380613327,
      "learning_rate": 4.277753964852122e-06,
      "loss": 0.1683,
      "step": 55020
    },
    {
      "epoch": 11.793827689669953,
      "grad_norm": 0.014212747104465961,
      "learning_rate": 4.274896413773397e-06,
      "loss": 0.0008,
      "step": 55030
    },
    {
      "epoch": 11.795970852978996,
      "grad_norm": 0.04376138746738434,
      "learning_rate": 4.272038862694671e-06,
      "loss": 0.3718,
      "step": 55040
    },
    {
      "epoch": 11.798114016288041,
      "grad_norm": 0.028299925848841667,
      "learning_rate": 4.269181311615946e-06,
      "loss": 0.3695,
      "step": 55050
    },
    {
      "epoch": 11.800257179597086,
      "grad_norm": 43.27444076538086,
      "learning_rate": 4.2663237605372196e-06,
      "loss": 0.1437,
      "step": 55060
    },
    {
      "epoch": 11.802400342906129,
      "grad_norm": 0.042048778384923935,
      "learning_rate": 4.263466209458494e-06,
      "loss": 0.1066,
      "step": 55070
    },
    {
      "epoch": 11.804543506215174,
      "grad_norm": 0.5750913619995117,
      "learning_rate": 4.260608658379769e-06,
      "loss": 0.002,
      "step": 55080
    },
    {
      "epoch": 11.806686669524218,
      "grad_norm": 1.4410486221313477,
      "learning_rate": 4.257751107301044e-06,
      "loss": 0.219,
      "step": 55090
    },
    {
      "epoch": 11.808829832833261,
      "grad_norm": 0.2216673493385315,
      "learning_rate": 4.254893556222318e-06,
      "loss": 0.3733,
      "step": 55100
    },
    {
      "epoch": 11.810972996142306,
      "grad_norm": 68.05155181884766,
      "learning_rate": 4.252036005143592e-06,
      "loss": 0.2513,
      "step": 55110
    },
    {
      "epoch": 11.81311615945135,
      "grad_norm": 0.05239871144294739,
      "learning_rate": 4.2491784540648665e-06,
      "loss": 0.0008,
      "step": 55120
    },
    {
      "epoch": 11.815259322760394,
      "grad_norm": 0.06641476601362228,
      "learning_rate": 4.246320902986141e-06,
      "loss": 0.0103,
      "step": 55130
    },
    {
      "epoch": 11.817402486069438,
      "grad_norm": 0.03842613846063614,
      "learning_rate": 4.243463351907416e-06,
      "loss": 0.2102,
      "step": 55140
    },
    {
      "epoch": 11.819545649378483,
      "grad_norm": 20.528608322143555,
      "learning_rate": 4.24060580082869e-06,
      "loss": 0.3861,
      "step": 55150
    },
    {
      "epoch": 11.821688812687526,
      "grad_norm": 0.003946264740079641,
      "learning_rate": 4.237748249749965e-06,
      "loss": 0.0008,
      "step": 55160
    },
    {
      "epoch": 11.82383197599657,
      "grad_norm": 0.34540024399757385,
      "learning_rate": 4.234890698671239e-06,
      "loss": 0.334,
      "step": 55170
    },
    {
      "epoch": 11.825975139305616,
      "grad_norm": 0.002010457217693329,
      "learning_rate": 4.2320331475925135e-06,
      "loss": 0.448,
      "step": 55180
    },
    {
      "epoch": 11.828118302614659,
      "grad_norm": 2.1488587856292725,
      "learning_rate": 4.229175596513788e-06,
      "loss": 0.1487,
      "step": 55190
    },
    {
      "epoch": 11.830261465923703,
      "grad_norm": 0.0007863475475460291,
      "learning_rate": 4.226318045435063e-06,
      "loss": 0.0012,
      "step": 55200
    },
    {
      "epoch": 11.832404629232748,
      "grad_norm": 0.13608042895793915,
      "learning_rate": 4.223460494356337e-06,
      "loss": 0.0014,
      "step": 55210
    },
    {
      "epoch": 11.834547792541791,
      "grad_norm": 0.18582363426685333,
      "learning_rate": 4.220602943277612e-06,
      "loss": 0.0778,
      "step": 55220
    },
    {
      "epoch": 11.836690955850836,
      "grad_norm": 0.000296036247164011,
      "learning_rate": 4.217745392198886e-06,
      "loss": 0.1502,
      "step": 55230
    },
    {
      "epoch": 11.83883411915988,
      "grad_norm": 29.539640426635742,
      "learning_rate": 4.2148878411201604e-06,
      "loss": 0.2613,
      "step": 55240
    },
    {
      "epoch": 11.840977282468923,
      "grad_norm": 0.006023153197020292,
      "learning_rate": 4.212030290041435e-06,
      "loss": 0.0015,
      "step": 55250
    },
    {
      "epoch": 11.843120445777968,
      "grad_norm": 0.3267529010772705,
      "learning_rate": 4.209172738962709e-06,
      "loss": 0.3175,
      "step": 55260
    },
    {
      "epoch": 11.845263609087013,
      "grad_norm": 0.06362246721982956,
      "learning_rate": 4.206315187883984e-06,
      "loss": 0.1879,
      "step": 55270
    },
    {
      "epoch": 11.847406772396056,
      "grad_norm": 0.07149557769298553,
      "learning_rate": 4.203457636805258e-06,
      "loss": 0.0014,
      "step": 55280
    },
    {
      "epoch": 11.8495499357051,
      "grad_norm": 0.00027515567489899695,
      "learning_rate": 4.200600085726533e-06,
      "loss": 0.0007,
      "step": 55290
    },
    {
      "epoch": 11.851693099014145,
      "grad_norm": 0.0021465530153363943,
      "learning_rate": 4.197742534647807e-06,
      "loss": 0.1932,
      "step": 55300
    },
    {
      "epoch": 11.853836262323188,
      "grad_norm": 0.000429844600148499,
      "learning_rate": 4.194884983569082e-06,
      "loss": 0.2853,
      "step": 55310
    },
    {
      "epoch": 11.855979425632233,
      "grad_norm": 0.0012116199359297752,
      "learning_rate": 4.192027432490356e-06,
      "loss": 0.0007,
      "step": 55320
    },
    {
      "epoch": 11.858122588941278,
      "grad_norm": 0.05757061392068863,
      "learning_rate": 4.189169881411631e-06,
      "loss": 0.0015,
      "step": 55330
    },
    {
      "epoch": 11.86026575225032,
      "grad_norm": 0.0010206194128841162,
      "learning_rate": 4.186312330332905e-06,
      "loss": 0.0019,
      "step": 55340
    },
    {
      "epoch": 11.862408915559365,
      "grad_norm": 0.07578253000974655,
      "learning_rate": 4.1834547792541796e-06,
      "loss": 0.124,
      "step": 55350
    },
    {
      "epoch": 11.86455207886841,
      "grad_norm": 21.250877380371094,
      "learning_rate": 4.180597228175454e-06,
      "loss": 0.5159,
      "step": 55360
    },
    {
      "epoch": 11.866695242177453,
      "grad_norm": 0.0018801750848069787,
      "learning_rate": 4.177739677096728e-06,
      "loss": 0.0007,
      "step": 55370
    },
    {
      "epoch": 11.868838405486498,
      "grad_norm": 0.0953226238489151,
      "learning_rate": 4.174882126018003e-06,
      "loss": 0.0005,
      "step": 55380
    },
    {
      "epoch": 11.870981568795543,
      "grad_norm": 0.00019362683815415949,
      "learning_rate": 4.172024574939277e-06,
      "loss": 0.1342,
      "step": 55390
    },
    {
      "epoch": 11.873124732104586,
      "grad_norm": 0.00039609026862308383,
      "learning_rate": 4.169167023860552e-06,
      "loss": 0.0005,
      "step": 55400
    },
    {
      "epoch": 11.87526789541363,
      "grad_norm": 0.08256854116916656,
      "learning_rate": 4.1663094727818265e-06,
      "loss": 0.0002,
      "step": 55410
    },
    {
      "epoch": 11.877411058722675,
      "grad_norm": 0.10740464180707932,
      "learning_rate": 4.1634519217031005e-06,
      "loss": 0.001,
      "step": 55420
    },
    {
      "epoch": 11.879554222031718,
      "grad_norm": 0.010645932517945766,
      "learning_rate": 4.160594370624375e-06,
      "loss": 0.248,
      "step": 55430
    },
    {
      "epoch": 11.881697385340763,
      "grad_norm": 0.15220987796783447,
      "learning_rate": 4.15773681954565e-06,
      "loss": 0.001,
      "step": 55440
    },
    {
      "epoch": 11.883840548649808,
      "grad_norm": 0.13122332096099854,
      "learning_rate": 4.154879268466924e-06,
      "loss": 0.0017,
      "step": 55450
    },
    {
      "epoch": 11.88598371195885,
      "grad_norm": 0.020116010680794716,
      "learning_rate": 4.152021717388199e-06,
      "loss": 0.1935,
      "step": 55460
    },
    {
      "epoch": 11.888126875267895,
      "grad_norm": 0.0011061333352699876,
      "learning_rate": 4.1491641663094735e-06,
      "loss": 0.0006,
      "step": 55470
    },
    {
      "epoch": 11.89027003857694,
      "grad_norm": 0.03954622521996498,
      "learning_rate": 4.146306615230747e-06,
      "loss": 0.2431,
      "step": 55480
    },
    {
      "epoch": 11.892413201885983,
      "grad_norm": 20.545597076416016,
      "learning_rate": 4.143449064152022e-06,
      "loss": 0.146,
      "step": 55490
    },
    {
      "epoch": 11.894556365195028,
      "grad_norm": 0.07031895220279694,
      "learning_rate": 4.140591513073296e-06,
      "loss": 0.168,
      "step": 55500
    },
    {
      "epoch": 11.896699528504072,
      "grad_norm": 0.00045191298704594374,
      "learning_rate": 4.137733961994571e-06,
      "loss": 0.0007,
      "step": 55510
    },
    {
      "epoch": 11.898842691813115,
      "grad_norm": 0.0473264716565609,
      "learning_rate": 4.134876410915846e-06,
      "loss": 0.1859,
      "step": 55520
    },
    {
      "epoch": 11.90098585512216,
      "grad_norm": 0.05269144847989082,
      "learning_rate": 4.13201885983712e-06,
      "loss": 0.0037,
      "step": 55530
    },
    {
      "epoch": 11.903129018431205,
      "grad_norm": 0.0004834341525565833,
      "learning_rate": 4.129161308758394e-06,
      "loss": 0.0007,
      "step": 55540
    },
    {
      "epoch": 11.905272181740248,
      "grad_norm": 0.002293134108185768,
      "learning_rate": 4.126303757679669e-06,
      "loss": 0.0021,
      "step": 55550
    },
    {
      "epoch": 11.907415345049293,
      "grad_norm": 0.005583653226494789,
      "learning_rate": 4.123446206600943e-06,
      "loss": 0.38,
      "step": 55560
    },
    {
      "epoch": 11.909558508358337,
      "grad_norm": 0.053459566086530685,
      "learning_rate": 4.120588655522218e-06,
      "loss": 0.0005,
      "step": 55570
    },
    {
      "epoch": 11.91170167166738,
      "grad_norm": 0.10227328538894653,
      "learning_rate": 4.117731104443492e-06,
      "loss": 0.0006,
      "step": 55580
    },
    {
      "epoch": 11.913844834976425,
      "grad_norm": 0.0016991302836686373,
      "learning_rate": 4.1148735533647666e-06,
      "loss": 0.5232,
      "step": 55590
    },
    {
      "epoch": 11.91598799828547,
      "grad_norm": 0.038356974720954895,
      "learning_rate": 4.112016002286041e-06,
      "loss": 0.001,
      "step": 55600
    },
    {
      "epoch": 11.918131161594513,
      "grad_norm": 0.005197318736463785,
      "learning_rate": 4.109158451207316e-06,
      "loss": 0.0022,
      "step": 55610
    },
    {
      "epoch": 11.920274324903557,
      "grad_norm": 0.015197732485830784,
      "learning_rate": 4.10630090012859e-06,
      "loss": 0.3356,
      "step": 55620
    },
    {
      "epoch": 11.922417488212602,
      "grad_norm": 0.0071363430470228195,
      "learning_rate": 4.103443349049865e-06,
      "loss": 0.1844,
      "step": 55630
    },
    {
      "epoch": 11.924560651521645,
      "grad_norm": 0.04435619339346886,
      "learning_rate": 4.100585797971139e-06,
      "loss": 0.1357,
      "step": 55640
    },
    {
      "epoch": 11.92670381483069,
      "grad_norm": 68.49039459228516,
      "learning_rate": 4.0977282468924135e-06,
      "loss": 0.114,
      "step": 55650
    },
    {
      "epoch": 11.928846978139735,
      "grad_norm": 0.002431703032925725,
      "learning_rate": 4.094870695813688e-06,
      "loss": 0.2509,
      "step": 55660
    },
    {
      "epoch": 11.930990141448778,
      "grad_norm": 0.0059742103330791,
      "learning_rate": 4.092013144734962e-06,
      "loss": 0.0018,
      "step": 55670
    },
    {
      "epoch": 11.933133304757822,
      "grad_norm": 23.4938907623291,
      "learning_rate": 4.089155593656237e-06,
      "loss": 0.3314,
      "step": 55680
    },
    {
      "epoch": 11.935276468066867,
      "grad_norm": 0.018152108415961266,
      "learning_rate": 4.086298042577511e-06,
      "loss": 0.1264,
      "step": 55690
    },
    {
      "epoch": 11.93741963137591,
      "grad_norm": 0.12200304120779037,
      "learning_rate": 4.083440491498786e-06,
      "loss": 0.0023,
      "step": 55700
    },
    {
      "epoch": 11.939562794684955,
      "grad_norm": 0.04739386588335037,
      "learning_rate": 4.0805829404200605e-06,
      "loss": 0.002,
      "step": 55710
    },
    {
      "epoch": 11.941705957994,
      "grad_norm": 0.00157466484233737,
      "learning_rate": 4.077725389341335e-06,
      "loss": 0.001,
      "step": 55720
    },
    {
      "epoch": 11.943849121303042,
      "grad_norm": 0.0011043534614145756,
      "learning_rate": 4.074867838262609e-06,
      "loss": 0.1475,
      "step": 55730
    },
    {
      "epoch": 11.945992284612087,
      "grad_norm": 0.03000345639884472,
      "learning_rate": 4.072010287183883e-06,
      "loss": 0.0012,
      "step": 55740
    },
    {
      "epoch": 11.948135447921132,
      "grad_norm": 0.4381488561630249,
      "learning_rate": 4.069152736105158e-06,
      "loss": 0.1382,
      "step": 55750
    },
    {
      "epoch": 11.950278611230175,
      "grad_norm": 0.03422578051686287,
      "learning_rate": 4.066295185026433e-06,
      "loss": 0.0003,
      "step": 55760
    },
    {
      "epoch": 11.95242177453922,
      "grad_norm": 0.005672392435371876,
      "learning_rate": 4.0634376339477074e-06,
      "loss": 0.0024,
      "step": 55770
    },
    {
      "epoch": 11.954564937848264,
      "grad_norm": 0.03719625249505043,
      "learning_rate": 4.060580082868981e-06,
      "loss": 0.1219,
      "step": 55780
    },
    {
      "epoch": 11.956708101157307,
      "grad_norm": 0.0007887256215326488,
      "learning_rate": 4.057722531790256e-06,
      "loss": 0.1172,
      "step": 55790
    },
    {
      "epoch": 11.958851264466352,
      "grad_norm": 0.03427973389625549,
      "learning_rate": 4.05486498071153e-06,
      "loss": 0.0203,
      "step": 55800
    },
    {
      "epoch": 11.960994427775397,
      "grad_norm": 0.0023007311392575502,
      "learning_rate": 4.052007429632805e-06,
      "loss": 0.0007,
      "step": 55810
    },
    {
      "epoch": 11.96313759108444,
      "grad_norm": 0.054958149790763855,
      "learning_rate": 4.04914987855408e-06,
      "loss": 0.0005,
      "step": 55820
    },
    {
      "epoch": 11.965280754393484,
      "grad_norm": 0.019595788791775703,
      "learning_rate": 4.046292327475354e-06,
      "loss": 0.1667,
      "step": 55830
    },
    {
      "epoch": 11.96742391770253,
      "grad_norm": 0.24181319773197174,
      "learning_rate": 4.043434776396628e-06,
      "loss": 0.0005,
      "step": 55840
    },
    {
      "epoch": 11.969567081011572,
      "grad_norm": 0.04460113123059273,
      "learning_rate": 4.040577225317902e-06,
      "loss": 0.2221,
      "step": 55850
    },
    {
      "epoch": 11.971710244320617,
      "grad_norm": 0.007063502911478281,
      "learning_rate": 4.037719674239177e-06,
      "loss": 0.0007,
      "step": 55860
    },
    {
      "epoch": 11.973853407629662,
      "grad_norm": 0.03793538734316826,
      "learning_rate": 4.034862123160452e-06,
      "loss": 0.2095,
      "step": 55870
    },
    {
      "epoch": 11.975996570938705,
      "grad_norm": 0.0016294940141960979,
      "learning_rate": 4.0320045720817266e-06,
      "loss": 0.0005,
      "step": 55880
    },
    {
      "epoch": 11.97813973424775,
      "grad_norm": 25.210004806518555,
      "learning_rate": 4.029147021003001e-06,
      "loss": 0.1519,
      "step": 55890
    },
    {
      "epoch": 11.980282897556794,
      "grad_norm": 0.0017322275089100003,
      "learning_rate": 4.026289469924275e-06,
      "loss": 0.1785,
      "step": 55900
    },
    {
      "epoch": 11.982426060865837,
      "grad_norm": 0.0038005090318620205,
      "learning_rate": 4.023431918845549e-06,
      "loss": 0.0018,
      "step": 55910
    },
    {
      "epoch": 11.984569224174882,
      "grad_norm": 0.0004108041466679424,
      "learning_rate": 4.020574367766824e-06,
      "loss": 0.0013,
      "step": 55920
    },
    {
      "epoch": 11.986712387483927,
      "grad_norm": 0.07778014987707138,
      "learning_rate": 4.017716816688099e-06,
      "loss": 0.1584,
      "step": 55930
    },
    {
      "epoch": 11.98885555079297,
      "grad_norm": 0.00037215888733044267,
      "learning_rate": 4.0148592656093735e-06,
      "loss": 0.3749,
      "step": 55940
    },
    {
      "epoch": 11.990998714102014,
      "grad_norm": 0.005010644905269146,
      "learning_rate": 4.0120017145306475e-06,
      "loss": 0.0008,
      "step": 55950
    },
    {
      "epoch": 11.993141877411059,
      "grad_norm": 0.007461023982614279,
      "learning_rate": 4.009144163451922e-06,
      "loss": 0.0006,
      "step": 55960
    },
    {
      "epoch": 11.995285040720102,
      "grad_norm": 0.03315466642379761,
      "learning_rate": 4.006286612373196e-06,
      "loss": 0.001,
      "step": 55970
    },
    {
      "epoch": 11.997428204029147,
      "grad_norm": 0.0034929350949823856,
      "learning_rate": 4.003429061294471e-06,
      "loss": 0.2887,
      "step": 55980
    },
    {
      "epoch": 11.999571367338191,
      "grad_norm": 0.009265168569982052,
      "learning_rate": 4.000571510215746e-06,
      "loss": 0.2485,
      "step": 55990
    },
    {
      "epoch": 12.0,
      "eval_accuracy": 0.966,
      "eval_f1": 0.8591160220994477,
      "eval_loss": 0.1963447630405426,
      "eval_precision": 0.9014492753623189,
      "eval_recall": 0.820580474934037,
      "eval_runtime": 703.7952,
      "eval_samples_per_second": 4.263,
      "eval_steps_per_second": 1.421,
      "step": 55992
    },
    {
      "epoch": 12.001714530647236,
      "grad_norm": 0.6308561563491821,
      "learning_rate": 3.9977139591370205e-06,
      "loss": 0.0011,
      "step": 56000
    },
    {
      "epoch": 12.003857693956279,
      "grad_norm": 0.02809596247971058,
      "learning_rate": 3.994856408058294e-06,
      "loss": 0.0003,
      "step": 56010
    },
    {
      "epoch": 12.006000857265324,
      "grad_norm": 0.002629476133733988,
      "learning_rate": 3.991998856979568e-06,
      "loss": 0.001,
      "step": 56020
    },
    {
      "epoch": 12.008144020574369,
      "grad_norm": 0.013593743555247784,
      "learning_rate": 3.989141305900843e-06,
      "loss": 0.1369,
      "step": 56030
    },
    {
      "epoch": 12.010287183883412,
      "grad_norm": 0.017689427360892296,
      "learning_rate": 3.986283754822118e-06,
      "loss": 0.4291,
      "step": 56040
    },
    {
      "epoch": 12.012430347192456,
      "grad_norm": 0.053941480815410614,
      "learning_rate": 3.983426203743393e-06,
      "loss": 0.0014,
      "step": 56050
    },
    {
      "epoch": 12.014573510501501,
      "grad_norm": 0.0572514645755291,
      "learning_rate": 3.980568652664667e-06,
      "loss": 0.4179,
      "step": 56060
    },
    {
      "epoch": 12.016716673810544,
      "grad_norm": 0.2922867238521576,
      "learning_rate": 3.977711101585941e-06,
      "loss": 0.122,
      "step": 56070
    },
    {
      "epoch": 12.018859837119589,
      "grad_norm": 0.03270123153924942,
      "learning_rate": 3.974853550507215e-06,
      "loss": 0.0013,
      "step": 56080
    },
    {
      "epoch": 12.021003000428633,
      "grad_norm": 0.04412458464503288,
      "learning_rate": 3.97199599942849e-06,
      "loss": 0.1304,
      "step": 56090
    },
    {
      "epoch": 12.023146163737676,
      "grad_norm": 20.34652328491211,
      "learning_rate": 3.969138448349765e-06,
      "loss": 0.1306,
      "step": 56100
    },
    {
      "epoch": 12.025289327046721,
      "grad_norm": 0.01314612664282322,
      "learning_rate": 3.96628089727104e-06,
      "loss": 0.0022,
      "step": 56110
    },
    {
      "epoch": 12.027432490355766,
      "grad_norm": 0.048220984637737274,
      "learning_rate": 3.9634233461923136e-06,
      "loss": 0.0007,
      "step": 56120
    },
    {
      "epoch": 12.029575653664809,
      "grad_norm": 0.023474855348467827,
      "learning_rate": 3.9605657951135875e-06,
      "loss": 0.0008,
      "step": 56130
    },
    {
      "epoch": 12.031718816973854,
      "grad_norm": 0.007592637091875076,
      "learning_rate": 3.957708244034862e-06,
      "loss": 0.2032,
      "step": 56140
    },
    {
      "epoch": 12.033861980282898,
      "grad_norm": 0.02323320135474205,
      "learning_rate": 3.954850692956137e-06,
      "loss": 0.2989,
      "step": 56150
    },
    {
      "epoch": 12.036005143591941,
      "grad_norm": 0.03241385892033577,
      "learning_rate": 3.951993141877412e-06,
      "loss": 0.0011,
      "step": 56160
    },
    {
      "epoch": 12.038148306900986,
      "grad_norm": 0.02977217175066471,
      "learning_rate": 3.949135590798686e-06,
      "loss": 0.0009,
      "step": 56170
    },
    {
      "epoch": 12.04029147021003,
      "grad_norm": 0.06440407782793045,
      "learning_rate": 3.9462780397199605e-06,
      "loss": 0.1979,
      "step": 56180
    },
    {
      "epoch": 12.042434633519074,
      "grad_norm": 0.024448107928037643,
      "learning_rate": 3.9434204886412344e-06,
      "loss": 0.3365,
      "step": 56190
    },
    {
      "epoch": 12.044577796828118,
      "grad_norm": 0.018430307507514954,
      "learning_rate": 3.940562937562509e-06,
      "loss": 0.0007,
      "step": 56200
    },
    {
      "epoch": 12.046720960137163,
      "grad_norm": 0.04501756280660629,
      "learning_rate": 3.937705386483784e-06,
      "loss": 0.1566,
      "step": 56210
    },
    {
      "epoch": 12.048864123446206,
      "grad_norm": 20.538625717163086,
      "learning_rate": 3.934847835405059e-06,
      "loss": 0.7273,
      "step": 56220
    },
    {
      "epoch": 12.051007286755251,
      "grad_norm": 17.27059555053711,
      "learning_rate": 3.931990284326333e-06,
      "loss": 0.3146,
      "step": 56230
    },
    {
      "epoch": 12.053150450064296,
      "grad_norm": 0.0027511727530509233,
      "learning_rate": 3.929132733247607e-06,
      "loss": 0.0005,
      "step": 56240
    },
    {
      "epoch": 12.055293613373339,
      "grad_norm": 0.03128505498170853,
      "learning_rate": 3.926275182168881e-06,
      "loss": 0.0011,
      "step": 56250
    },
    {
      "epoch": 12.057436776682383,
      "grad_norm": 0.016352351754903793,
      "learning_rate": 3.923417631090156e-06,
      "loss": 0.2932,
      "step": 56260
    },
    {
      "epoch": 12.059579939991428,
      "grad_norm": 0.0013509468408301473,
      "learning_rate": 3.920560080011431e-06,
      "loss": 0.0003,
      "step": 56270
    },
    {
      "epoch": 12.061723103300471,
      "grad_norm": 22.609342575073242,
      "learning_rate": 3.917702528932705e-06,
      "loss": 0.3995,
      "step": 56280
    },
    {
      "epoch": 12.063866266609516,
      "grad_norm": 0.025840185582637787,
      "learning_rate": 3.91484497785398e-06,
      "loss": 0.0022,
      "step": 56290
    },
    {
      "epoch": 12.06600942991856,
      "grad_norm": 0.02058682218194008,
      "learning_rate": 3.911987426775254e-06,
      "loss": 0.1321,
      "step": 56300
    },
    {
      "epoch": 12.068152593227603,
      "grad_norm": 0.0581539012491703,
      "learning_rate": 3.909129875696528e-06,
      "loss": 0.0012,
      "step": 56310
    },
    {
      "epoch": 12.070295756536648,
      "grad_norm": 0.003137498628348112,
      "learning_rate": 3.906272324617803e-06,
      "loss": 0.1722,
      "step": 56320
    },
    {
      "epoch": 12.072438919845693,
      "grad_norm": 0.03251078724861145,
      "learning_rate": 3.903414773539078e-06,
      "loss": 0.0005,
      "step": 56330
    },
    {
      "epoch": 12.074582083154736,
      "grad_norm": 0.021123623475432396,
      "learning_rate": 3.900557222460352e-06,
      "loss": 0.0002,
      "step": 56340
    },
    {
      "epoch": 12.07672524646378,
      "grad_norm": 0.016700517386198044,
      "learning_rate": 3.897699671381627e-06,
      "loss": 0.2219,
      "step": 56350
    },
    {
      "epoch": 12.078868409772825,
      "grad_norm": 0.004431852605193853,
      "learning_rate": 3.8948421203029005e-06,
      "loss": 0.1393,
      "step": 56360
    },
    {
      "epoch": 12.081011573081868,
      "grad_norm": 0.32960641384124756,
      "learning_rate": 3.891984569224175e-06,
      "loss": 0.3342,
      "step": 56370
    },
    {
      "epoch": 12.083154736390913,
      "grad_norm": 0.04600249230861664,
      "learning_rate": 3.88912701814545e-06,
      "loss": 0.0008,
      "step": 56380
    },
    {
      "epoch": 12.085297899699958,
      "grad_norm": 0.018522413447499275,
      "learning_rate": 3.886269467066724e-06,
      "loss": 0.196,
      "step": 56390
    },
    {
      "epoch": 12.087441063009,
      "grad_norm": 0.02026944048702717,
      "learning_rate": 3.883411915987999e-06,
      "loss": 0.1844,
      "step": 56400
    },
    {
      "epoch": 12.089584226318046,
      "grad_norm": 0.010325828567147255,
      "learning_rate": 3.880554364909273e-06,
      "loss": 0.1566,
      "step": 56410
    },
    {
      "epoch": 12.09172738962709,
      "grad_norm": 0.09215764701366425,
      "learning_rate": 3.8776968138305475e-06,
      "loss": 0.001,
      "step": 56420
    },
    {
      "epoch": 12.093870552936133,
      "grad_norm": 0.0475003756582737,
      "learning_rate": 3.874839262751822e-06,
      "loss": 0.0003,
      "step": 56430
    },
    {
      "epoch": 12.096013716245178,
      "grad_norm": 0.013571142219007015,
      "learning_rate": 3.871981711673096e-06,
      "loss": 0.0006,
      "step": 56440
    },
    {
      "epoch": 12.098156879554223,
      "grad_norm": 24.121458053588867,
      "learning_rate": 3.869124160594371e-06,
      "loss": 0.275,
      "step": 56450
    },
    {
      "epoch": 12.100300042863266,
      "grad_norm": 0.087995745241642,
      "learning_rate": 3.866266609515646e-06,
      "loss": 0.0081,
      "step": 56460
    },
    {
      "epoch": 12.10244320617231,
      "grad_norm": 0.41898220777511597,
      "learning_rate": 3.86340905843692e-06,
      "loss": 0.0012,
      "step": 56470
    },
    {
      "epoch": 12.104586369481355,
      "grad_norm": 0.01254684291779995,
      "learning_rate": 3.8605515073581945e-06,
      "loss": 0.1619,
      "step": 56480
    },
    {
      "epoch": 12.106729532790398,
      "grad_norm": 0.011673425324261189,
      "learning_rate": 3.857693956279469e-06,
      "loss": 0.0003,
      "step": 56490
    },
    {
      "epoch": 12.108872696099443,
      "grad_norm": 0.05494312569499016,
      "learning_rate": 3.854836405200743e-06,
      "loss": 0.1948,
      "step": 56500
    },
    {
      "epoch": 12.111015859408488,
      "grad_norm": 0.0070831747725605965,
      "learning_rate": 3.851978854122018e-06,
      "loss": 0.0007,
      "step": 56510
    },
    {
      "epoch": 12.11315902271753,
      "grad_norm": 0.003386711934581399,
      "learning_rate": 3.849121303043292e-06,
      "loss": 0.097,
      "step": 56520
    },
    {
      "epoch": 12.115302186026575,
      "grad_norm": 0.38746464252471924,
      "learning_rate": 3.846263751964567e-06,
      "loss": 0.1922,
      "step": 56530
    },
    {
      "epoch": 12.11744534933562,
      "grad_norm": 0.0012270946754142642,
      "learning_rate": 3.843406200885841e-06,
      "loss": 0.0004,
      "step": 56540
    },
    {
      "epoch": 12.119588512644663,
      "grad_norm": 0.09240826219320297,
      "learning_rate": 3.840548649807115e-06,
      "loss": 0.0017,
      "step": 56550
    },
    {
      "epoch": 12.121731675953708,
      "grad_norm": 0.05430605635046959,
      "learning_rate": 3.83769109872839e-06,
      "loss": 0.2082,
      "step": 56560
    },
    {
      "epoch": 12.123874839262752,
      "grad_norm": 0.003495813813060522,
      "learning_rate": 3.834833547649665e-06,
      "loss": 0.0999,
      "step": 56570
    },
    {
      "epoch": 12.126018002571795,
      "grad_norm": 0.10407834500074387,
      "learning_rate": 3.831975996570939e-06,
      "loss": 0.0005,
      "step": 56580
    },
    {
      "epoch": 12.12816116588084,
      "grad_norm": 0.010150743648409843,
      "learning_rate": 3.829118445492214e-06,
      "loss": 0.0007,
      "step": 56590
    },
    {
      "epoch": 12.130304329189885,
      "grad_norm": 0.03171848878264427,
      "learning_rate": 3.8262608944134875e-06,
      "loss": 0.0007,
      "step": 56600
    },
    {
      "epoch": 12.132447492498928,
      "grad_norm": 0.941612958908081,
      "learning_rate": 3.823403343334762e-06,
      "loss": 0.1405,
      "step": 56610
    },
    {
      "epoch": 12.134590655807973,
      "grad_norm": 0.03345165774226189,
      "learning_rate": 3.820545792256037e-06,
      "loss": 0.2241,
      "step": 56620
    },
    {
      "epoch": 12.136733819117017,
      "grad_norm": 0.006767503451555967,
      "learning_rate": 3.817688241177311e-06,
      "loss": 0.0466,
      "step": 56630
    },
    {
      "epoch": 12.13887698242606,
      "grad_norm": 0.0004633578937500715,
      "learning_rate": 3.814830690098586e-06,
      "loss": 0.0005,
      "step": 56640
    },
    {
      "epoch": 12.141020145735105,
      "grad_norm": 0.030589327216148376,
      "learning_rate": 3.8119731390198606e-06,
      "loss": 0.2781,
      "step": 56650
    },
    {
      "epoch": 12.14316330904415,
      "grad_norm": 0.033846478909254074,
      "learning_rate": 3.8091155879411345e-06,
      "loss": 0.0002,
      "step": 56660
    },
    {
      "epoch": 12.145306472353193,
      "grad_norm": 0.003083649789914489,
      "learning_rate": 3.8062580368624093e-06,
      "loss": 0.0003,
      "step": 56670
    },
    {
      "epoch": 12.147449635662237,
      "grad_norm": 0.015451506711542606,
      "learning_rate": 3.8034004857836836e-06,
      "loss": 0.0007,
      "step": 56680
    },
    {
      "epoch": 12.149592798971282,
      "grad_norm": 0.4396545886993408,
      "learning_rate": 3.8005429347049584e-06,
      "loss": 0.0011,
      "step": 56690
    },
    {
      "epoch": 12.151735962280325,
      "grad_norm": 0.003611417952924967,
      "learning_rate": 3.7976853836262327e-06,
      "loss": 0.0003,
      "step": 56700
    },
    {
      "epoch": 12.15387912558937,
      "grad_norm": 0.009609744884073734,
      "learning_rate": 3.7948278325475067e-06,
      "loss": 0.0001,
      "step": 56710
    },
    {
      "epoch": 12.156022288898415,
      "grad_norm": 0.016586584970355034,
      "learning_rate": 3.7919702814687814e-06,
      "loss": 0.0002,
      "step": 56720
    },
    {
      "epoch": 12.158165452207458,
      "grad_norm": 0.013917115516960621,
      "learning_rate": 3.7891127303900562e-06,
      "loss": 0.0001,
      "step": 56730
    },
    {
      "epoch": 12.160308615516502,
      "grad_norm": 0.001046103541739285,
      "learning_rate": 3.7862551793113306e-06,
      "loss": 0.001,
      "step": 56740
    },
    {
      "epoch": 12.162451778825547,
      "grad_norm": 0.01970910094678402,
      "learning_rate": 3.7833976282326053e-06,
      "loss": 0.0004,
      "step": 56750
    },
    {
      "epoch": 12.16459494213459,
      "grad_norm": 0.0031601155642420053,
      "learning_rate": 3.7805400771538793e-06,
      "loss": 0.0001,
      "step": 56760
    },
    {
      "epoch": 12.166738105443635,
      "grad_norm": 0.004057693760842085,
      "learning_rate": 3.7776825260751536e-06,
      "loss": 0.2085,
      "step": 56770
    },
    {
      "epoch": 12.16888126875268,
      "grad_norm": 0.00047074994654394686,
      "learning_rate": 3.7748249749964284e-06,
      "loss": 0.2395,
      "step": 56780
    },
    {
      "epoch": 12.171024432061722,
      "grad_norm": 0.005581303499639034,
      "learning_rate": 3.7719674239177027e-06,
      "loss": 0.1426,
      "step": 56790
    },
    {
      "epoch": 12.173167595370767,
      "grad_norm": 0.00891117099672556,
      "learning_rate": 3.7691098728389775e-06,
      "loss": 0.0001,
      "step": 56800
    },
    {
      "epoch": 12.175310758679812,
      "grad_norm": 0.009602617472410202,
      "learning_rate": 3.766252321760252e-06,
      "loss": 0.4291,
      "step": 56810
    },
    {
      "epoch": 12.177453921988855,
      "grad_norm": 0.009641929529607296,
      "learning_rate": 3.7633947706815262e-06,
      "loss": 0.0009,
      "step": 56820
    },
    {
      "epoch": 12.1795970852979,
      "grad_norm": 0.0008397747296839952,
      "learning_rate": 3.7605372196028006e-06,
      "loss": 0.1509,
      "step": 56830
    },
    {
      "epoch": 12.181740248606944,
      "grad_norm": 0.030378786846995354,
      "learning_rate": 3.7576796685240754e-06,
      "loss": 0.0007,
      "step": 56840
    },
    {
      "epoch": 12.183883411915987,
      "grad_norm": 0.07260502129793167,
      "learning_rate": 3.7548221174453497e-06,
      "loss": 0.25,
      "step": 56850
    },
    {
      "epoch": 12.186026575225032,
      "grad_norm": 19.455411911010742,
      "learning_rate": 3.7519645663666245e-06,
      "loss": 0.1553,
      "step": 56860
    },
    {
      "epoch": 12.188169738534077,
      "grad_norm": 0.05431457236409187,
      "learning_rate": 3.7491070152878984e-06,
      "loss": 0.4046,
      "step": 56870
    },
    {
      "epoch": 12.19031290184312,
      "grad_norm": 0.02685881033539772,
      "learning_rate": 3.7462494642091728e-06,
      "loss": 0.0004,
      "step": 56880
    },
    {
      "epoch": 12.192456065152165,
      "grad_norm": 0.26230478286743164,
      "learning_rate": 3.7433919131304475e-06,
      "loss": 0.0008,
      "step": 56890
    },
    {
      "epoch": 12.19459922846121,
      "grad_norm": 0.00040279942913912237,
      "learning_rate": 3.740534362051722e-06,
      "loss": 0.1393,
      "step": 56900
    },
    {
      "epoch": 12.196742391770252,
      "grad_norm": 0.00428348034620285,
      "learning_rate": 3.7376768109729967e-06,
      "loss": 0.0014,
      "step": 56910
    },
    {
      "epoch": 12.198885555079297,
      "grad_norm": 0.049142949283123016,
      "learning_rate": 3.734819259894271e-06,
      "loss": 0.2402,
      "step": 56920
    },
    {
      "epoch": 12.201028718388342,
      "grad_norm": 0.2666376233100891,
      "learning_rate": 3.7319617088155454e-06,
      "loss": 0.0008,
      "step": 56930
    },
    {
      "epoch": 12.203171881697385,
      "grad_norm": 0.0012592324055731297,
      "learning_rate": 3.7291041577368197e-06,
      "loss": 0.0006,
      "step": 56940
    },
    {
      "epoch": 12.20531504500643,
      "grad_norm": 0.03380376473069191,
      "learning_rate": 3.7262466066580945e-06,
      "loss": 0.0047,
      "step": 56950
    },
    {
      "epoch": 12.207458208315474,
      "grad_norm": 0.00038426928222179413,
      "learning_rate": 3.723389055579369e-06,
      "loss": 0.0001,
      "step": 56960
    },
    {
      "epoch": 12.209601371624517,
      "grad_norm": 36.308921813964844,
      "learning_rate": 3.7205315045006436e-06,
      "loss": 0.4129,
      "step": 56970
    },
    {
      "epoch": 12.211744534933562,
      "grad_norm": 0.016130762174725533,
      "learning_rate": 3.7176739534219175e-06,
      "loss": 0.1595,
      "step": 56980
    },
    {
      "epoch": 12.213887698242607,
      "grad_norm": 0.14454488456249237,
      "learning_rate": 3.714816402343192e-06,
      "loss": 0.2308,
      "step": 56990
    },
    {
      "epoch": 12.21603086155165,
      "grad_norm": 0.15395508706569672,
      "learning_rate": 3.7119588512644667e-06,
      "loss": 0.0012,
      "step": 57000
    },
    {
      "epoch": 12.218174024860694,
      "grad_norm": 0.3074449300765991,
      "learning_rate": 3.709101300185741e-06,
      "loss": 0.0013,
      "step": 57010
    },
    {
      "epoch": 12.220317188169739,
      "grad_norm": 0.02578561007976532,
      "learning_rate": 3.706243749107016e-06,
      "loss": 0.0003,
      "step": 57020
    },
    {
      "epoch": 12.222460351478782,
      "grad_norm": 0.013094511814415455,
      "learning_rate": 3.7033861980282897e-06,
      "loss": 0.0004,
      "step": 57030
    },
    {
      "epoch": 12.224603514787827,
      "grad_norm": 0.012942050583660603,
      "learning_rate": 3.7005286469495645e-06,
      "loss": 0.3591,
      "step": 57040
    },
    {
      "epoch": 12.226746678096871,
      "grad_norm": 30.1202392578125,
      "learning_rate": 3.697671095870839e-06,
      "loss": 0.1994,
      "step": 57050
    },
    {
      "epoch": 12.228889841405914,
      "grad_norm": 0.005436840932816267,
      "learning_rate": 3.6948135447921136e-06,
      "loss": 0.0005,
      "step": 57060
    },
    {
      "epoch": 12.23103300471496,
      "grad_norm": 0.06614746898412704,
      "learning_rate": 3.691955993713388e-06,
      "loss": 0.1693,
      "step": 57070
    },
    {
      "epoch": 12.233176168024004,
      "grad_norm": 0.0005330366548150778,
      "learning_rate": 3.6890984426346628e-06,
      "loss": 0.0005,
      "step": 57080
    },
    {
      "epoch": 12.235319331333047,
      "grad_norm": 0.001800975645892322,
      "learning_rate": 3.6862408915559367e-06,
      "loss": 0.0005,
      "step": 57090
    },
    {
      "epoch": 12.237462494642092,
      "grad_norm": 0.0011601562146097422,
      "learning_rate": 3.683383340477211e-06,
      "loss": 0.2538,
      "step": 57100
    },
    {
      "epoch": 12.239605657951136,
      "grad_norm": 21.62029457092285,
      "learning_rate": 3.680525789398486e-06,
      "loss": 0.3172,
      "step": 57110
    },
    {
      "epoch": 12.24174882126018,
      "grad_norm": 2094.445068359375,
      "learning_rate": 3.6776682383197606e-06,
      "loss": 0.2457,
      "step": 57120
    },
    {
      "epoch": 12.243891984569224,
      "grad_norm": 25.122509002685547,
      "learning_rate": 3.674810687241035e-06,
      "loss": 0.3562,
      "step": 57130
    },
    {
      "epoch": 12.246035147878269,
      "grad_norm": 0.026728034019470215,
      "learning_rate": 3.671953136162309e-06,
      "loss": 0.1653,
      "step": 57140
    },
    {
      "epoch": 12.248178311187312,
      "grad_norm": 0.006989357061684132,
      "learning_rate": 3.6690955850835836e-06,
      "loss": 0.0019,
      "step": 57150
    },
    {
      "epoch": 12.250321474496356,
      "grad_norm": 0.001993941841647029,
      "learning_rate": 3.666238034004858e-06,
      "loss": 0.0033,
      "step": 57160
    },
    {
      "epoch": 12.252464637805401,
      "grad_norm": 0.044054433703422546,
      "learning_rate": 3.6633804829261328e-06,
      "loss": 0.3586,
      "step": 57170
    },
    {
      "epoch": 12.254607801114444,
      "grad_norm": 0.0006859709392301738,
      "learning_rate": 3.660522931847407e-06,
      "loss": 0.1378,
      "step": 57180
    },
    {
      "epoch": 12.256750964423489,
      "grad_norm": 0.3117161691188812,
      "learning_rate": 3.6576653807686815e-06,
      "loss": 0.0014,
      "step": 57190
    },
    {
      "epoch": 12.258894127732534,
      "grad_norm": 43.8901481628418,
      "learning_rate": 3.654807829689956e-06,
      "loss": 0.4605,
      "step": 57200
    },
    {
      "epoch": 12.261037291041577,
      "grad_norm": 0.008464427664875984,
      "learning_rate": 3.6519502786112306e-06,
      "loss": 0.1172,
      "step": 57210
    },
    {
      "epoch": 12.263180454350621,
      "grad_norm": 0.04288959130644798,
      "learning_rate": 3.649092727532505e-06,
      "loss": 0.1489,
      "step": 57220
    },
    {
      "epoch": 12.265323617659666,
      "grad_norm": 0.05478387698531151,
      "learning_rate": 3.6462351764537797e-06,
      "loss": 0.001,
      "step": 57230
    },
    {
      "epoch": 12.267466780968709,
      "grad_norm": 0.0017074448987841606,
      "learning_rate": 3.643377625375054e-06,
      "loss": 0.1256,
      "step": 57240
    },
    {
      "epoch": 12.269609944277754,
      "grad_norm": 0.0020389712881296873,
      "learning_rate": 3.640520074296328e-06,
      "loss": 0.1295,
      "step": 57250
    },
    {
      "epoch": 12.271753107586798,
      "grad_norm": 0.015209159813821316,
      "learning_rate": 3.6376625232176028e-06,
      "loss": 0.3028,
      "step": 57260
    },
    {
      "epoch": 12.273896270895841,
      "grad_norm": 24.498559951782227,
      "learning_rate": 3.634804972138877e-06,
      "loss": 0.1255,
      "step": 57270
    },
    {
      "epoch": 12.276039434204886,
      "grad_norm": 0.8465656638145447,
      "learning_rate": 3.631947421060152e-06,
      "loss": 0.122,
      "step": 57280
    },
    {
      "epoch": 12.278182597513931,
      "grad_norm": 0.025500547140836716,
      "learning_rate": 3.6290898699814263e-06,
      "loss": 0.0008,
      "step": 57290
    },
    {
      "epoch": 12.280325760822974,
      "grad_norm": 0.09571798145771027,
      "learning_rate": 3.6262323189027006e-06,
      "loss": 0.1807,
      "step": 57300
    },
    {
      "epoch": 12.282468924132019,
      "grad_norm": 0.01167169027030468,
      "learning_rate": 3.623374767823975e-06,
      "loss": 0.0002,
      "step": 57310
    },
    {
      "epoch": 12.284612087441063,
      "grad_norm": 0.06958412379026413,
      "learning_rate": 3.6205172167452497e-06,
      "loss": 0.0002,
      "step": 57320
    },
    {
      "epoch": 12.286755250750106,
      "grad_norm": 17.72817039489746,
      "learning_rate": 3.617659665666524e-06,
      "loss": 0.1226,
      "step": 57330
    },
    {
      "epoch": 12.288898414059151,
      "grad_norm": 0.002557696308940649,
      "learning_rate": 3.614802114587799e-06,
      "loss": 0.12,
      "step": 57340
    },
    {
      "epoch": 12.291041577368196,
      "grad_norm": 0.027726521715521812,
      "learning_rate": 3.611944563509073e-06,
      "loss": 0.001,
      "step": 57350
    },
    {
      "epoch": 12.293184740677239,
      "grad_norm": 0.0024581870529800653,
      "learning_rate": 3.609087012430347e-06,
      "loss": 0.0017,
      "step": 57360
    },
    {
      "epoch": 12.295327903986284,
      "grad_norm": 0.009221473708748817,
      "learning_rate": 3.606229461351622e-06,
      "loss": 0.0002,
      "step": 57370
    },
    {
      "epoch": 12.297471067295328,
      "grad_norm": 0.030833454802632332,
      "learning_rate": 3.6033719102728963e-06,
      "loss": 0.2001,
      "step": 57380
    },
    {
      "epoch": 12.299614230604371,
      "grad_norm": 0.002857095329090953,
      "learning_rate": 3.600514359194171e-06,
      "loss": 0.1988,
      "step": 57390
    },
    {
      "epoch": 12.301757393913416,
      "grad_norm": 0.027198240160942078,
      "learning_rate": 3.5976568081154454e-06,
      "loss": 0.0006,
      "step": 57400
    },
    {
      "epoch": 12.30390055722246,
      "grad_norm": 0.1384519636631012,
      "learning_rate": 3.5947992570367198e-06,
      "loss": 0.0009,
      "step": 57410
    },
    {
      "epoch": 12.306043720531505,
      "grad_norm": 0.0012318291701376438,
      "learning_rate": 3.591941705957994e-06,
      "loss": 0.1806,
      "step": 57420
    },
    {
      "epoch": 12.308186883840548,
      "grad_norm": 0.009759380482137203,
      "learning_rate": 3.589084154879269e-06,
      "loss": 0.0006,
      "step": 57430
    },
    {
      "epoch": 12.310330047149593,
      "grad_norm": 0.05375804752111435,
      "learning_rate": 3.5862266038005432e-06,
      "loss": 0.3588,
      "step": 57440
    },
    {
      "epoch": 12.312473210458638,
      "grad_norm": 0.0002678383607417345,
      "learning_rate": 3.583369052721818e-06,
      "loss": 0.0004,
      "step": 57450
    },
    {
      "epoch": 12.31461637376768,
      "grad_norm": 0.0062078130431473255,
      "learning_rate": 3.580511501643092e-06,
      "loss": 0.2283,
      "step": 57460
    },
    {
      "epoch": 12.316759537076726,
      "grad_norm": 0.005665122997015715,
      "learning_rate": 3.5776539505643663e-06,
      "loss": 0.1738,
      "step": 57470
    },
    {
      "epoch": 12.31890270038577,
      "grad_norm": 0.05993252247571945,
      "learning_rate": 3.574796399485641e-06,
      "loss": 0.0012,
      "step": 57480
    },
    {
      "epoch": 12.321045863694813,
      "grad_norm": 0.00019567810522858053,
      "learning_rate": 3.5719388484069154e-06,
      "loss": 0.0004,
      "step": 57490
    },
    {
      "epoch": 12.323189027003858,
      "grad_norm": 0.01851886510848999,
      "learning_rate": 3.56908129732819e-06,
      "loss": 0.1438,
      "step": 57500
    },
    {
      "epoch": 12.325332190312903,
      "grad_norm": 0.0016390254022553563,
      "learning_rate": 3.566223746249465e-06,
      "loss": 0.2322,
      "step": 57510
    },
    {
      "epoch": 12.327475353621946,
      "grad_norm": 0.006009917706251144,
      "learning_rate": 3.563366195170739e-06,
      "loss": 0.0009,
      "step": 57520
    },
    {
      "epoch": 12.32961851693099,
      "grad_norm": 18.950149536132812,
      "learning_rate": 3.5605086440920133e-06,
      "loss": 0.3718,
      "step": 57530
    },
    {
      "epoch": 12.331761680240035,
      "grad_norm": 0.003396127140149474,
      "learning_rate": 3.557651093013288e-06,
      "loss": 0.0002,
      "step": 57540
    },
    {
      "epoch": 12.333904843549078,
      "grad_norm": 0.0009525378700345755,
      "learning_rate": 3.5547935419345624e-06,
      "loss": 0.1934,
      "step": 57550
    },
    {
      "epoch": 12.336048006858123,
      "grad_norm": 0.033089302480220795,
      "learning_rate": 3.551935990855837e-06,
      "loss": 0.1154,
      "step": 57560
    },
    {
      "epoch": 12.338191170167168,
      "grad_norm": 12.321133613586426,
      "learning_rate": 3.549078439777111e-06,
      "loss": 0.1271,
      "step": 57570
    },
    {
      "epoch": 12.34033433347621,
      "grad_norm": 0.14830857515335083,
      "learning_rate": 3.546220888698386e-06,
      "loss": 0.0015,
      "step": 57580
    },
    {
      "epoch": 12.342477496785255,
      "grad_norm": 0.00875944271683693,
      "learning_rate": 3.54336333761966e-06,
      "loss": 0.1066,
      "step": 57590
    },
    {
      "epoch": 12.3446206600943,
      "grad_norm": 0.005566652864217758,
      "learning_rate": 3.540505786540935e-06,
      "loss": 0.0023,
      "step": 57600
    },
    {
      "epoch": 12.346763823403343,
      "grad_norm": 0.6949318647384644,
      "learning_rate": 3.5376482354622093e-06,
      "loss": 0.1659,
      "step": 57610
    },
    {
      "epoch": 12.348906986712388,
      "grad_norm": 0.15830549597740173,
      "learning_rate": 3.5347906843834833e-06,
      "loss": 0.0007,
      "step": 57620
    },
    {
      "epoch": 12.351050150021432,
      "grad_norm": 0.10355501621961594,
      "learning_rate": 3.531933133304758e-06,
      "loss": 0.141,
      "step": 57630
    },
    {
      "epoch": 12.353193313330475,
      "grad_norm": 0.0018645442323759198,
      "learning_rate": 3.5290755822260324e-06,
      "loss": 0.0003,
      "step": 57640
    },
    {
      "epoch": 12.35533647663952,
      "grad_norm": 0.0008077831007540226,
      "learning_rate": 3.526218031147307e-06,
      "loss": 0.0001,
      "step": 57650
    },
    {
      "epoch": 12.357479639948565,
      "grad_norm": 0.024015799164772034,
      "learning_rate": 3.5233604800685815e-06,
      "loss": 0.0001,
      "step": 57660
    },
    {
      "epoch": 12.359622803257608,
      "grad_norm": 0.0011440881062299013,
      "learning_rate": 3.5205029289898563e-06,
      "loss": 0.3018,
      "step": 57670
    },
    {
      "epoch": 12.361765966566653,
      "grad_norm": 17.29474639892578,
      "learning_rate": 3.5176453779111302e-06,
      "loss": 0.175,
      "step": 57680
    },
    {
      "epoch": 12.363909129875697,
      "grad_norm": 0.10387173295021057,
      "learning_rate": 3.514787826832405e-06,
      "loss": 0.1017,
      "step": 57690
    },
    {
      "epoch": 12.36605229318474,
      "grad_norm": 0.0003015496185980737,
      "learning_rate": 3.5119302757536793e-06,
      "loss": 0.1145,
      "step": 57700
    },
    {
      "epoch": 12.368195456493785,
      "grad_norm": 0.013611944392323494,
      "learning_rate": 3.509072724674954e-06,
      "loss": 0.0005,
      "step": 57710
    },
    {
      "epoch": 12.37033861980283,
      "grad_norm": 0.0029922949615865946,
      "learning_rate": 3.5062151735962285e-06,
      "loss": 0.1886,
      "step": 57720
    },
    {
      "epoch": 12.372481783111873,
      "grad_norm": 17.460617065429688,
      "learning_rate": 3.5033576225175024e-06,
      "loss": 0.1053,
      "step": 57730
    },
    {
      "epoch": 12.374624946420917,
      "grad_norm": 0.02584908716380596,
      "learning_rate": 3.500500071438777e-06,
      "loss": 0.0018,
      "step": 57740
    },
    {
      "epoch": 12.376768109729962,
      "grad_norm": 0.03328576683998108,
      "learning_rate": 3.4976425203600515e-06,
      "loss": 0.125,
      "step": 57750
    },
    {
      "epoch": 12.378911273039005,
      "grad_norm": 0.1720067411661148,
      "learning_rate": 3.4947849692813263e-06,
      "loss": 0.0003,
      "step": 57760
    },
    {
      "epoch": 12.38105443634805,
      "grad_norm": 20.152873992919922,
      "learning_rate": 3.4919274182026007e-06,
      "loss": 0.2399,
      "step": 57770
    },
    {
      "epoch": 12.383197599657095,
      "grad_norm": 0.007933071814477444,
      "learning_rate": 3.489069867123875e-06,
      "loss": 0.1148,
      "step": 57780
    },
    {
      "epoch": 12.385340762966138,
      "grad_norm": 0.0004927750560455024,
      "learning_rate": 3.4862123160451494e-06,
      "loss": 0.0004,
      "step": 57790
    },
    {
      "epoch": 12.387483926275182,
      "grad_norm": 0.01347634568810463,
      "learning_rate": 3.483354764966424e-06,
      "loss": 0.0782,
      "step": 57800
    },
    {
      "epoch": 12.389627089584227,
      "grad_norm": 0.018636511638760567,
      "learning_rate": 3.4804972138876985e-06,
      "loss": 0.1024,
      "step": 57810
    },
    {
      "epoch": 12.39177025289327,
      "grad_norm": 0.02245175465941429,
      "learning_rate": 3.4776396628089733e-06,
      "loss": 0.2404,
      "step": 57820
    },
    {
      "epoch": 12.393913416202315,
      "grad_norm": 0.22792300581932068,
      "learning_rate": 3.4747821117302476e-06,
      "loss": 0.0005,
      "step": 57830
    },
    {
      "epoch": 12.39605657951136,
      "grad_norm": 0.01523654442280531,
      "learning_rate": 3.4719245606515215e-06,
      "loss": 0.0003,
      "step": 57840
    },
    {
      "epoch": 12.398199742820402,
      "grad_norm": 0.0008291153935715556,
      "learning_rate": 3.4690670095727963e-06,
      "loss": 0.1108,
      "step": 57850
    },
    {
      "epoch": 12.400342906129447,
      "grad_norm": 0.0006870835204608738,
      "learning_rate": 3.4662094584940707e-06,
      "loss": 0.0003,
      "step": 57860
    },
    {
      "epoch": 12.402486069438492,
      "grad_norm": 0.0007088215788826346,
      "learning_rate": 3.4633519074153454e-06,
      "loss": 0.0007,
      "step": 57870
    },
    {
      "epoch": 12.404629232747535,
      "grad_norm": 0.009270834736526012,
      "learning_rate": 3.46049435633662e-06,
      "loss": 0.0002,
      "step": 57880
    },
    {
      "epoch": 12.40677239605658,
      "grad_norm": 0.018668508157134056,
      "learning_rate": 3.457636805257894e-06,
      "loss": 0.0008,
      "step": 57890
    },
    {
      "epoch": 12.408915559365624,
      "grad_norm": 0.0010443811770528555,
      "learning_rate": 3.4547792541791685e-06,
      "loss": 0.0003,
      "step": 57900
    },
    {
      "epoch": 12.411058722674667,
      "grad_norm": 0.02153615467250347,
      "learning_rate": 3.4519217031004433e-06,
      "loss": 0.0002,
      "step": 57910
    },
    {
      "epoch": 12.413201885983712,
      "grad_norm": 0.036867085844278336,
      "learning_rate": 3.4490641520217176e-06,
      "loss": 0.0007,
      "step": 57920
    },
    {
      "epoch": 12.415345049292757,
      "grad_norm": 21.252614974975586,
      "learning_rate": 3.4462066009429924e-06,
      "loss": 0.0005,
      "step": 57930
    },
    {
      "epoch": 12.4174882126018,
      "grad_norm": 0.006281771697103977,
      "learning_rate": 3.4433490498642668e-06,
      "loss": 0.5611,
      "step": 57940
    },
    {
      "epoch": 12.419631375910845,
      "grad_norm": 0.000643963401671499,
      "learning_rate": 3.4404914987855407e-06,
      "loss": 0.2187,
      "step": 57950
    },
    {
      "epoch": 12.42177453921989,
      "grad_norm": 0.013219965621829033,
      "learning_rate": 3.4376339477068155e-06,
      "loss": 0.3243,
      "step": 57960
    },
    {
      "epoch": 12.423917702528932,
      "grad_norm": 0.03690319135785103,
      "learning_rate": 3.4347763966280902e-06,
      "loss": 0.0023,
      "step": 57970
    },
    {
      "epoch": 12.426060865837977,
      "grad_norm": 0.00792787317186594,
      "learning_rate": 3.4319188455493646e-06,
      "loss": 0.1169,
      "step": 57980
    },
    {
      "epoch": 12.428204029147022,
      "grad_norm": 0.0005025888094678521,
      "learning_rate": 3.4290612944706394e-06,
      "loss": 0.0013,
      "step": 57990
    },
    {
      "epoch": 12.430347192456065,
      "grad_norm": 0.00037982111098244786,
      "learning_rate": 3.4262037433919133e-06,
      "loss": 0.2084,
      "step": 58000
    },
    {
      "epoch": 12.43249035576511,
      "grad_norm": 0.015152125619351864,
      "learning_rate": 3.4233461923131876e-06,
      "loss": 0.0005,
      "step": 58010
    },
    {
      "epoch": 12.434633519074154,
      "grad_norm": 0.012575131841003895,
      "learning_rate": 3.4204886412344624e-06,
      "loss": 0.2244,
      "step": 58020
    },
    {
      "epoch": 12.436776682383197,
      "grad_norm": 0.01995319500565529,
      "learning_rate": 3.4176310901557368e-06,
      "loss": 0.1856,
      "step": 58030
    },
    {
      "epoch": 12.438919845692242,
      "grad_norm": 0.002676201518625021,
      "learning_rate": 3.4147735390770115e-06,
      "loss": 0.0007,
      "step": 58040
    },
    {
      "epoch": 12.441063009001287,
      "grad_norm": 0.048124995082616806,
      "learning_rate": 3.4119159879982855e-06,
      "loss": 0.001,
      "step": 58050
    },
    {
      "epoch": 12.44320617231033,
      "grad_norm": 1.3028216361999512,
      "learning_rate": 3.4090584369195602e-06,
      "loss": 0.2794,
      "step": 58060
    },
    {
      "epoch": 12.445349335619374,
      "grad_norm": 0.000629047688562423,
      "learning_rate": 3.4062008858408346e-06,
      "loss": 0.0012,
      "step": 58070
    },
    {
      "epoch": 12.447492498928419,
      "grad_norm": 0.05699026957154274,
      "learning_rate": 3.4033433347621094e-06,
      "loss": 0.0005,
      "step": 58080
    },
    {
      "epoch": 12.449635662237462,
      "grad_norm": 0.020443767309188843,
      "learning_rate": 3.4004857836833837e-06,
      "loss": 0.1842,
      "step": 58090
    },
    {
      "epoch": 12.451778825546507,
      "grad_norm": 0.01892513781785965,
      "learning_rate": 3.3976282326046585e-06,
      "loss": 0.0006,
      "step": 58100
    },
    {
      "epoch": 12.453921988855551,
      "grad_norm": 0.005260945297777653,
      "learning_rate": 3.3947706815259324e-06,
      "loss": 0.0001,
      "step": 58110
    },
    {
      "epoch": 12.456065152164594,
      "grad_norm": 0.00014119199477136135,
      "learning_rate": 3.3919131304472068e-06,
      "loss": 0.0006,
      "step": 58120
    },
    {
      "epoch": 12.45820831547364,
      "grad_norm": 0.01964407227933407,
      "learning_rate": 3.3890555793684816e-06,
      "loss": 0.0009,
      "step": 58130
    },
    {
      "epoch": 12.460351478782684,
      "grad_norm": 0.014676380902528763,
      "learning_rate": 3.386198028289756e-06,
      "loss": 0.0001,
      "step": 58140
    },
    {
      "epoch": 12.462494642091727,
      "grad_norm": 0.00020949995086994022,
      "learning_rate": 3.3833404772110307e-06,
      "loss": 0.0003,
      "step": 58150
    },
    {
      "epoch": 12.464637805400772,
      "grad_norm": 0.040050942450761795,
      "learning_rate": 3.3804829261323046e-06,
      "loss": 0.0002,
      "step": 58160
    },
    {
      "epoch": 12.466780968709816,
      "grad_norm": 0.018659444525837898,
      "learning_rate": 3.3776253750535794e-06,
      "loss": 0.3623,
      "step": 58170
    },
    {
      "epoch": 12.46892413201886,
      "grad_norm": 0.0016141470987349749,
      "learning_rate": 3.3747678239748537e-06,
      "loss": 0.1245,
      "step": 58180
    },
    {
      "epoch": 12.471067295327904,
      "grad_norm": 0.09875920414924622,
      "learning_rate": 3.3719102728961285e-06,
      "loss": 0.0002,
      "step": 58190
    },
    {
      "epoch": 12.473210458636949,
      "grad_norm": 0.0009364541620016098,
      "learning_rate": 3.369052721817403e-06,
      "loss": 0.0001,
      "step": 58200
    },
    {
      "epoch": 12.475353621945992,
      "grad_norm": 0.0002618991711642593,
      "learning_rate": 3.366195170738677e-06,
      "loss": 0.2949,
      "step": 58210
    },
    {
      "epoch": 12.477496785255036,
      "grad_norm": 0.012917272746562958,
      "learning_rate": 3.3633376196599516e-06,
      "loss": 0.0001,
      "step": 58220
    },
    {
      "epoch": 12.479639948564081,
      "grad_norm": 0.0001771724346326664,
      "learning_rate": 3.360480068581226e-06,
      "loss": 0.0004,
      "step": 58230
    },
    {
      "epoch": 12.481783111873124,
      "grad_norm": 0.03127007931470871,
      "learning_rate": 3.3576225175025007e-06,
      "loss": 0.6136,
      "step": 58240
    },
    {
      "epoch": 12.483926275182169,
      "grad_norm": 0.05935570225119591,
      "learning_rate": 3.354764966423775e-06,
      "loss": 0.1235,
      "step": 58250
    },
    {
      "epoch": 12.486069438491214,
      "grad_norm": 31.86337661743164,
      "learning_rate": 3.35190741534505e-06,
      "loss": 0.9021,
      "step": 58260
    },
    {
      "epoch": 12.488212601800257,
      "grad_norm": 24.13583755493164,
      "learning_rate": 3.3490498642663238e-06,
      "loss": 0.1428,
      "step": 58270
    },
    {
      "epoch": 12.490355765109301,
      "grad_norm": 0.021628662943840027,
      "learning_rate": 3.3461923131875985e-06,
      "loss": 0.0012,
      "step": 58280
    },
    {
      "epoch": 12.492498928418346,
      "grad_norm": 0.040279168635606766,
      "learning_rate": 3.343334762108873e-06,
      "loss": 0.0022,
      "step": 58290
    },
    {
      "epoch": 12.494642091727389,
      "grad_norm": 0.04813506826758385,
      "learning_rate": 3.3404772110301477e-06,
      "loss": 0.1741,
      "step": 58300
    },
    {
      "epoch": 12.496785255036434,
      "grad_norm": 0.12671077251434326,
      "learning_rate": 3.337619659951422e-06,
      "loss": 0.0008,
      "step": 58310
    },
    {
      "epoch": 12.498928418345479,
      "grad_norm": 0.0016900472110137343,
      "learning_rate": 3.334762108872696e-06,
      "loss": 0.0011,
      "step": 58320
    },
    {
      "epoch": 12.501071581654521,
      "grad_norm": 0.027407454326748848,
      "learning_rate": 3.3319045577939707e-06,
      "loss": 0.0005,
      "step": 58330
    },
    {
      "epoch": 12.503214744963566,
      "grad_norm": 0.014718438498675823,
      "learning_rate": 3.3290470067152455e-06,
      "loss": 0.0005,
      "step": 58340
    },
    {
      "epoch": 12.505357908272611,
      "grad_norm": 0.00037077857996337116,
      "learning_rate": 3.32618945563652e-06,
      "loss": 0.1643,
      "step": 58350
    },
    {
      "epoch": 12.507501071581654,
      "grad_norm": 0.004909595008939505,
      "learning_rate": 3.3233319045577946e-06,
      "loss": 0.0003,
      "step": 58360
    },
    {
      "epoch": 12.509644234890699,
      "grad_norm": 0.011100415140390396,
      "learning_rate": 3.3204743534790685e-06,
      "loss": 0.0008,
      "step": 58370
    },
    {
      "epoch": 12.511787398199743,
      "grad_norm": 0.01313413493335247,
      "learning_rate": 3.317616802400343e-06,
      "loss": 0.2414,
      "step": 58380
    },
    {
      "epoch": 12.513930561508786,
      "grad_norm": 0.0011369953863322735,
      "learning_rate": 3.3147592513216177e-06,
      "loss": 0.1744,
      "step": 58390
    },
    {
      "epoch": 12.516073724817831,
      "grad_norm": 0.02631012722849846,
      "learning_rate": 3.311901700242892e-06,
      "loss": 0.0008,
      "step": 58400
    },
    {
      "epoch": 12.518216888126876,
      "grad_norm": 0.0006029625074006617,
      "learning_rate": 3.309044149164167e-06,
      "loss": 0.0008,
      "step": 58410
    },
    {
      "epoch": 12.520360051435919,
      "grad_norm": 0.0024078276474028826,
      "learning_rate": 3.306186598085441e-06,
      "loss": 0.0001,
      "step": 58420
    },
    {
      "epoch": 12.522503214744964,
      "grad_norm": 33.84392166137695,
      "learning_rate": 3.3033290470067155e-06,
      "loss": 0.1169,
      "step": 58430
    },
    {
      "epoch": 12.524646378054008,
      "grad_norm": 0.0016818715957924724,
      "learning_rate": 3.30047149592799e-06,
      "loss": 0.0005,
      "step": 58440
    },
    {
      "epoch": 12.526789541363051,
      "grad_norm": 14.831865310668945,
      "learning_rate": 3.2976139448492646e-06,
      "loss": 0.0014,
      "step": 58450
    },
    {
      "epoch": 12.528932704672096,
      "grad_norm": 0.0003502324689179659,
      "learning_rate": 3.294756393770539e-06,
      "loss": 0.0004,
      "step": 58460
    },
    {
      "epoch": 12.53107586798114,
      "grad_norm": 0.0004583138506859541,
      "learning_rate": 3.2918988426918138e-06,
      "loss": 0.2256,
      "step": 58470
    },
    {
      "epoch": 12.533219031290184,
      "grad_norm": 0.0007717699045315385,
      "learning_rate": 3.2890412916130877e-06,
      "loss": 0.0002,
      "step": 58480
    },
    {
      "epoch": 12.535362194599228,
      "grad_norm": 0.14359208941459656,
      "learning_rate": 3.286183740534362e-06,
      "loss": 0.1672,
      "step": 58490
    },
    {
      "epoch": 12.537505357908273,
      "grad_norm": 0.015690652653574944,
      "learning_rate": 3.283326189455637e-06,
      "loss": 0.0004,
      "step": 58500
    },
    {
      "epoch": 12.539648521217316,
      "grad_norm": 0.0019418909214437008,
      "learning_rate": 3.280468638376911e-06,
      "loss": 0.3432,
      "step": 58510
    },
    {
      "epoch": 12.54179168452636,
      "grad_norm": 0.0040849051438272,
      "learning_rate": 3.277611087298186e-06,
      "loss": 0.0013,
      "step": 58520
    },
    {
      "epoch": 12.543934847835406,
      "grad_norm": 0.012232777662575245,
      "learning_rate": 3.2747535362194603e-06,
      "loss": 0.0001,
      "step": 58530
    },
    {
      "epoch": 12.546078011144449,
      "grad_norm": 0.017208244651556015,
      "learning_rate": 3.2718959851407346e-06,
      "loss": 0.0005,
      "step": 58540
    },
    {
      "epoch": 12.548221174453493,
      "grad_norm": 0.008219386450946331,
      "learning_rate": 3.269038434062009e-06,
      "loss": 0.0019,
      "step": 58550
    },
    {
      "epoch": 12.550364337762538,
      "grad_norm": 0.004040507599711418,
      "learning_rate": 3.2661808829832838e-06,
      "loss": 0.0001,
      "step": 58560
    },
    {
      "epoch": 12.552507501071581,
      "grad_norm": 0.0008668910595588386,
      "learning_rate": 3.263323331904558e-06,
      "loss": 0.0008,
      "step": 58570
    },
    {
      "epoch": 12.554650664380626,
      "grad_norm": 0.015260668471455574,
      "learning_rate": 3.260465780825833e-06,
      "loss": 0.3624,
      "step": 58580
    },
    {
      "epoch": 12.55679382768967,
      "grad_norm": 0.011989166028797626,
      "learning_rate": 3.257608229747107e-06,
      "loss": 0.2415,
      "step": 58590
    },
    {
      "epoch": 12.558936990998713,
      "grad_norm": 0.033355385065078735,
      "learning_rate": 3.254750678668381e-06,
      "loss": 0.0008,
      "step": 58600
    },
    {
      "epoch": 12.561080154307758,
      "grad_norm": 22.41141128540039,
      "learning_rate": 3.251893127589656e-06,
      "loss": 0.1789,
      "step": 58610
    },
    {
      "epoch": 12.563223317616803,
      "grad_norm": 0.012678183615207672,
      "learning_rate": 3.2490355765109303e-06,
      "loss": 0.1325,
      "step": 58620
    },
    {
      "epoch": 12.565366480925846,
      "grad_norm": 0.0034432278480380774,
      "learning_rate": 3.246178025432205e-06,
      "loss": 0.1467,
      "step": 58630
    },
    {
      "epoch": 12.56750964423489,
      "grad_norm": 12.005949020385742,
      "learning_rate": 3.243320474353479e-06,
      "loss": 0.3709,
      "step": 58640
    },
    {
      "epoch": 12.569652807543935,
      "grad_norm": 0.03158002346754074,
      "learning_rate": 3.2404629232747538e-06,
      "loss": 0.1221,
      "step": 58650
    },
    {
      "epoch": 12.571795970852978,
      "grad_norm": 23.74732208251953,
      "learning_rate": 3.237605372196028e-06,
      "loss": 0.2284,
      "step": 58660
    },
    {
      "epoch": 12.573939134162023,
      "grad_norm": 0.008551640436053276,
      "learning_rate": 3.234747821117303e-06,
      "loss": 0.1184,
      "step": 58670
    },
    {
      "epoch": 12.576082297471068,
      "grad_norm": 26.97425079345703,
      "learning_rate": 3.2318902700385773e-06,
      "loss": 0.2562,
      "step": 58680
    },
    {
      "epoch": 12.57822546078011,
      "grad_norm": 0.010227656923234463,
      "learning_rate": 3.229032718959852e-06,
      "loss": 0.0009,
      "step": 58690
    },
    {
      "epoch": 12.580368624089155,
      "grad_norm": 0.010866384021937847,
      "learning_rate": 3.226175167881126e-06,
      "loss": 0.0004,
      "step": 58700
    },
    {
      "epoch": 12.5825117873982,
      "grad_norm": 0.13675065338611603,
      "learning_rate": 3.2233176168024003e-06,
      "loss": 0.0018,
      "step": 58710
    },
    {
      "epoch": 12.584654950707243,
      "grad_norm": 0.013714516535401344,
      "learning_rate": 3.220460065723675e-06,
      "loss": 0.0024,
      "step": 58720
    },
    {
      "epoch": 12.586798114016288,
      "grad_norm": 0.01773933321237564,
      "learning_rate": 3.21760251464495e-06,
      "loss": 0.0019,
      "step": 58730
    },
    {
      "epoch": 12.588941277325333,
      "grad_norm": 0.08887391537427902,
      "learning_rate": 3.2147449635662242e-06,
      "loss": 0.001,
      "step": 58740
    },
    {
      "epoch": 12.591084440634376,
      "grad_norm": 0.024507654830813408,
      "learning_rate": 3.211887412487498e-06,
      "loss": 0.2458,
      "step": 58750
    },
    {
      "epoch": 12.59322760394342,
      "grad_norm": 0.008289888501167297,
      "learning_rate": 3.209029861408773e-06,
      "loss": 0.0002,
      "step": 58760
    },
    {
      "epoch": 12.595370767252465,
      "grad_norm": 0.0016628371085971594,
      "learning_rate": 3.2061723103300473e-06,
      "loss": 0.1532,
      "step": 58770
    },
    {
      "epoch": 12.597513930561508,
      "grad_norm": 0.0008430371526628733,
      "learning_rate": 3.203314759251322e-06,
      "loss": 0.1561,
      "step": 58780
    },
    {
      "epoch": 12.599657093870553,
      "grad_norm": 0.01488101202994585,
      "learning_rate": 3.2004572081725964e-06,
      "loss": 0.0005,
      "step": 58790
    },
    {
      "epoch": 12.601800257179598,
      "grad_norm": 4.033265113830566,
      "learning_rate": 3.1975996570938708e-06,
      "loss": 0.0007,
      "step": 58800
    },
    {
      "epoch": 12.60394342048864,
      "grad_norm": 0.015099150128662586,
      "learning_rate": 3.194742106015145e-06,
      "loss": 0.0018,
      "step": 58810
    },
    {
      "epoch": 12.606086583797685,
      "grad_norm": 0.0031737934332340956,
      "learning_rate": 3.19188455493642e-06,
      "loss": 0.0001,
      "step": 58820
    },
    {
      "epoch": 12.60822974710673,
      "grad_norm": 0.012160008773207664,
      "learning_rate": 3.1890270038576942e-06,
      "loss": 0.0006,
      "step": 58830
    },
    {
      "epoch": 12.610372910415773,
      "grad_norm": 0.16372451186180115,
      "learning_rate": 3.186169452778969e-06,
      "loss": 0.0005,
      "step": 58840
    },
    {
      "epoch": 12.612516073724818,
      "grad_norm": 0.009610841050744057,
      "learning_rate": 3.1833119017002434e-06,
      "loss": 0.0005,
      "step": 58850
    },
    {
      "epoch": 12.614659237033862,
      "grad_norm": 22.262880325317383,
      "learning_rate": 3.1804543506215173e-06,
      "loss": 0.2897,
      "step": 58860
    },
    {
      "epoch": 12.616802400342905,
      "grad_norm": 0.0015073721297085285,
      "learning_rate": 3.177596799542792e-06,
      "loss": 0.0001,
      "step": 58870
    },
    {
      "epoch": 12.61894556365195,
      "grad_norm": 0.0036460391711443663,
      "learning_rate": 3.1747392484640664e-06,
      "loss": 0.0003,
      "step": 58880
    },
    {
      "epoch": 12.621088726960995,
      "grad_norm": 0.0009450819925405085,
      "learning_rate": 3.171881697385341e-06,
      "loss": 0.0016,
      "step": 58890
    },
    {
      "epoch": 12.623231890270038,
      "grad_norm": 0.00969326589256525,
      "learning_rate": 3.1690241463066155e-06,
      "loss": 0.4248,
      "step": 58900
    },
    {
      "epoch": 12.625375053579083,
      "grad_norm": 0.0017979482654482126,
      "learning_rate": 3.16616659522789e-06,
      "loss": 0.1341,
      "step": 58910
    },
    {
      "epoch": 12.627518216888127,
      "grad_norm": 0.13175524771213531,
      "learning_rate": 3.1633090441491642e-06,
      "loss": 0.1463,
      "step": 58920
    },
    {
      "epoch": 12.62966138019717,
      "grad_norm": 0.0008393694297410548,
      "learning_rate": 3.160451493070439e-06,
      "loss": 0.3931,
      "step": 58930
    },
    {
      "epoch": 12.631804543506215,
      "grad_norm": 0.00064786960138008,
      "learning_rate": 3.1575939419917134e-06,
      "loss": 0.0018,
      "step": 58940
    },
    {
      "epoch": 12.63394770681526,
      "grad_norm": 0.002973533235490322,
      "learning_rate": 3.154736390912988e-06,
      "loss": 0.2912,
      "step": 58950
    },
    {
      "epoch": 12.636090870124303,
      "grad_norm": 0.04896663501858711,
      "learning_rate": 3.1518788398342625e-06,
      "loss": 0.0003,
      "step": 58960
    },
    {
      "epoch": 12.638234033433347,
      "grad_norm": 0.0005819304496981204,
      "learning_rate": 3.1490212887555364e-06,
      "loss": 0.1632,
      "step": 58970
    },
    {
      "epoch": 12.640377196742392,
      "grad_norm": 0.00040303225978277624,
      "learning_rate": 3.146163737676811e-06,
      "loss": 0.0014,
      "step": 58980
    },
    {
      "epoch": 12.642520360051435,
      "grad_norm": 35.26175308227539,
      "learning_rate": 3.1433061865980856e-06,
      "loss": 0.1129,
      "step": 58990
    },
    {
      "epoch": 12.64466352336048,
      "grad_norm": 0.0030224560759961605,
      "learning_rate": 3.1404486355193603e-06,
      "loss": 0.1399,
      "step": 59000
    },
    {
      "epoch": 12.646806686669525,
      "grad_norm": 0.0009978170273825526,
      "learning_rate": 3.1375910844406347e-06,
      "loss": 0.1477,
      "step": 59010
    },
    {
      "epoch": 12.648949849978568,
      "grad_norm": 0.04187712445855141,
      "learning_rate": 3.134733533361909e-06,
      "loss": 0.001,
      "step": 59020
    },
    {
      "epoch": 12.651093013287612,
      "grad_norm": 0.0030482166912406683,
      "learning_rate": 3.1318759822831834e-06,
      "loss": 0.0023,
      "step": 59030
    },
    {
      "epoch": 12.653236176596657,
      "grad_norm": 0.010227660648524761,
      "learning_rate": 3.129018431204458e-06,
      "loss": 0.0002,
      "step": 59040
    },
    {
      "epoch": 12.6553793399057,
      "grad_norm": 0.0016469932161271572,
      "learning_rate": 3.1261608801257325e-06,
      "loss": 0.2529,
      "step": 59050
    },
    {
      "epoch": 12.657522503214745,
      "grad_norm": 0.5188189148902893,
      "learning_rate": 3.1233033290470073e-06,
      "loss": 0.0012,
      "step": 59060
    },
    {
      "epoch": 12.65966566652379,
      "grad_norm": 0.010678403079509735,
      "learning_rate": 3.1204457779682812e-06,
      "loss": 0.0002,
      "step": 59070
    },
    {
      "epoch": 12.661808829832832,
      "grad_norm": 0.025236666202545166,
      "learning_rate": 3.1175882268895556e-06,
      "loss": 0.001,
      "step": 59080
    },
    {
      "epoch": 12.663951993141877,
      "grad_norm": 0.0004908719565719366,
      "learning_rate": 3.1147306758108303e-06,
      "loss": 0.0004,
      "step": 59090
    },
    {
      "epoch": 12.666095156450922,
      "grad_norm": 0.0037456105928868055,
      "learning_rate": 3.1118731247321047e-06,
      "loss": 0.0005,
      "step": 59100
    },
    {
      "epoch": 12.668238319759965,
      "grad_norm": 0.02933734655380249,
      "learning_rate": 3.1090155736533795e-06,
      "loss": 0.0011,
      "step": 59110
    },
    {
      "epoch": 12.67038148306901,
      "grad_norm": 0.6387749910354614,
      "learning_rate": 3.1061580225746542e-06,
      "loss": 0.1485,
      "step": 59120
    },
    {
      "epoch": 12.672524646378054,
      "grad_norm": 0.00048445467837154865,
      "learning_rate": 3.103300471495928e-06,
      "loss": 0.2455,
      "step": 59130
    },
    {
      "epoch": 12.674667809687097,
      "grad_norm": 1803.42724609375,
      "learning_rate": 3.1004429204172025e-06,
      "loss": 0.3804,
      "step": 59140
    },
    {
      "epoch": 12.676810972996142,
      "grad_norm": 0.0034670690074563026,
      "learning_rate": 3.0975853693384773e-06,
      "loss": 0.0001,
      "step": 59150
    },
    {
      "epoch": 12.678954136305187,
      "grad_norm": 41.62236022949219,
      "learning_rate": 3.0947278182597517e-06,
      "loss": 0.1641,
      "step": 59160
    },
    {
      "epoch": 12.68109729961423,
      "grad_norm": 1.1208913326263428,
      "learning_rate": 3.0918702671810264e-06,
      "loss": 0.0003,
      "step": 59170
    },
    {
      "epoch": 12.683240462923274,
      "grad_norm": 0.00030365365091711283,
      "learning_rate": 3.0890127161023004e-06,
      "loss": 0.0006,
      "step": 59180
    },
    {
      "epoch": 12.68538362623232,
      "grad_norm": 0.011273236945271492,
      "learning_rate": 3.086155165023575e-06,
      "loss": 0.1888,
      "step": 59190
    },
    {
      "epoch": 12.687526789541362,
      "grad_norm": 0.018041547387838364,
      "learning_rate": 3.0832976139448495e-06,
      "loss": 0.1665,
      "step": 59200
    },
    {
      "epoch": 12.689669952850407,
      "grad_norm": 0.0013678789837285876,
      "learning_rate": 3.0804400628661243e-06,
      "loss": 0.0002,
      "step": 59210
    },
    {
      "epoch": 12.691813116159452,
      "grad_norm": 0.005154266022145748,
      "learning_rate": 3.0775825117873986e-06,
      "loss": 0.0002,
      "step": 59220
    },
    {
      "epoch": 12.693956279468495,
      "grad_norm": 0.0007945183315314353,
      "learning_rate": 3.0747249607086725e-06,
      "loss": 0.0002,
      "step": 59230
    },
    {
      "epoch": 12.69609944277754,
      "grad_norm": 0.001067024772055447,
      "learning_rate": 3.0718674096299473e-06,
      "loss": 0.0005,
      "step": 59240
    },
    {
      "epoch": 12.698242606086584,
      "grad_norm": 0.02275223657488823,
      "learning_rate": 3.0690098585512217e-06,
      "loss": 0.1637,
      "step": 59250
    },
    {
      "epoch": 12.700385769395627,
      "grad_norm": 0.010760988108813763,
      "learning_rate": 3.0661523074724964e-06,
      "loss": 0.2948,
      "step": 59260
    },
    {
      "epoch": 12.702528932704672,
      "grad_norm": 0.015478530898690224,
      "learning_rate": 3.063294756393771e-06,
      "loss": 0.001,
      "step": 59270
    },
    {
      "epoch": 12.704672096013716,
      "grad_norm": 0.00016755885735619813,
      "learning_rate": 3.0604372053150456e-06,
      "loss": 0.0008,
      "step": 59280
    },
    {
      "epoch": 12.70681525932276,
      "grad_norm": 0.0010913732694461942,
      "learning_rate": 3.0575796542363195e-06,
      "loss": 0.0001,
      "step": 59290
    },
    {
      "epoch": 12.708958422631804,
      "grad_norm": 0.017215970903635025,
      "learning_rate": 3.0547221031575943e-06,
      "loss": 0.0004,
      "step": 59300
    },
    {
      "epoch": 12.711101585940849,
      "grad_norm": 0.0057549555785954,
      "learning_rate": 3.0518645520788686e-06,
      "loss": 0.0006,
      "step": 59310
    },
    {
      "epoch": 12.713244749249894,
      "grad_norm": 0.0008113342337310314,
      "learning_rate": 3.0490070010001434e-06,
      "loss": 0.2741,
      "step": 59320
    },
    {
      "epoch": 12.715387912558937,
      "grad_norm": 0.0025660155806690454,
      "learning_rate": 3.0461494499214177e-06,
      "loss": 0.1466,
      "step": 59330
    },
    {
      "epoch": 12.717531075867981,
      "grad_norm": 0.014787672087550163,
      "learning_rate": 3.0432918988426917e-06,
      "loss": 0.0004,
      "step": 59340
    },
    {
      "epoch": 12.719674239177026,
      "grad_norm": 0.004007100593298674,
      "learning_rate": 3.0404343477639665e-06,
      "loss": 0.229,
      "step": 59350
    },
    {
      "epoch": 12.721817402486069,
      "grad_norm": 0.01921394281089306,
      "learning_rate": 3.037576796685241e-06,
      "loss": 0.0001,
      "step": 59360
    },
    {
      "epoch": 12.723960565795114,
      "grad_norm": 0.012474129907786846,
      "learning_rate": 3.0347192456065156e-06,
      "loss": 0.0001,
      "step": 59370
    },
    {
      "epoch": 12.726103729104159,
      "grad_norm": 0.005019545555114746,
      "learning_rate": 3.03186169452779e-06,
      "loss": 0.0011,
      "step": 59380
    },
    {
      "epoch": 12.728246892413202,
      "grad_norm": 0.02030184119939804,
      "learning_rate": 3.0290041434490643e-06,
      "loss": 0.0002,
      "step": 59390
    },
    {
      "epoch": 12.730390055722246,
      "grad_norm": 0.0003021408338099718,
      "learning_rate": 3.0261465923703386e-06,
      "loss": 0.0005,
      "step": 59400
    },
    {
      "epoch": 12.732533219031291,
      "grad_norm": 0.0015728871803730726,
      "learning_rate": 3.0232890412916134e-06,
      "loss": 0.1692,
      "step": 59410
    },
    {
      "epoch": 12.734676382340334,
      "grad_norm": 0.004177531693130732,
      "learning_rate": 3.0204314902128878e-06,
      "loss": 0.0008,
      "step": 59420
    },
    {
      "epoch": 12.736819545649379,
      "grad_norm": 0.006737018469721079,
      "learning_rate": 3.0175739391341625e-06,
      "loss": 0.0006,
      "step": 59430
    },
    {
      "epoch": 12.738962708958423,
      "grad_norm": 0.06358472257852554,
      "learning_rate": 3.014716388055437e-06,
      "loss": 0.4529,
      "step": 59440
    },
    {
      "epoch": 12.741105872267466,
      "grad_norm": 0.008913395926356316,
      "learning_rate": 3.011858836976711e-06,
      "loss": 0.0002,
      "step": 59450
    },
    {
      "epoch": 12.743249035576511,
      "grad_norm": 0.0018018389819189906,
      "learning_rate": 3.0090012858979856e-06,
      "loss": 0.146,
      "step": 59460
    },
    {
      "epoch": 12.745392198885556,
      "grad_norm": 0.0041067469865083694,
      "learning_rate": 3.00614373481926e-06,
      "loss": 0.0001,
      "step": 59470
    },
    {
      "epoch": 12.747535362194599,
      "grad_norm": 0.049256548285484314,
      "learning_rate": 3.0032861837405347e-06,
      "loss": 0.0004,
      "step": 59480
    },
    {
      "epoch": 12.749678525503644,
      "grad_norm": 0.0008247136720456183,
      "learning_rate": 3.000428632661809e-06,
      "loss": 0.0002,
      "step": 59490
    },
    {
      "epoch": 12.751821688812688,
      "grad_norm": 16.87589454650879,
      "learning_rate": 2.9975710815830834e-06,
      "loss": 0.5204,
      "step": 59500
    },
    {
      "epoch": 12.753964852121731,
      "grad_norm": 0.010315745137631893,
      "learning_rate": 2.9947135305043578e-06,
      "loss": 0.2076,
      "step": 59510
    },
    {
      "epoch": 12.756108015430776,
      "grad_norm": 0.0004133110342081636,
      "learning_rate": 2.9918559794256326e-06,
      "loss": 0.0003,
      "step": 59520
    },
    {
      "epoch": 12.75825117873982,
      "grad_norm": 0.02832498401403427,
      "learning_rate": 2.988998428346907e-06,
      "loss": 0.001,
      "step": 59530
    },
    {
      "epoch": 12.760394342048864,
      "grad_norm": 0.023667600005865097,
      "learning_rate": 2.9861408772681817e-06,
      "loss": 0.2137,
      "step": 59540
    },
    {
      "epoch": 12.762537505357908,
      "grad_norm": 0.06536605209112167,
      "learning_rate": 2.983283326189456e-06,
      "loss": 0.0006,
      "step": 59550
    },
    {
      "epoch": 12.764680668666953,
      "grad_norm": 0.010444726794958115,
      "learning_rate": 2.98042577511073e-06,
      "loss": 0.0001,
      "step": 59560
    },
    {
      "epoch": 12.766823831975996,
      "grad_norm": 0.009046424180269241,
      "learning_rate": 2.9775682240320047e-06,
      "loss": 0.1862,
      "step": 59570
    },
    {
      "epoch": 12.76896699528504,
      "grad_norm": 0.3963955342769623,
      "learning_rate": 2.9747106729532795e-06,
      "loss": 0.0016,
      "step": 59580
    },
    {
      "epoch": 12.771110158594086,
      "grad_norm": 0.008845582604408264,
      "learning_rate": 2.971853121874554e-06,
      "loss": 0.1409,
      "step": 59590
    },
    {
      "epoch": 12.773253321903129,
      "grad_norm": 0.2631387412548065,
      "learning_rate": 2.9689955707958286e-06,
      "loss": 0.0008,
      "step": 59600
    },
    {
      "epoch": 12.775396485212173,
      "grad_norm": 0.00047281404840759933,
      "learning_rate": 2.9661380197171026e-06,
      "loss": 0.1601,
      "step": 59610
    },
    {
      "epoch": 12.777539648521218,
      "grad_norm": 0.00022203590197023004,
      "learning_rate": 2.963280468638377e-06,
      "loss": 0.1399,
      "step": 59620
    },
    {
      "epoch": 12.779682811830261,
      "grad_norm": 0.035619452595710754,
      "learning_rate": 2.9604229175596517e-06,
      "loss": 0.5197,
      "step": 59630
    },
    {
      "epoch": 12.781825975139306,
      "grad_norm": 0.03397246450185776,
      "learning_rate": 2.957565366480926e-06,
      "loss": 0.0005,
      "step": 59640
    },
    {
      "epoch": 12.78396913844835,
      "grad_norm": 0.031684331595897675,
      "learning_rate": 2.954707815402201e-06,
      "loss": 0.4066,
      "step": 59650
    },
    {
      "epoch": 12.786112301757393,
      "grad_norm": 0.017655886709690094,
      "learning_rate": 2.9518502643234747e-06,
      "loss": 0.0003,
      "step": 59660
    },
    {
      "epoch": 12.788255465066438,
      "grad_norm": 0.07679759711027145,
      "learning_rate": 2.9489927132447495e-06,
      "loss": 0.0011,
      "step": 59670
    },
    {
      "epoch": 12.790398628375483,
      "grad_norm": 0.03926108404994011,
      "learning_rate": 2.946135162166024e-06,
      "loss": 0.0007,
      "step": 59680
    },
    {
      "epoch": 12.792541791684526,
      "grad_norm": 0.0010072416625916958,
      "learning_rate": 2.9432776110872986e-06,
      "loss": 0.1691,
      "step": 59690
    },
    {
      "epoch": 12.79468495499357,
      "grad_norm": 0.008621648885309696,
      "learning_rate": 2.940420060008573e-06,
      "loss": 0.1516,
      "step": 59700
    },
    {
      "epoch": 12.796828118302615,
      "grad_norm": 0.009036797098815441,
      "learning_rate": 2.9375625089298478e-06,
      "loss": 0.0007,
      "step": 59710
    },
    {
      "epoch": 12.798971281611658,
      "grad_norm": 0.052867624908685684,
      "learning_rate": 2.9347049578511217e-06,
      "loss": 0.0005,
      "step": 59720
    },
    {
      "epoch": 12.801114444920703,
      "grad_norm": 0.0005137734115123749,
      "learning_rate": 2.931847406772396e-06,
      "loss": 0.174,
      "step": 59730
    },
    {
      "epoch": 12.803257608229748,
      "grad_norm": 0.008971481584012508,
      "learning_rate": 2.928989855693671e-06,
      "loss": 0.4057,
      "step": 59740
    },
    {
      "epoch": 12.80540077153879,
      "grad_norm": 32.38916015625,
      "learning_rate": 2.926132304614945e-06,
      "loss": 0.1837,
      "step": 59750
    },
    {
      "epoch": 12.807543934847835,
      "grad_norm": 0.00037212419556453824,
      "learning_rate": 2.92327475353622e-06,
      "loss": 0.1672,
      "step": 59760
    },
    {
      "epoch": 12.80968709815688,
      "grad_norm": 0.028592178598046303,
      "learning_rate": 2.920417202457494e-06,
      "loss": 0.1836,
      "step": 59770
    },
    {
      "epoch": 12.811830261465923,
      "grad_norm": 0.0008181107114069164,
      "learning_rate": 2.9175596513787687e-06,
      "loss": 0.0006,
      "step": 59780
    },
    {
      "epoch": 12.813973424774968,
      "grad_norm": 20.63014793395996,
      "learning_rate": 2.914702100300043e-06,
      "loss": 0.1726,
      "step": 59790
    },
    {
      "epoch": 12.816116588084013,
      "grad_norm": 0.0004540738300420344,
      "learning_rate": 2.911844549221318e-06,
      "loss": 0.1264,
      "step": 59800
    },
    {
      "epoch": 12.818259751393056,
      "grad_norm": 0.0002321817009942606,
      "learning_rate": 2.908986998142592e-06,
      "loss": 0.0015,
      "step": 59810
    },
    {
      "epoch": 12.8204029147021,
      "grad_norm": 16.38010025024414,
      "learning_rate": 2.906129447063866e-06,
      "loss": 0.1491,
      "step": 59820
    },
    {
      "epoch": 12.822546078011145,
      "grad_norm": 0.0014033702900633216,
      "learning_rate": 2.903271895985141e-06,
      "loss": 0.0015,
      "step": 59830
    },
    {
      "epoch": 12.824689241320188,
      "grad_norm": 0.0003925717028323561,
      "learning_rate": 2.900414344906415e-06,
      "loss": 0.001,
      "step": 59840
    },
    {
      "epoch": 12.826832404629233,
      "grad_norm": 0.0005893387133255601,
      "learning_rate": 2.89755679382769e-06,
      "loss": 0.2764,
      "step": 59850
    },
    {
      "epoch": 12.828975567938278,
      "grad_norm": 0.032501477748155594,
      "learning_rate": 2.8946992427489643e-06,
      "loss": 0.2187,
      "step": 59860
    },
    {
      "epoch": 12.83111873124732,
      "grad_norm": 0.2447032779455185,
      "learning_rate": 2.891841691670239e-06,
      "loss": 0.001,
      "step": 59870
    },
    {
      "epoch": 12.833261894556365,
      "grad_norm": 0.476689875125885,
      "learning_rate": 2.888984140591513e-06,
      "loss": 0.2362,
      "step": 59880
    },
    {
      "epoch": 12.83540505786541,
      "grad_norm": 0.00036852515768259764,
      "learning_rate": 2.886126589512788e-06,
      "loss": 0.1414,
      "step": 59890
    },
    {
      "epoch": 12.837548221174453,
      "grad_norm": 0.00042672152630984783,
      "learning_rate": 2.883269038434062e-06,
      "loss": 0.0017,
      "step": 59900
    },
    {
      "epoch": 12.839691384483498,
      "grad_norm": 1.6812667846679688,
      "learning_rate": 2.880411487355337e-06,
      "loss": 0.0019,
      "step": 59910
    },
    {
      "epoch": 12.841834547792542,
      "grad_norm": 0.005016721785068512,
      "learning_rate": 2.8775539362766113e-06,
      "loss": 0.1451,
      "step": 59920
    },
    {
      "epoch": 12.843977711101585,
      "grad_norm": 0.027986697852611542,
      "learning_rate": 2.874696385197885e-06,
      "loss": 0.0007,
      "step": 59930
    },
    {
      "epoch": 12.84612087441063,
      "grad_norm": 0.0025275161024183035,
      "learning_rate": 2.87183883411916e-06,
      "loss": 0.001,
      "step": 59940
    },
    {
      "epoch": 12.848264037719675,
      "grad_norm": 0.009425393305718899,
      "learning_rate": 2.8689812830404348e-06,
      "loss": 0.0004,
      "step": 59950
    },
    {
      "epoch": 12.850407201028718,
      "grad_norm": 0.017313886433839798,
      "learning_rate": 2.866123731961709e-06,
      "loss": 0.1073,
      "step": 59960
    },
    {
      "epoch": 12.852550364337763,
      "grad_norm": 0.0004043659719172865,
      "learning_rate": 2.863266180882984e-06,
      "loss": 0.582,
      "step": 59970
    },
    {
      "epoch": 12.854693527646807,
      "grad_norm": 0.012383329682052135,
      "learning_rate": 2.860408629804258e-06,
      "loss": 0.1899,
      "step": 59980
    },
    {
      "epoch": 12.85683669095585,
      "grad_norm": 0.009277267381548882,
      "learning_rate": 2.857551078725532e-06,
      "loss": 0.0006,
      "step": 59990
    },
    {
      "epoch": 12.858979854264895,
      "grad_norm": 0.032392993569374084,
      "learning_rate": 2.854693527646807e-06,
      "loss": 0.3315,
      "step": 60000
    },
    {
      "epoch": 12.86112301757394,
      "grad_norm": 0.02109585888683796,
      "learning_rate": 2.8518359765680813e-06,
      "loss": 0.0011,
      "step": 60010
    },
    {
      "epoch": 12.863266180882983,
      "grad_norm": 0.13877348601818085,
      "learning_rate": 2.848978425489356e-06,
      "loss": 0.0012,
      "step": 60020
    },
    {
      "epoch": 12.865409344192027,
      "grad_norm": 0.0420590415596962,
      "learning_rate": 2.8461208744106304e-06,
      "loss": 0.0003,
      "step": 60030
    },
    {
      "epoch": 12.867552507501072,
      "grad_norm": 0.024795543402433395,
      "learning_rate": 2.8432633233319048e-06,
      "loss": 0.1674,
      "step": 60040
    },
    {
      "epoch": 12.869695670810115,
      "grad_norm": 0.0010895398445427418,
      "learning_rate": 2.840405772253179e-06,
      "loss": 0.1718,
      "step": 60050
    },
    {
      "epoch": 12.87183883411916,
      "grad_norm": 0.05380440503358841,
      "learning_rate": 2.837548221174454e-06,
      "loss": 0.0007,
      "step": 60060
    },
    {
      "epoch": 12.873981997428205,
      "grad_norm": 0.007673528511077166,
      "learning_rate": 2.8346906700957283e-06,
      "loss": 0.0008,
      "step": 60070
    },
    {
      "epoch": 12.876125160737248,
      "grad_norm": 0.005234780255705118,
      "learning_rate": 2.831833119017003e-06,
      "loss": 0.2622,
      "step": 60080
    },
    {
      "epoch": 12.878268324046292,
      "grad_norm": 0.013109940104186535,
      "learning_rate": 2.828975567938277e-06,
      "loss": 0.0007,
      "step": 60090
    },
    {
      "epoch": 12.880411487355337,
      "grad_norm": 0.007054837420582771,
      "learning_rate": 2.8261180168595513e-06,
      "loss": 0.0002,
      "step": 60100
    },
    {
      "epoch": 12.88255465066438,
      "grad_norm": 0.007271112874150276,
      "learning_rate": 2.823260465780826e-06,
      "loss": 0.0005,
      "step": 60110
    },
    {
      "epoch": 12.884697813973425,
      "grad_norm": 0.18790598213672638,
      "learning_rate": 2.8204029147021004e-06,
      "loss": 0.1428,
      "step": 60120
    },
    {
      "epoch": 12.88684097728247,
      "grad_norm": 0.03351534157991409,
      "learning_rate": 2.817545363623375e-06,
      "loss": 0.1605,
      "step": 60130
    },
    {
      "epoch": 12.888984140591512,
      "grad_norm": 0.008510104380548,
      "learning_rate": 2.8146878125446496e-06,
      "loss": 0.3929,
      "step": 60140
    },
    {
      "epoch": 12.891127303900557,
      "grad_norm": 0.004670802038162947,
      "learning_rate": 2.811830261465924e-06,
      "loss": 0.3151,
      "step": 60150
    },
    {
      "epoch": 12.893270467209602,
      "grad_norm": 0.056294187903404236,
      "learning_rate": 2.8089727103871983e-06,
      "loss": 0.0004,
      "step": 60160
    },
    {
      "epoch": 12.895413630518645,
      "grad_norm": 0.18816912174224854,
      "learning_rate": 2.806115159308473e-06,
      "loss": 0.0007,
      "step": 60170
    },
    {
      "epoch": 12.89755679382769,
      "grad_norm": 0.012995611876249313,
      "learning_rate": 2.8032576082297474e-06,
      "loss": 0.2154,
      "step": 60180
    },
    {
      "epoch": 12.899699957136734,
      "grad_norm": 0.2505364418029785,
      "learning_rate": 2.800400057151022e-06,
      "loss": 0.0006,
      "step": 60190
    },
    {
      "epoch": 12.901843120445777,
      "grad_norm": 0.002580174244940281,
      "learning_rate": 2.797542506072296e-06,
      "loss": 0.1301,
      "step": 60200
    },
    {
      "epoch": 12.903986283754822,
      "grad_norm": 0.0016674750950187445,
      "learning_rate": 2.7946849549935704e-06,
      "loss": 0.0004,
      "step": 60210
    },
    {
      "epoch": 12.906129447063867,
      "grad_norm": 0.010062388144433498,
      "learning_rate": 2.7918274039148452e-06,
      "loss": 0.0002,
      "step": 60220
    },
    {
      "epoch": 12.90827261037291,
      "grad_norm": 0.0009422305738553405,
      "learning_rate": 2.7889698528361196e-06,
      "loss": 0.1385,
      "step": 60230
    },
    {
      "epoch": 12.910415773681954,
      "grad_norm": 0.03482748568058014,
      "learning_rate": 2.7861123017573943e-06,
      "loss": 0.0004,
      "step": 60240
    },
    {
      "epoch": 12.912558936991,
      "grad_norm": 0.004776076879352331,
      "learning_rate": 2.7832547506786683e-06,
      "loss": 0.0037,
      "step": 60250
    },
    {
      "epoch": 12.914702100300042,
      "grad_norm": 24.73999786376953,
      "learning_rate": 2.780397199599943e-06,
      "loss": 0.2056,
      "step": 60260
    },
    {
      "epoch": 12.916845263609087,
      "grad_norm": 0.0030815848149359226,
      "learning_rate": 2.7775396485212174e-06,
      "loss": 0.1441,
      "step": 60270
    },
    {
      "epoch": 12.918988426918132,
      "grad_norm": 0.00043877417920157313,
      "learning_rate": 2.774682097442492e-06,
      "loss": 0.0012,
      "step": 60280
    },
    {
      "epoch": 12.921131590227175,
      "grad_norm": 0.010225987993180752,
      "learning_rate": 2.7718245463637665e-06,
      "loss": 0.0012,
      "step": 60290
    },
    {
      "epoch": 12.92327475353622,
      "grad_norm": 0.2809596657752991,
      "learning_rate": 2.7689669952850413e-06,
      "loss": 0.001,
      "step": 60300
    },
    {
      "epoch": 12.925417916845264,
      "grad_norm": 0.03188289701938629,
      "learning_rate": 2.7661094442063152e-06,
      "loss": 0.0002,
      "step": 60310
    },
    {
      "epoch": 12.927561080154307,
      "grad_norm": 0.0010947503615170717,
      "learning_rate": 2.7632518931275896e-06,
      "loss": 0.0006,
      "step": 60320
    },
    {
      "epoch": 12.929704243463352,
      "grad_norm": 0.0009646433754824102,
      "learning_rate": 2.7603943420488644e-06,
      "loss": 0.0001,
      "step": 60330
    },
    {
      "epoch": 12.931847406772397,
      "grad_norm": 0.009985895827412605,
      "learning_rate": 2.757536790970139e-06,
      "loss": 0.0004,
      "step": 60340
    },
    {
      "epoch": 12.93399057008144,
      "grad_norm": 0.1341637670993805,
      "learning_rate": 2.7546792398914135e-06,
      "loss": 0.0009,
      "step": 60350
    },
    {
      "epoch": 12.936133733390484,
      "grad_norm": 0.005760222673416138,
      "learning_rate": 2.7518216888126874e-06,
      "loss": 0.0003,
      "step": 60360
    },
    {
      "epoch": 12.938276896699529,
      "grad_norm": 0.0006294412305578589,
      "learning_rate": 2.748964137733962e-06,
      "loss": 0.0001,
      "step": 60370
    },
    {
      "epoch": 12.940420060008572,
      "grad_norm": 0.019783494994044304,
      "learning_rate": 2.7461065866552365e-06,
      "loss": 0.0001,
      "step": 60380
    },
    {
      "epoch": 12.942563223317617,
      "grad_norm": 0.0009465828770771623,
      "learning_rate": 2.7432490355765113e-06,
      "loss": 0.5355,
      "step": 60390
    },
    {
      "epoch": 12.944706386626661,
      "grad_norm": 0.024941425770521164,
      "learning_rate": 2.7403914844977857e-06,
      "loss": 0.2697,
      "step": 60400
    },
    {
      "epoch": 12.946849549935704,
      "grad_norm": 0.1128012165427208,
      "learning_rate": 2.73753393341906e-06,
      "loss": 0.1566,
      "step": 60410
    },
    {
      "epoch": 12.948992713244749,
      "grad_norm": 26.28449249267578,
      "learning_rate": 2.7346763823403344e-06,
      "loss": 0.3881,
      "step": 60420
    },
    {
      "epoch": 12.951135876553794,
      "grad_norm": 0.009498938918113708,
      "learning_rate": 2.731818831261609e-06,
      "loss": 0.0006,
      "step": 60430
    },
    {
      "epoch": 12.953279039862837,
      "grad_norm": 0.0011558603728190064,
      "learning_rate": 2.7289612801828835e-06,
      "loss": 0.0013,
      "step": 60440
    },
    {
      "epoch": 12.955422203171882,
      "grad_norm": 0.0061976006254553795,
      "learning_rate": 2.7261037291041583e-06,
      "loss": 0.0006,
      "step": 60450
    },
    {
      "epoch": 12.957565366480926,
      "grad_norm": 30.002378463745117,
      "learning_rate": 2.7232461780254326e-06,
      "loss": 0.3624,
      "step": 60460
    },
    {
      "epoch": 12.95970852978997,
      "grad_norm": 0.02730061486363411,
      "learning_rate": 2.7203886269467066e-06,
      "loss": 0.0417,
      "step": 60470
    },
    {
      "epoch": 12.961851693099014,
      "grad_norm": 0.0023714418057352304,
      "learning_rate": 2.7175310758679813e-06,
      "loss": 0.0009,
      "step": 60480
    },
    {
      "epoch": 12.963994856408059,
      "grad_norm": 0.017572710290551186,
      "learning_rate": 2.7146735247892557e-06,
      "loss": 0.2913,
      "step": 60490
    },
    {
      "epoch": 12.966138019717102,
      "grad_norm": 0.28106480836868286,
      "learning_rate": 2.7118159737105305e-06,
      "loss": 0.1516,
      "step": 60500
    },
    {
      "epoch": 12.968281183026146,
      "grad_norm": 0.015717944130301476,
      "learning_rate": 2.708958422631805e-06,
      "loss": 0.0005,
      "step": 60510
    },
    {
      "epoch": 12.970424346335191,
      "grad_norm": 0.00021896047110203654,
      "learning_rate": 2.706100871553079e-06,
      "loss": 0.2383,
      "step": 60520
    },
    {
      "epoch": 12.972567509644234,
      "grad_norm": 0.0052934568375349045,
      "learning_rate": 2.7032433204743535e-06,
      "loss": 0.2542,
      "step": 60530
    },
    {
      "epoch": 12.974710672953279,
      "grad_norm": 0.04896905645728111,
      "learning_rate": 2.7003857693956283e-06,
      "loss": 0.1894,
      "step": 60540
    },
    {
      "epoch": 12.976853836262324,
      "grad_norm": 0.029745545238256454,
      "learning_rate": 2.6975282183169026e-06,
      "loss": 0.2917,
      "step": 60550
    },
    {
      "epoch": 12.978996999571367,
      "grad_norm": 0.0023883641697466373,
      "learning_rate": 2.6946706672381774e-06,
      "loss": 0.0009,
      "step": 60560
    },
    {
      "epoch": 12.981140162880411,
      "grad_norm": 0.022704364731907845,
      "learning_rate": 2.6918131161594518e-06,
      "loss": 0.3782,
      "step": 60570
    },
    {
      "epoch": 12.983283326189456,
      "grad_norm": 0.09016076475381851,
      "learning_rate": 2.6889555650807257e-06,
      "loss": 0.0014,
      "step": 60580
    },
    {
      "epoch": 12.985426489498499,
      "grad_norm": 0.058788906782865524,
      "learning_rate": 2.6860980140020005e-06,
      "loss": 0.0013,
      "step": 60590
    },
    {
      "epoch": 12.987569652807544,
      "grad_norm": 0.04635200276970863,
      "learning_rate": 2.683240462923275e-06,
      "loss": 0.4078,
      "step": 60600
    },
    {
      "epoch": 12.989712816116588,
      "grad_norm": 0.0005481911939568818,
      "learning_rate": 2.6803829118445496e-06,
      "loss": 0.0009,
      "step": 60610
    },
    {
      "epoch": 12.991855979425631,
      "grad_norm": 0.2791753113269806,
      "learning_rate": 2.677525360765824e-06,
      "loss": 0.3423,
      "step": 60620
    },
    {
      "epoch": 12.993999142734676,
      "grad_norm": 27.71098518371582,
      "learning_rate": 2.6746678096870983e-06,
      "loss": 0.1581,
      "step": 60630
    },
    {
      "epoch": 12.996142306043721,
      "grad_norm": 0.026765132322907448,
      "learning_rate": 2.6718102586083727e-06,
      "loss": 0.4067,
      "step": 60640
    },
    {
      "epoch": 12.998285469352764,
      "grad_norm": 0.007849589921534061,
      "learning_rate": 2.6689527075296474e-06,
      "loss": 0.0015,
      "step": 60650
    },
    {
      "epoch": 13.0,
      "eval_accuracy": 0.966,
      "eval_f1": 0.8617886178861788,
      "eval_loss": 0.19281156361103058,
      "eval_precision": 0.8857938718662952,
      "eval_recall": 0.8390501319261213,
      "eval_runtime": 750.6513,
      "eval_samples_per_second": 3.997,
      "eval_steps_per_second": 1.332,
      "step": 60658
    },
    {
      "epoch": 13.000428632661809,
      "grad_norm": 0.00021650981216225773,
      "learning_rate": 2.6660951564509218e-06,
      "loss": 0.0004,
      "step": 60660
    },
    {
      "epoch": 13.002571795970853,
      "grad_norm": 0.016209453344345093,
      "learning_rate": 2.6632376053721966e-06,
      "loss": 0.0016,
      "step": 60670
    },
    {
      "epoch": 13.004714959279896,
      "grad_norm": 0.0001864157384261489,
      "learning_rate": 2.6603800542934705e-06,
      "loss": 0.1233,
      "step": 60680
    },
    {
      "epoch": 13.006858122588941,
      "grad_norm": 0.009005265310406685,
      "learning_rate": 2.657522503214745e-06,
      "loss": 0.0005,
      "step": 60690
    },
    {
      "epoch": 13.009001285897986,
      "grad_norm": 0.0004897214821539819,
      "learning_rate": 2.6546649521360196e-06,
      "loss": 0.2174,
      "step": 60700
    },
    {
      "epoch": 13.011144449207029,
      "grad_norm": 0.2697908580303192,
      "learning_rate": 2.651807401057294e-06,
      "loss": 0.1769,
      "step": 60710
    },
    {
      "epoch": 13.013287612516073,
      "grad_norm": 0.0003417320258449763,
      "learning_rate": 2.6489498499785687e-06,
      "loss": 0.0001,
      "step": 60720
    },
    {
      "epoch": 13.015430775825118,
      "grad_norm": 0.013698357157409191,
      "learning_rate": 2.6460922988998435e-06,
      "loss": 0.0302,
      "step": 60730
    },
    {
      "epoch": 13.017573939134161,
      "grad_norm": 0.008407079614698887,
      "learning_rate": 2.6432347478211174e-06,
      "loss": 0.5163,
      "step": 60740
    },
    {
      "epoch": 13.019717102443206,
      "grad_norm": 23.028650283813477,
      "learning_rate": 2.640377196742392e-06,
      "loss": 0.2052,
      "step": 60750
    },
    {
      "epoch": 13.02186026575225,
      "grad_norm": 0.04568234086036682,
      "learning_rate": 2.6375196456636666e-06,
      "loss": 0.0011,
      "step": 60760
    },
    {
      "epoch": 13.024003429061294,
      "grad_norm": 0.02798549085855484,
      "learning_rate": 2.634662094584941e-06,
      "loss": 0.0014,
      "step": 60770
    },
    {
      "epoch": 13.026146592370338,
      "grad_norm": 0.2547677457332611,
      "learning_rate": 2.6318045435062157e-06,
      "loss": 0.1451,
      "step": 60780
    },
    {
      "epoch": 13.028289755679383,
      "grad_norm": 0.008920551277697086,
      "learning_rate": 2.6289469924274896e-06,
      "loss": 0.0003,
      "step": 60790
    },
    {
      "epoch": 13.030432918988426,
      "grad_norm": 0.007622677832841873,
      "learning_rate": 2.6260894413487644e-06,
      "loss": 0.1748,
      "step": 60800
    },
    {
      "epoch": 13.03257608229747,
      "grad_norm": 0.00787047017365694,
      "learning_rate": 2.6232318902700388e-06,
      "loss": 0.0004,
      "step": 60810
    },
    {
      "epoch": 13.034719245606516,
      "grad_norm": 0.0890946090221405,
      "learning_rate": 2.6203743391913135e-06,
      "loss": 0.1359,
      "step": 60820
    },
    {
      "epoch": 13.03686240891556,
      "grad_norm": 0.04016042873263359,
      "learning_rate": 2.617516788112588e-06,
      "loss": 0.1916,
      "step": 60830
    },
    {
      "epoch": 13.039005572224603,
      "grad_norm": 0.1808108538389206,
      "learning_rate": 2.614659237033862e-06,
      "loss": 0.0005,
      "step": 60840
    },
    {
      "epoch": 13.041148735533648,
      "grad_norm": 0.03310709446668625,
      "learning_rate": 2.6118016859551366e-06,
      "loss": 0.0008,
      "step": 60850
    },
    {
      "epoch": 13.043291898842693,
      "grad_norm": 0.01884891279041767,
      "learning_rate": 2.608944134876411e-06,
      "loss": 0.1321,
      "step": 60860
    },
    {
      "epoch": 13.045435062151736,
      "grad_norm": 0.24098657071590424,
      "learning_rate": 2.6060865837976857e-06,
      "loss": 0.0013,
      "step": 60870
    },
    {
      "epoch": 13.04757822546078,
      "grad_norm": 0.010728616267442703,
      "learning_rate": 2.60322903271896e-06,
      "loss": 0.0003,
      "step": 60880
    },
    {
      "epoch": 13.049721388769825,
      "grad_norm": 21.197872161865234,
      "learning_rate": 2.600371481640235e-06,
      "loss": 0.1945,
      "step": 60890
    },
    {
      "epoch": 13.051864552078868,
      "grad_norm": 0.07562711089849472,
      "learning_rate": 2.5975139305615088e-06,
      "loss": 0.0011,
      "step": 60900
    },
    {
      "epoch": 13.054007715387913,
      "grad_norm": 0.05521845817565918,
      "learning_rate": 2.5946563794827835e-06,
      "loss": 0.0187,
      "step": 60910
    },
    {
      "epoch": 13.056150878696958,
      "grad_norm": 0.021085740998387337,
      "learning_rate": 2.591798828404058e-06,
      "loss": 0.2444,
      "step": 60920
    },
    {
      "epoch": 13.058294042006,
      "grad_norm": 0.2650553286075592,
      "learning_rate": 2.5889412773253327e-06,
      "loss": 0.1617,
      "step": 60930
    },
    {
      "epoch": 13.060437205315045,
      "grad_norm": 0.09648061543703079,
      "learning_rate": 2.586083726246607e-06,
      "loss": 0.0009,
      "step": 60940
    },
    {
      "epoch": 13.06258036862409,
      "grad_norm": 0.008071104064583778,
      "learning_rate": 2.583226175167881e-06,
      "loss": 0.0005,
      "step": 60950
    },
    {
      "epoch": 13.064723531933133,
      "grad_norm": 19.249759674072266,
      "learning_rate": 2.5803686240891557e-06,
      "loss": 0.1391,
      "step": 60960
    },
    {
      "epoch": 13.066866695242178,
      "grad_norm": 0.0011918202508240938,
      "learning_rate": 2.57751107301043e-06,
      "loss": 0.3408,
      "step": 60970
    },
    {
      "epoch": 13.069009858551222,
      "grad_norm": 0.06981197744607925,
      "learning_rate": 2.574653521931705e-06,
      "loss": 0.0007,
      "step": 60980
    },
    {
      "epoch": 13.071153021860265,
      "grad_norm": 0.0009242250234819949,
      "learning_rate": 2.571795970852979e-06,
      "loss": 0.2052,
      "step": 60990
    },
    {
      "epoch": 13.07329618516931,
      "grad_norm": 0.01370320189744234,
      "learning_rate": 2.5689384197742536e-06,
      "loss": 0.0007,
      "step": 61000
    },
    {
      "epoch": 13.075439348478355,
      "grad_norm": 0.09550703316926956,
      "learning_rate": 2.566080868695528e-06,
      "loss": 0.1466,
      "step": 61010
    },
    {
      "epoch": 13.077582511787398,
      "grad_norm": 0.007838007993996143,
      "learning_rate": 2.5632233176168027e-06,
      "loss": 0.0008,
      "step": 61020
    },
    {
      "epoch": 13.079725675096443,
      "grad_norm": 0.0022626619320362806,
      "learning_rate": 2.560365766538077e-06,
      "loss": 0.0001,
      "step": 61030
    },
    {
      "epoch": 13.081868838405487,
      "grad_norm": 0.019177546724677086,
      "learning_rate": 2.557508215459352e-06,
      "loss": 0.4519,
      "step": 61040
    },
    {
      "epoch": 13.08401200171453,
      "grad_norm": 0.06644131243228912,
      "learning_rate": 2.554650664380626e-06,
      "loss": 0.2081,
      "step": 61050
    },
    {
      "epoch": 13.086155165023575,
      "grad_norm": 0.0525524877011776,
      "learning_rate": 2.5517931133019e-06,
      "loss": 0.0005,
      "step": 61060
    },
    {
      "epoch": 13.08829832833262,
      "grad_norm": 0.0015808497555553913,
      "learning_rate": 2.548935562223175e-06,
      "loss": 0.1288,
      "step": 61070
    },
    {
      "epoch": 13.090441491641663,
      "grad_norm": 0.007398510351777077,
      "learning_rate": 2.5460780111444492e-06,
      "loss": 0.1394,
      "step": 61080
    },
    {
      "epoch": 13.092584654950707,
      "grad_norm": 0.05168725550174713,
      "learning_rate": 2.543220460065724e-06,
      "loss": 0.0003,
      "step": 61090
    },
    {
      "epoch": 13.094727818259752,
      "grad_norm": 0.00033824218553490937,
      "learning_rate": 2.5403629089869983e-06,
      "loss": 0.0003,
      "step": 61100
    },
    {
      "epoch": 13.096870981568795,
      "grad_norm": 0.011529680341482162,
      "learning_rate": 2.5375053579082727e-06,
      "loss": 0.1662,
      "step": 61110
    },
    {
      "epoch": 13.09901414487784,
      "grad_norm": 0.01110642496496439,
      "learning_rate": 2.534647806829547e-06,
      "loss": 0.0005,
      "step": 61120
    },
    {
      "epoch": 13.101157308186885,
      "grad_norm": 0.12661202251911163,
      "learning_rate": 2.531790255750822e-06,
      "loss": 0.0012,
      "step": 61130
    },
    {
      "epoch": 13.103300471495928,
      "grad_norm": 0.00934639759361744,
      "learning_rate": 2.528932704672096e-06,
      "loss": 0.0029,
      "step": 61140
    },
    {
      "epoch": 13.105443634804972,
      "grad_norm": 0.0007232107454910874,
      "learning_rate": 2.526075153593371e-06,
      "loss": 0.0016,
      "step": 61150
    },
    {
      "epoch": 13.107586798114017,
      "grad_norm": 0.00016755005344748497,
      "learning_rate": 2.5232176025146453e-06,
      "loss": 0.1805,
      "step": 61160
    },
    {
      "epoch": 13.10972996142306,
      "grad_norm": 0.06106586381793022,
      "learning_rate": 2.5203600514359192e-06,
      "loss": 0.1432,
      "step": 61170
    },
    {
      "epoch": 13.111873124732105,
      "grad_norm": 0.011674052104353905,
      "learning_rate": 2.517502500357194e-06,
      "loss": 0.0015,
      "step": 61180
    },
    {
      "epoch": 13.11401628804115,
      "grad_norm": 21.791460037231445,
      "learning_rate": 2.5146449492784688e-06,
      "loss": 0.1992,
      "step": 61190
    },
    {
      "epoch": 13.116159451350192,
      "grad_norm": 0.004926774650812149,
      "learning_rate": 2.511787398199743e-06,
      "loss": 0.0011,
      "step": 61200
    },
    {
      "epoch": 13.118302614659237,
      "grad_norm": 0.0025539181660860777,
      "learning_rate": 2.508929847121018e-06,
      "loss": 0.0002,
      "step": 61210
    },
    {
      "epoch": 13.120445777968282,
      "grad_norm": 0.0003645656688604504,
      "learning_rate": 2.506072296042292e-06,
      "loss": 0.3057,
      "step": 61220
    },
    {
      "epoch": 13.122588941277325,
      "grad_norm": 0.005321639124304056,
      "learning_rate": 2.503214744963566e-06,
      "loss": 0.1229,
      "step": 61230
    },
    {
      "epoch": 13.12473210458637,
      "grad_norm": 24.39979362487793,
      "learning_rate": 2.500357193884841e-06,
      "loss": 0.4949,
      "step": 61240
    },
    {
      "epoch": 13.126875267895414,
      "grad_norm": 0.02432701550424099,
      "learning_rate": 2.4974996428061153e-06,
      "loss": 0.0018,
      "step": 61250
    },
    {
      "epoch": 13.129018431204457,
      "grad_norm": 0.001361797796562314,
      "learning_rate": 2.4946420917273897e-06,
      "loss": 0.0012,
      "step": 61260
    },
    {
      "epoch": 13.131161594513502,
      "grad_norm": 0.024818314239382744,
      "learning_rate": 2.4917845406486644e-06,
      "loss": 0.0019,
      "step": 61270
    },
    {
      "epoch": 13.133304757822547,
      "grad_norm": 0.013843582011759281,
      "learning_rate": 2.488926989569939e-06,
      "loss": 0.0003,
      "step": 61280
    },
    {
      "epoch": 13.13544792113159,
      "grad_norm": 27.086055755615234,
      "learning_rate": 2.486069438491213e-06,
      "loss": 0.2357,
      "step": 61290
    },
    {
      "epoch": 13.137591084440635,
      "grad_norm": 0.006659502163529396,
      "learning_rate": 2.483211887412488e-06,
      "loss": 0.0007,
      "step": 61300
    },
    {
      "epoch": 13.13973424774968,
      "grad_norm": 0.0068763443268835545,
      "learning_rate": 2.480354336333762e-06,
      "loss": 0.0006,
      "step": 61310
    },
    {
      "epoch": 13.141877411058722,
      "grad_norm": 0.4459676146507263,
      "learning_rate": 2.4774967852550366e-06,
      "loss": 0.0011,
      "step": 61320
    },
    {
      "epoch": 13.144020574367767,
      "grad_norm": 0.04035419225692749,
      "learning_rate": 2.4746392341763114e-06,
      "loss": 0.1275,
      "step": 61330
    },
    {
      "epoch": 13.146163737676812,
      "grad_norm": 0.00851881131529808,
      "learning_rate": 2.4717816830975853e-06,
      "loss": 0.1444,
      "step": 61340
    },
    {
      "epoch": 13.148306900985855,
      "grad_norm": 29.97652244567871,
      "learning_rate": 2.46892413201886e-06,
      "loss": 0.4097,
      "step": 61350
    },
    {
      "epoch": 13.1504500642949,
      "grad_norm": 0.04093825817108154,
      "learning_rate": 2.4660665809401345e-06,
      "loss": 0.0005,
      "step": 61360
    },
    {
      "epoch": 13.152593227603944,
      "grad_norm": 0.01387718040496111,
      "learning_rate": 2.463209029861409e-06,
      "loss": 0.0004,
      "step": 61370
    },
    {
      "epoch": 13.154736390912987,
      "grad_norm": 0.000120136137411464,
      "learning_rate": 2.4603514787826836e-06,
      "loss": 0.123,
      "step": 61380
    },
    {
      "epoch": 13.156879554222032,
      "grad_norm": 0.00033864742727018893,
      "learning_rate": 2.457493927703958e-06,
      "loss": 0.0004,
      "step": 61390
    },
    {
      "epoch": 13.159022717531077,
      "grad_norm": 0.028041526675224304,
      "learning_rate": 2.4546363766252323e-06,
      "loss": 0.0005,
      "step": 61400
    },
    {
      "epoch": 13.16116588084012,
      "grad_norm": 0.0074914912693202496,
      "learning_rate": 2.451778825546507e-06,
      "loss": 0.0012,
      "step": 61410
    },
    {
      "epoch": 13.163309044149164,
      "grad_norm": 0.004639635793864727,
      "learning_rate": 2.4489212744677814e-06,
      "loss": 0.0001,
      "step": 61420
    },
    {
      "epoch": 13.165452207458209,
      "grad_norm": 0.3968646228313446,
      "learning_rate": 2.4460637233890558e-06,
      "loss": 0.0018,
      "step": 61430
    },
    {
      "epoch": 13.167595370767252,
      "grad_norm": 0.013311311602592468,
      "learning_rate": 2.4432061723103305e-06,
      "loss": 0.1507,
      "step": 61440
    },
    {
      "epoch": 13.169738534076297,
      "grad_norm": 16.867216110229492,
      "learning_rate": 2.4403486212316045e-06,
      "loss": 0.3762,
      "step": 61450
    },
    {
      "epoch": 13.171881697385341,
      "grad_norm": 0.4130169153213501,
      "learning_rate": 2.4374910701528792e-06,
      "loss": 0.0016,
      "step": 61460
    },
    {
      "epoch": 13.174024860694384,
      "grad_norm": 0.014627687633037567,
      "learning_rate": 2.4346335190741536e-06,
      "loss": 0.002,
      "step": 61470
    },
    {
      "epoch": 13.17616802400343,
      "grad_norm": 0.3471130430698395,
      "learning_rate": 2.431775967995428e-06,
      "loss": 0.3596,
      "step": 61480
    },
    {
      "epoch": 13.178311187312474,
      "grad_norm": 18.018712997436523,
      "learning_rate": 2.4289184169167027e-06,
      "loss": 0.1459,
      "step": 61490
    },
    {
      "epoch": 13.180454350621517,
      "grad_norm": 0.011028562672436237,
      "learning_rate": 2.426060865837977e-06,
      "loss": 0.0008,
      "step": 61500
    },
    {
      "epoch": 13.182597513930562,
      "grad_norm": 0.00204653711989522,
      "learning_rate": 2.4232033147592514e-06,
      "loss": 0.0009,
      "step": 61510
    },
    {
      "epoch": 13.184740677239606,
      "grad_norm": 0.001216796925291419,
      "learning_rate": 2.420345763680526e-06,
      "loss": 0.1388,
      "step": 61520
    },
    {
      "epoch": 13.18688384054865,
      "grad_norm": 0.008446484804153442,
      "learning_rate": 2.4174882126018006e-06,
      "loss": 0.0006,
      "step": 61530
    },
    {
      "epoch": 13.189027003857694,
      "grad_norm": 0.285602867603302,
      "learning_rate": 2.414630661523075e-06,
      "loss": 0.1698,
      "step": 61540
    },
    {
      "epoch": 13.191170167166739,
      "grad_norm": 0.04852721840143204,
      "learning_rate": 2.4117731104443493e-06,
      "loss": 0.4854,
      "step": 61550
    },
    {
      "epoch": 13.193313330475782,
      "grad_norm": 0.0005392296006903052,
      "learning_rate": 2.408915559365624e-06,
      "loss": 0.0012,
      "step": 61560
    },
    {
      "epoch": 13.195456493784826,
      "grad_norm": 0.00046209717402234674,
      "learning_rate": 2.4060580082868984e-06,
      "loss": 0.1447,
      "step": 61570
    },
    {
      "epoch": 13.197599657093871,
      "grad_norm": 0.009230573661625385,
      "learning_rate": 2.4032004572081727e-06,
      "loss": 0.1254,
      "step": 61580
    },
    {
      "epoch": 13.199742820402914,
      "grad_norm": 0.00062667386373505,
      "learning_rate": 2.400342906129447e-06,
      "loss": 0.0006,
      "step": 61590
    },
    {
      "epoch": 13.201885983711959,
      "grad_norm": 0.008804542012512684,
      "learning_rate": 2.397485355050722e-06,
      "loss": 0.1319,
      "step": 61600
    },
    {
      "epoch": 13.204029147021004,
      "grad_norm": 0.3674231767654419,
      "learning_rate": 2.3946278039719962e-06,
      "loss": 0.3385,
      "step": 61610
    },
    {
      "epoch": 13.206172310330047,
      "grad_norm": 0.002369487192481756,
      "learning_rate": 2.3917702528932706e-06,
      "loss": 0.281,
      "step": 61620
    },
    {
      "epoch": 13.208315473639091,
      "grad_norm": 0.016360970214009285,
      "learning_rate": 2.3889127018145453e-06,
      "loss": 0.0959,
      "step": 61630
    },
    {
      "epoch": 13.210458636948136,
      "grad_norm": 0.028821207582950592,
      "learning_rate": 2.3860551507358197e-06,
      "loss": 0.0008,
      "step": 61640
    },
    {
      "epoch": 13.212601800257179,
      "grad_norm": 0.33698156476020813,
      "learning_rate": 2.383197599657094e-06,
      "loss": 0.0011,
      "step": 61650
    },
    {
      "epoch": 13.214744963566224,
      "grad_norm": 0.007425057701766491,
      "learning_rate": 2.3803400485783684e-06,
      "loss": 0.2648,
      "step": 61660
    },
    {
      "epoch": 13.216888126875268,
      "grad_norm": 0.3398221731185913,
      "learning_rate": 2.377482497499643e-06,
      "loss": 0.138,
      "step": 61670
    },
    {
      "epoch": 13.219031290184311,
      "grad_norm": 0.006197127979248762,
      "learning_rate": 2.3746249464209175e-06,
      "loss": 0.1429,
      "step": 61680
    },
    {
      "epoch": 13.221174453493356,
      "grad_norm": 0.045799702405929565,
      "learning_rate": 2.371767395342192e-06,
      "loss": 0.001,
      "step": 61690
    },
    {
      "epoch": 13.223317616802401,
      "grad_norm": 0.008869864046573639,
      "learning_rate": 2.3689098442634662e-06,
      "loss": 0.0002,
      "step": 61700
    },
    {
      "epoch": 13.225460780111444,
      "grad_norm": 0.03637286648154259,
      "learning_rate": 2.366052293184741e-06,
      "loss": 0.1817,
      "step": 61710
    },
    {
      "epoch": 13.227603943420489,
      "grad_norm": 0.2167014628648758,
      "learning_rate": 2.3631947421060154e-06,
      "loss": 0.0005,
      "step": 61720
    },
    {
      "epoch": 13.229747106729533,
      "grad_norm": 0.0005324093508534133,
      "learning_rate": 2.3603371910272897e-06,
      "loss": 0.0002,
      "step": 61730
    },
    {
      "epoch": 13.231890270038576,
      "grad_norm": 0.0005795095348730683,
      "learning_rate": 2.357479639948564e-06,
      "loss": 0.0036,
      "step": 61740
    },
    {
      "epoch": 13.234033433347621,
      "grad_norm": 0.00034939523902721703,
      "learning_rate": 2.354622088869839e-06,
      "loss": 0.0002,
      "step": 61750
    },
    {
      "epoch": 13.236176596656666,
      "grad_norm": 0.0005000696983188391,
      "learning_rate": 2.351764537791113e-06,
      "loss": 0.1777,
      "step": 61760
    },
    {
      "epoch": 13.238319759965709,
      "grad_norm": 0.008668988011777401,
      "learning_rate": 2.3489069867123875e-06,
      "loss": 0.2248,
      "step": 61770
    },
    {
      "epoch": 13.240462923274753,
      "grad_norm": 0.025200365111231804,
      "learning_rate": 2.3460494356336623e-06,
      "loss": 0.0011,
      "step": 61780
    },
    {
      "epoch": 13.242606086583798,
      "grad_norm": 0.00043532243580557406,
      "learning_rate": 2.3431918845549367e-06,
      "loss": 0.0001,
      "step": 61790
    },
    {
      "epoch": 13.244749249892841,
      "grad_norm": 0.05770020931959152,
      "learning_rate": 2.340334333476211e-06,
      "loss": 0.002,
      "step": 61800
    },
    {
      "epoch": 13.246892413201886,
      "grad_norm": 0.00032035508775152266,
      "learning_rate": 2.337476782397486e-06,
      "loss": 0.0007,
      "step": 61810
    },
    {
      "epoch": 13.24903557651093,
      "grad_norm": 0.01918654516339302,
      "learning_rate": 2.3346192313187597e-06,
      "loss": 0.0002,
      "step": 61820
    },
    {
      "epoch": 13.251178739819974,
      "grad_norm": 0.001806059037335217,
      "learning_rate": 2.3317616802400345e-06,
      "loss": 0.1071,
      "step": 61830
    },
    {
      "epoch": 13.253321903129018,
      "grad_norm": 0.00021033243683632463,
      "learning_rate": 2.328904129161309e-06,
      "loss": 0.1289,
      "step": 61840
    },
    {
      "epoch": 13.255465066438063,
      "grad_norm": 0.010105659253895283,
      "learning_rate": 2.326046578082583e-06,
      "loss": 0.0004,
      "step": 61850
    },
    {
      "epoch": 13.257608229747106,
      "grad_norm": 0.16420501470565796,
      "learning_rate": 2.323189027003858e-06,
      "loss": 0.1182,
      "step": 61860
    },
    {
      "epoch": 13.25975139305615,
      "grad_norm": 0.0008400807273574173,
      "learning_rate": 2.3203314759251323e-06,
      "loss": 0.3744,
      "step": 61870
    },
    {
      "epoch": 13.261894556365196,
      "grad_norm": 0.00015092315152287483,
      "learning_rate": 2.3174739248464067e-06,
      "loss": 0.1233,
      "step": 61880
    },
    {
      "epoch": 13.264037719674239,
      "grad_norm": 0.006582014728337526,
      "learning_rate": 2.3146163737676815e-06,
      "loss": 0.0017,
      "step": 61890
    },
    {
      "epoch": 13.266180882983283,
      "grad_norm": 0.0162593275308609,
      "learning_rate": 2.311758822688956e-06,
      "loss": 0.1451,
      "step": 61900
    },
    {
      "epoch": 13.268324046292328,
      "grad_norm": 0.0005621465970762074,
      "learning_rate": 2.30890127161023e-06,
      "loss": 0.2031,
      "step": 61910
    },
    {
      "epoch": 13.270467209601371,
      "grad_norm": 0.014315927401185036,
      "learning_rate": 2.306043720531505e-06,
      "loss": 0.1215,
      "step": 61920
    },
    {
      "epoch": 13.272610372910416,
      "grad_norm": 0.5671420097351074,
      "learning_rate": 2.303186169452779e-06,
      "loss": 0.001,
      "step": 61930
    },
    {
      "epoch": 13.27475353621946,
      "grad_norm": 0.0008990077185444534,
      "learning_rate": 2.3003286183740536e-06,
      "loss": 0.0006,
      "step": 61940
    },
    {
      "epoch": 13.276896699528503,
      "grad_norm": 0.0003819061676040292,
      "learning_rate": 2.2974710672953284e-06,
      "loss": 0.0008,
      "step": 61950
    },
    {
      "epoch": 13.279039862837548,
      "grad_norm": 0.0015427711186930537,
      "learning_rate": 2.2946135162166023e-06,
      "loss": 0.2001,
      "step": 61960
    },
    {
      "epoch": 13.281183026146593,
      "grad_norm": 0.0062034595757722855,
      "learning_rate": 2.291755965137877e-06,
      "loss": 0.0005,
      "step": 61970
    },
    {
      "epoch": 13.283326189455636,
      "grad_norm": 0.0002555984247010201,
      "learning_rate": 2.2888984140591515e-06,
      "loss": 0.1895,
      "step": 61980
    },
    {
      "epoch": 13.28546935276468,
      "grad_norm": 0.0006385745364241302,
      "learning_rate": 2.286040862980426e-06,
      "loss": 0.135,
      "step": 61990
    },
    {
      "epoch": 13.287612516073725,
      "grad_norm": 0.006458759773522615,
      "learning_rate": 2.2831833119017006e-06,
      "loss": 0.0018,
      "step": 62000
    },
    {
      "epoch": 13.289755679382768,
      "grad_norm": 0.0003360504051670432,
      "learning_rate": 2.280325760822975e-06,
      "loss": 0.5173,
      "step": 62010
    },
    {
      "epoch": 13.291898842691813,
      "grad_norm": 23.257482528686523,
      "learning_rate": 2.2774682097442493e-06,
      "loss": 0.2848,
      "step": 62020
    },
    {
      "epoch": 13.294042006000858,
      "grad_norm": 0.017418375238776207,
      "learning_rate": 2.274610658665524e-06,
      "loss": 0.3259,
      "step": 62030
    },
    {
      "epoch": 13.2961851693099,
      "grad_norm": 0.14967241883277893,
      "learning_rate": 2.2717531075867984e-06,
      "loss": 0.1274,
      "step": 62040
    },
    {
      "epoch": 13.298328332618945,
      "grad_norm": 0.011265017092227936,
      "learning_rate": 2.2688955565080728e-06,
      "loss": 0.001,
      "step": 62050
    },
    {
      "epoch": 13.30047149592799,
      "grad_norm": 0.0132670933380723,
      "learning_rate": 2.266038005429347e-06,
      "loss": 0.0001,
      "step": 62060
    },
    {
      "epoch": 13.302614659237033,
      "grad_norm": 0.019976243376731873,
      "learning_rate": 2.2631804543506215e-06,
      "loss": 0.1409,
      "step": 62070
    },
    {
      "epoch": 13.304757822546078,
      "grad_norm": 0.0010349249932914972,
      "learning_rate": 2.2603229032718963e-06,
      "loss": 0.0001,
      "step": 62080
    },
    {
      "epoch": 13.306900985855123,
      "grad_norm": 0.3284126818180084,
      "learning_rate": 2.2574653521931706e-06,
      "loss": 0.0009,
      "step": 62090
    },
    {
      "epoch": 13.309044149164166,
      "grad_norm": 0.009358398616313934,
      "learning_rate": 2.254607801114445e-06,
      "loss": 0.1544,
      "step": 62100
    },
    {
      "epoch": 13.31118731247321,
      "grad_norm": 0.3936307430267334,
      "learning_rate": 2.2517502500357197e-06,
      "loss": 0.001,
      "step": 62110
    },
    {
      "epoch": 13.313330475782255,
      "grad_norm": 0.00700040627270937,
      "learning_rate": 2.248892698956994e-06,
      "loss": 0.0003,
      "step": 62120
    },
    {
      "epoch": 13.315473639091298,
      "grad_norm": 0.03979506343603134,
      "learning_rate": 2.2460351478782684e-06,
      "loss": 0.0006,
      "step": 62130
    },
    {
      "epoch": 13.317616802400343,
      "grad_norm": 0.09458031505346298,
      "learning_rate": 2.2431775967995432e-06,
      "loss": 0.2676,
      "step": 62140
    },
    {
      "epoch": 13.319759965709387,
      "grad_norm": 0.0758172795176506,
      "learning_rate": 2.2403200457208176e-06,
      "loss": 0.0018,
      "step": 62150
    },
    {
      "epoch": 13.32190312901843,
      "grad_norm": 0.10790113359689713,
      "learning_rate": 2.237462494642092e-06,
      "loss": 0.0015,
      "step": 62160
    },
    {
      "epoch": 13.324046292327475,
      "grad_norm": 0.1229172870516777,
      "learning_rate": 2.2346049435633663e-06,
      "loss": 0.3295,
      "step": 62170
    },
    {
      "epoch": 13.32618945563652,
      "grad_norm": 0.005064390599727631,
      "learning_rate": 2.231747392484641e-06,
      "loss": 0.3378,
      "step": 62180
    },
    {
      "epoch": 13.328332618945563,
      "grad_norm": 0.0005651598912663758,
      "learning_rate": 2.2288898414059154e-06,
      "loss": 0.0001,
      "step": 62190
    },
    {
      "epoch": 13.330475782254608,
      "grad_norm": 0.01129077561199665,
      "learning_rate": 2.2260322903271897e-06,
      "loss": 0.157,
      "step": 62200
    },
    {
      "epoch": 13.332618945563652,
      "grad_norm": 0.0014296750305220485,
      "learning_rate": 2.223174739248464e-06,
      "loss": 0.1271,
      "step": 62210
    },
    {
      "epoch": 13.334762108872695,
      "grad_norm": 0.022201500833034515,
      "learning_rate": 2.220317188169739e-06,
      "loss": 0.0002,
      "step": 62220
    },
    {
      "epoch": 13.33690527218174,
      "grad_norm": 0.0055912574753165245,
      "learning_rate": 2.2174596370910132e-06,
      "loss": 0.0005,
      "step": 62230
    },
    {
      "epoch": 13.339048435490785,
      "grad_norm": 0.00446717394515872,
      "learning_rate": 2.2146020860122876e-06,
      "loss": 0.3218,
      "step": 62240
    },
    {
      "epoch": 13.341191598799828,
      "grad_norm": 0.02087632566690445,
      "learning_rate": 2.211744534933562e-06,
      "loss": 0.0002,
      "step": 62250
    },
    {
      "epoch": 13.343334762108872,
      "grad_norm": 0.0017980725970119238,
      "learning_rate": 2.2088869838548367e-06,
      "loss": 0.1809,
      "step": 62260
    },
    {
      "epoch": 13.345477925417917,
      "grad_norm": 0.010933415964245796,
      "learning_rate": 2.206029432776111e-06,
      "loss": 0.0003,
      "step": 62270
    },
    {
      "epoch": 13.34762108872696,
      "grad_norm": 0.0044851405546069145,
      "learning_rate": 2.2031718816973854e-06,
      "loss": 0.0002,
      "step": 62280
    },
    {
      "epoch": 13.349764252036005,
      "grad_norm": 0.0006933518452569842,
      "learning_rate": 2.20031433061866e-06,
      "loss": 0.0002,
      "step": 62290
    },
    {
      "epoch": 13.35190741534505,
      "grad_norm": 0.10752768069505692,
      "learning_rate": 2.1974567795399345e-06,
      "loss": 0.1909,
      "step": 62300
    },
    {
      "epoch": 13.354050578654093,
      "grad_norm": 0.009227653034031391,
      "learning_rate": 2.194599228461209e-06,
      "loss": 0.5447,
      "step": 62310
    },
    {
      "epoch": 13.356193741963137,
      "grad_norm": 0.0045606098137795925,
      "learning_rate": 2.1917416773824832e-06,
      "loss": 0.0004,
      "step": 62320
    },
    {
      "epoch": 13.358336905272182,
      "grad_norm": 0.09014303982257843,
      "learning_rate": 2.1888841263037576e-06,
      "loss": 0.0025,
      "step": 62330
    },
    {
      "epoch": 13.360480068581225,
      "grad_norm": 0.2890644073486328,
      "learning_rate": 2.1860265752250324e-06,
      "loss": 0.1237,
      "step": 62340
    },
    {
      "epoch": 13.36262323189027,
      "grad_norm": 0.0069953626953065395,
      "learning_rate": 2.1831690241463067e-06,
      "loss": 0.129,
      "step": 62350
    },
    {
      "epoch": 13.364766395199315,
      "grad_norm": 0.054015640169382095,
      "learning_rate": 2.180311473067581e-06,
      "loss": 0.0017,
      "step": 62360
    },
    {
      "epoch": 13.366909558508357,
      "grad_norm": 0.0018416744424030185,
      "learning_rate": 2.177453921988856e-06,
      "loss": 0.002,
      "step": 62370
    },
    {
      "epoch": 13.369052721817402,
      "grad_norm": 0.020902547985315323,
      "learning_rate": 2.17459637091013e-06,
      "loss": 0.001,
      "step": 62380
    },
    {
      "epoch": 13.371195885126447,
      "grad_norm": 0.00861539226025343,
      "learning_rate": 2.1717388198314045e-06,
      "loss": 0.1281,
      "step": 62390
    },
    {
      "epoch": 13.37333904843549,
      "grad_norm": 0.008734393864870071,
      "learning_rate": 2.1688812687526793e-06,
      "loss": 0.0008,
      "step": 62400
    },
    {
      "epoch": 13.375482211744535,
      "grad_norm": 0.0014353785663843155,
      "learning_rate": 2.1660237176739537e-06,
      "loss": 0.2545,
      "step": 62410
    },
    {
      "epoch": 13.37762537505358,
      "grad_norm": 0.02928321249783039,
      "learning_rate": 2.163166166595228e-06,
      "loss": 0.25,
      "step": 62420
    },
    {
      "epoch": 13.379768538362622,
      "grad_norm": 0.017840664833784103,
      "learning_rate": 2.160308615516503e-06,
      "loss": 0.0008,
      "step": 62430
    },
    {
      "epoch": 13.381911701671667,
      "grad_norm": 0.0007447571842931211,
      "learning_rate": 2.1574510644377767e-06,
      "loss": 0.0002,
      "step": 62440
    },
    {
      "epoch": 13.384054864980712,
      "grad_norm": 0.0012247348204255104,
      "learning_rate": 2.1545935133590515e-06,
      "loss": 0.0007,
      "step": 62450
    },
    {
      "epoch": 13.386198028289755,
      "grad_norm": 0.0014579108683392406,
      "learning_rate": 2.151735962280326e-06,
      "loss": 0.0012,
      "step": 62460
    },
    {
      "epoch": 13.3883411915988,
      "grad_norm": 0.010706432163715363,
      "learning_rate": 2.1488784112016e-06,
      "loss": 0.0002,
      "step": 62470
    },
    {
      "epoch": 13.390484354907844,
      "grad_norm": 0.0005646790377795696,
      "learning_rate": 2.146020860122875e-06,
      "loss": 0.1377,
      "step": 62480
    },
    {
      "epoch": 13.392627518216887,
      "grad_norm": 0.00010954937170026824,
      "learning_rate": 2.1431633090441493e-06,
      "loss": 0.1317,
      "step": 62490
    },
    {
      "epoch": 13.394770681525932,
      "grad_norm": 23.680448532104492,
      "learning_rate": 2.1403057579654237e-06,
      "loss": 0.1843,
      "step": 62500
    },
    {
      "epoch": 13.396913844834977,
      "grad_norm": 0.0015401451382786036,
      "learning_rate": 2.1374482068866985e-06,
      "loss": 0.0005,
      "step": 62510
    },
    {
      "epoch": 13.39905700814402,
      "grad_norm": 0.00020592058717738837,
      "learning_rate": 2.134590655807973e-06,
      "loss": 0.0003,
      "step": 62520
    },
    {
      "epoch": 13.401200171453064,
      "grad_norm": 0.2384568750858307,
      "learning_rate": 2.131733104729247e-06,
      "loss": 0.3342,
      "step": 62530
    },
    {
      "epoch": 13.40334333476211,
      "grad_norm": 0.32917681336402893,
      "learning_rate": 2.128875553650522e-06,
      "loss": 0.0014,
      "step": 62540
    },
    {
      "epoch": 13.405486498071152,
      "grad_norm": 0.19051937758922577,
      "learning_rate": 2.126018002571796e-06,
      "loss": 0.1434,
      "step": 62550
    },
    {
      "epoch": 13.407629661380197,
      "grad_norm": 0.06381804496049881,
      "learning_rate": 2.1231604514930706e-06,
      "loss": 0.1411,
      "step": 62560
    },
    {
      "epoch": 13.409772824689242,
      "grad_norm": 0.007764277048408985,
      "learning_rate": 2.120302900414345e-06,
      "loss": 0.0001,
      "step": 62570
    },
    {
      "epoch": 13.411915987998286,
      "grad_norm": 0.0012833446962758899,
      "learning_rate": 2.1174453493356193e-06,
      "loss": 0.1375,
      "step": 62580
    },
    {
      "epoch": 13.41405915130733,
      "grad_norm": 0.0009121171315200627,
      "learning_rate": 2.114587798256894e-06,
      "loss": 0.0004,
      "step": 62590
    },
    {
      "epoch": 13.416202314616374,
      "grad_norm": 0.008342773653566837,
      "learning_rate": 2.1117302471781685e-06,
      "loss": 0.0009,
      "step": 62600
    },
    {
      "epoch": 13.418345477925419,
      "grad_norm": 0.012790980748832226,
      "learning_rate": 2.108872696099443e-06,
      "loss": 0.1995,
      "step": 62610
    },
    {
      "epoch": 13.420488641234462,
      "grad_norm": 0.9568807482719421,
      "learning_rate": 2.1060151450207176e-06,
      "loss": 0.1599,
      "step": 62620
    },
    {
      "epoch": 13.422631804543506,
      "grad_norm": 0.0015621497295796871,
      "learning_rate": 2.103157593941992e-06,
      "loss": 0.0007,
      "step": 62630
    },
    {
      "epoch": 13.424774967852551,
      "grad_norm": 0.0004601402906700969,
      "learning_rate": 2.1003000428632663e-06,
      "loss": 0.0016,
      "step": 62640
    },
    {
      "epoch": 13.426918131161594,
      "grad_norm": 0.005284872837364674,
      "learning_rate": 2.097442491784541e-06,
      "loss": 0.0011,
      "step": 62650
    },
    {
      "epoch": 13.429061294470639,
      "grad_norm": 0.2090352475643158,
      "learning_rate": 2.0945849407058154e-06,
      "loss": 0.0002,
      "step": 62660
    },
    {
      "epoch": 13.431204457779684,
      "grad_norm": 0.0013640900142490864,
      "learning_rate": 2.0917273896270898e-06,
      "loss": 0.0002,
      "step": 62670
    },
    {
      "epoch": 13.433347621088727,
      "grad_norm": 0.0008017392246983945,
      "learning_rate": 2.088869838548364e-06,
      "loss": 0.001,
      "step": 62680
    },
    {
      "epoch": 13.435490784397771,
      "grad_norm": 0.007808296009898186,
      "learning_rate": 2.0860122874696385e-06,
      "loss": 0.0001,
      "step": 62690
    },
    {
      "epoch": 13.437633947706816,
      "grad_norm": 0.002996808383613825,
      "learning_rate": 2.0831547363909133e-06,
      "loss": 0.001,
      "step": 62700
    },
    {
      "epoch": 13.439777111015859,
      "grad_norm": 0.0004194566863588989,
      "learning_rate": 2.0802971853121876e-06,
      "loss": 0.0003,
      "step": 62710
    },
    {
      "epoch": 13.441920274324904,
      "grad_norm": 0.001146382070146501,
      "learning_rate": 2.077439634233462e-06,
      "loss": 0.0003,
      "step": 62720
    },
    {
      "epoch": 13.444063437633949,
      "grad_norm": 0.011646075174212456,
      "learning_rate": 2.0745820831547367e-06,
      "loss": 0.0001,
      "step": 62730
    },
    {
      "epoch": 13.446206600942991,
      "grad_norm": 0.009835287928581238,
      "learning_rate": 2.071724532076011e-06,
      "loss": 0.0001,
      "step": 62740
    },
    {
      "epoch": 13.448349764252036,
      "grad_norm": 0.00014341442147269845,
      "learning_rate": 2.0688669809972854e-06,
      "loss": 0.0001,
      "step": 62750
    },
    {
      "epoch": 13.450492927561081,
      "grad_norm": 0.0018496910342946649,
      "learning_rate": 2.06600942991856e-06,
      "loss": 0.0001,
      "step": 62760
    },
    {
      "epoch": 13.452636090870124,
      "grad_norm": 0.004816665779799223,
      "learning_rate": 2.0631518788398346e-06,
      "loss": 0.0002,
      "step": 62770
    },
    {
      "epoch": 13.454779254179169,
      "grad_norm": 0.044900715351104736,
      "learning_rate": 2.060294327761109e-06,
      "loss": 0.0001,
      "step": 62780
    },
    {
      "epoch": 13.456922417488213,
      "grad_norm": 0.00037597777554765344,
      "learning_rate": 2.0574367766823833e-06,
      "loss": 0.0001,
      "step": 62790
    },
    {
      "epoch": 13.459065580797256,
      "grad_norm": 0.0020533266942948103,
      "learning_rate": 2.054579225603658e-06,
      "loss": 0.1533,
      "step": 62800
    },
    {
      "epoch": 13.461208744106301,
      "grad_norm": 0.0033898481633514166,
      "learning_rate": 2.0517216745249324e-06,
      "loss": 0.0001,
      "step": 62810
    },
    {
      "epoch": 13.463351907415346,
      "grad_norm": 0.0004738165298476815,
      "learning_rate": 2.0488641234462068e-06,
      "loss": 0.0002,
      "step": 62820
    },
    {
      "epoch": 13.465495070724389,
      "grad_norm": 0.000813570455648005,
      "learning_rate": 2.046006572367481e-06,
      "loss": 0.0001,
      "step": 62830
    },
    {
      "epoch": 13.467638234033434,
      "grad_norm": 0.000319734332151711,
      "learning_rate": 2.0431490212887555e-06,
      "loss": 0.0001,
      "step": 62840
    },
    {
      "epoch": 13.469781397342478,
      "grad_norm": 0.013200566172599792,
      "learning_rate": 2.0402914702100302e-06,
      "loss": 0.1926,
      "step": 62850
    },
    {
      "epoch": 13.471924560651521,
      "grad_norm": 0.0003434082318563014,
      "learning_rate": 2.0374339191313046e-06,
      "loss": 0.0004,
      "step": 62860
    },
    {
      "epoch": 13.474067723960566,
      "grad_norm": 0.0007230510818772018,
      "learning_rate": 2.034576368052579e-06,
      "loss": 0.0001,
      "step": 62870
    },
    {
      "epoch": 13.47621088726961,
      "grad_norm": 0.0027355460915714502,
      "learning_rate": 2.0317188169738537e-06,
      "loss": 0.0005,
      "step": 62880
    },
    {
      "epoch": 13.478354050578654,
      "grad_norm": 0.009872548282146454,
      "learning_rate": 2.028861265895128e-06,
      "loss": 0.0001,
      "step": 62890
    },
    {
      "epoch": 13.480497213887698,
      "grad_norm": 0.005902859847992659,
      "learning_rate": 2.0260037148164024e-06,
      "loss": 0.0001,
      "step": 62900
    },
    {
      "epoch": 13.482640377196743,
      "grad_norm": 0.1617915779352188,
      "learning_rate": 2.023146163737677e-06,
      "loss": 0.0005,
      "step": 62910
    },
    {
      "epoch": 13.484783540505786,
      "grad_norm": 0.0007895882590673864,
      "learning_rate": 2.020288612658951e-06,
      "loss": 0.1664,
      "step": 62920
    },
    {
      "epoch": 13.48692670381483,
      "grad_norm": 0.00030298353522084653,
      "learning_rate": 2.017431061580226e-06,
      "loss": 0.1261,
      "step": 62930
    },
    {
      "epoch": 13.489069867123876,
      "grad_norm": 0.004236558917909861,
      "learning_rate": 2.0145735105015007e-06,
      "loss": 0.0,
      "step": 62940
    },
    {
      "epoch": 13.491213030432919,
      "grad_norm": 0.00025978664052672684,
      "learning_rate": 2.0117159594227746e-06,
      "loss": 0.0001,
      "step": 62950
    },
    {
      "epoch": 13.493356193741963,
      "grad_norm": 0.005385029129683971,
      "learning_rate": 2.0088584083440494e-06,
      "loss": 0.0002,
      "step": 62960
    },
    {
      "epoch": 13.495499357051008,
      "grad_norm": 0.00436076195910573,
      "learning_rate": 2.0060008572653237e-06,
      "loss": 0.0001,
      "step": 62970
    },
    {
      "epoch": 13.497642520360051,
      "grad_norm": 0.07278154790401459,
      "learning_rate": 2.003143306186598e-06,
      "loss": 0.286,
      "step": 62980
    },
    {
      "epoch": 13.499785683669096,
      "grad_norm": 0.00023411911388393492,
      "learning_rate": 2.000285755107873e-06,
      "loss": 0.0001,
      "step": 62990
    },
    {
      "epoch": 13.50192884697814,
      "grad_norm": 0.01281250361353159,
      "learning_rate": 1.997428204029147e-06,
      "loss": 0.0001,
      "step": 63000
    },
    {
      "epoch": 13.504072010287183,
      "grad_norm": 0.00011825522233266383,
      "learning_rate": 1.9945706529504216e-06,
      "loss": 0.0006,
      "step": 63010
    },
    {
      "epoch": 13.506215173596228,
      "grad_norm": 0.006176886614412069,
      "learning_rate": 1.9917131018716963e-06,
      "loss": 0.174,
      "step": 63020
    },
    {
      "epoch": 13.508358336905273,
      "grad_norm": 0.00010948458657367155,
      "learning_rate": 1.9888555507929707e-06,
      "loss": 0.0003,
      "step": 63030
    },
    {
      "epoch": 13.510501500214316,
      "grad_norm": 0.2169380784034729,
      "learning_rate": 1.985997999714245e-06,
      "loss": 0.1588,
      "step": 63040
    },
    {
      "epoch": 13.51264466352336,
      "grad_norm": 0.028825653716921806,
      "learning_rate": 1.98314044863552e-06,
      "loss": 0.2117,
      "step": 63050
    },
    {
      "epoch": 13.514787826832405,
      "grad_norm": 0.0047623468562960625,
      "learning_rate": 1.9802828975567937e-06,
      "loss": 0.1422,
      "step": 63060
    },
    {
      "epoch": 13.516930990141448,
      "grad_norm": 0.006169116590172052,
      "learning_rate": 1.9774253464780685e-06,
      "loss": 0.0003,
      "step": 63070
    },
    {
      "epoch": 13.519074153450493,
      "grad_norm": 0.0007198103121481836,
      "learning_rate": 1.974567795399343e-06,
      "loss": 0.2966,
      "step": 63080
    },
    {
      "epoch": 13.521217316759538,
      "grad_norm": 0.010913783684372902,
      "learning_rate": 1.9717102443206172e-06,
      "loss": 0.001,
      "step": 63090
    },
    {
      "epoch": 13.52336048006858,
      "grad_norm": 0.0003486759669613093,
      "learning_rate": 1.968852693241892e-06,
      "loss": 0.1126,
      "step": 63100
    },
    {
      "epoch": 13.525503643377625,
      "grad_norm": 0.014156539924442768,
      "learning_rate": 1.9659951421631663e-06,
      "loss": 0.0002,
      "step": 63110
    },
    {
      "epoch": 13.52764680668667,
      "grad_norm": 0.009956516325473785,
      "learning_rate": 1.9631375910844407e-06,
      "loss": 0.0001,
      "step": 63120
    },
    {
      "epoch": 13.529789969995713,
      "grad_norm": 0.015290869399905205,
      "learning_rate": 1.9602800400057155e-06,
      "loss": 0.0,
      "step": 63130
    },
    {
      "epoch": 13.531933133304758,
      "grad_norm": 28.481182098388672,
      "learning_rate": 1.95742248892699e-06,
      "loss": 0.2375,
      "step": 63140
    },
    {
      "epoch": 13.534076296613803,
      "grad_norm": 0.01390665490180254,
      "learning_rate": 1.954564937848264e-06,
      "loss": 0.0005,
      "step": 63150
    },
    {
      "epoch": 13.536219459922846,
      "grad_norm": 0.06389947235584259,
      "learning_rate": 1.951707386769539e-06,
      "loss": 0.1519,
      "step": 63160
    },
    {
      "epoch": 13.53836262323189,
      "grad_norm": 0.000517968728672713,
      "learning_rate": 1.9488498356908133e-06,
      "loss": 0.2465,
      "step": 63170
    },
    {
      "epoch": 13.540505786540935,
      "grad_norm": 0.00211508315987885,
      "learning_rate": 1.9459922846120877e-06,
      "loss": 0.0003,
      "step": 63180
    },
    {
      "epoch": 13.542648949849978,
      "grad_norm": 0.04775875434279442,
      "learning_rate": 1.943134733533362e-06,
      "loss": 0.001,
      "step": 63190
    },
    {
      "epoch": 13.544792113159023,
      "grad_norm": 266.0097961425781,
      "learning_rate": 1.9402771824546364e-06,
      "loss": 0.34,
      "step": 63200
    },
    {
      "epoch": 13.546935276468067,
      "grad_norm": 0.06004446744918823,
      "learning_rate": 1.937419631375911e-06,
      "loss": 0.1498,
      "step": 63210
    },
    {
      "epoch": 13.54907843977711,
      "grad_norm": 0.008850407786667347,
      "learning_rate": 1.9345620802971855e-06,
      "loss": 0.0008,
      "step": 63220
    },
    {
      "epoch": 13.551221603086155,
      "grad_norm": 35.95061111450195,
      "learning_rate": 1.93170452921846e-06,
      "loss": 0.4773,
      "step": 63230
    },
    {
      "epoch": 13.5533647663952,
      "grad_norm": 0.021083401516079903,
      "learning_rate": 1.9288469781397346e-06,
      "loss": 0.0008,
      "step": 63240
    },
    {
      "epoch": 13.555507929704243,
      "grad_norm": 0.0019062570063397288,
      "learning_rate": 1.925989427061009e-06,
      "loss": 0.193,
      "step": 63250
    },
    {
      "epoch": 13.557651093013288,
      "grad_norm": 0.11416444182395935,
      "learning_rate": 1.9231318759822833e-06,
      "loss": 0.0004,
      "step": 63260
    },
    {
      "epoch": 13.559794256322332,
      "grad_norm": 0.0019383563194423914,
      "learning_rate": 1.9202743249035577e-06,
      "loss": 0.7324,
      "step": 63270
    },
    {
      "epoch": 13.561937419631375,
      "grad_norm": 0.04887138307094574,
      "learning_rate": 1.9174167738248324e-06,
      "loss": 0.0014,
      "step": 63280
    },
    {
      "epoch": 13.56408058294042,
      "grad_norm": 0.02075844816863537,
      "learning_rate": 1.914559222746107e-06,
      "loss": 0.0004,
      "step": 63290
    },
    {
      "epoch": 13.566223746249465,
      "grad_norm": 22.44117546081543,
      "learning_rate": 1.911701671667381e-06,
      "loss": 0.1912,
      "step": 63300
    },
    {
      "epoch": 13.568366909558508,
      "grad_norm": 0.00038076448254287243,
      "learning_rate": 1.9088441205886555e-06,
      "loss": 0.1362,
      "step": 63310
    },
    {
      "epoch": 13.570510072867553,
      "grad_norm": 0.0010215534130111337,
      "learning_rate": 1.9059865695099303e-06,
      "loss": 0.0002,
      "step": 63320
    },
    {
      "epoch": 13.572653236176597,
      "grad_norm": 0.011417798697948456,
      "learning_rate": 1.9031290184312046e-06,
      "loss": 0.2043,
      "step": 63330
    },
    {
      "epoch": 13.57479639948564,
      "grad_norm": 0.3244982659816742,
      "learning_rate": 1.9002714673524792e-06,
      "loss": 0.1542,
      "step": 63340
    },
    {
      "epoch": 13.576939562794685,
      "grad_norm": 0.004379907622933388,
      "learning_rate": 1.8974139162737533e-06,
      "loss": 0.0005,
      "step": 63350
    },
    {
      "epoch": 13.57908272610373,
      "grad_norm": 0.00894670095294714,
      "learning_rate": 1.8945563651950281e-06,
      "loss": 0.0003,
      "step": 63360
    },
    {
      "epoch": 13.581225889412773,
      "grad_norm": 0.040372494608163834,
      "learning_rate": 1.8916988141163027e-06,
      "loss": 0.1365,
      "step": 63370
    },
    {
      "epoch": 13.583369052721817,
      "grad_norm": 0.005098902154713869,
      "learning_rate": 1.8888412630375768e-06,
      "loss": 0.1394,
      "step": 63380
    },
    {
      "epoch": 13.585512216030862,
      "grad_norm": 0.024278810247778893,
      "learning_rate": 1.8859837119588514e-06,
      "loss": 0.0012,
      "step": 63390
    },
    {
      "epoch": 13.587655379339905,
      "grad_norm": 0.010918249376118183,
      "learning_rate": 1.883126160880126e-06,
      "loss": 0.0003,
      "step": 63400
    },
    {
      "epoch": 13.58979854264895,
      "grad_norm": 0.014675035141408443,
      "learning_rate": 1.8802686098014003e-06,
      "loss": 0.0004,
      "step": 63410
    },
    {
      "epoch": 13.591941705957995,
      "grad_norm": 0.01259826309978962,
      "learning_rate": 1.8774110587226749e-06,
      "loss": 0.0011,
      "step": 63420
    },
    {
      "epoch": 13.594084869267038,
      "grad_norm": 0.28296327590942383,
      "learning_rate": 1.8745535076439492e-06,
      "loss": 0.0008,
      "step": 63430
    },
    {
      "epoch": 13.596228032576082,
      "grad_norm": 0.013168060220777988,
      "learning_rate": 1.8716959565652238e-06,
      "loss": 0.0014,
      "step": 63440
    },
    {
      "epoch": 13.598371195885127,
      "grad_norm": 0.00036313102464191616,
      "learning_rate": 1.8688384054864983e-06,
      "loss": 0.0001,
      "step": 63450
    },
    {
      "epoch": 13.60051435919417,
      "grad_norm": 0.01180196087807417,
      "learning_rate": 1.8659808544077727e-06,
      "loss": 0.1994,
      "step": 63460
    },
    {
      "epoch": 13.602657522503215,
      "grad_norm": 0.0024200021289288998,
      "learning_rate": 1.8631233033290472e-06,
      "loss": 0.1241,
      "step": 63470
    },
    {
      "epoch": 13.60480068581226,
      "grad_norm": 0.11838025599718094,
      "learning_rate": 1.8602657522503218e-06,
      "loss": 0.0005,
      "step": 63480
    },
    {
      "epoch": 13.606943849121302,
      "grad_norm": 0.009302081540226936,
      "learning_rate": 1.857408201171596e-06,
      "loss": 0.0005,
      "step": 63490
    },
    {
      "epoch": 13.609087012430347,
      "grad_norm": 0.00060491036856547,
      "learning_rate": 1.8545506500928705e-06,
      "loss": 0.3188,
      "step": 63500
    },
    {
      "epoch": 13.611230175739392,
      "grad_norm": 0.02399473637342453,
      "learning_rate": 1.8516930990141449e-06,
      "loss": 0.1338,
      "step": 63510
    },
    {
      "epoch": 13.613373339048435,
      "grad_norm": 0.002248259959742427,
      "learning_rate": 1.8488355479354194e-06,
      "loss": 0.0004,
      "step": 63520
    },
    {
      "epoch": 13.61551650235748,
      "grad_norm": 0.004184979945421219,
      "learning_rate": 1.845977996856694e-06,
      "loss": 0.0015,
      "step": 63530
    },
    {
      "epoch": 13.617659665666524,
      "grad_norm": 0.6335241794586182,
      "learning_rate": 1.8431204457779683e-06,
      "loss": 0.0004,
      "step": 63540
    },
    {
      "epoch": 13.619802828975567,
      "grad_norm": 0.0006348012830130756,
      "learning_rate": 1.840262894699243e-06,
      "loss": 0.2086,
      "step": 63550
    },
    {
      "epoch": 13.621945992284612,
      "grad_norm": 0.03482549637556076,
      "learning_rate": 1.8374053436205175e-06,
      "loss": 0.1815,
      "step": 63560
    },
    {
      "epoch": 13.624089155593657,
      "grad_norm": 0.0005347842234186828,
      "learning_rate": 1.8345477925417918e-06,
      "loss": 0.0002,
      "step": 63570
    },
    {
      "epoch": 13.6262323189027,
      "grad_norm": 0.000808294746093452,
      "learning_rate": 1.8316902414630664e-06,
      "loss": 0.0002,
      "step": 63580
    },
    {
      "epoch": 13.628375482211744,
      "grad_norm": 0.002293018391355872,
      "learning_rate": 1.8288326903843407e-06,
      "loss": 0.0015,
      "step": 63590
    },
    {
      "epoch": 13.63051864552079,
      "grad_norm": 0.0014021025272086263,
      "learning_rate": 1.8259751393056153e-06,
      "loss": 0.1121,
      "step": 63600
    },
    {
      "epoch": 13.632661808829832,
      "grad_norm": 0.00563126802444458,
      "learning_rate": 1.8231175882268899e-06,
      "loss": 0.1754,
      "step": 63610
    },
    {
      "epoch": 13.634804972138877,
      "grad_norm": 0.007462428882718086,
      "learning_rate": 1.820260037148164e-06,
      "loss": 0.0003,
      "step": 63620
    },
    {
      "epoch": 13.636948135447922,
      "grad_norm": 0.0040252599865198135,
      "learning_rate": 1.8174024860694386e-06,
      "loss": 0.1768,
      "step": 63630
    },
    {
      "epoch": 13.639091298756965,
      "grad_norm": 0.00028431732789613307,
      "learning_rate": 1.8145449349907131e-06,
      "loss": 0.0001,
      "step": 63640
    },
    {
      "epoch": 13.64123446206601,
      "grad_norm": 0.0024504167959094048,
      "learning_rate": 1.8116873839119875e-06,
      "loss": 0.001,
      "step": 63650
    },
    {
      "epoch": 13.643377625375054,
      "grad_norm": 0.007016246672719717,
      "learning_rate": 1.808829832833262e-06,
      "loss": 0.1895,
      "step": 63660
    },
    {
      "epoch": 13.645520788684097,
      "grad_norm": 0.007562573999166489,
      "learning_rate": 1.8059722817545364e-06,
      "loss": 0.001,
      "step": 63670
    },
    {
      "epoch": 13.647663951993142,
      "grad_norm": 0.0004619551473297179,
      "learning_rate": 1.803114730675811e-06,
      "loss": 0.0002,
      "step": 63680
    },
    {
      "epoch": 13.649807115302186,
      "grad_norm": 0.004191952291876078,
      "learning_rate": 1.8002571795970855e-06,
      "loss": 0.1369,
      "step": 63690
    },
    {
      "epoch": 13.65195027861123,
      "grad_norm": 0.001433994504623115,
      "learning_rate": 1.7973996285183599e-06,
      "loss": 0.1443,
      "step": 63700
    },
    {
      "epoch": 13.654093441920274,
      "grad_norm": 148.99810791015625,
      "learning_rate": 1.7945420774396344e-06,
      "loss": 0.2003,
      "step": 63710
    },
    {
      "epoch": 13.656236605229319,
      "grad_norm": 0.016908349469304085,
      "learning_rate": 1.791684526360909e-06,
      "loss": 0.2604,
      "step": 63720
    },
    {
      "epoch": 13.658379768538362,
      "grad_norm": 0.0008529808255843818,
      "learning_rate": 1.7888269752821831e-06,
      "loss": 0.001,
      "step": 63730
    },
    {
      "epoch": 13.660522931847407,
      "grad_norm": 0.000188047721167095,
      "learning_rate": 1.7859694242034577e-06,
      "loss": 0.0001,
      "step": 63740
    },
    {
      "epoch": 13.662666095156451,
      "grad_norm": 73.4869384765625,
      "learning_rate": 1.7831118731247325e-06,
      "loss": 0.3592,
      "step": 63750
    },
    {
      "epoch": 13.664809258465494,
      "grad_norm": 0.013275422155857086,
      "learning_rate": 1.7802543220460066e-06,
      "loss": 0.0005,
      "step": 63760
    },
    {
      "epoch": 13.666952421774539,
      "grad_norm": 74.54055786132812,
      "learning_rate": 1.7773967709672812e-06,
      "loss": 0.2136,
      "step": 63770
    },
    {
      "epoch": 13.669095585083584,
      "grad_norm": 2271.330810546875,
      "learning_rate": 1.7745392198885555e-06,
      "loss": 0.1392,
      "step": 63780
    },
    {
      "epoch": 13.671238748392627,
      "grad_norm": 0.01952284760773182,
      "learning_rate": 1.77168166880983e-06,
      "loss": 0.0007,
      "step": 63790
    },
    {
      "epoch": 13.673381911701671,
      "grad_norm": 0.015568146482110023,
      "learning_rate": 1.7688241177311047e-06,
      "loss": 0.1406,
      "step": 63800
    },
    {
      "epoch": 13.675525075010716,
      "grad_norm": 0.5496740937232971,
      "learning_rate": 1.765966566652379e-06,
      "loss": 0.1653,
      "step": 63810
    },
    {
      "epoch": 13.67766823831976,
      "grad_norm": 0.02515239827334881,
      "learning_rate": 1.7631090155736536e-06,
      "loss": 0.0002,
      "step": 63820
    },
    {
      "epoch": 13.679811401628804,
      "grad_norm": 0.004474794492125511,
      "learning_rate": 1.7602514644949281e-06,
      "loss": 0.0006,
      "step": 63830
    },
    {
      "epoch": 13.681954564937849,
      "grad_norm": 0.001887484802864492,
      "learning_rate": 1.7573939134162025e-06,
      "loss": 0.0001,
      "step": 63840
    },
    {
      "epoch": 13.684097728246892,
      "grad_norm": 0.002499047201126814,
      "learning_rate": 1.754536362337477e-06,
      "loss": 0.1077,
      "step": 63850
    },
    {
      "epoch": 13.686240891555936,
      "grad_norm": 0.0005866314750164747,
      "learning_rate": 1.7516788112587512e-06,
      "loss": 0.0002,
      "step": 63860
    },
    {
      "epoch": 13.688384054864981,
      "grad_norm": 0.007956522516906261,
      "learning_rate": 1.7488212601800258e-06,
      "loss": 0.393,
      "step": 63870
    },
    {
      "epoch": 13.690527218174024,
      "grad_norm": 0.037847064435482025,
      "learning_rate": 1.7459637091013003e-06,
      "loss": 0.0014,
      "step": 63880
    },
    {
      "epoch": 13.692670381483069,
      "grad_norm": 0.045630861073732376,
      "learning_rate": 1.7431061580225747e-06,
      "loss": 0.1327,
      "step": 63890
    },
    {
      "epoch": 13.694813544792114,
      "grad_norm": 0.007831677794456482,
      "learning_rate": 1.7402486069438492e-06,
      "loss": 0.2633,
      "step": 63900
    },
    {
      "epoch": 13.696956708101157,
      "grad_norm": 0.00014525542792398483,
      "learning_rate": 1.7373910558651238e-06,
      "loss": 0.0006,
      "step": 63910
    },
    {
      "epoch": 13.699099871410201,
      "grad_norm": 0.00015015115786809474,
      "learning_rate": 1.7345335047863982e-06,
      "loss": 0.0001,
      "step": 63920
    },
    {
      "epoch": 13.701243034719246,
      "grad_norm": 0.00665281293913722,
      "learning_rate": 1.7316759537076727e-06,
      "loss": 0.1954,
      "step": 63930
    },
    {
      "epoch": 13.703386198028289,
      "grad_norm": 0.015221980400383472,
      "learning_rate": 1.728818402628947e-06,
      "loss": 0.0011,
      "step": 63940
    },
    {
      "epoch": 13.705529361337334,
      "grad_norm": 0.006699893157929182,
      "learning_rate": 1.7259608515502216e-06,
      "loss": 0.0006,
      "step": 63950
    },
    {
      "epoch": 13.707672524646378,
      "grad_norm": 0.015283603221178055,
      "learning_rate": 1.7231033004714962e-06,
      "loss": 0.0007,
      "step": 63960
    },
    {
      "epoch": 13.709815687955421,
      "grad_norm": 0.001389539334923029,
      "learning_rate": 1.7202457493927703e-06,
      "loss": 0.0001,
      "step": 63970
    },
    {
      "epoch": 13.711958851264466,
      "grad_norm": 80.60236358642578,
      "learning_rate": 1.7173881983140451e-06,
      "loss": 0.5027,
      "step": 63980
    },
    {
      "epoch": 13.71410201457351,
      "grad_norm": 0.05734645202755928,
      "learning_rate": 1.7145306472353197e-06,
      "loss": 0.1351,
      "step": 63990
    },
    {
      "epoch": 13.716245177882554,
      "grad_norm": 0.0016412993427366018,
      "learning_rate": 1.7116730961565938e-06,
      "loss": 0.0016,
      "step": 64000
    },
    {
      "epoch": 13.718388341191599,
      "grad_norm": 0.037096403539180756,
      "learning_rate": 1.7088155450778684e-06,
      "loss": 0.1835,
      "step": 64010
    },
    {
      "epoch": 13.720531504500643,
      "grad_norm": 0.01206906046718359,
      "learning_rate": 1.7059579939991427e-06,
      "loss": 0.1266,
      "step": 64020
    },
    {
      "epoch": 13.722674667809688,
      "grad_norm": 0.0800798237323761,
      "learning_rate": 1.7031004429204173e-06,
      "loss": 0.0005,
      "step": 64030
    },
    {
      "epoch": 13.724817831118731,
      "grad_norm": 0.029076192528009415,
      "learning_rate": 1.7002428918416919e-06,
      "loss": 0.0007,
      "step": 64040
    },
    {
      "epoch": 13.726960994427776,
      "grad_norm": 0.005862888880074024,
      "learning_rate": 1.6973853407629662e-06,
      "loss": 0.0002,
      "step": 64050
    },
    {
      "epoch": 13.72910415773682,
      "grad_norm": 0.011605259962379932,
      "learning_rate": 1.6945277896842408e-06,
      "loss": 0.0008,
      "step": 64060
    },
    {
      "epoch": 13.731247321045863,
      "grad_norm": 0.010218883864581585,
      "learning_rate": 1.6916702386055153e-06,
      "loss": 0.1395,
      "step": 64070
    },
    {
      "epoch": 13.733390484354908,
      "grad_norm": 0.002149044768884778,
      "learning_rate": 1.6888126875267897e-06,
      "loss": 0.0008,
      "step": 64080
    },
    {
      "epoch": 13.735533647663953,
      "grad_norm": 0.9037620425224304,
      "learning_rate": 1.6859551364480643e-06,
      "loss": 0.1343,
      "step": 64090
    },
    {
      "epoch": 13.737676810972996,
      "grad_norm": 0.0005931772175244987,
      "learning_rate": 1.6830975853693384e-06,
      "loss": 0.1292,
      "step": 64100
    },
    {
      "epoch": 13.73981997428204,
      "grad_norm": 0.0007921071955934167,
      "learning_rate": 1.680240034290613e-06,
      "loss": 0.2143,
      "step": 64110
    },
    {
      "epoch": 13.741963137591085,
      "grad_norm": 0.00036685934173874557,
      "learning_rate": 1.6773824832118875e-06,
      "loss": 0.0011,
      "step": 64120
    },
    {
      "epoch": 13.744106300900128,
      "grad_norm": 0.011771115474402905,
      "learning_rate": 1.6745249321331619e-06,
      "loss": 0.0004,
      "step": 64130
    },
    {
      "epoch": 13.746249464209173,
      "grad_norm": 0.005963745526969433,
      "learning_rate": 1.6716673810544364e-06,
      "loss": 0.1564,
      "step": 64140
    },
    {
      "epoch": 13.748392627518218,
      "grad_norm": 0.0018143089255318046,
      "learning_rate": 1.668809829975711e-06,
      "loss": 0.0019,
      "step": 64150
    },
    {
      "epoch": 13.75053579082726,
      "grad_norm": 0.0006460722652263939,
      "learning_rate": 1.6659522788969854e-06,
      "loss": 0.0001,
      "step": 64160
    },
    {
      "epoch": 13.752678954136305,
      "grad_norm": 0.10773461312055588,
      "learning_rate": 1.66309472781826e-06,
      "loss": 0.1721,
      "step": 64170
    },
    {
      "epoch": 13.75482211744535,
      "grad_norm": 23.491043090820312,
      "learning_rate": 1.6602371767395343e-06,
      "loss": 0.2153,
      "step": 64180
    },
    {
      "epoch": 13.756965280754393,
      "grad_norm": 0.0004747401981148869,
      "learning_rate": 1.6573796256608088e-06,
      "loss": 0.1933,
      "step": 64190
    },
    {
      "epoch": 13.759108444063438,
      "grad_norm": 0.0008120639831759036,
      "learning_rate": 1.6545220745820834e-06,
      "loss": 0.001,
      "step": 64200
    },
    {
      "epoch": 13.761251607372483,
      "grad_norm": 0.0023031020537018776,
      "learning_rate": 1.6516645235033577e-06,
      "loss": 0.001,
      "step": 64210
    },
    {
      "epoch": 13.763394770681526,
      "grad_norm": 0.00017315025615971535,
      "learning_rate": 1.6488069724246323e-06,
      "loss": 0.2775,
      "step": 64220
    },
    {
      "epoch": 13.76553793399057,
      "grad_norm": 0.005904152989387512,
      "learning_rate": 1.6459494213459069e-06,
      "loss": 0.0003,
      "step": 64230
    },
    {
      "epoch": 13.767681097299615,
      "grad_norm": 0.004021570552140474,
      "learning_rate": 1.643091870267181e-06,
      "loss": 0.0006,
      "step": 64240
    },
    {
      "epoch": 13.769824260608658,
      "grad_norm": 0.001380423316732049,
      "learning_rate": 1.6402343191884556e-06,
      "loss": 0.0003,
      "step": 64250
    },
    {
      "epoch": 13.771967423917703,
      "grad_norm": 25.23096466064453,
      "learning_rate": 1.6373767681097301e-06,
      "loss": 0.1819,
      "step": 64260
    },
    {
      "epoch": 13.774110587226748,
      "grad_norm": 0.00289064715616405,
      "learning_rate": 1.6345192170310045e-06,
      "loss": 0.0005,
      "step": 64270
    },
    {
      "epoch": 13.77625375053579,
      "grad_norm": 0.027359796687960625,
      "learning_rate": 1.631661665952279e-06,
      "loss": 0.0001,
      "step": 64280
    },
    {
      "epoch": 13.778396913844835,
      "grad_norm": 0.005902532022446394,
      "learning_rate": 1.6288041148735534e-06,
      "loss": 0.0019,
      "step": 64290
    },
    {
      "epoch": 13.78054007715388,
      "grad_norm": 0.46977052092552185,
      "learning_rate": 1.625946563794828e-06,
      "loss": 0.0004,
      "step": 64300
    },
    {
      "epoch": 13.782683240462923,
      "grad_norm": 0.0013274315278977156,
      "learning_rate": 1.6230890127161025e-06,
      "loss": 0.0001,
      "step": 64310
    },
    {
      "epoch": 13.784826403771968,
      "grad_norm": 0.18433333933353424,
      "learning_rate": 1.6202314616373769e-06,
      "loss": 0.0003,
      "step": 64320
    },
    {
      "epoch": 13.786969567081012,
      "grad_norm": 0.011763466522097588,
      "learning_rate": 1.6173739105586515e-06,
      "loss": 0.0009,
      "step": 64330
    },
    {
      "epoch": 13.789112730390055,
      "grad_norm": 0.00021305200061760843,
      "learning_rate": 1.614516359479926e-06,
      "loss": 0.0005,
      "step": 64340
    },
    {
      "epoch": 13.7912558936991,
      "grad_norm": 0.004255648236721754,
      "learning_rate": 1.6116588084012002e-06,
      "loss": 0.0001,
      "step": 64350
    },
    {
      "epoch": 13.793399057008145,
      "grad_norm": 0.0001316073175985366,
      "learning_rate": 1.608801257322475e-06,
      "loss": 0.0007,
      "step": 64360
    },
    {
      "epoch": 13.795542220317188,
      "grad_norm": 0.002663322724401951,
      "learning_rate": 1.605943706243749e-06,
      "loss": 0.1353,
      "step": 64370
    },
    {
      "epoch": 13.797685383626233,
      "grad_norm": 0.005375643260776997,
      "learning_rate": 1.6030861551650236e-06,
      "loss": 0.0004,
      "step": 64380
    },
    {
      "epoch": 13.799828546935277,
      "grad_norm": 0.0002220936439698562,
      "learning_rate": 1.6002286040862982e-06,
      "loss": 0.0003,
      "step": 64390
    },
    {
      "epoch": 13.80197171024432,
      "grad_norm": 0.001009326078929007,
      "learning_rate": 1.5973710530075726e-06,
      "loss": 0.0001,
      "step": 64400
    },
    {
      "epoch": 13.804114873553365,
      "grad_norm": 0.0013875683071091771,
      "learning_rate": 1.5945135019288471e-06,
      "loss": 0.1937,
      "step": 64410
    },
    {
      "epoch": 13.80625803686241,
      "grad_norm": 0.00020071027392987162,
      "learning_rate": 1.5916559508501217e-06,
      "loss": 0.0002,
      "step": 64420
    },
    {
      "epoch": 13.808401200171453,
      "grad_norm": 0.00016975475591607392,
      "learning_rate": 1.588798399771396e-06,
      "loss": 0.0008,
      "step": 64430
    },
    {
      "epoch": 13.810544363480497,
      "grad_norm": 0.04345373064279556,
      "learning_rate": 1.5859408486926706e-06,
      "loss": 0.2084,
      "step": 64440
    },
    {
      "epoch": 13.812687526789542,
      "grad_norm": 0.05320374667644501,
      "learning_rate": 1.583083297613945e-06,
      "loss": 0.156,
      "step": 64450
    },
    {
      "epoch": 13.814830690098585,
      "grad_norm": 19.130258560180664,
      "learning_rate": 1.5802257465352195e-06,
      "loss": 0.3067,
      "step": 64460
    },
    {
      "epoch": 13.81697385340763,
      "grad_norm": 0.0020341542549431324,
      "learning_rate": 1.577368195456494e-06,
      "loss": 0.1164,
      "step": 64470
    },
    {
      "epoch": 13.819117016716675,
      "grad_norm": 0.003252309048548341,
      "learning_rate": 1.5745106443777682e-06,
      "loss": 0.0005,
      "step": 64480
    },
    {
      "epoch": 13.821260180025718,
      "grad_norm": 23.23375701904297,
      "learning_rate": 1.5716530932990428e-06,
      "loss": 0.2395,
      "step": 64490
    },
    {
      "epoch": 13.823403343334762,
      "grad_norm": 0.09036976844072342,
      "learning_rate": 1.5687955422203173e-06,
      "loss": 0.0008,
      "step": 64500
    },
    {
      "epoch": 13.825546506643807,
      "grad_norm": 0.00013245396257843822,
      "learning_rate": 1.5659379911415917e-06,
      "loss": 0.0008,
      "step": 64510
    },
    {
      "epoch": 13.82768966995285,
      "grad_norm": 0.20900028944015503,
      "learning_rate": 1.5630804400628663e-06,
      "loss": 0.001,
      "step": 64520
    },
    {
      "epoch": 13.829832833261895,
      "grad_norm": 9.374890942126513e-05,
      "learning_rate": 1.5602228889841406e-06,
      "loss": 0.001,
      "step": 64530
    },
    {
      "epoch": 13.83197599657094,
      "grad_norm": 0.2717302441596985,
      "learning_rate": 1.5573653379054152e-06,
      "loss": 0.0009,
      "step": 64540
    },
    {
      "epoch": 13.834119159879982,
      "grad_norm": 0.00043590590939857066,
      "learning_rate": 1.5545077868266897e-06,
      "loss": 0.0011,
      "step": 64550
    },
    {
      "epoch": 13.836262323189027,
      "grad_norm": 0.0049719661474227905,
      "learning_rate": 1.551650235747964e-06,
      "loss": 0.1727,
      "step": 64560
    },
    {
      "epoch": 13.838405486498072,
      "grad_norm": 0.0002703515056055039,
      "learning_rate": 1.5487926846692386e-06,
      "loss": 0.0,
      "step": 64570
    },
    {
      "epoch": 13.840548649807115,
      "grad_norm": 0.00017994824156630784,
      "learning_rate": 1.5459351335905132e-06,
      "loss": 0.1612,
      "step": 64580
    },
    {
      "epoch": 13.84269181311616,
      "grad_norm": 0.002173148561269045,
      "learning_rate": 1.5430775825117876e-06,
      "loss": 0.0002,
      "step": 64590
    },
    {
      "epoch": 13.844834976425204,
      "grad_norm": 0.0002138254785677418,
      "learning_rate": 1.5402200314330621e-06,
      "loss": 0.0005,
      "step": 64600
    },
    {
      "epoch": 13.846978139734247,
      "grad_norm": 0.055093761533498764,
      "learning_rate": 1.5373624803543363e-06,
      "loss": 0.0006,
      "step": 64610
    },
    {
      "epoch": 13.849121303043292,
      "grad_norm": 6.752122135367244e-05,
      "learning_rate": 1.5345049292756108e-06,
      "loss": 0.2561,
      "step": 64620
    },
    {
      "epoch": 13.851264466352337,
      "grad_norm": 0.0002783758973237127,
      "learning_rate": 1.5316473781968854e-06,
      "loss": 0.1372,
      "step": 64630
    },
    {
      "epoch": 13.85340762966138,
      "grad_norm": 0.009638403542339802,
      "learning_rate": 1.5287898271181597e-06,
      "loss": 0.3699,
      "step": 64640
    },
    {
      "epoch": 13.855550792970424,
      "grad_norm": 0.0019913793075829744,
      "learning_rate": 1.5259322760394343e-06,
      "loss": 0.0005,
      "step": 64650
    },
    {
      "epoch": 13.85769395627947,
      "grad_norm": 0.01799885369837284,
      "learning_rate": 1.5230747249607089e-06,
      "loss": 0.0006,
      "step": 64660
    },
    {
      "epoch": 13.859837119588512,
      "grad_norm": 0.13828465342521667,
      "learning_rate": 1.5202171738819832e-06,
      "loss": 0.0007,
      "step": 64670
    },
    {
      "epoch": 13.861980282897557,
      "grad_norm": 0.07323049008846283,
      "learning_rate": 1.5173596228032578e-06,
      "loss": 0.0011,
      "step": 64680
    },
    {
      "epoch": 13.864123446206602,
      "grad_norm": 0.0008303269860334694,
      "learning_rate": 1.5145020717245321e-06,
      "loss": 0.0006,
      "step": 64690
    },
    {
      "epoch": 13.866266609515645,
      "grad_norm": 0.26249921321868896,
      "learning_rate": 1.5116445206458067e-06,
      "loss": 0.1809,
      "step": 64700
    },
    {
      "epoch": 13.86840977282469,
      "grad_norm": 0.0652775838971138,
      "learning_rate": 1.5087869695670813e-06,
      "loss": 0.0007,
      "step": 64710
    },
    {
      "epoch": 13.870552936133734,
      "grad_norm": 0.2990361452102661,
      "learning_rate": 1.5059294184883554e-06,
      "loss": 0.0018,
      "step": 64720
    },
    {
      "epoch": 13.872696099442777,
      "grad_norm": 0.0610828697681427,
      "learning_rate": 1.50307186740963e-06,
      "loss": 0.0009,
      "step": 64730
    },
    {
      "epoch": 13.874839262751822,
      "grad_norm": 0.004399457480758429,
      "learning_rate": 1.5002143163309045e-06,
      "loss": 0.0006,
      "step": 64740
    },
    {
      "epoch": 13.876982426060867,
      "grad_norm": 0.004685560241341591,
      "learning_rate": 1.4973567652521789e-06,
      "loss": 0.0005,
      "step": 64750
    },
    {
      "epoch": 13.87912558936991,
      "grad_norm": 0.12833423912525177,
      "learning_rate": 1.4944992141734535e-06,
      "loss": 0.1614,
      "step": 64760
    },
    {
      "epoch": 13.881268752678954,
      "grad_norm": 0.0030725421383976936,
      "learning_rate": 1.491641663094728e-06,
      "loss": 0.0007,
      "step": 64770
    },
    {
      "epoch": 13.883411915987999,
      "grad_norm": 0.0009258189820684493,
      "learning_rate": 1.4887841120160024e-06,
      "loss": 0.1588,
      "step": 64780
    },
    {
      "epoch": 13.885555079297042,
      "grad_norm": 0.0002199180016759783,
      "learning_rate": 1.485926560937277e-06,
      "loss": 0.1531,
      "step": 64790
    },
    {
      "epoch": 13.887698242606087,
      "grad_norm": 0.17867769300937653,
      "learning_rate": 1.4830690098585513e-06,
      "loss": 0.0002,
      "step": 64800
    },
    {
      "epoch": 13.889841405915131,
      "grad_norm": 0.0008756304159760475,
      "learning_rate": 1.4802114587798258e-06,
      "loss": 0.1345,
      "step": 64810
    },
    {
      "epoch": 13.891984569224174,
      "grad_norm": 0.0017153299413621426,
      "learning_rate": 1.4773539077011004e-06,
      "loss": 0.1637,
      "step": 64820
    },
    {
      "epoch": 13.894127732533219,
      "grad_norm": 0.011148260906338692,
      "learning_rate": 1.4744963566223748e-06,
      "loss": 0.2105,
      "step": 64830
    },
    {
      "epoch": 13.896270895842264,
      "grad_norm": 0.0061452072113752365,
      "learning_rate": 1.4716388055436493e-06,
      "loss": 0.1888,
      "step": 64840
    },
    {
      "epoch": 13.898414059151307,
      "grad_norm": 0.06737730652093887,
      "learning_rate": 1.4687812544649239e-06,
      "loss": 0.4842,
      "step": 64850
    },
    {
      "epoch": 13.900557222460352,
      "grad_norm": 0.024223465472459793,
      "learning_rate": 1.465923703386198e-06,
      "loss": 0.0008,
      "step": 64860
    },
    {
      "epoch": 13.902700385769396,
      "grad_norm": 0.0008269338286481798,
      "learning_rate": 1.4630661523074726e-06,
      "loss": 0.0001,
      "step": 64870
    },
    {
      "epoch": 13.90484354907844,
      "grad_norm": 0.0015842870343476534,
      "learning_rate": 1.460208601228747e-06,
      "loss": 0.1413,
      "step": 64880
    },
    {
      "epoch": 13.906986712387484,
      "grad_norm": 0.0018541686004027724,
      "learning_rate": 1.4573510501500215e-06,
      "loss": 0.0002,
      "step": 64890
    },
    {
      "epoch": 13.909129875696529,
      "grad_norm": 0.0031795762479305267,
      "learning_rate": 1.454493499071296e-06,
      "loss": 0.0002,
      "step": 64900
    },
    {
      "epoch": 13.911273039005572,
      "grad_norm": 0.05842868611216545,
      "learning_rate": 1.4516359479925704e-06,
      "loss": 0.0007,
      "step": 64910
    },
    {
      "epoch": 13.913416202314616,
      "grad_norm": 0.20827902853488922,
      "learning_rate": 1.448778396913845e-06,
      "loss": 0.0007,
      "step": 64920
    },
    {
      "epoch": 13.915559365623661,
      "grad_norm": 0.00035402202047407627,
      "learning_rate": 1.4459208458351195e-06,
      "loss": 0.0,
      "step": 64930
    },
    {
      "epoch": 13.917702528932704,
      "grad_norm": 0.0017993960063904524,
      "learning_rate": 1.443063294756394e-06,
      "loss": 0.0002,
      "step": 64940
    },
    {
      "epoch": 13.919845692241749,
      "grad_norm": 0.008536815643310547,
      "learning_rate": 1.4402057436776685e-06,
      "loss": 0.1019,
      "step": 64950
    },
    {
      "epoch": 13.921988855550794,
      "grad_norm": 0.0016859992174431682,
      "learning_rate": 1.4373481925989426e-06,
      "loss": 0.0001,
      "step": 64960
    },
    {
      "epoch": 13.924132018859837,
      "grad_norm": 0.005654043983668089,
      "learning_rate": 1.4344906415202174e-06,
      "loss": 0.0001,
      "step": 64970
    },
    {
      "epoch": 13.926275182168881,
      "grad_norm": 0.00019504528609104455,
      "learning_rate": 1.431633090441492e-06,
      "loss": 0.1628,
      "step": 64980
    },
    {
      "epoch": 13.928418345477926,
      "grad_norm": 0.005681236274540424,
      "learning_rate": 1.428775539362766e-06,
      "loss": 0.0001,
      "step": 64990
    },
    {
      "epoch": 13.930561508786969,
      "grad_norm": 0.004790244158357382,
      "learning_rate": 1.4259179882840406e-06,
      "loss": 0.0003,
      "step": 65000
    },
    {
      "epoch": 13.932704672096014,
      "grad_norm": 0.0007566004060208797,
      "learning_rate": 1.4230604372053152e-06,
      "loss": 0.0002,
      "step": 65010
    },
    {
      "epoch": 13.934847835405058,
      "grad_norm": 0.04277953505516052,
      "learning_rate": 1.4202028861265896e-06,
      "loss": 0.2768,
      "step": 65020
    },
    {
      "epoch": 13.936990998714101,
      "grad_norm": 0.005411907099187374,
      "learning_rate": 1.4173453350478641e-06,
      "loss": 0.0001,
      "step": 65030
    },
    {
      "epoch": 13.939134162023146,
      "grad_norm": 0.0069899349473416805,
      "learning_rate": 1.4144877839691385e-06,
      "loss": 0.0,
      "step": 65040
    },
    {
      "epoch": 13.94127732533219,
      "grad_norm": 0.0019396492280066013,
      "learning_rate": 1.411630232890413e-06,
      "loss": 0.2261,
      "step": 65050
    },
    {
      "epoch": 13.943420488641234,
      "grad_norm": 0.01068683061748743,
      "learning_rate": 1.4087726818116876e-06,
      "loss": 0.2659,
      "step": 65060
    },
    {
      "epoch": 13.945563651950279,
      "grad_norm": 0.0036404719576239586,
      "learning_rate": 1.405915130732962e-06,
      "loss": 0.1885,
      "step": 65070
    },
    {
      "epoch": 13.947706815259323,
      "grad_norm": 0.0001697828120086342,
      "learning_rate": 1.4030575796542365e-06,
      "loss": 0.1091,
      "step": 65080
    },
    {
      "epoch": 13.949849978568366,
      "grad_norm": 0.002646160777658224,
      "learning_rate": 1.400200028575511e-06,
      "loss": 0.0003,
      "step": 65090
    },
    {
      "epoch": 13.951993141877411,
      "grad_norm": 0.015382868237793446,
      "learning_rate": 1.3973424774967852e-06,
      "loss": 0.0002,
      "step": 65100
    },
    {
      "epoch": 13.954136305186456,
      "grad_norm": 26.680381774902344,
      "learning_rate": 1.3944849264180598e-06,
      "loss": 0.1167,
      "step": 65110
    },
    {
      "epoch": 13.956279468495499,
      "grad_norm": 0.010152358561754227,
      "learning_rate": 1.3916273753393341e-06,
      "loss": 0.2728,
      "step": 65120
    },
    {
      "epoch": 13.958422631804543,
      "grad_norm": 0.04873855412006378,
      "learning_rate": 1.3887698242606087e-06,
      "loss": 0.0008,
      "step": 65130
    },
    {
      "epoch": 13.960565795113588,
      "grad_norm": 0.028446046635508537,
      "learning_rate": 1.3859122731818833e-06,
      "loss": 0.0006,
      "step": 65140
    },
    {
      "epoch": 13.962708958422631,
      "grad_norm": 0.018593167886137962,
      "learning_rate": 1.3830547221031576e-06,
      "loss": 0.0007,
      "step": 65150
    },
    {
      "epoch": 13.964852121731676,
      "grad_norm": 0.006523401476442814,
      "learning_rate": 1.3801971710244322e-06,
      "loss": 0.0,
      "step": 65160
    },
    {
      "epoch": 13.96699528504072,
      "grad_norm": 0.0005418963264673948,
      "learning_rate": 1.3773396199457067e-06,
      "loss": 0.0001,
      "step": 65170
    },
    {
      "epoch": 13.969138448349764,
      "grad_norm": 0.01004796288907528,
      "learning_rate": 1.374482068866981e-06,
      "loss": 0.0005,
      "step": 65180
    },
    {
      "epoch": 13.971281611658808,
      "grad_norm": 0.0003073647676501423,
      "learning_rate": 1.3716245177882557e-06,
      "loss": 0.001,
      "step": 65190
    },
    {
      "epoch": 13.973424774967853,
      "grad_norm": 0.22396039962768555,
      "learning_rate": 1.36876696670953e-06,
      "loss": 0.0016,
      "step": 65200
    },
    {
      "epoch": 13.975567938276896,
      "grad_norm": 0.0007328017381951213,
      "learning_rate": 1.3659094156308046e-06,
      "loss": 0.0006,
      "step": 65210
    },
    {
      "epoch": 13.97771110158594,
      "grad_norm": 0.07194629311561584,
      "learning_rate": 1.3630518645520791e-06,
      "loss": 0.0002,
      "step": 65220
    },
    {
      "epoch": 13.979854264894986,
      "grad_norm": 0.0001293683162657544,
      "learning_rate": 1.3601943134733533e-06,
      "loss": 0.2474,
      "step": 65230
    },
    {
      "epoch": 13.981997428204028,
      "grad_norm": 0.0006667762063443661,
      "learning_rate": 1.3573367623946278e-06,
      "loss": 0.0001,
      "step": 65240
    },
    {
      "epoch": 13.984140591513073,
      "grad_norm": 0.003315955400466919,
      "learning_rate": 1.3544792113159024e-06,
      "loss": 0.0001,
      "step": 65250
    },
    {
      "epoch": 13.986283754822118,
      "grad_norm": 0.010768752545118332,
      "learning_rate": 1.3516216602371768e-06,
      "loss": 0.0005,
      "step": 65260
    },
    {
      "epoch": 13.988426918131161,
      "grad_norm": 0.010114396922290325,
      "learning_rate": 1.3487641091584513e-06,
      "loss": 0.1535,
      "step": 65270
    },
    {
      "epoch": 13.990570081440206,
      "grad_norm": 0.008784941397607327,
      "learning_rate": 1.3459065580797259e-06,
      "loss": 0.0409,
      "step": 65280
    },
    {
      "epoch": 13.99271324474925,
      "grad_norm": 0.001690716133452952,
      "learning_rate": 1.3430490070010002e-06,
      "loss": 0.167,
      "step": 65290
    },
    {
      "epoch": 13.994856408058293,
      "grad_norm": 0.005445445887744427,
      "learning_rate": 1.3401914559222748e-06,
      "loss": 0.1579,
      "step": 65300
    },
    {
      "epoch": 13.996999571367338,
      "grad_norm": 0.030048027634620667,
      "learning_rate": 1.3373339048435492e-06,
      "loss": 0.3073,
      "step": 65310
    },
    {
      "epoch": 13.999142734676383,
      "grad_norm": 0.001946203992702067,
      "learning_rate": 1.3344763537648237e-06,
      "loss": 0.1052,
      "step": 65320
    },
    {
      "epoch": 14.0,
      "eval_accuracy": 0.966,
      "eval_f1": 0.8654353562005277,
      "eval_loss": 0.21383188664913177,
      "eval_precision": 0.8654353562005277,
      "eval_recall": 0.8654353562005277,
      "eval_runtime": 430.2372,
      "eval_samples_per_second": 6.973,
      "eval_steps_per_second": 2.324,
      "step": 65324
    },
    {
      "epoch": 14.001285897985426,
      "grad_norm": 0.0034988338593393564,
      "learning_rate": 1.3316188026860983e-06,
      "loss": 0.1952,
      "step": 65330
    },
    {
      "epoch": 14.00342906129447,
      "grad_norm": 0.007873809896409512,
      "learning_rate": 1.3287612516073724e-06,
      "loss": 0.0001,
      "step": 65340
    },
    {
      "epoch": 14.005572224603515,
      "grad_norm": 0.0006554364808835089,
      "learning_rate": 1.325903700528647e-06,
      "loss": 0.0005,
      "step": 65350
    },
    {
      "epoch": 14.007715387912558,
      "grad_norm": 0.011863082647323608,
      "learning_rate": 1.3230461494499218e-06,
      "loss": 0.0002,
      "step": 65360
    },
    {
      "epoch": 14.009858551221603,
      "grad_norm": 0.00022713435464538634,
      "learning_rate": 1.320188598371196e-06,
      "loss": 0.1914,
      "step": 65370
    },
    {
      "epoch": 14.012001714530648,
      "grad_norm": 0.13495640456676483,
      "learning_rate": 1.3173310472924705e-06,
      "loss": 0.0003,
      "step": 65380
    },
    {
      "epoch": 14.01414487783969,
      "grad_norm": 0.2113264501094818,
      "learning_rate": 1.3144734962137448e-06,
      "loss": 0.0006,
      "step": 65390
    },
    {
      "epoch": 14.016288041148735,
      "grad_norm": 9.555099677527323e-05,
      "learning_rate": 1.3116159451350194e-06,
      "loss": 0.2693,
      "step": 65400
    },
    {
      "epoch": 14.01843120445778,
      "grad_norm": 0.10069183260202408,
      "learning_rate": 1.308758394056294e-06,
      "loss": 0.0002,
      "step": 65410
    },
    {
      "epoch": 14.020574367766823,
      "grad_norm": 0.0392213799059391,
      "learning_rate": 1.3059008429775683e-06,
      "loss": 0.0007,
      "step": 65420
    },
    {
      "epoch": 14.022717531075868,
      "grad_norm": 0.03226945176720619,
      "learning_rate": 1.3030432918988429e-06,
      "loss": 0.0002,
      "step": 65430
    },
    {
      "epoch": 14.024860694384913,
      "grad_norm": 0.008185752667486668,
      "learning_rate": 1.3001857408201174e-06,
      "loss": 0.0006,
      "step": 65440
    },
    {
      "epoch": 14.027003857693956,
      "grad_norm": 0.1559993475675583,
      "learning_rate": 1.2973281897413918e-06,
      "loss": 0.0015,
      "step": 65450
    },
    {
      "epoch": 14.029147021003,
      "grad_norm": 0.008194881491363049,
      "learning_rate": 1.2944706386626663e-06,
      "loss": 0.1589,
      "step": 65460
    },
    {
      "epoch": 14.031290184312045,
      "grad_norm": 0.0066675953567028046,
      "learning_rate": 1.2916130875839405e-06,
      "loss": 0.0001,
      "step": 65470
    },
    {
      "epoch": 14.033433347621088,
      "grad_norm": 0.0005573716480284929,
      "learning_rate": 1.288755536505215e-06,
      "loss": 0.1621,
      "step": 65480
    },
    {
      "epoch": 14.035576510930133,
      "grad_norm": 0.00015563686611130834,
      "learning_rate": 1.2858979854264896e-06,
      "loss": 0.0008,
      "step": 65490
    },
    {
      "epoch": 14.037719674239177,
      "grad_norm": 0.0001362878829240799,
      "learning_rate": 1.283040434347764e-06,
      "loss": 0.0001,
      "step": 65500
    },
    {
      "epoch": 14.03986283754822,
      "grad_norm": 0.07833662629127502,
      "learning_rate": 1.2801828832690385e-06,
      "loss": 0.0002,
      "step": 65510
    },
    {
      "epoch": 14.042006000857265,
      "grad_norm": 0.02068616822361946,
      "learning_rate": 1.277325332190313e-06,
      "loss": 0.0009,
      "step": 65520
    },
    {
      "epoch": 14.04414916416631,
      "grad_norm": 0.001660879235714674,
      "learning_rate": 1.2744677811115874e-06,
      "loss": 0.0001,
      "step": 65530
    },
    {
      "epoch": 14.046292327475353,
      "grad_norm": 0.061654817312955856,
      "learning_rate": 1.271610230032862e-06,
      "loss": 0.0007,
      "step": 65540
    },
    {
      "epoch": 14.048435490784398,
      "grad_norm": 0.00024377960653509945,
      "learning_rate": 1.2687526789541363e-06,
      "loss": 0.0007,
      "step": 65550
    },
    {
      "epoch": 14.050578654093442,
      "grad_norm": 0.0011890889145433903,
      "learning_rate": 1.265895127875411e-06,
      "loss": 0.0007,
      "step": 65560
    },
    {
      "epoch": 14.052721817402485,
      "grad_norm": 0.0020046564750373363,
      "learning_rate": 1.2630375767966855e-06,
      "loss": 0.0008,
      "step": 65570
    },
    {
      "epoch": 14.05486498071153,
      "grad_norm": 0.00038847015821374953,
      "learning_rate": 1.2601800257179596e-06,
      "loss": 0.0,
      "step": 65580
    },
    {
      "epoch": 14.057008144020575,
      "grad_norm": 7.970427395775914e-05,
      "learning_rate": 1.2573224746392344e-06,
      "loss": 0.0002,
      "step": 65590
    },
    {
      "epoch": 14.059151307329618,
      "grad_norm": 0.004143108148127794,
      "learning_rate": 1.254464923560509e-06,
      "loss": 0.0,
      "step": 65600
    },
    {
      "epoch": 14.061294470638662,
      "grad_norm": 0.0008032493642531335,
      "learning_rate": 1.251607372481783e-06,
      "loss": 0.0,
      "step": 65610
    },
    {
      "epoch": 14.063437633947707,
      "grad_norm": 0.007929244078695774,
      "learning_rate": 1.2487498214030577e-06,
      "loss": 0.0005,
      "step": 65620
    },
    {
      "epoch": 14.06558079725675,
      "grad_norm": 0.0017526085721328855,
      "learning_rate": 1.2458922703243322e-06,
      "loss": 0.0002,
      "step": 65630
    },
    {
      "epoch": 14.067723960565795,
      "grad_norm": 0.000486943987198174,
      "learning_rate": 1.2430347192456066e-06,
      "loss": 0.1744,
      "step": 65640
    },
    {
      "epoch": 14.06986712387484,
      "grad_norm": 0.0008959858678281307,
      "learning_rate": 1.240177168166881e-06,
      "loss": 0.0013,
      "step": 65650
    },
    {
      "epoch": 14.072010287183883,
      "grad_norm": 0.0007176675135269761,
      "learning_rate": 1.2373196170881557e-06,
      "loss": 0.0005,
      "step": 65660
    },
    {
      "epoch": 14.074153450492927,
      "grad_norm": 0.00414994964376092,
      "learning_rate": 1.23446206600943e-06,
      "loss": 0.0002,
      "step": 65670
    },
    {
      "epoch": 14.076296613801972,
      "grad_norm": 0.03910449147224426,
      "learning_rate": 1.2316045149307044e-06,
      "loss": 0.0096,
      "step": 65680
    },
    {
      "epoch": 14.078439777111015,
      "grad_norm": 0.0015712410677224398,
      "learning_rate": 1.228746963851979e-06,
      "loss": 0.0,
      "step": 65690
    },
    {
      "epoch": 14.08058294042006,
      "grad_norm": 0.009322934783995152,
      "learning_rate": 1.2258894127732535e-06,
      "loss": 0.0001,
      "step": 65700
    },
    {
      "epoch": 14.082726103729104,
      "grad_norm": 0.06825213879346848,
      "learning_rate": 1.2230318616945279e-06,
      "loss": 0.0002,
      "step": 65710
    },
    {
      "epoch": 14.084869267038147,
      "grad_norm": 0.006357827689498663,
      "learning_rate": 1.2201743106158022e-06,
      "loss": 0.0005,
      "step": 65720
    },
    {
      "epoch": 14.087012430347192,
      "grad_norm": 0.008339283987879753,
      "learning_rate": 1.2173167595370768e-06,
      "loss": 0.0001,
      "step": 65730
    },
    {
      "epoch": 14.089155593656237,
      "grad_norm": 18.214529037475586,
      "learning_rate": 1.2144592084583514e-06,
      "loss": 0.1516,
      "step": 65740
    },
    {
      "epoch": 14.09129875696528,
      "grad_norm": 0.002900468185544014,
      "learning_rate": 1.2116016573796257e-06,
      "loss": 0.1862,
      "step": 65750
    },
    {
      "epoch": 14.093441920274325,
      "grad_norm": 0.006842226255685091,
      "learning_rate": 1.2087441063009003e-06,
      "loss": 0.0001,
      "step": 65760
    },
    {
      "epoch": 14.09558508358337,
      "grad_norm": 0.00021855950762983412,
      "learning_rate": 1.2058865552221746e-06,
      "loss": 0.0003,
      "step": 65770
    },
    {
      "epoch": 14.097728246892412,
      "grad_norm": 0.00520005589351058,
      "learning_rate": 1.2030290041434492e-06,
      "loss": 0.0001,
      "step": 65780
    },
    {
      "epoch": 14.099871410201457,
      "grad_norm": 37.88663101196289,
      "learning_rate": 1.2001714530647235e-06,
      "loss": 0.5878,
      "step": 65790
    },
    {
      "epoch": 14.102014573510502,
      "grad_norm": 9.752180631039664e-05,
      "learning_rate": 1.1973139019859981e-06,
      "loss": 0.0001,
      "step": 65800
    },
    {
      "epoch": 14.104157736819545,
      "grad_norm": 0.0012663633096963167,
      "learning_rate": 1.1944563509072727e-06,
      "loss": 0.0005,
      "step": 65810
    },
    {
      "epoch": 14.10630090012859,
      "grad_norm": 0.0013560638763010502,
      "learning_rate": 1.191598799828547e-06,
      "loss": 0.0,
      "step": 65820
    },
    {
      "epoch": 14.108444063437634,
      "grad_norm": 0.00037395660183392465,
      "learning_rate": 1.1887412487498216e-06,
      "loss": 0.0,
      "step": 65830
    },
    {
      "epoch": 14.110587226746677,
      "grad_norm": 0.00038891242002137005,
      "learning_rate": 1.185883697671096e-06,
      "loss": 0.0011,
      "step": 65840
    },
    {
      "epoch": 14.112730390055722,
      "grad_norm": 0.02466144599020481,
      "learning_rate": 1.1830261465923705e-06,
      "loss": 0.2564,
      "step": 65850
    },
    {
      "epoch": 14.114873553364767,
      "grad_norm": 7.791732059558854e-05,
      "learning_rate": 1.1801685955136449e-06,
      "loss": 0.3154,
      "step": 65860
    },
    {
      "epoch": 14.117016716673811,
      "grad_norm": 0.00024042246513999999,
      "learning_rate": 1.1773110444349194e-06,
      "loss": 0.0005,
      "step": 65870
    },
    {
      "epoch": 14.119159879982854,
      "grad_norm": 0.005342146381735802,
      "learning_rate": 1.1744534933561938e-06,
      "loss": 0.1468,
      "step": 65880
    },
    {
      "epoch": 14.1213030432919,
      "grad_norm": 0.00030154455453157425,
      "learning_rate": 1.1715959422774683e-06,
      "loss": 0.336,
      "step": 65890
    },
    {
      "epoch": 14.123446206600944,
      "grad_norm": 0.0012541995383799076,
      "learning_rate": 1.168738391198743e-06,
      "loss": 0.0008,
      "step": 65900
    },
    {
      "epoch": 14.125589369909987,
      "grad_norm": 0.0007966976845636964,
      "learning_rate": 1.1658808401200172e-06,
      "loss": 0.1373,
      "step": 65910
    },
    {
      "epoch": 14.127732533219032,
      "grad_norm": 0.001985329668968916,
      "learning_rate": 1.1630232890412916e-06,
      "loss": 0.1381,
      "step": 65920
    },
    {
      "epoch": 14.129875696528076,
      "grad_norm": 0.006064079236239195,
      "learning_rate": 1.1601657379625662e-06,
      "loss": 0.1175,
      "step": 65930
    },
    {
      "epoch": 14.13201885983712,
      "grad_norm": 0.00016796514682937413,
      "learning_rate": 1.1573081868838407e-06,
      "loss": 0.0004,
      "step": 65940
    },
    {
      "epoch": 14.134162023146164,
      "grad_norm": 0.00098987587261945,
      "learning_rate": 1.154450635805115e-06,
      "loss": 0.1103,
      "step": 65950
    },
    {
      "epoch": 14.136305186455209,
      "grad_norm": 0.005123344715684652,
      "learning_rate": 1.1515930847263894e-06,
      "loss": 0.1378,
      "step": 65960
    },
    {
      "epoch": 14.138448349764252,
      "grad_norm": 0.0062327515333890915,
      "learning_rate": 1.1487355336476642e-06,
      "loss": 0.1582,
      "step": 65970
    },
    {
      "epoch": 14.140591513073296,
      "grad_norm": 0.002311548450961709,
      "learning_rate": 1.1458779825689386e-06,
      "loss": 0.0005,
      "step": 65980
    },
    {
      "epoch": 14.142734676382341,
      "grad_norm": 0.00087499845540151,
      "learning_rate": 1.143020431490213e-06,
      "loss": 0.0002,
      "step": 65990
    },
    {
      "epoch": 14.144877839691384,
      "grad_norm": 10.249297142028809,
      "learning_rate": 1.1401628804114875e-06,
      "loss": 0.0025,
      "step": 66000
    },
    {
      "epoch": 14.147021003000429,
      "grad_norm": 0.005609780550003052,
      "learning_rate": 1.137305329332762e-06,
      "loss": 0.0002,
      "step": 66010
    },
    {
      "epoch": 14.149164166309474,
      "grad_norm": 0.0001684058952378109,
      "learning_rate": 1.1344477782540364e-06,
      "loss": 0.2303,
      "step": 66020
    },
    {
      "epoch": 14.151307329618517,
      "grad_norm": 0.013047297485172749,
      "learning_rate": 1.1315902271753107e-06,
      "loss": 0.0003,
      "step": 66030
    },
    {
      "epoch": 14.153450492927561,
      "grad_norm": 0.09595530480146408,
      "learning_rate": 1.1287326760965853e-06,
      "loss": 0.0002,
      "step": 66040
    },
    {
      "epoch": 14.155593656236606,
      "grad_norm": 7.616637594765052e-05,
      "learning_rate": 1.1258751250178599e-06,
      "loss": 0.0008,
      "step": 66050
    },
    {
      "epoch": 14.157736819545649,
      "grad_norm": 0.031863003969192505,
      "learning_rate": 1.1230175739391342e-06,
      "loss": 0.0002,
      "step": 66060
    },
    {
      "epoch": 14.159879982854694,
      "grad_norm": 5.6903576478362083e-05,
      "learning_rate": 1.1201600228604088e-06,
      "loss": 0.0003,
      "step": 66070
    },
    {
      "epoch": 14.162023146163738,
      "grad_norm": 0.0010415655560791492,
      "learning_rate": 1.1173024717816831e-06,
      "loss": 0.2023,
      "step": 66080
    },
    {
      "epoch": 14.164166309472781,
      "grad_norm": 0.0003278927179053426,
      "learning_rate": 1.1144449207029577e-06,
      "loss": 0.0001,
      "step": 66090
    },
    {
      "epoch": 14.166309472781826,
      "grad_norm": 8.118827099679038e-05,
      "learning_rate": 1.111587369624232e-06,
      "loss": 0.1516,
      "step": 66100
    },
    {
      "epoch": 14.168452636090871,
      "grad_norm": 0.0008866494172252715,
      "learning_rate": 1.1087298185455066e-06,
      "loss": 0.1296,
      "step": 66110
    },
    {
      "epoch": 14.170595799399914,
      "grad_norm": 0.0009400341659784317,
      "learning_rate": 1.105872267466781e-06,
      "loss": 0.1781,
      "step": 66120
    },
    {
      "epoch": 14.172738962708959,
      "grad_norm": 0.10824645310640335,
      "learning_rate": 1.1030147163880555e-06,
      "loss": 0.0774,
      "step": 66130
    },
    {
      "epoch": 14.174882126018003,
      "grad_norm": 0.04958009719848633,
      "learning_rate": 1.10015716530933e-06,
      "loss": 0.0009,
      "step": 66140
    },
    {
      "epoch": 14.177025289327046,
      "grad_norm": 0.014789453707635403,
      "learning_rate": 1.0972996142306044e-06,
      "loss": 0.0001,
      "step": 66150
    },
    {
      "epoch": 14.179168452636091,
      "grad_norm": 0.0010214158101007342,
      "learning_rate": 1.0944420631518788e-06,
      "loss": 0.0005,
      "step": 66160
    },
    {
      "epoch": 14.181311615945136,
      "grad_norm": 0.3261543810367584,
      "learning_rate": 1.0915845120731534e-06,
      "loss": 0.0019,
      "step": 66170
    },
    {
      "epoch": 14.183454779254179,
      "grad_norm": 0.33620163798332214,
      "learning_rate": 1.088726960994428e-06,
      "loss": 0.1275,
      "step": 66180
    },
    {
      "epoch": 14.185597942563223,
      "grad_norm": 0.010185335762798786,
      "learning_rate": 1.0858694099157023e-06,
      "loss": 0.0002,
      "step": 66190
    },
    {
      "epoch": 14.187741105872268,
      "grad_norm": 1.6421723365783691,
      "learning_rate": 1.0830118588369768e-06,
      "loss": 0.0006,
      "step": 66200
    },
    {
      "epoch": 14.189884269181311,
      "grad_norm": 0.00010091873991768807,
      "learning_rate": 1.0801543077582514e-06,
      "loss": 0.0001,
      "step": 66210
    },
    {
      "epoch": 14.192027432490356,
      "grad_norm": 0.00010565578850219026,
      "learning_rate": 1.0772967566795258e-06,
      "loss": 0.3514,
      "step": 66220
    },
    {
      "epoch": 14.1941705957994,
      "grad_norm": 0.10740763694047928,
      "learning_rate": 1.0744392056008e-06,
      "loss": 0.2232,
      "step": 66230
    },
    {
      "epoch": 14.196313759108444,
      "grad_norm": 0.05213171988725662,
      "learning_rate": 1.0715816545220747e-06,
      "loss": 0.0001,
      "step": 66240
    },
    {
      "epoch": 14.198456922417488,
      "grad_norm": 0.0012734696501865983,
      "learning_rate": 1.0687241034433492e-06,
      "loss": 0.0006,
      "step": 66250
    },
    {
      "epoch": 14.200600085726533,
      "grad_norm": 0.158191978931427,
      "learning_rate": 1.0658665523646236e-06,
      "loss": 0.2406,
      "step": 66260
    },
    {
      "epoch": 14.202743249035576,
      "grad_norm": 0.0019674107898026705,
      "learning_rate": 1.063009001285898e-06,
      "loss": 0.1299,
      "step": 66270
    },
    {
      "epoch": 14.20488641234462,
      "grad_norm": 0.03287038952112198,
      "learning_rate": 1.0601514502071725e-06,
      "loss": 0.0001,
      "step": 66280
    },
    {
      "epoch": 14.207029575653666,
      "grad_norm": 0.005472747143357992,
      "learning_rate": 1.057293899128447e-06,
      "loss": 0.0,
      "step": 66290
    },
    {
      "epoch": 14.209172738962708,
      "grad_norm": 0.012603011913597584,
      "learning_rate": 1.0544363480497214e-06,
      "loss": 0.0003,
      "step": 66300
    },
    {
      "epoch": 14.211315902271753,
      "grad_norm": 0.0016347767086699605,
      "learning_rate": 1.051578796970996e-06,
      "loss": 0.0003,
      "step": 66310
    },
    {
      "epoch": 14.213459065580798,
      "grad_norm": 0.003017389914020896,
      "learning_rate": 1.0487212458922705e-06,
      "loss": 0.1864,
      "step": 66320
    },
    {
      "epoch": 14.215602228889841,
      "grad_norm": 0.00014307137462310493,
      "learning_rate": 1.0458636948135449e-06,
      "loss": 0.0008,
      "step": 66330
    },
    {
      "epoch": 14.217745392198886,
      "grad_norm": 0.0045067258179187775,
      "learning_rate": 1.0430061437348192e-06,
      "loss": 0.0016,
      "step": 66340
    },
    {
      "epoch": 14.21988855550793,
      "grad_norm": 0.0001616882800590247,
      "learning_rate": 1.0401485926560938e-06,
      "loss": 0.0001,
      "step": 66350
    },
    {
      "epoch": 14.222031718816973,
      "grad_norm": 0.0004472621367312968,
      "learning_rate": 1.0372910415773684e-06,
      "loss": 0.1232,
      "step": 66360
    },
    {
      "epoch": 14.224174882126018,
      "grad_norm": 26.28383445739746,
      "learning_rate": 1.0344334904986427e-06,
      "loss": 0.1996,
      "step": 66370
    },
    {
      "epoch": 14.226318045435063,
      "grad_norm": 0.0002156364353140816,
      "learning_rate": 1.0315759394199173e-06,
      "loss": 0.001,
      "step": 66380
    },
    {
      "epoch": 14.228461208744106,
      "grad_norm": 37.486083984375,
      "learning_rate": 1.0287183883411916e-06,
      "loss": 0.2999,
      "step": 66390
    },
    {
      "epoch": 14.23060437205315,
      "grad_norm": 0.00038648006739094853,
      "learning_rate": 1.0258608372624662e-06,
      "loss": 0.0005,
      "step": 66400
    },
    {
      "epoch": 14.232747535362195,
      "grad_norm": 0.007472524885088205,
      "learning_rate": 1.0230032861837406e-06,
      "loss": 0.0011,
      "step": 66410
    },
    {
      "epoch": 14.234890698671238,
      "grad_norm": 0.0015827234601601958,
      "learning_rate": 1.0201457351050151e-06,
      "loss": 0.1249,
      "step": 66420
    },
    {
      "epoch": 14.237033861980283,
      "grad_norm": 0.0005720077897422016,
      "learning_rate": 1.0172881840262895e-06,
      "loss": 0.0001,
      "step": 66430
    },
    {
      "epoch": 14.239177025289328,
      "grad_norm": 0.0006051853997632861,
      "learning_rate": 1.014430632947564e-06,
      "loss": 0.0,
      "step": 66440
    },
    {
      "epoch": 14.24132018859837,
      "grad_norm": 4.4925734982825816e-05,
      "learning_rate": 1.0115730818688386e-06,
      "loss": 0.0003,
      "step": 66450
    },
    {
      "epoch": 14.243463351907415,
      "grad_norm": 0.04722720757126808,
      "learning_rate": 1.008715530790113e-06,
      "loss": 0.1468,
      "step": 66460
    },
    {
      "epoch": 14.24560651521646,
      "grad_norm": 0.00035570343607105315,
      "learning_rate": 1.0058579797113873e-06,
      "loss": 0.0004,
      "step": 66470
    },
    {
      "epoch": 14.247749678525503,
      "grad_norm": 0.0002088472683681175,
      "learning_rate": 1.0030004286326619e-06,
      "loss": 0.001,
      "step": 66480
    },
    {
      "epoch": 14.249892841834548,
      "grad_norm": 0.006044026464223862,
      "learning_rate": 1.0001428775539364e-06,
      "loss": 0.0001,
      "step": 66490
    },
    {
      "epoch": 14.252036005143593,
      "grad_norm": 0.0002657622389961034,
      "learning_rate": 9.972853264752108e-07,
      "loss": 0.0006,
      "step": 66500
    },
    {
      "epoch": 14.254179168452636,
      "grad_norm": 0.11702533811330795,
      "learning_rate": 9.944277753964853e-07,
      "loss": 0.0005,
      "step": 66510
    },
    {
      "epoch": 14.25632233176168,
      "grad_norm": 0.002971307374536991,
      "learning_rate": 9.9157022431776e-07,
      "loss": 0.0001,
      "step": 66520
    },
    {
      "epoch": 14.258465495070725,
      "grad_norm": 0.00012163811334175989,
      "learning_rate": 9.887126732390343e-07,
      "loss": 0.0001,
      "step": 66530
    },
    {
      "epoch": 14.260608658379768,
      "grad_norm": 5.381597293308005e-05,
      "learning_rate": 9.858551221603086e-07,
      "loss": 0.0004,
      "step": 66540
    },
    {
      "epoch": 14.262751821688813,
      "grad_norm": 0.0008598252898082137,
      "learning_rate": 9.829975710815832e-07,
      "loss": 0.1781,
      "step": 66550
    },
    {
      "epoch": 14.264894984997857,
      "grad_norm": 0.0050031752325594425,
      "learning_rate": 9.801400200028577e-07,
      "loss": 0.2045,
      "step": 66560
    },
    {
      "epoch": 14.2670381483069,
      "grad_norm": 0.003443925641477108,
      "learning_rate": 9.77282468924132e-07,
      "loss": 0.3251,
      "step": 66570
    },
    {
      "epoch": 14.269181311615945,
      "grad_norm": 9.76646042545326e-05,
      "learning_rate": 9.744249178454067e-07,
      "loss": 0.0001,
      "step": 66580
    },
    {
      "epoch": 14.27132447492499,
      "grad_norm": 0.009351618587970734,
      "learning_rate": 9.71567366766681e-07,
      "loss": 0.0005,
      "step": 66590
    },
    {
      "epoch": 14.273467638234033,
      "grad_norm": 0.12987522780895233,
      "learning_rate": 9.687098156879556e-07,
      "loss": 0.0003,
      "step": 66600
    },
    {
      "epoch": 14.275610801543078,
      "grad_norm": 18.608455657958984,
      "learning_rate": 9.6585226460923e-07,
      "loss": 0.1244,
      "step": 66610
    },
    {
      "epoch": 14.277753964852122,
      "grad_norm": 0.00011592702503548935,
      "learning_rate": 9.629947135305045e-07,
      "loss": 0.0008,
      "step": 66620
    },
    {
      "epoch": 14.279897128161165,
      "grad_norm": 0.005143570713698864,
      "learning_rate": 9.601371624517788e-07,
      "loss": 0.129,
      "step": 66630
    },
    {
      "epoch": 14.28204029147021,
      "grad_norm": 0.0006292494363151491,
      "learning_rate": 9.572796113730534e-07,
      "loss": 0.0001,
      "step": 66640
    },
    {
      "epoch": 14.284183454779255,
      "grad_norm": 0.0005656361463479698,
      "learning_rate": 9.544220602943278e-07,
      "loss": 0.0005,
      "step": 66650
    },
    {
      "epoch": 14.286326618088298,
      "grad_norm": 0.21123306453227997,
      "learning_rate": 9.515645092156023e-07,
      "loss": 0.1405,
      "step": 66660
    },
    {
      "epoch": 14.288469781397342,
      "grad_norm": 0.06077568233013153,
      "learning_rate": 9.487069581368767e-07,
      "loss": 0.0002,
      "step": 66670
    },
    {
      "epoch": 14.290612944706387,
      "grad_norm": 0.13419799506664276,
      "learning_rate": 9.458494070581513e-07,
      "loss": 0.001,
      "step": 66680
    },
    {
      "epoch": 14.29275610801543,
      "grad_norm": 0.000508731696754694,
      "learning_rate": 9.429918559794257e-07,
      "loss": 0.1394,
      "step": 66690
    },
    {
      "epoch": 14.294899271324475,
      "grad_norm": 0.0012285950360819697,
      "learning_rate": 9.401343049007001e-07,
      "loss": 0.2623,
      "step": 66700
    },
    {
      "epoch": 14.29704243463352,
      "grad_norm": 0.004362454637885094,
      "learning_rate": 9.372767538219746e-07,
      "loss": 0.0001,
      "step": 66710
    },
    {
      "epoch": 14.299185597942563,
      "grad_norm": 0.006160873454064131,
      "learning_rate": 9.344192027432492e-07,
      "loss": 0.1655,
      "step": 66720
    },
    {
      "epoch": 14.301328761251607,
      "grad_norm": 0.0013370574451982975,
      "learning_rate": 9.315616516645236e-07,
      "loss": 0.1167,
      "step": 66730
    },
    {
      "epoch": 14.303471924560652,
      "grad_norm": 0.0006884156027808785,
      "learning_rate": 9.28704100585798e-07,
      "loss": 0.0001,
      "step": 66740
    },
    {
      "epoch": 14.305615087869695,
      "grad_norm": 0.00018454169912729412,
      "learning_rate": 9.258465495070724e-07,
      "loss": 0.0001,
      "step": 66750
    },
    {
      "epoch": 14.30775825117874,
      "grad_norm": 0.004062274470925331,
      "learning_rate": 9.22988998428347e-07,
      "loss": 0.0018,
      "step": 66760
    },
    {
      "epoch": 14.309901414487785,
      "grad_norm": 0.0016182021936401725,
      "learning_rate": 9.201314473496215e-07,
      "loss": 0.0008,
      "step": 66770
    },
    {
      "epoch": 14.312044577796827,
      "grad_norm": 0.0004914166638627648,
      "learning_rate": 9.172738962708959e-07,
      "loss": 0.0002,
      "step": 66780
    },
    {
      "epoch": 14.314187741105872,
      "grad_norm": 0.000299749372061342,
      "learning_rate": 9.144163451921704e-07,
      "loss": 0.1299,
      "step": 66790
    },
    {
      "epoch": 14.316330904414917,
      "grad_norm": 27.483619689941406,
      "learning_rate": 9.115587941134449e-07,
      "loss": 0.333,
      "step": 66800
    },
    {
      "epoch": 14.31847406772396,
      "grad_norm": 0.012671324424445629,
      "learning_rate": 9.087012430347193e-07,
      "loss": 0.2132,
      "step": 66810
    },
    {
      "epoch": 14.320617231033005,
      "grad_norm": 0.0032876343466341496,
      "learning_rate": 9.058436919559937e-07,
      "loss": 0.1283,
      "step": 66820
    },
    {
      "epoch": 14.32276039434205,
      "grad_norm": 0.0007112190360203385,
      "learning_rate": 9.029861408772682e-07,
      "loss": 0.1356,
      "step": 66830
    },
    {
      "epoch": 14.324903557651092,
      "grad_norm": 0.025181900709867477,
      "learning_rate": 9.001285897985428e-07,
      "loss": 0.1586,
      "step": 66840
    },
    {
      "epoch": 14.327046720960137,
      "grad_norm": 3.60911653842777e-05,
      "learning_rate": 8.972710387198172e-07,
      "loss": 0.0006,
      "step": 66850
    },
    {
      "epoch": 14.329189884269182,
      "grad_norm": 0.0006502046016976237,
      "learning_rate": 8.944134876410916e-07,
      "loss": 0.12,
      "step": 66860
    },
    {
      "epoch": 14.331333047578225,
      "grad_norm": 0.0012105631176382303,
      "learning_rate": 8.915559365623662e-07,
      "loss": 0.22,
      "step": 66870
    },
    {
      "epoch": 14.33347621088727,
      "grad_norm": 0.000325848872307688,
      "learning_rate": 8.886983854836406e-07,
      "loss": 0.2089,
      "step": 66880
    },
    {
      "epoch": 14.335619374196314,
      "grad_norm": 0.0010415156139060855,
      "learning_rate": 8.85840834404915e-07,
      "loss": 0.2516,
      "step": 66890
    },
    {
      "epoch": 14.337762537505357,
      "grad_norm": 0.007374810986220837,
      "learning_rate": 8.829832833261895e-07,
      "loss": 0.0001,
      "step": 66900
    },
    {
      "epoch": 14.339905700814402,
      "grad_norm": 0.8056610822677612,
      "learning_rate": 8.801257322474641e-07,
      "loss": 0.0015,
      "step": 66910
    },
    {
      "epoch": 14.342048864123447,
      "grad_norm": 0.00029861036455258727,
      "learning_rate": 8.772681811687385e-07,
      "loss": 0.2189,
      "step": 66920
    },
    {
      "epoch": 14.34419202743249,
      "grad_norm": 0.0015833050711080432,
      "learning_rate": 8.744106300900129e-07,
      "loss": 0.0001,
      "step": 66930
    },
    {
      "epoch": 14.346335190741534,
      "grad_norm": 0.007390840444713831,
      "learning_rate": 8.715530790112873e-07,
      "loss": 0.0001,
      "step": 66940
    },
    {
      "epoch": 14.34847835405058,
      "grad_norm": 0.34041130542755127,
      "learning_rate": 8.686955279325619e-07,
      "loss": 0.0014,
      "step": 66950
    },
    {
      "epoch": 14.350621517359622,
      "grad_norm": 0.009910207241773605,
      "learning_rate": 8.658379768538364e-07,
      "loss": 0.0002,
      "step": 66960
    },
    {
      "epoch": 14.352764680668667,
      "grad_norm": 0.0004654047661460936,
      "learning_rate": 8.629804257751108e-07,
      "loss": 0.001,
      "step": 66970
    },
    {
      "epoch": 14.354907843977712,
      "grad_norm": 0.0009847895707935095,
      "learning_rate": 8.601228746963852e-07,
      "loss": 0.0,
      "step": 66980
    },
    {
      "epoch": 14.357051007286755,
      "grad_norm": 0.004734185058623552,
      "learning_rate": 8.572653236176598e-07,
      "loss": 0.3452,
      "step": 66990
    },
    {
      "epoch": 14.3591941705958,
      "grad_norm": 0.0017144187586382031,
      "learning_rate": 8.544077725389342e-07,
      "loss": 0.3093,
      "step": 67000
    },
    {
      "epoch": 14.361337333904844,
      "grad_norm": 0.004471581894904375,
      "learning_rate": 8.515502214602087e-07,
      "loss": 0.1171,
      "step": 67010
    },
    {
      "epoch": 14.363480497213887,
      "grad_norm": 0.0009897799463942647,
      "learning_rate": 8.486926703814831e-07,
      "loss": 0.0001,
      "step": 67020
    },
    {
      "epoch": 14.365623660522932,
      "grad_norm": 0.0003920512390322983,
      "learning_rate": 8.458351193027577e-07,
      "loss": 0.0005,
      "step": 67030
    },
    {
      "epoch": 14.367766823831976,
      "grad_norm": 0.0039385538548231125,
      "learning_rate": 8.429775682240321e-07,
      "loss": 0.0001,
      "step": 67040
    },
    {
      "epoch": 14.36990998714102,
      "grad_norm": 0.0004891249118372798,
      "learning_rate": 8.401200171453065e-07,
      "loss": 0.0006,
      "step": 67050
    },
    {
      "epoch": 14.372053150450064,
      "grad_norm": 0.018071141093969345,
      "learning_rate": 8.372624660665809e-07,
      "loss": 0.0002,
      "step": 67060
    },
    {
      "epoch": 14.374196313759109,
      "grad_norm": 0.008623699657619,
      "learning_rate": 8.344049149878555e-07,
      "loss": 0.2793,
      "step": 67070
    },
    {
      "epoch": 14.376339477068152,
      "grad_norm": 0.00035220960853621364,
      "learning_rate": 8.3154736390913e-07,
      "loss": 0.2591,
      "step": 67080
    },
    {
      "epoch": 14.378482640377197,
      "grad_norm": 0.19692353904247284,
      "learning_rate": 8.286898128304044e-07,
      "loss": 0.0003,
      "step": 67090
    },
    {
      "epoch": 14.380625803686241,
      "grad_norm": 0.00031738419784232974,
      "learning_rate": 8.258322617516789e-07,
      "loss": 0.2312,
      "step": 67100
    },
    {
      "epoch": 14.382768966995284,
      "grad_norm": 0.005178472958505154,
      "learning_rate": 8.229747106729534e-07,
      "loss": 0.0002,
      "step": 67110
    },
    {
      "epoch": 14.384912130304329,
      "grad_norm": 0.39289984107017517,
      "learning_rate": 8.201171595942278e-07,
      "loss": 0.5047,
      "step": 67120
    },
    {
      "epoch": 14.387055293613374,
      "grad_norm": 6.337098602671176e-05,
      "learning_rate": 8.172596085155022e-07,
      "loss": 0.1131,
      "step": 67130
    },
    {
      "epoch": 14.389198456922417,
      "grad_norm": 8.507021266268566e-05,
      "learning_rate": 8.144020574367767e-07,
      "loss": 0.0012,
      "step": 67140
    },
    {
      "epoch": 14.391341620231461,
      "grad_norm": 0.0002027796726906672,
      "learning_rate": 8.115445063580513e-07,
      "loss": 0.0004,
      "step": 67150
    },
    {
      "epoch": 14.393484783540506,
      "grad_norm": 0.00014167993504088372,
      "learning_rate": 8.086869552793257e-07,
      "loss": 0.0002,
      "step": 67160
    },
    {
      "epoch": 14.39562794684955,
      "grad_norm": 0.0029299582820385695,
      "learning_rate": 8.058294042006001e-07,
      "loss": 0.0003,
      "step": 67170
    },
    {
      "epoch": 14.397771110158594,
      "grad_norm": 0.06757732480764389,
      "learning_rate": 8.029718531218745e-07,
      "loss": 0.0006,
      "step": 67180
    },
    {
      "epoch": 14.399914273467639,
      "grad_norm": 0.02476830594241619,
      "learning_rate": 8.001143020431491e-07,
      "loss": 0.0431,
      "step": 67190
    },
    {
      "epoch": 14.402057436776682,
      "grad_norm": 0.006240530870854855,
      "learning_rate": 7.972567509644236e-07,
      "loss": 0.141,
      "step": 67200
    },
    {
      "epoch": 14.404200600085726,
      "grad_norm": 0.001986338524147868,
      "learning_rate": 7.94399199885698e-07,
      "loss": 0.0005,
      "step": 67210
    },
    {
      "epoch": 14.406343763394771,
      "grad_norm": 7.262542931130156e-05,
      "learning_rate": 7.915416488069725e-07,
      "loss": 0.0001,
      "step": 67220
    },
    {
      "epoch": 14.408486926703814,
      "grad_norm": 0.006199214141815901,
      "learning_rate": 7.88684097728247e-07,
      "loss": 0.1344,
      "step": 67230
    },
    {
      "epoch": 14.410630090012859,
      "grad_norm": 0.001358863664790988,
      "learning_rate": 7.858265466495214e-07,
      "loss": 0.1074,
      "step": 67240
    },
    {
      "epoch": 14.412773253321904,
      "grad_norm": 0.005339331459254026,
      "learning_rate": 7.829689955707958e-07,
      "loss": 0.0003,
      "step": 67250
    },
    {
      "epoch": 14.414916416630946,
      "grad_norm": 0.007156938314437866,
      "learning_rate": 7.801114444920703e-07,
      "loss": 0.0003,
      "step": 67260
    },
    {
      "epoch": 14.417059579939991,
      "grad_norm": 28.370132446289062,
      "learning_rate": 7.772538934133449e-07,
      "loss": 0.1587,
      "step": 67270
    },
    {
      "epoch": 14.419202743249036,
      "grad_norm": 0.008796174079179764,
      "learning_rate": 7.743963423346193e-07,
      "loss": 0.0008,
      "step": 67280
    },
    {
      "epoch": 14.421345906558079,
      "grad_norm": 0.615260124206543,
      "learning_rate": 7.715387912558938e-07,
      "loss": 0.0008,
      "step": 67290
    },
    {
      "epoch": 14.423489069867124,
      "grad_norm": 3.536849908414297e-05,
      "learning_rate": 7.686812401771681e-07,
      "loss": 0.0008,
      "step": 67300
    },
    {
      "epoch": 14.425632233176168,
      "grad_norm": 0.0009957035072147846,
      "learning_rate": 7.658236890984427e-07,
      "loss": 0.0011,
      "step": 67310
    },
    {
      "epoch": 14.427775396485211,
      "grad_norm": 0.027789684012532234,
      "learning_rate": 7.629661380197172e-07,
      "loss": 0.0003,
      "step": 67320
    },
    {
      "epoch": 14.429918559794256,
      "grad_norm": 0.15953989326953888,
      "learning_rate": 7.601085869409916e-07,
      "loss": 0.0018,
      "step": 67330
    },
    {
      "epoch": 14.4320617231033,
      "grad_norm": 0.006771480664610863,
      "learning_rate": 7.572510358622661e-07,
      "loss": 0.1674,
      "step": 67340
    },
    {
      "epoch": 14.434204886412344,
      "grad_norm": 0.0010554419131949544,
      "learning_rate": 7.543934847835406e-07,
      "loss": 0.1647,
      "step": 67350
    },
    {
      "epoch": 14.436348049721389,
      "grad_norm": 0.000547732983250171,
      "learning_rate": 7.51535933704815e-07,
      "loss": 0.1626,
      "step": 67360
    },
    {
      "epoch": 14.438491213030433,
      "grad_norm": 0.045213520526885986,
      "learning_rate": 7.486783826260894e-07,
      "loss": 0.1165,
      "step": 67370
    },
    {
      "epoch": 14.440634376339476,
      "grad_norm": 0.005362540949136019,
      "learning_rate": 7.45820831547364e-07,
      "loss": 0.195,
      "step": 67380
    },
    {
      "epoch": 14.442777539648521,
      "grad_norm": 0.0016827074578031898,
      "learning_rate": 7.429632804686385e-07,
      "loss": 0.189,
      "step": 67390
    },
    {
      "epoch": 14.444920702957566,
      "grad_norm": 0.00041501972009427845,
      "learning_rate": 7.401057293899129e-07,
      "loss": 0.186,
      "step": 67400
    },
    {
      "epoch": 14.447063866266609,
      "grad_norm": 0.008906916715204716,
      "learning_rate": 7.372481783111874e-07,
      "loss": 0.0016,
      "step": 67410
    },
    {
      "epoch": 14.449207029575653,
      "grad_norm": 0.0012699218932539225,
      "learning_rate": 7.343906272324619e-07,
      "loss": 0.0009,
      "step": 67420
    },
    {
      "epoch": 14.451350192884698,
      "grad_norm": 0.00010975099576171488,
      "learning_rate": 7.315330761537363e-07,
      "loss": 0.0001,
      "step": 67430
    },
    {
      "epoch": 14.453493356193743,
      "grad_norm": 0.00012366469309199601,
      "learning_rate": 7.286755250750108e-07,
      "loss": 0.1747,
      "step": 67440
    },
    {
      "epoch": 14.455636519502786,
      "grad_norm": 0.04587920382618904,
      "learning_rate": 7.258179739962852e-07,
      "loss": 0.1363,
      "step": 67450
    },
    {
      "epoch": 14.45777968281183,
      "grad_norm": 0.0012554817367345095,
      "learning_rate": 7.229604229175598e-07,
      "loss": 0.1322,
      "step": 67460
    },
    {
      "epoch": 14.459922846120875,
      "grad_norm": 0.0004375964344944805,
      "learning_rate": 7.201028718388342e-07,
      "loss": 0.1427,
      "step": 67470
    },
    {
      "epoch": 14.462066009429918,
      "grad_norm": 0.4036395251750946,
      "learning_rate": 7.172453207601087e-07,
      "loss": 0.0019,
      "step": 67480
    },
    {
      "epoch": 14.464209172738963,
      "grad_norm": 0.006307951640337706,
      "learning_rate": 7.14387769681383e-07,
      "loss": 0.0001,
      "step": 67490
    },
    {
      "epoch": 14.466352336048008,
      "grad_norm": 0.016170388087630272,
      "learning_rate": 7.115302186026576e-07,
      "loss": 0.0011,
      "step": 67500
    },
    {
      "epoch": 14.46849549935705,
      "grad_norm": 0.0002334192831767723,
      "learning_rate": 7.086726675239321e-07,
      "loss": 0.3279,
      "step": 67510
    },
    {
      "epoch": 14.470638662666095,
      "grad_norm": 0.0001764856424415484,
      "learning_rate": 7.058151164452065e-07,
      "loss": 0.0005,
      "step": 67520
    },
    {
      "epoch": 14.47278182597514,
      "grad_norm": 0.06153858080506325,
      "learning_rate": 7.02957565366481e-07,
      "loss": 0.0002,
      "step": 67530
    },
    {
      "epoch": 14.474924989284183,
      "grad_norm": 0.0032114542555063963,
      "learning_rate": 7.001000142877555e-07,
      "loss": 0.1523,
      "step": 67540
    },
    {
      "epoch": 14.477068152593228,
      "grad_norm": 0.0010692673968151212,
      "learning_rate": 6.972424632090299e-07,
      "loss": 0.0002,
      "step": 67550
    },
    {
      "epoch": 14.479211315902273,
      "grad_norm": 0.0025245954748243093,
      "learning_rate": 6.943849121303044e-07,
      "loss": 0.0012,
      "step": 67560
    },
    {
      "epoch": 14.481354479211316,
      "grad_norm": 0.0008836998604238033,
      "learning_rate": 6.915273610515788e-07,
      "loss": 0.1265,
      "step": 67570
    },
    {
      "epoch": 14.48349764252036,
      "grad_norm": 0.00010901789937634021,
      "learning_rate": 6.886698099728534e-07,
      "loss": 0.2702,
      "step": 67580
    },
    {
      "epoch": 14.485640805829405,
      "grad_norm": 0.2847544252872467,
      "learning_rate": 6.858122588941278e-07,
      "loss": 0.0003,
      "step": 67590
    },
    {
      "epoch": 14.487783969138448,
      "grad_norm": 0.0014607174089178443,
      "learning_rate": 6.829547078154023e-07,
      "loss": 0.0003,
      "step": 67600
    },
    {
      "epoch": 14.489927132447493,
      "grad_norm": 34.88652038574219,
      "learning_rate": 6.800971567366766e-07,
      "loss": 0.2867,
      "step": 67610
    },
    {
      "epoch": 14.492070295756537,
      "grad_norm": 6.15716417087242e-05,
      "learning_rate": 6.772396056579512e-07,
      "loss": 0.001,
      "step": 67620
    },
    {
      "epoch": 14.49421345906558,
      "grad_norm": 0.1853431612253189,
      "learning_rate": 6.743820545792257e-07,
      "loss": 0.0002,
      "step": 67630
    },
    {
      "epoch": 14.496356622374625,
      "grad_norm": 0.002628544345498085,
      "learning_rate": 6.715245035005001e-07,
      "loss": 0.1711,
      "step": 67640
    },
    {
      "epoch": 14.49849978568367,
      "grad_norm": 0.05753610283136368,
      "learning_rate": 6.686669524217746e-07,
      "loss": 0.0019,
      "step": 67650
    },
    {
      "epoch": 14.500642948992713,
      "grad_norm": 0.15029622614383698,
      "learning_rate": 6.658094013430491e-07,
      "loss": 0.0003,
      "step": 67660
    },
    {
      "epoch": 14.502786112301758,
      "grad_norm": 0.00016049838450271636,
      "learning_rate": 6.629518502643235e-07,
      "loss": 0.0015,
      "step": 67670
    },
    {
      "epoch": 14.504929275610802,
      "grad_norm": 8.048424933804199e-05,
      "learning_rate": 6.60094299185598e-07,
      "loss": 0.0001,
      "step": 67680
    },
    {
      "epoch": 14.507072438919845,
      "grad_norm": 0.000534320599399507,
      "learning_rate": 6.572367481068724e-07,
      "loss": 0.0003,
      "step": 67690
    },
    {
      "epoch": 14.50921560222889,
      "grad_norm": 0.003929272294044495,
      "learning_rate": 6.54379197028147e-07,
      "loss": 0.0005,
      "step": 67700
    },
    {
      "epoch": 14.511358765537935,
      "grad_norm": 5.116644751979038e-05,
      "learning_rate": 6.515216459494214e-07,
      "loss": 0.1506,
      "step": 67710
    },
    {
      "epoch": 14.513501928846978,
      "grad_norm": 0.004144572652876377,
      "learning_rate": 6.486640948706959e-07,
      "loss": 0.0001,
      "step": 67720
    },
    {
      "epoch": 14.515645092156022,
      "grad_norm": 0.002370159374549985,
      "learning_rate": 6.458065437919702e-07,
      "loss": 0.1333,
      "step": 67730
    },
    {
      "epoch": 14.517788255465067,
      "grad_norm": 0.0004431576526258141,
      "learning_rate": 6.429489927132448e-07,
      "loss": 0.0002,
      "step": 67740
    },
    {
      "epoch": 14.51993141877411,
      "grad_norm": 0.0015885872999206185,
      "learning_rate": 6.400914416345193e-07,
      "loss": 0.1359,
      "step": 67750
    },
    {
      "epoch": 14.522074582083155,
      "grad_norm": 0.008642151951789856,
      "learning_rate": 6.372338905557937e-07,
      "loss": 0.001,
      "step": 67760
    },
    {
      "epoch": 14.5242177453922,
      "grad_norm": 0.016438230872154236,
      "learning_rate": 6.343763394770682e-07,
      "loss": 0.0006,
      "step": 67770
    },
    {
      "epoch": 14.526360908701243,
      "grad_norm": 0.06823953986167908,
      "learning_rate": 6.315187883983427e-07,
      "loss": 0.0001,
      "step": 67780
    },
    {
      "epoch": 14.528504072010287,
      "grad_norm": 31.346792221069336,
      "learning_rate": 6.286612373196172e-07,
      "loss": 0.3272,
      "step": 67790
    },
    {
      "epoch": 14.530647235319332,
      "grad_norm": 0.00012471027730498463,
      "learning_rate": 6.258036862408915e-07,
      "loss": 0.2222,
      "step": 67800
    },
    {
      "epoch": 14.532790398628375,
      "grad_norm": 0.0005166016635484993,
      "learning_rate": 6.229461351621661e-07,
      "loss": 0.0,
      "step": 67810
    },
    {
      "epoch": 14.53493356193742,
      "grad_norm": 0.000600821862462908,
      "learning_rate": 6.200885840834405e-07,
      "loss": 0.222,
      "step": 67820
    },
    {
      "epoch": 14.537076725246465,
      "grad_norm": 0.00042421810212545097,
      "learning_rate": 6.17231033004715e-07,
      "loss": 0.0003,
      "step": 67830
    },
    {
      "epoch": 14.539219888555508,
      "grad_norm": 0.008610355667769909,
      "learning_rate": 6.143734819259895e-07,
      "loss": 0.2936,
      "step": 67840
    },
    {
      "epoch": 14.541363051864552,
      "grad_norm": 0.008758960291743279,
      "learning_rate": 6.115159308472639e-07,
      "loss": 0.159,
      "step": 67850
    },
    {
      "epoch": 14.543506215173597,
      "grad_norm": 161.33889770507812,
      "learning_rate": 6.086583797685384e-07,
      "loss": 0.0878,
      "step": 67860
    },
    {
      "epoch": 14.54564937848264,
      "grad_norm": 0.00038876099279150367,
      "learning_rate": 6.058008286898129e-07,
      "loss": 0.1414,
      "step": 67870
    },
    {
      "epoch": 14.547792541791685,
      "grad_norm": 0.010444266721606255,
      "learning_rate": 6.029432776110873e-07,
      "loss": 0.0001,
      "step": 67880
    },
    {
      "epoch": 14.54993570510073,
      "grad_norm": 0.00284657534211874,
      "learning_rate": 6.000857265323618e-07,
      "loss": 0.0001,
      "step": 67890
    },
    {
      "epoch": 14.552078868409772,
      "grad_norm": 0.002123791491612792,
      "learning_rate": 5.972281754536363e-07,
      "loss": 0.0005,
      "step": 67900
    },
    {
      "epoch": 14.554222031718817,
      "grad_norm": 0.004151982255280018,
      "learning_rate": 5.943706243749108e-07,
      "loss": 0.0001,
      "step": 67910
    },
    {
      "epoch": 14.556365195027862,
      "grad_norm": 0.0031673116609454155,
      "learning_rate": 5.915130732961853e-07,
      "loss": 0.3214,
      "step": 67920
    },
    {
      "epoch": 14.558508358336905,
      "grad_norm": 0.00016940361820161343,
      "learning_rate": 5.886555222174597e-07,
      "loss": 0.1324,
      "step": 67930
    },
    {
      "epoch": 14.56065152164595,
      "grad_norm": 0.008099955506622791,
      "learning_rate": 5.857979711387342e-07,
      "loss": 0.5926,
      "step": 67940
    },
    {
      "epoch": 14.562794684954994,
      "grad_norm": 0.00118625583127141,
      "learning_rate": 5.829404200600086e-07,
      "loss": 0.1705,
      "step": 67950
    },
    {
      "epoch": 14.564937848264037,
      "grad_norm": 7.584103150293231e-05,
      "learning_rate": 5.800828689812831e-07,
      "loss": 0.1968,
      "step": 67960
    },
    {
      "epoch": 14.567081011573082,
      "grad_norm": 0.0027819073293358088,
      "learning_rate": 5.772253179025575e-07,
      "loss": 0.2711,
      "step": 67970
    },
    {
      "epoch": 14.569224174882127,
      "grad_norm": 0.0035804074723273516,
      "learning_rate": 5.743677668238321e-07,
      "loss": 0.2878,
      "step": 67980
    },
    {
      "epoch": 14.57136733819117,
      "grad_norm": 0.006339346989989281,
      "learning_rate": 5.715102157451065e-07,
      "loss": 0.0005,
      "step": 67990
    },
    {
      "epoch": 14.573510501500214,
      "grad_norm": 0.03497231751680374,
      "learning_rate": 5.68652664666381e-07,
      "loss": 0.1409,
      "step": 68000
    },
    {
      "epoch": 14.57565366480926,
      "grad_norm": 0.014398333616554737,
      "learning_rate": 5.657951135876554e-07,
      "loss": 0.0005,
      "step": 68010
    },
    {
      "epoch": 14.577796828118302,
      "grad_norm": 0.10606774687767029,
      "learning_rate": 5.629375625089299e-07,
      "loss": 0.2824,
      "step": 68020
    },
    {
      "epoch": 14.579939991427347,
      "grad_norm": 0.0036123848985880613,
      "learning_rate": 5.600800114302044e-07,
      "loss": 0.0005,
      "step": 68030
    },
    {
      "epoch": 14.582083154736392,
      "grad_norm": 0.0015123888151720166,
      "learning_rate": 5.572224603514788e-07,
      "loss": 0.1347,
      "step": 68040
    },
    {
      "epoch": 14.584226318045435,
      "grad_norm": 0.006552473176270723,
      "learning_rate": 5.543649092727533e-07,
      "loss": 0.0001,
      "step": 68050
    },
    {
      "epoch": 14.58636948135448,
      "grad_norm": 0.014250900596380234,
      "learning_rate": 5.515073581940278e-07,
      "loss": 0.0013,
      "step": 68060
    },
    {
      "epoch": 14.588512644663524,
      "grad_norm": 0.05108853057026863,
      "learning_rate": 5.486498071153022e-07,
      "loss": 0.1845,
      "step": 68070
    },
    {
      "epoch": 14.590655807972567,
      "grad_norm": 0.02062232978641987,
      "learning_rate": 5.457922560365767e-07,
      "loss": 0.2477,
      "step": 68080
    },
    {
      "epoch": 14.592798971281612,
      "grad_norm": 5.589765351032838e-05,
      "learning_rate": 5.429347049578511e-07,
      "loss": 0.1183,
      "step": 68090
    },
    {
      "epoch": 14.594942134590656,
      "grad_norm": 6.336073420243338e-05,
      "learning_rate": 5.400771538791257e-07,
      "loss": 0.0004,
      "step": 68100
    },
    {
      "epoch": 14.5970852978997,
      "grad_norm": 0.01685648038983345,
      "learning_rate": 5.372196028004e-07,
      "loss": 0.0003,
      "step": 68110
    },
    {
      "epoch": 14.599228461208744,
      "grad_norm": 0.00909009575843811,
      "learning_rate": 5.343620517216746e-07,
      "loss": 0.0002,
      "step": 68120
    },
    {
      "epoch": 14.601371624517789,
      "grad_norm": 0.008115263655781746,
      "learning_rate": 5.31504500642949e-07,
      "loss": 0.0002,
      "step": 68130
    },
    {
      "epoch": 14.603514787826832,
      "grad_norm": 9.001810394693166e-05,
      "learning_rate": 5.286469495642235e-07,
      "loss": 0.0002,
      "step": 68140
    },
    {
      "epoch": 14.605657951135877,
      "grad_norm": 0.0002341417275601998,
      "learning_rate": 5.25789398485498e-07,
      "loss": 0.1802,
      "step": 68150
    },
    {
      "epoch": 14.607801114444921,
      "grad_norm": 4.4813456042902544e-05,
      "learning_rate": 5.229318474067724e-07,
      "loss": 0.2492,
      "step": 68160
    },
    {
      "epoch": 14.609944277753964,
      "grad_norm": 0.0008043485577218235,
      "learning_rate": 5.200742963280469e-07,
      "loss": 0.0001,
      "step": 68170
    },
    {
      "epoch": 14.612087441063009,
      "grad_norm": 0.12494131922721863,
      "learning_rate": 5.172167452493214e-07,
      "loss": 0.0009,
      "step": 68180
    },
    {
      "epoch": 14.614230604372054,
      "grad_norm": 0.014422966167330742,
      "learning_rate": 5.143591941705958e-07,
      "loss": 0.0001,
      "step": 68190
    },
    {
      "epoch": 14.616373767681097,
      "grad_norm": 0.0002865991264116019,
      "learning_rate": 5.115016430918703e-07,
      "loss": 0.0004,
      "step": 68200
    },
    {
      "epoch": 14.618516930990141,
      "grad_norm": 0.00734124518930912,
      "learning_rate": 5.086440920131447e-07,
      "loss": 0.1778,
      "step": 68210
    },
    {
      "epoch": 14.620660094299186,
      "grad_norm": 0.0016264674486592412,
      "learning_rate": 5.057865409344193e-07,
      "loss": 0.0003,
      "step": 68220
    },
    {
      "epoch": 14.62280325760823,
      "grad_norm": 0.050414372235536575,
      "learning_rate": 5.029289898556937e-07,
      "loss": 0.0015,
      "step": 68230
    },
    {
      "epoch": 14.624946420917274,
      "grad_norm": 0.0026160937268286943,
      "learning_rate": 5.000714387769682e-07,
      "loss": 0.0001,
      "step": 68240
    },
    {
      "epoch": 14.627089584226319,
      "grad_norm": 0.001438845880329609,
      "learning_rate": 4.972138876982427e-07,
      "loss": 0.0025,
      "step": 68250
    },
    {
      "epoch": 14.629232747535362,
      "grad_norm": 9.067011706065387e-05,
      "learning_rate": 4.943563366195171e-07,
      "loss": 0.0001,
      "step": 68260
    },
    {
      "epoch": 14.631375910844406,
      "grad_norm": 9.515850979369134e-05,
      "learning_rate": 4.914987855407916e-07,
      "loss": 0.0001,
      "step": 68270
    },
    {
      "epoch": 14.633519074153451,
      "grad_norm": 0.0002393001050222665,
      "learning_rate": 4.88641234462066e-07,
      "loss": 0.0004,
      "step": 68280
    },
    {
      "epoch": 14.635662237462494,
      "grad_norm": 0.00011511977209011093,
      "learning_rate": 4.857836833833405e-07,
      "loss": 0.0885,
      "step": 68290
    },
    {
      "epoch": 14.637805400771539,
      "grad_norm": 0.00014499911048915237,
      "learning_rate": 4.82926132304615e-07,
      "loss": 0.0018,
      "step": 68300
    },
    {
      "epoch": 14.639948564080584,
      "grad_norm": 0.0006399932899512351,
      "learning_rate": 4.800685812258894e-07,
      "loss": 0.0,
      "step": 68310
    },
    {
      "epoch": 14.642091727389626,
      "grad_norm": 0.0031803951133042574,
      "learning_rate": 4.772110301471639e-07,
      "loss": 0.0004,
      "step": 68320
    },
    {
      "epoch": 14.644234890698671,
      "grad_norm": 0.003381323767825961,
      "learning_rate": 4.7435347906843833e-07,
      "loss": 0.0008,
      "step": 68330
    },
    {
      "epoch": 14.646378054007716,
      "grad_norm": 0.0031602338422089815,
      "learning_rate": 4.7149592798971284e-07,
      "loss": 0.0001,
      "step": 68340
    },
    {
      "epoch": 14.648521217316759,
      "grad_norm": 0.004757595714181662,
      "learning_rate": 4.686383769109873e-07,
      "loss": 0.001,
      "step": 68350
    },
    {
      "epoch": 14.650664380625804,
      "grad_norm": 0.3208441138267517,
      "learning_rate": 4.657808258322618e-07,
      "loss": 0.0011,
      "step": 68360
    },
    {
      "epoch": 14.652807543934848,
      "grad_norm": 0.00018017854017671198,
      "learning_rate": 4.629232747535362e-07,
      "loss": 0.0001,
      "step": 68370
    },
    {
      "epoch": 14.654950707243891,
      "grad_norm": 0.13277532160282135,
      "learning_rate": 4.6006572367481073e-07,
      "loss": 0.0006,
      "step": 68380
    },
    {
      "epoch": 14.657093870552936,
      "grad_norm": 0.03566783666610718,
      "learning_rate": 4.572081725960852e-07,
      "loss": 0.1624,
      "step": 68390
    },
    {
      "epoch": 14.65923703386198,
      "grad_norm": 0.011741502210497856,
      "learning_rate": 4.5435062151735964e-07,
      "loss": 0.1379,
      "step": 68400
    },
    {
      "epoch": 14.661380197171024,
      "grad_norm": 0.0007308672065846622,
      "learning_rate": 4.514930704386341e-07,
      "loss": 0.1223,
      "step": 68410
    },
    {
      "epoch": 14.663523360480069,
      "grad_norm": 0.0029344947542995214,
      "learning_rate": 4.486355193599086e-07,
      "loss": 0.0001,
      "step": 68420
    },
    {
      "epoch": 14.665666523789113,
      "grad_norm": 0.00041992729529738426,
      "learning_rate": 4.457779682811831e-07,
      "loss": 0.0005,
      "step": 68430
    },
    {
      "epoch": 14.667809687098156,
      "grad_norm": 0.0006366036250256002,
      "learning_rate": 4.429204172024575e-07,
      "loss": 0.0006,
      "step": 68440
    },
    {
      "epoch": 14.669952850407201,
      "grad_norm": 0.0033619124442338943,
      "learning_rate": 4.4006286612373204e-07,
      "loss": 0.0008,
      "step": 68450
    },
    {
      "epoch": 14.672096013716246,
      "grad_norm": 0.004843698814511299,
      "learning_rate": 4.3720531504500644e-07,
      "loss": 0.0001,
      "step": 68460
    },
    {
      "epoch": 14.674239177025289,
      "grad_norm": 0.036170680075883865,
      "learning_rate": 4.3434776396628095e-07,
      "loss": 0.0011,
      "step": 68470
    },
    {
      "epoch": 14.676382340334333,
      "grad_norm": 7.607285078847781e-05,
      "learning_rate": 4.314902128875554e-07,
      "loss": 0.1203,
      "step": 68480
    },
    {
      "epoch": 14.678525503643378,
      "grad_norm": 21.246469497680664,
      "learning_rate": 4.286326618088299e-07,
      "loss": 0.1164,
      "step": 68490
    },
    {
      "epoch": 14.680668666952421,
      "grad_norm": 0.002536667278036475,
      "learning_rate": 4.257751107301043e-07,
      "loss": 0.0001,
      "step": 68500
    },
    {
      "epoch": 14.682811830261466,
      "grad_norm": 30.695693969726562,
      "learning_rate": 4.2291755965137884e-07,
      "loss": 0.5339,
      "step": 68510
    },
    {
      "epoch": 14.68495499357051,
      "grad_norm": 0.0020558852702379227,
      "learning_rate": 4.2006000857265324e-07,
      "loss": 0.0001,
      "step": 68520
    },
    {
      "epoch": 14.687098156879554,
      "grad_norm": 0.0026205298490822315,
      "learning_rate": 4.1720245749392775e-07,
      "loss": 0.0005,
      "step": 68530
    },
    {
      "epoch": 14.689241320188598,
      "grad_norm": 0.0025035040453076363,
      "learning_rate": 4.143449064152022e-07,
      "loss": 0.0005,
      "step": 68540
    },
    {
      "epoch": 14.691384483497643,
      "grad_norm": 0.007657644338905811,
      "learning_rate": 4.114873553364767e-07,
      "loss": 0.0063,
      "step": 68550
    },
    {
      "epoch": 14.693527646806686,
      "grad_norm": 0.0035925228148698807,
      "learning_rate": 4.086298042577511e-07,
      "loss": 0.0001,
      "step": 68560
    },
    {
      "epoch": 14.69567081011573,
      "grad_norm": 0.0005426594871096313,
      "learning_rate": 4.0577225317902563e-07,
      "loss": 0.0003,
      "step": 68570
    },
    {
      "epoch": 14.697813973424775,
      "grad_norm": 0.0026219040155410767,
      "learning_rate": 4.0291470210030004e-07,
      "loss": 0.0003,
      "step": 68580
    },
    {
      "epoch": 14.699957136733818,
      "grad_norm": 0.004354208242148161,
      "learning_rate": 4.0005715102157455e-07,
      "loss": 0.1954,
      "step": 68590
    },
    {
      "epoch": 14.702100300042863,
      "grad_norm": 0.003031008644029498,
      "learning_rate": 3.97199599942849e-07,
      "loss": 0.0004,
      "step": 68600
    },
    {
      "epoch": 14.704243463351908,
      "grad_norm": 29.865432739257812,
      "learning_rate": 3.943420488641235e-07,
      "loss": 0.2085,
      "step": 68610
    },
    {
      "epoch": 14.70638662666095,
      "grad_norm": 0.0002122322766808793,
      "learning_rate": 3.914844977853979e-07,
      "loss": 0.2009,
      "step": 68620
    },
    {
      "epoch": 14.708529789969996,
      "grad_norm": 0.21321839094161987,
      "learning_rate": 3.8862694670667243e-07,
      "loss": 0.0008,
      "step": 68630
    },
    {
      "epoch": 14.71067295327904,
      "grad_norm": 0.004617996513843536,
      "learning_rate": 3.857693956279469e-07,
      "loss": 0.0003,
      "step": 68640
    },
    {
      "epoch": 14.712816116588083,
      "grad_norm": 22.705284118652344,
      "learning_rate": 3.8291184454922135e-07,
      "loss": 0.2128,
      "step": 68650
    },
    {
      "epoch": 14.714959279897128,
      "grad_norm": 0.05193965137004852,
      "learning_rate": 3.800542934704958e-07,
      "loss": 0.3209,
      "step": 68660
    },
    {
      "epoch": 14.717102443206173,
      "grad_norm": 21.376741409301758,
      "learning_rate": 3.771967423917703e-07,
      "loss": 0.3049,
      "step": 68670
    },
    {
      "epoch": 14.719245606515216,
      "grad_norm": 7.656272646272555e-05,
      "learning_rate": 3.743391913130447e-07,
      "loss": 0.001,
      "step": 68680
    },
    {
      "epoch": 14.72138876982426,
      "grad_norm": 0.05039656534790993,
      "learning_rate": 3.7148164023431923e-07,
      "loss": 0.001,
      "step": 68690
    },
    {
      "epoch": 14.723531933133305,
      "grad_norm": 0.00021499884314835072,
      "learning_rate": 3.686240891555937e-07,
      "loss": 0.1293,
      "step": 68700
    },
    {
      "epoch": 14.725675096442348,
      "grad_norm": 0.004075607284903526,
      "learning_rate": 3.6576653807686815e-07,
      "loss": 0.0001,
      "step": 68710
    },
    {
      "epoch": 14.727818259751393,
      "grad_norm": 0.0006963266059756279,
      "learning_rate": 3.629089869981426e-07,
      "loss": 0.0001,
      "step": 68720
    },
    {
      "epoch": 14.729961423060438,
      "grad_norm": 0.0023091768380254507,
      "learning_rate": 3.600514359194171e-07,
      "loss": 0.0005,
      "step": 68730
    },
    {
      "epoch": 14.73210458636948,
      "grad_norm": 0.00010140511585632339,
      "learning_rate": 3.571938848406915e-07,
      "loss": 0.0001,
      "step": 68740
    },
    {
      "epoch": 14.734247749678525,
      "grad_norm": 0.0028303044382482767,
      "learning_rate": 3.5433633376196603e-07,
      "loss": 0.0005,
      "step": 68750
    },
    {
      "epoch": 14.73639091298757,
      "grad_norm": 0.005220875609666109,
      "learning_rate": 3.514787826832405e-07,
      "loss": 0.0002,
      "step": 68760
    },
    {
      "epoch": 14.738534076296613,
      "grad_norm": 0.006029773037880659,
      "learning_rate": 3.4862123160451495e-07,
      "loss": 0.0002,
      "step": 68770
    },
    {
      "epoch": 14.740677239605658,
      "grad_norm": 0.000826750707346946,
      "learning_rate": 3.457636805257894e-07,
      "loss": 0.0002,
      "step": 68780
    },
    {
      "epoch": 14.742820402914703,
      "grad_norm": 0.002076832577586174,
      "learning_rate": 3.429061294470639e-07,
      "loss": 0.1975,
      "step": 68790
    },
    {
      "epoch": 14.744963566223745,
      "grad_norm": 0.004639343358576298,
      "learning_rate": 3.400485783683383e-07,
      "loss": 0.0001,
      "step": 68800
    },
    {
      "epoch": 14.74710672953279,
      "grad_norm": 0.09036830812692642,
      "learning_rate": 3.3719102728961283e-07,
      "loss": 0.3124,
      "step": 68810
    },
    {
      "epoch": 14.749249892841835,
      "grad_norm": 0.00014582423318643123,
      "learning_rate": 3.343334762108873e-07,
      "loss": 0.3055,
      "step": 68820
    },
    {
      "epoch": 14.751393056150878,
      "grad_norm": 0.03445294126868248,
      "learning_rate": 3.3147592513216175e-07,
      "loss": 0.2821,
      "step": 68830
    },
    {
      "epoch": 14.753536219459923,
      "grad_norm": 0.0017510835314169526,
      "learning_rate": 3.286183740534362e-07,
      "loss": 0.0004,
      "step": 68840
    },
    {
      "epoch": 14.755679382768967,
      "grad_norm": 0.002387724816799164,
      "learning_rate": 3.257608229747107e-07,
      "loss": 0.0001,
      "step": 68850
    },
    {
      "epoch": 14.75782254607801,
      "grad_norm": 0.0070547079667449,
      "learning_rate": 3.229032718959851e-07,
      "loss": 0.0001,
      "step": 68860
    },
    {
      "epoch": 14.759965709387055,
      "grad_norm": 0.014408964663743973,
      "learning_rate": 3.2004572081725963e-07,
      "loss": 0.302,
      "step": 68870
    },
    {
      "epoch": 14.7621088726961,
      "grad_norm": 7.194514910224825e-05,
      "learning_rate": 3.171881697385341e-07,
      "loss": 0.1581,
      "step": 68880
    },
    {
      "epoch": 14.764252036005143,
      "grad_norm": 0.0013810917735099792,
      "learning_rate": 3.143306186598086e-07,
      "loss": 0.1483,
      "step": 68890
    },
    {
      "epoch": 14.766395199314188,
      "grad_norm": 0.0052336142398417,
      "learning_rate": 3.1147306758108306e-07,
      "loss": 0.0004,
      "step": 68900
    },
    {
      "epoch": 14.768538362623232,
      "grad_norm": 0.00023138345568440855,
      "learning_rate": 3.086155165023575e-07,
      "loss": 0.1972,
      "step": 68910
    },
    {
      "epoch": 14.770681525932275,
      "grad_norm": 0.0033509538043290377,
      "learning_rate": 3.0575796542363197e-07,
      "loss": 0.0002,
      "step": 68920
    },
    {
      "epoch": 14.77282468924132,
      "grad_norm": 0.0002530756755732,
      "learning_rate": 3.0290041434490643e-07,
      "loss": 0.0001,
      "step": 68930
    },
    {
      "epoch": 14.774967852550365,
      "grad_norm": 0.00350083876401186,
      "learning_rate": 3.000428632661809e-07,
      "loss": 0.0012,
      "step": 68940
    },
    {
      "epoch": 14.777111015859408,
      "grad_norm": 4.7240639105439186e-05,
      "learning_rate": 2.971853121874554e-07,
      "loss": 0.0004,
      "step": 68950
    },
    {
      "epoch": 14.779254179168452,
      "grad_norm": 0.005558169912546873,
      "learning_rate": 2.9432776110872985e-07,
      "loss": 0.4229,
      "step": 68960
    },
    {
      "epoch": 14.781397342477497,
      "grad_norm": 0.37580159306526184,
      "learning_rate": 2.914702100300043e-07,
      "loss": 0.0006,
      "step": 68970
    },
    {
      "epoch": 14.78354050578654,
      "grad_norm": 0.11225462704896927,
      "learning_rate": 2.8861265895127877e-07,
      "loss": 0.0004,
      "step": 68980
    },
    {
      "epoch": 14.785683669095585,
      "grad_norm": 0.002541910856962204,
      "learning_rate": 2.8575510787255323e-07,
      "loss": 0.0002,
      "step": 68990
    },
    {
      "epoch": 14.78782683240463,
      "grad_norm": 0.0015010255156084895,
      "learning_rate": 2.828975567938277e-07,
      "loss": 0.0008,
      "step": 69000
    },
    {
      "epoch": 14.789969995713673,
      "grad_norm": 0.0012797282543033361,
      "learning_rate": 2.800400057151022e-07,
      "loss": 0.1412,
      "step": 69010
    },
    {
      "epoch": 14.792113159022717,
      "grad_norm": 0.004649179056286812,
      "learning_rate": 2.7718245463637665e-07,
      "loss": 0.0002,
      "step": 69020
    },
    {
      "epoch": 14.794256322331762,
      "grad_norm": 0.0001038355112541467,
      "learning_rate": 2.743249035576511e-07,
      "loss": 0.0,
      "step": 69030
    },
    {
      "epoch": 14.796399485640805,
      "grad_norm": 0.002966732019558549,
      "learning_rate": 2.7146735247892557e-07,
      "loss": 0.0006,
      "step": 69040
    },
    {
      "epoch": 14.79854264894985,
      "grad_norm": 0.00433845492079854,
      "learning_rate": 2.686098014002e-07,
      "loss": 0.2092,
      "step": 69050
    },
    {
      "epoch": 14.800685812258894,
      "grad_norm": 0.0025866993237286806,
      "learning_rate": 2.657522503214745e-07,
      "loss": 0.0,
      "step": 69060
    },
    {
      "epoch": 14.802828975567937,
      "grad_norm": 0.000196327586309053,
      "learning_rate": 2.62894699242749e-07,
      "loss": 0.0007,
      "step": 69070
    },
    {
      "epoch": 14.804972138876982,
      "grad_norm": 0.024252548813819885,
      "learning_rate": 2.6003714816402345e-07,
      "loss": 0.0003,
      "step": 69080
    },
    {
      "epoch": 14.807115302186027,
      "grad_norm": 0.007033767644315958,
      "learning_rate": 2.571795970852979e-07,
      "loss": 0.0001,
      "step": 69090
    },
    {
      "epoch": 14.80925846549507,
      "grad_norm": 0.018098903819918633,
      "learning_rate": 2.5432204600657237e-07,
      "loss": 0.0001,
      "step": 69100
    },
    {
      "epoch": 14.811401628804115,
      "grad_norm": 0.008149882778525352,
      "learning_rate": 2.514644949278468e-07,
      "loss": 0.0007,
      "step": 69110
    },
    {
      "epoch": 14.81354479211316,
      "grad_norm": 0.006953940261155367,
      "learning_rate": 2.4860694384912134e-07,
      "loss": 0.0001,
      "step": 69120
    },
    {
      "epoch": 14.815687955422202,
      "grad_norm": 0.0040291245095431805,
      "learning_rate": 2.457493927703958e-07,
      "loss": 0.0001,
      "step": 69130
    },
    {
      "epoch": 14.817831118731247,
      "grad_norm": 0.052326880395412445,
      "learning_rate": 2.4289184169167025e-07,
      "loss": 0.1514,
      "step": 69140
    },
    {
      "epoch": 14.819974282040292,
      "grad_norm": 0.004312163684517145,
      "learning_rate": 2.400342906129447e-07,
      "loss": 0.1421,
      "step": 69150
    },
    {
      "epoch": 14.822117445349335,
      "grad_norm": 0.0009082888718694448,
      "learning_rate": 2.3717673953421917e-07,
      "loss": 0.1667,
      "step": 69160
    },
    {
      "epoch": 14.82426060865838,
      "grad_norm": 0.0356869101524353,
      "learning_rate": 2.3431918845549365e-07,
      "loss": 0.1311,
      "step": 69170
    },
    {
      "epoch": 14.826403771967424,
      "grad_norm": 0.0003399947891011834,
      "learning_rate": 2.314616373767681e-07,
      "loss": 0.0005,
      "step": 69180
    },
    {
      "epoch": 14.828546935276467,
      "grad_norm": 0.004278314299881458,
      "learning_rate": 2.286040862980426e-07,
      "loss": 0.0001,
      "step": 69190
    },
    {
      "epoch": 14.830690098585512,
      "grad_norm": 39.42283248901367,
      "learning_rate": 2.2574653521931705e-07,
      "loss": 0.2445,
      "step": 69200
    },
    {
      "epoch": 14.832833261894557,
      "grad_norm": 36.952144622802734,
      "learning_rate": 2.2288898414059156e-07,
      "loss": 0.3334,
      "step": 69210
    },
    {
      "epoch": 14.8349764252036,
      "grad_norm": 0.0020945812575519085,
      "learning_rate": 2.2003143306186602e-07,
      "loss": 0.0002,
      "step": 69220
    },
    {
      "epoch": 14.837119588512644,
      "grad_norm": 1982.44091796875,
      "learning_rate": 2.1717388198314048e-07,
      "loss": 0.1156,
      "step": 69230
    },
    {
      "epoch": 14.839262751821689,
      "grad_norm": 471.1180725097656,
      "learning_rate": 2.1431633090441496e-07,
      "loss": 0.1065,
      "step": 69240
    },
    {
      "epoch": 14.841405915130732,
      "grad_norm": 0.004530816804617643,
      "learning_rate": 2.1145877982568942e-07,
      "loss": 0.0001,
      "step": 69250
    },
    {
      "epoch": 14.843549078439777,
      "grad_norm": 0.0003187500697094947,
      "learning_rate": 2.0860122874696388e-07,
      "loss": 0.0001,
      "step": 69260
    },
    {
      "epoch": 14.845692241748822,
      "grad_norm": 0.02974964864552021,
      "learning_rate": 2.0574367766823836e-07,
      "loss": 0.1248,
      "step": 69270
    },
    {
      "epoch": 14.847835405057864,
      "grad_norm": 0.0007861988269723952,
      "learning_rate": 2.0288612658951282e-07,
      "loss": 0.0003,
      "step": 69280
    },
    {
      "epoch": 14.84997856836691,
      "grad_norm": 3.9010661566862836e-05,
      "learning_rate": 2.0002857551078727e-07,
      "loss": 0.0001,
      "step": 69290
    },
    {
      "epoch": 14.852121731675954,
      "grad_norm": 0.0026796457823365927,
      "learning_rate": 1.9717102443206176e-07,
      "loss": 0.3011,
      "step": 69300
    },
    {
      "epoch": 14.854264894984997,
      "grad_norm": 0.0004669048357754946,
      "learning_rate": 1.9431347335333622e-07,
      "loss": 0.0112,
      "step": 69310
    },
    {
      "epoch": 14.856408058294042,
      "grad_norm": 5.734129445045255e-05,
      "learning_rate": 1.9145592227461067e-07,
      "loss": 0.0001,
      "step": 69320
    },
    {
      "epoch": 14.858551221603086,
      "grad_norm": 0.0062803891487419605,
      "learning_rate": 1.8859837119588516e-07,
      "loss": 0.2938,
      "step": 69330
    },
    {
      "epoch": 14.860694384912131,
      "grad_norm": 0.7679603099822998,
      "learning_rate": 1.8574082011715962e-07,
      "loss": 0.0006,
      "step": 69340
    },
    {
      "epoch": 14.862837548221174,
      "grad_norm": 0.0023097556550055742,
      "learning_rate": 1.8288326903843407e-07,
      "loss": 0.0003,
      "step": 69350
    },
    {
      "epoch": 14.864980711530219,
      "grad_norm": 0.004882222041487694,
      "learning_rate": 1.8002571795970856e-07,
      "loss": 0.241,
      "step": 69360
    },
    {
      "epoch": 14.867123874839264,
      "grad_norm": 0.00119360804092139,
      "learning_rate": 1.7716816688098302e-07,
      "loss": 0.1937,
      "step": 69370
    },
    {
      "epoch": 14.869267038148307,
      "grad_norm": 0.0018831415800377727,
      "learning_rate": 1.7431061580225747e-07,
      "loss": 0.0001,
      "step": 69380
    },
    {
      "epoch": 14.871410201457351,
      "grad_norm": 0.0032749539241194725,
      "learning_rate": 1.7145306472353196e-07,
      "loss": 0.1168,
      "step": 69390
    },
    {
      "epoch": 14.873553364766396,
      "grad_norm": 0.0768708884716034,
      "learning_rate": 1.6859551364480642e-07,
      "loss": 0.0003,
      "step": 69400
    },
    {
      "epoch": 14.875696528075439,
      "grad_norm": 0.0014005046105012298,
      "learning_rate": 1.6573796256608087e-07,
      "loss": 0.0001,
      "step": 69410
    },
    {
      "epoch": 14.877839691384484,
      "grad_norm": 0.032274991273880005,
      "learning_rate": 1.6288041148735536e-07,
      "loss": 0.3084,
      "step": 69420
    },
    {
      "epoch": 14.879982854693528,
      "grad_norm": 0.0638040229678154,
      "learning_rate": 1.6002286040862981e-07,
      "loss": 0.1252,
      "step": 69430
    },
    {
      "epoch": 14.882126018002571,
      "grad_norm": 0.0029024071991443634,
      "learning_rate": 1.571653093299043e-07,
      "loss": 0.0001,
      "step": 69440
    },
    {
      "epoch": 14.884269181311616,
      "grad_norm": 0.0004249236371833831,
      "learning_rate": 1.5430775825117876e-07,
      "loss": 0.0003,
      "step": 69450
    },
    {
      "epoch": 14.88641234462066,
      "grad_norm": 0.12131178379058838,
      "learning_rate": 1.5145020717245321e-07,
      "loss": 0.0006,
      "step": 69460
    },
    {
      "epoch": 14.888555507929704,
      "grad_norm": 0.0052610659040510654,
      "learning_rate": 1.485926560937277e-07,
      "loss": 0.1221,
      "step": 69470
    },
    {
      "epoch": 14.890698671238749,
      "grad_norm": 0.011905711144208908,
      "learning_rate": 1.4573510501500216e-07,
      "loss": 0.1373,
      "step": 69480
    },
    {
      "epoch": 14.892841834547793,
      "grad_norm": 0.00017164964810945094,
      "learning_rate": 1.4287755393627661e-07,
      "loss": 0.1055,
      "step": 69490
    },
    {
      "epoch": 14.894984997856836,
      "grad_norm": 0.009205656126141548,
      "learning_rate": 1.400200028575511e-07,
      "loss": 0.0005,
      "step": 69500
    },
    {
      "epoch": 14.897128161165881,
      "grad_norm": 0.4014991819858551,
      "learning_rate": 1.3716245177882556e-07,
      "loss": 0.0006,
      "step": 69510
    },
    {
      "epoch": 14.899271324474926,
      "grad_norm": 0.0007208619499579072,
      "learning_rate": 1.343049007001e-07,
      "loss": 0.0001,
      "step": 69520
    },
    {
      "epoch": 14.901414487783969,
      "grad_norm": 0.0029234648682177067,
      "learning_rate": 1.314473496213745e-07,
      "loss": 0.0014,
      "step": 69530
    },
    {
      "epoch": 14.903557651093013,
      "grad_norm": 0.07160015404224396,
      "learning_rate": 1.2858979854264895e-07,
      "loss": 0.001,
      "step": 69540
    },
    {
      "epoch": 14.905700814402058,
      "grad_norm": 0.008366151712834835,
      "learning_rate": 1.257322474639234e-07,
      "loss": 0.0001,
      "step": 69550
    },
    {
      "epoch": 14.907843977711101,
      "grad_norm": 5.01830036228057e-05,
      "learning_rate": 1.228746963851979e-07,
      "loss": 0.0864,
      "step": 69560
    },
    {
      "epoch": 14.909987141020146,
      "grad_norm": 0.11326202005147934,
      "learning_rate": 1.2001714530647235e-07,
      "loss": 0.0005,
      "step": 69570
    },
    {
      "epoch": 14.91213030432919,
      "grad_norm": 4.093595271115191e-05,
      "learning_rate": 1.1715959422774683e-07,
      "loss": 0.0,
      "step": 69580
    },
    {
      "epoch": 14.914273467638234,
      "grad_norm": 0.010499815456569195,
      "learning_rate": 1.143020431490213e-07,
      "loss": 0.0001,
      "step": 69590
    },
    {
      "epoch": 14.916416630947278,
      "grad_norm": 0.0015806135488674045,
      "learning_rate": 1.1144449207029578e-07,
      "loss": 0.2492,
      "step": 69600
    },
    {
      "epoch": 14.918559794256323,
      "grad_norm": 0.00842383410781622,
      "learning_rate": 1.0858694099157024e-07,
      "loss": 0.2097,
      "step": 69610
    },
    {
      "epoch": 14.920702957565366,
      "grad_norm": 0.19332924485206604,
      "learning_rate": 1.0572938991284471e-07,
      "loss": 0.0006,
      "step": 69620
    },
    {
      "epoch": 14.92284612087441,
      "grad_norm": 0.0012607569806277752,
      "learning_rate": 1.0287183883411918e-07,
      "loss": 0.0001,
      "step": 69630
    },
    {
      "epoch": 14.924989284183455,
      "grad_norm": 0.002859540283679962,
      "learning_rate": 1.0001428775539364e-07,
      "loss": 0.0001,
      "step": 69640
    },
    {
      "epoch": 14.927132447492498,
      "grad_norm": 0.003991013392806053,
      "learning_rate": 9.715673667666811e-08,
      "loss": 0.0001,
      "step": 69650
    },
    {
      "epoch": 14.929275610801543,
      "grad_norm": 0.00011848275607917458,
      "learning_rate": 9.429918559794258e-08,
      "loss": 0.2163,
      "step": 69660
    },
    {
      "epoch": 14.931418774110588,
      "grad_norm": 0.016125790774822235,
      "learning_rate": 9.144163451921704e-08,
      "loss": 0.0004,
      "step": 69670
    },
    {
      "epoch": 14.933561937419631,
      "grad_norm": 0.0013082015793770552,
      "learning_rate": 8.858408344049151e-08,
      "loss": 0.0001,
      "step": 69680
    },
    {
      "epoch": 14.935705100728676,
      "grad_norm": 0.00015807300223968923,
      "learning_rate": 8.572653236176598e-08,
      "loss": 0.2184,
      "step": 69690
    },
    {
      "epoch": 14.93784826403772,
      "grad_norm": 0.00713846692815423,
      "learning_rate": 8.286898128304044e-08,
      "loss": 0.4838,
      "step": 69700
    },
    {
      "epoch": 14.939991427346763,
      "grad_norm": 0.00136546918656677,
      "learning_rate": 8.001143020431491e-08,
      "loss": 0.0,
      "step": 69710
    },
    {
      "epoch": 14.942134590655808,
      "grad_norm": 0.0003912069951184094,
      "learning_rate": 7.715387912558938e-08,
      "loss": 0.1537,
      "step": 69720
    },
    {
      "epoch": 14.944277753964853,
      "grad_norm": 0.00012330243771430105,
      "learning_rate": 7.429632804686385e-08,
      "loss": 0.0656,
      "step": 69730
    },
    {
      "epoch": 14.946420917273896,
      "grad_norm": 0.005702865310013294,
      "learning_rate": 7.143877696813831e-08,
      "loss": 0.2555,
      "step": 69740
    },
    {
      "epoch": 14.94856408058294,
      "grad_norm": 22.917640686035156,
      "learning_rate": 6.858122588941278e-08,
      "loss": 0.4252,
      "step": 69750
    },
    {
      "epoch": 14.950707243891985,
      "grad_norm": 0.005919214338064194,
      "learning_rate": 6.572367481068725e-08,
      "loss": 0.0001,
      "step": 69760
    },
    {
      "epoch": 14.952850407201028,
      "grad_norm": 5.391311788116582e-05,
      "learning_rate": 6.28661237319617e-08,
      "loss": 0.0,
      "step": 69770
    },
    {
      "epoch": 14.954993570510073,
      "grad_norm": 0.0027855618391186,
      "learning_rate": 6.000857265323618e-08,
      "loss": 0.0002,
      "step": 69780
    },
    {
      "epoch": 14.957136733819118,
      "grad_norm": 0.0010689377086237073,
      "learning_rate": 5.715102157451065e-08,
      "loss": 0.0004,
      "step": 69790
    },
    {
      "epoch": 14.95927989712816,
      "grad_norm": 0.00013836940343026072,
      "learning_rate": 5.429347049578512e-08,
      "loss": 0.0008,
      "step": 69800
    },
    {
      "epoch": 14.961423060437205,
      "grad_norm": 0.05910663306713104,
      "learning_rate": 5.143591941705959e-08,
      "loss": 0.0001,
      "step": 69810
    },
    {
      "epoch": 14.96356622374625,
      "grad_norm": 0.004716057796031237,
      "learning_rate": 4.8578368338334054e-08,
      "loss": 0.0013,
      "step": 69820
    },
    {
      "epoch": 14.965709387055293,
      "grad_norm": 0.00010853483399841934,
      "learning_rate": 4.572081725960852e-08,
      "loss": 0.0005,
      "step": 69830
    },
    {
      "epoch": 14.967852550364338,
      "grad_norm": 0.005865518003702164,
      "learning_rate": 4.286326618088299e-08,
      "loss": 0.0,
      "step": 69840
    },
    {
      "epoch": 14.969995713673383,
      "grad_norm": 0.38734304904937744,
      "learning_rate": 4.0005715102157454e-08,
      "loss": 0.0018,
      "step": 69850
    },
    {
      "epoch": 14.972138876982426,
      "grad_norm": 0.006057164166122675,
      "learning_rate": 3.7148164023431925e-08,
      "loss": 0.0007,
      "step": 69860
    },
    {
      "epoch": 14.97428204029147,
      "grad_norm": 0.004147053696215153,
      "learning_rate": 3.429061294470639e-08,
      "loss": 0.0007,
      "step": 69870
    },
    {
      "epoch": 14.976425203600515,
      "grad_norm": 0.00017842714441940188,
      "learning_rate": 3.143306186598085e-08,
      "loss": 0.0003,
      "step": 69880
    },
    {
      "epoch": 14.978568366909558,
      "grad_norm": 0.00023719151795376092,
      "learning_rate": 2.8575510787255324e-08,
      "loss": 0.0001,
      "step": 69890
    },
    {
      "epoch": 14.980711530218603,
      "grad_norm": 0.08775074779987335,
      "learning_rate": 2.5717959708529795e-08,
      "loss": 0.257,
      "step": 69900
    },
    {
      "epoch": 14.982854693527647,
      "grad_norm": 0.012157709337770939,
      "learning_rate": 2.286040862980426e-08,
      "loss": 0.0001,
      "step": 69910
    },
    {
      "epoch": 14.98499785683669,
      "grad_norm": 0.8501979112625122,
      "learning_rate": 2.0002857551078727e-08,
      "loss": 0.1909,
      "step": 69920
    },
    {
      "epoch": 14.987141020145735,
      "grad_norm": 25.578563690185547,
      "learning_rate": 1.7145306472353194e-08,
      "loss": 0.1213,
      "step": 69930
    },
    {
      "epoch": 14.98928418345478,
      "grad_norm": 0.006524831522256136,
      "learning_rate": 1.4287755393627662e-08,
      "loss": 0.0001,
      "step": 69940
    },
    {
      "epoch": 14.991427346763823,
      "grad_norm": 0.005122786853462458,
      "learning_rate": 1.143020431490213e-08,
      "loss": 0.0005,
      "step": 69950
    },
    {
      "epoch": 14.993570510072868,
      "grad_norm": 0.0015393879730254412,
      "learning_rate": 8.572653236176597e-09,
      "loss": 0.2815,
      "step": 69960
    },
    {
      "epoch": 14.995713673381912,
      "grad_norm": 0.00818261131644249,
      "learning_rate": 5.715102157451065e-09,
      "loss": 0.0001,
      "step": 69970
    },
    {
      "epoch": 14.997856836690955,
      "grad_norm": 0.021128986030817032,
      "learning_rate": 2.8575510787255324e-09,
      "loss": 0.0021,
      "step": 69980
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.04279766231775284,
      "learning_rate": 0.0,
      "loss": 0.0006,
      "step": 69990
    },
    {
      "epoch": 15.0,
      "eval_accuracy": 0.969,
      "eval_f1": 0.8748317631224766,
      "eval_loss": 0.19964353740215302,
      "eval_precision": 0.8928571428571429,
      "eval_recall": 0.8575197889182058,
      "eval_runtime": 428.8633,
      "eval_samples_per_second": 6.995,
      "eval_steps_per_second": 2.332,
      "step": 69990
    }
  ],
  "logging_steps": 10,
  "max_steps": 69990,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.0228207609094317e+17,
  "train_batch_size": 3,
  "trial_name": null,
  "trial_params": null
}
