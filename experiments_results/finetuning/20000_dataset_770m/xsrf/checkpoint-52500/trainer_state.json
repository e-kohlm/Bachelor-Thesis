{
  "best_metric": 0.7442528735632185,
  "best_model_checkpoint": "../saved_models/xsrf_20000_770/checkpoint-52500",
  "epoch": 15.0,
  "eval_steps": 500,
  "global_step": 52500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00028571428571428574,
      "grad_norm": 55.09519577026367,
      "learning_rate": 1.999961904761905e-05,
      "loss": 0.4503,
      "step": 1
    },
    {
      "epoch": 0.002857142857142857,
      "grad_norm": 8.500561714172363,
      "learning_rate": 1.9996190476190477e-05,
      "loss": 0.3615,
      "step": 10
    },
    {
      "epoch": 0.005714285714285714,
      "grad_norm": 67.42021179199219,
      "learning_rate": 1.9992380952380953e-05,
      "loss": 0.7963,
      "step": 20
    },
    {
      "epoch": 0.008571428571428572,
      "grad_norm": 0.3785379230976105,
      "learning_rate": 1.9988571428571432e-05,
      "loss": 0.4631,
      "step": 30
    },
    {
      "epoch": 0.011428571428571429,
      "grad_norm": 62.108890533447266,
      "learning_rate": 1.9984761904761907e-05,
      "loss": 0.8978,
      "step": 40
    },
    {
      "epoch": 0.014285714285714285,
      "grad_norm": 60.437313079833984,
      "learning_rate": 1.9980952380952383e-05,
      "loss": 0.9036,
      "step": 50
    },
    {
      "epoch": 0.017142857142857144,
      "grad_norm": 0.10423482209444046,
      "learning_rate": 1.997714285714286e-05,
      "loss": 0.3258,
      "step": 60
    },
    {
      "epoch": 0.02,
      "grad_norm": 2.7046265602111816,
      "learning_rate": 1.9973333333333334e-05,
      "loss": 1.1483,
      "step": 70
    },
    {
      "epoch": 0.022857142857142857,
      "grad_norm": 51.40941619873047,
      "learning_rate": 1.9969523809523813e-05,
      "loss": 0.793,
      "step": 80
    },
    {
      "epoch": 0.025714285714285714,
      "grad_norm": 0.28653207421302795,
      "learning_rate": 1.996571428571429e-05,
      "loss": 0.3631,
      "step": 90
    },
    {
      "epoch": 0.02857142857142857,
      "grad_norm": 66.28955841064453,
      "learning_rate": 1.9961904761904764e-05,
      "loss": 1.4114,
      "step": 100
    },
    {
      "epoch": 0.03142857142857143,
      "grad_norm": 67.26824188232422,
      "learning_rate": 1.995809523809524e-05,
      "loss": 0.7398,
      "step": 110
    },
    {
      "epoch": 0.03428571428571429,
      "grad_norm": 2.8743813037872314,
      "learning_rate": 1.9954285714285715e-05,
      "loss": 0.5243,
      "step": 120
    },
    {
      "epoch": 0.037142857142857144,
      "grad_norm": 52.79320526123047,
      "learning_rate": 1.995047619047619e-05,
      "loss": 0.5422,
      "step": 130
    },
    {
      "epoch": 0.04,
      "grad_norm": 47.93938064575195,
      "learning_rate": 1.9946666666666667e-05,
      "loss": 0.6171,
      "step": 140
    },
    {
      "epoch": 0.04285714285714286,
      "grad_norm": 79.40613555908203,
      "learning_rate": 1.9942857142857142e-05,
      "loss": 0.6442,
      "step": 150
    },
    {
      "epoch": 0.045714285714285714,
      "grad_norm": 42.29698181152344,
      "learning_rate": 1.993904761904762e-05,
      "loss": 0.6106,
      "step": 160
    },
    {
      "epoch": 0.04857142857142857,
      "grad_norm": 7.217770099639893,
      "learning_rate": 1.9935238095238097e-05,
      "loss": 0.9007,
      "step": 170
    },
    {
      "epoch": 0.05142857142857143,
      "grad_norm": 0.045715730637311935,
      "learning_rate": 1.9931428571428572e-05,
      "loss": 0.3435,
      "step": 180
    },
    {
      "epoch": 0.054285714285714284,
      "grad_norm": 0.8202230334281921,
      "learning_rate": 1.9927619047619048e-05,
      "loss": 1.5618,
      "step": 190
    },
    {
      "epoch": 0.05714285714285714,
      "grad_norm": 0.5254529714584351,
      "learning_rate": 1.9923809523809527e-05,
      "loss": 0.6503,
      "step": 200
    },
    {
      "epoch": 0.06,
      "grad_norm": 0.030347084626555443,
      "learning_rate": 1.9920000000000002e-05,
      "loss": 0.075,
      "step": 210
    },
    {
      "epoch": 0.06285714285714286,
      "grad_norm": 0.11661780625581741,
      "learning_rate": 1.9916190476190478e-05,
      "loss": 0.8052,
      "step": 220
    },
    {
      "epoch": 0.06571428571428571,
      "grad_norm": 56.458290100097656,
      "learning_rate": 1.9912380952380954e-05,
      "loss": 0.6453,
      "step": 230
    },
    {
      "epoch": 0.06857142857142857,
      "grad_norm": 37.11115646362305,
      "learning_rate": 1.990857142857143e-05,
      "loss": 0.5125,
      "step": 240
    },
    {
      "epoch": 0.07142857142857142,
      "grad_norm": 13.329779624938965,
      "learning_rate": 1.9904761904761908e-05,
      "loss": 0.7205,
      "step": 250
    },
    {
      "epoch": 0.07428571428571429,
      "grad_norm": 42.95119857788086,
      "learning_rate": 1.9900952380952384e-05,
      "loss": 0.887,
      "step": 260
    },
    {
      "epoch": 0.07714285714285714,
      "grad_norm": 36.89399337768555,
      "learning_rate": 1.989714285714286e-05,
      "loss": 0.6946,
      "step": 270
    },
    {
      "epoch": 0.08,
      "grad_norm": 141.09378051757812,
      "learning_rate": 1.9893333333333335e-05,
      "loss": 0.6034,
      "step": 280
    },
    {
      "epoch": 0.08285714285714285,
      "grad_norm": 34.01966857910156,
      "learning_rate": 1.988952380952381e-05,
      "loss": 0.9281,
      "step": 290
    },
    {
      "epoch": 0.08571428571428572,
      "grad_norm": 0.1736118048429489,
      "learning_rate": 1.988571428571429e-05,
      "loss": 0.6352,
      "step": 300
    },
    {
      "epoch": 0.08857142857142856,
      "grad_norm": 0.966469407081604,
      "learning_rate": 1.9881904761904765e-05,
      "loss": 0.8086,
      "step": 310
    },
    {
      "epoch": 0.09142857142857143,
      "grad_norm": 35.80792236328125,
      "learning_rate": 1.987809523809524e-05,
      "loss": 0.5292,
      "step": 320
    },
    {
      "epoch": 0.09428571428571429,
      "grad_norm": 28.896881103515625,
      "learning_rate": 1.9874285714285716e-05,
      "loss": 0.634,
      "step": 330
    },
    {
      "epoch": 0.09714285714285714,
      "grad_norm": 4.893095970153809,
      "learning_rate": 1.9870476190476192e-05,
      "loss": 0.8736,
      "step": 340
    },
    {
      "epoch": 0.1,
      "grad_norm": 33.19411087036133,
      "learning_rate": 1.9866666666666667e-05,
      "loss": 0.3603,
      "step": 350
    },
    {
      "epoch": 0.10285714285714286,
      "grad_norm": 0.08287229388952255,
      "learning_rate": 1.9862857142857143e-05,
      "loss": 0.311,
      "step": 360
    },
    {
      "epoch": 0.10571428571428572,
      "grad_norm": 25.902433395385742,
      "learning_rate": 1.985904761904762e-05,
      "loss": 1.101,
      "step": 370
    },
    {
      "epoch": 0.10857142857142857,
      "grad_norm": 25.826580047607422,
      "learning_rate": 1.9855238095238097e-05,
      "loss": 0.5006,
      "step": 380
    },
    {
      "epoch": 0.11142857142857143,
      "grad_norm": 0.08755390346050262,
      "learning_rate": 1.9851428571428573e-05,
      "loss": 0.1775,
      "step": 390
    },
    {
      "epoch": 0.11428571428571428,
      "grad_norm": 0.1824149787425995,
      "learning_rate": 1.984761904761905e-05,
      "loss": 0.8683,
      "step": 400
    },
    {
      "epoch": 0.11714285714285715,
      "grad_norm": 36.2433967590332,
      "learning_rate": 1.9843809523809524e-05,
      "loss": 0.5689,
      "step": 410
    },
    {
      "epoch": 0.12,
      "grad_norm": 0.9743809700012207,
      "learning_rate": 1.9840000000000003e-05,
      "loss": 0.7077,
      "step": 420
    },
    {
      "epoch": 0.12285714285714286,
      "grad_norm": 0.3476428985595703,
      "learning_rate": 1.983619047619048e-05,
      "loss": 0.5283,
      "step": 430
    },
    {
      "epoch": 0.12571428571428572,
      "grad_norm": 0.06211766600608826,
      "learning_rate": 1.9832380952380954e-05,
      "loss": 0.0039,
      "step": 440
    },
    {
      "epoch": 0.12857142857142856,
      "grad_norm": 21.339868545532227,
      "learning_rate": 1.982857142857143e-05,
      "loss": 1.049,
      "step": 450
    },
    {
      "epoch": 0.13142857142857142,
      "grad_norm": 19.381959915161133,
      "learning_rate": 1.9824761904761905e-05,
      "loss": 0.6655,
      "step": 460
    },
    {
      "epoch": 0.13428571428571429,
      "grad_norm": 15.036827087402344,
      "learning_rate": 1.9820952380952384e-05,
      "loss": 0.8383,
      "step": 470
    },
    {
      "epoch": 0.13714285714285715,
      "grad_norm": 0.8209059238433838,
      "learning_rate": 1.981714285714286e-05,
      "loss": 0.5999,
      "step": 480
    },
    {
      "epoch": 0.14,
      "grad_norm": 17.393848419189453,
      "learning_rate": 1.9813333333333336e-05,
      "loss": 0.7472,
      "step": 490
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 19.91798973083496,
      "learning_rate": 1.980952380952381e-05,
      "loss": 0.3268,
      "step": 500
    },
    {
      "epoch": 0.1457142857142857,
      "grad_norm": 5.6947922706604,
      "learning_rate": 1.9805714285714287e-05,
      "loss": 0.6914,
      "step": 510
    },
    {
      "epoch": 0.14857142857142858,
      "grad_norm": 1.3055856227874756,
      "learning_rate": 1.9801904761904766e-05,
      "loss": 0.1818,
      "step": 520
    },
    {
      "epoch": 0.15142857142857144,
      "grad_norm": 0.5999532341957092,
      "learning_rate": 1.979809523809524e-05,
      "loss": 0.7721,
      "step": 530
    },
    {
      "epoch": 0.15428571428571428,
      "grad_norm": 0.42665204405784607,
      "learning_rate": 1.9794285714285717e-05,
      "loss": 0.4739,
      "step": 540
    },
    {
      "epoch": 0.15714285714285714,
      "grad_norm": 0.212212473154068,
      "learning_rate": 1.9790476190476193e-05,
      "loss": 0.4243,
      "step": 550
    },
    {
      "epoch": 0.16,
      "grad_norm": 14.990283012390137,
      "learning_rate": 1.9786666666666668e-05,
      "loss": 0.9473,
      "step": 560
    },
    {
      "epoch": 0.16285714285714287,
      "grad_norm": 13.066901206970215,
      "learning_rate": 1.9782857142857144e-05,
      "loss": 0.7948,
      "step": 570
    },
    {
      "epoch": 0.1657142857142857,
      "grad_norm": 0.6230149269104004,
      "learning_rate": 1.977904761904762e-05,
      "loss": 0.4343,
      "step": 580
    },
    {
      "epoch": 0.16857142857142857,
      "grad_norm": 15.696584701538086,
      "learning_rate": 1.9775238095238095e-05,
      "loss": 0.5118,
      "step": 590
    },
    {
      "epoch": 0.17142857142857143,
      "grad_norm": 1.2436010837554932,
      "learning_rate": 1.9771428571428574e-05,
      "loss": 0.8022,
      "step": 600
    },
    {
      "epoch": 0.1742857142857143,
      "grad_norm": 0.40198758244514465,
      "learning_rate": 1.976761904761905e-05,
      "loss": 0.3246,
      "step": 610
    },
    {
      "epoch": 0.17714285714285713,
      "grad_norm": 14.300725936889648,
      "learning_rate": 1.9763809523809525e-05,
      "loss": 0.5997,
      "step": 620
    },
    {
      "epoch": 0.18,
      "grad_norm": 13.315069198608398,
      "learning_rate": 1.976e-05,
      "loss": 0.7853,
      "step": 630
    },
    {
      "epoch": 0.18285714285714286,
      "grad_norm": 2.2871837615966797,
      "learning_rate": 1.9756190476190476e-05,
      "loss": 0.7391,
      "step": 640
    },
    {
      "epoch": 0.18571428571428572,
      "grad_norm": 1.8269257545471191,
      "learning_rate": 1.9752380952380955e-05,
      "loss": 0.4548,
      "step": 650
    },
    {
      "epoch": 0.18857142857142858,
      "grad_norm": 0.4656664729118347,
      "learning_rate": 1.974857142857143e-05,
      "loss": 0.6163,
      "step": 660
    },
    {
      "epoch": 0.19142857142857142,
      "grad_norm": 0.24909529089927673,
      "learning_rate": 1.9744761904761906e-05,
      "loss": 0.6552,
      "step": 670
    },
    {
      "epoch": 0.19428571428571428,
      "grad_norm": 16.414323806762695,
      "learning_rate": 1.9740952380952382e-05,
      "loss": 0.6301,
      "step": 680
    },
    {
      "epoch": 0.19714285714285715,
      "grad_norm": 15.387165069580078,
      "learning_rate": 1.973714285714286e-05,
      "loss": 0.554,
      "step": 690
    },
    {
      "epoch": 0.2,
      "grad_norm": 0.869575560092926,
      "learning_rate": 1.9733333333333336e-05,
      "loss": 0.4999,
      "step": 700
    },
    {
      "epoch": 0.20285714285714285,
      "grad_norm": 16.13109588623047,
      "learning_rate": 1.9729523809523812e-05,
      "loss": 0.6397,
      "step": 710
    },
    {
      "epoch": 0.2057142857142857,
      "grad_norm": 0.30003535747528076,
      "learning_rate": 1.9725714285714288e-05,
      "loss": 0.3516,
      "step": 720
    },
    {
      "epoch": 0.20857142857142857,
      "grad_norm": 15.299036979675293,
      "learning_rate": 1.9721904761904763e-05,
      "loss": 0.4498,
      "step": 730
    },
    {
      "epoch": 0.21142857142857144,
      "grad_norm": 15.495351791381836,
      "learning_rate": 1.9718095238095242e-05,
      "loss": 0.9116,
      "step": 740
    },
    {
      "epoch": 0.21428571428571427,
      "grad_norm": 3.2534432411193848,
      "learning_rate": 1.9714285714285718e-05,
      "loss": 0.5996,
      "step": 750
    },
    {
      "epoch": 0.21714285714285714,
      "grad_norm": 0.5800557136535645,
      "learning_rate": 1.971047619047619e-05,
      "loss": 0.4715,
      "step": 760
    },
    {
      "epoch": 0.22,
      "grad_norm": 0.20781973004341125,
      "learning_rate": 1.970666666666667e-05,
      "loss": 0.6848,
      "step": 770
    },
    {
      "epoch": 0.22285714285714286,
      "grad_norm": 0.19376172125339508,
      "learning_rate": 1.9702857142857144e-05,
      "loss": 0.1317,
      "step": 780
    },
    {
      "epoch": 0.2257142857142857,
      "grad_norm": 17.843101501464844,
      "learning_rate": 1.969904761904762e-05,
      "loss": 0.9294,
      "step": 790
    },
    {
      "epoch": 0.22857142857142856,
      "grad_norm": 0.6375122666358948,
      "learning_rate": 1.9695238095238096e-05,
      "loss": 0.385,
      "step": 800
    },
    {
      "epoch": 0.23142857142857143,
      "grad_norm": 0.186645045876503,
      "learning_rate": 1.969142857142857e-05,
      "loss": 0.5172,
      "step": 810
    },
    {
      "epoch": 0.2342857142857143,
      "grad_norm": 0.228572815656662,
      "learning_rate": 1.968761904761905e-05,
      "loss": 0.2823,
      "step": 820
    },
    {
      "epoch": 0.23714285714285716,
      "grad_norm": 0.37463638186454773,
      "learning_rate": 1.9683809523809526e-05,
      "loss": 0.5519,
      "step": 830
    },
    {
      "epoch": 0.24,
      "grad_norm": 27.317541122436523,
      "learning_rate": 1.968e-05,
      "loss": 0.8682,
      "step": 840
    },
    {
      "epoch": 0.24285714285714285,
      "grad_norm": 1.3688349723815918,
      "learning_rate": 1.9676190476190477e-05,
      "loss": 0.6668,
      "step": 850
    },
    {
      "epoch": 0.24571428571428572,
      "grad_norm": 27.03592872619629,
      "learning_rate": 1.9672380952380952e-05,
      "loss": 0.5464,
      "step": 860
    },
    {
      "epoch": 0.24857142857142858,
      "grad_norm": 0.33241260051727295,
      "learning_rate": 1.966857142857143e-05,
      "loss": 0.5676,
      "step": 870
    },
    {
      "epoch": 0.25142857142857145,
      "grad_norm": 14.758585929870605,
      "learning_rate": 1.9664761904761907e-05,
      "loss": 0.6657,
      "step": 880
    },
    {
      "epoch": 0.2542857142857143,
      "grad_norm": 0.772950291633606,
      "learning_rate": 1.9660952380952383e-05,
      "loss": 0.4409,
      "step": 890
    },
    {
      "epoch": 0.2571428571428571,
      "grad_norm": 0.23201261460781097,
      "learning_rate": 1.9657142857142858e-05,
      "loss": 0.5349,
      "step": 900
    },
    {
      "epoch": 0.26,
      "grad_norm": 0.22053630650043488,
      "learning_rate": 1.9653333333333334e-05,
      "loss": 0.406,
      "step": 910
    },
    {
      "epoch": 0.26285714285714284,
      "grad_norm": 16.57174301147461,
      "learning_rate": 1.9649523809523813e-05,
      "loss": 0.9423,
      "step": 920
    },
    {
      "epoch": 0.26571428571428574,
      "grad_norm": 0.3473609983921051,
      "learning_rate": 1.964571428571429e-05,
      "loss": 0.3684,
      "step": 930
    },
    {
      "epoch": 0.26857142857142857,
      "grad_norm": 0.815921425819397,
      "learning_rate": 1.9641904761904764e-05,
      "loss": 0.6912,
      "step": 940
    },
    {
      "epoch": 0.2714285714285714,
      "grad_norm": 0.39312517642974854,
      "learning_rate": 1.963809523809524e-05,
      "loss": 0.5498,
      "step": 950
    },
    {
      "epoch": 0.2742857142857143,
      "grad_norm": 1.366208553314209,
      "learning_rate": 1.963428571428572e-05,
      "loss": 0.7582,
      "step": 960
    },
    {
      "epoch": 0.27714285714285714,
      "grad_norm": 0.3582535684108734,
      "learning_rate": 1.9630476190476194e-05,
      "loss": 0.7242,
      "step": 970
    },
    {
      "epoch": 0.28,
      "grad_norm": 0.23941145837306976,
      "learning_rate": 1.9626666666666666e-05,
      "loss": 0.5054,
      "step": 980
    },
    {
      "epoch": 0.28285714285714286,
      "grad_norm": 0.5502306222915649,
      "learning_rate": 1.9622857142857142e-05,
      "loss": 0.4656,
      "step": 990
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 20.788726806640625,
      "learning_rate": 1.961904761904762e-05,
      "loss": 0.563,
      "step": 1000
    },
    {
      "epoch": 0.2885714285714286,
      "grad_norm": 19.049848556518555,
      "learning_rate": 1.9615238095238096e-05,
      "loss": 0.5935,
      "step": 1010
    },
    {
      "epoch": 0.2914285714285714,
      "grad_norm": 35.376922607421875,
      "learning_rate": 1.9611428571428572e-05,
      "loss": 0.7666,
      "step": 1020
    },
    {
      "epoch": 0.29428571428571426,
      "grad_norm": 21.97226333618164,
      "learning_rate": 1.9607619047619048e-05,
      "loss": 0.5695,
      "step": 1030
    },
    {
      "epoch": 0.29714285714285715,
      "grad_norm": 2.4859938621520996,
      "learning_rate": 1.9603809523809526e-05,
      "loss": 0.3003,
      "step": 1040
    },
    {
      "epoch": 0.3,
      "grad_norm": 0.11138644814491272,
      "learning_rate": 1.9600000000000002e-05,
      "loss": 0.3066,
      "step": 1050
    },
    {
      "epoch": 0.3028571428571429,
      "grad_norm": 14.836761474609375,
      "learning_rate": 1.9596190476190478e-05,
      "loss": 0.842,
      "step": 1060
    },
    {
      "epoch": 0.3057142857142857,
      "grad_norm": 17.639638900756836,
      "learning_rate": 1.9592380952380953e-05,
      "loss": 0.6713,
      "step": 1070
    },
    {
      "epoch": 0.30857142857142855,
      "grad_norm": 0.3894665837287903,
      "learning_rate": 1.958857142857143e-05,
      "loss": 0.34,
      "step": 1080
    },
    {
      "epoch": 0.31142857142857144,
      "grad_norm": 0.3690527081489563,
      "learning_rate": 1.9584761904761908e-05,
      "loss": 0.6877,
      "step": 1090
    },
    {
      "epoch": 0.3142857142857143,
      "grad_norm": 17.689697265625,
      "learning_rate": 1.9580952380952383e-05,
      "loss": 0.673,
      "step": 1100
    },
    {
      "epoch": 0.3171428571428571,
      "grad_norm": 17.029809951782227,
      "learning_rate": 1.957714285714286e-05,
      "loss": 0.6208,
      "step": 1110
    },
    {
      "epoch": 0.32,
      "grad_norm": 14.470847129821777,
      "learning_rate": 1.9573333333333335e-05,
      "loss": 0.5669,
      "step": 1120
    },
    {
      "epoch": 0.32285714285714284,
      "grad_norm": 1.4389289617538452,
      "learning_rate": 1.956952380952381e-05,
      "loss": 0.6512,
      "step": 1130
    },
    {
      "epoch": 0.32571428571428573,
      "grad_norm": 0.39851483702659607,
      "learning_rate": 1.956571428571429e-05,
      "loss": 0.6852,
      "step": 1140
    },
    {
      "epoch": 0.32857142857142857,
      "grad_norm": 26.13707733154297,
      "learning_rate": 1.9561904761904765e-05,
      "loss": 0.7387,
      "step": 1150
    },
    {
      "epoch": 0.3314285714285714,
      "grad_norm": 1.0372698307037354,
      "learning_rate": 1.955809523809524e-05,
      "loss": 0.6968,
      "step": 1160
    },
    {
      "epoch": 0.3342857142857143,
      "grad_norm": 12.985828399658203,
      "learning_rate": 1.9554285714285716e-05,
      "loss": 0.5299,
      "step": 1170
    },
    {
      "epoch": 0.33714285714285713,
      "grad_norm": 0.39678430557250977,
      "learning_rate": 1.955047619047619e-05,
      "loss": 0.2391,
      "step": 1180
    },
    {
      "epoch": 0.34,
      "grad_norm": 13.602680206298828,
      "learning_rate": 1.954666666666667e-05,
      "loss": 0.2802,
      "step": 1190
    },
    {
      "epoch": 0.34285714285714286,
      "grad_norm": 0.32423219084739685,
      "learning_rate": 1.9542857142857143e-05,
      "loss": 1.2197,
      "step": 1200
    },
    {
      "epoch": 0.3457142857142857,
      "grad_norm": 0.7754684686660767,
      "learning_rate": 1.9539047619047618e-05,
      "loss": 0.6594,
      "step": 1210
    },
    {
      "epoch": 0.3485714285714286,
      "grad_norm": 1.4912855625152588,
      "learning_rate": 1.9535238095238097e-05,
      "loss": 0.608,
      "step": 1220
    },
    {
      "epoch": 0.3514285714285714,
      "grad_norm": 16.073902130126953,
      "learning_rate": 1.9531428571428573e-05,
      "loss": 0.4993,
      "step": 1230
    },
    {
      "epoch": 0.35428571428571426,
      "grad_norm": 25.159034729003906,
      "learning_rate": 1.9527619047619048e-05,
      "loss": 0.8499,
      "step": 1240
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 11.525607109069824,
      "learning_rate": 1.9523809523809524e-05,
      "loss": 0.5283,
      "step": 1250
    },
    {
      "epoch": 0.36,
      "grad_norm": 15.017203330993652,
      "learning_rate": 1.9520000000000003e-05,
      "loss": 0.1951,
      "step": 1260
    },
    {
      "epoch": 0.3628571428571429,
      "grad_norm": 12.710832595825195,
      "learning_rate": 1.951619047619048e-05,
      "loss": 0.9522,
      "step": 1270
    },
    {
      "epoch": 0.3657142857142857,
      "grad_norm": 0.49266621470451355,
      "learning_rate": 1.9512380952380954e-05,
      "loss": 0.4079,
      "step": 1280
    },
    {
      "epoch": 0.36857142857142855,
      "grad_norm": 0.33838412165641785,
      "learning_rate": 1.950857142857143e-05,
      "loss": 0.6418,
      "step": 1290
    },
    {
      "epoch": 0.37142857142857144,
      "grad_norm": 12.258347511291504,
      "learning_rate": 1.9504761904761905e-05,
      "loss": 0.5544,
      "step": 1300
    },
    {
      "epoch": 0.3742857142857143,
      "grad_norm": 0.5552758574485779,
      "learning_rate": 1.9500952380952384e-05,
      "loss": 0.565,
      "step": 1310
    },
    {
      "epoch": 0.37714285714285717,
      "grad_norm": 14.347989082336426,
      "learning_rate": 1.949714285714286e-05,
      "loss": 0.5241,
      "step": 1320
    },
    {
      "epoch": 0.38,
      "grad_norm": 12.088249206542969,
      "learning_rate": 1.9493333333333335e-05,
      "loss": 0.636,
      "step": 1330
    },
    {
      "epoch": 0.38285714285714284,
      "grad_norm": 1.4642541408538818,
      "learning_rate": 1.948952380952381e-05,
      "loss": 0.4779,
      "step": 1340
    },
    {
      "epoch": 0.38571428571428573,
      "grad_norm": 1.4755691289901733,
      "learning_rate": 1.9485714285714286e-05,
      "loss": 0.7452,
      "step": 1350
    },
    {
      "epoch": 0.38857142857142857,
      "grad_norm": 15.817530632019043,
      "learning_rate": 1.9481904761904765e-05,
      "loss": 0.2865,
      "step": 1360
    },
    {
      "epoch": 0.3914285714285714,
      "grad_norm": 0.08084215968847275,
      "learning_rate": 1.947809523809524e-05,
      "loss": 0.3266,
      "step": 1370
    },
    {
      "epoch": 0.3942857142857143,
      "grad_norm": 18.772356033325195,
      "learning_rate": 1.9474285714285717e-05,
      "loss": 0.4942,
      "step": 1380
    },
    {
      "epoch": 0.39714285714285713,
      "grad_norm": 0.5546216368675232,
      "learning_rate": 1.9470476190476192e-05,
      "loss": 0.9193,
      "step": 1390
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.5647950172424316,
      "learning_rate": 1.9466666666666668e-05,
      "loss": 0.4683,
      "step": 1400
    },
    {
      "epoch": 0.40285714285714286,
      "grad_norm": 16.633583068847656,
      "learning_rate": 1.9462857142857147e-05,
      "loss": 0.6919,
      "step": 1410
    },
    {
      "epoch": 0.4057142857142857,
      "grad_norm": 13.740082740783691,
      "learning_rate": 1.945904761904762e-05,
      "loss": 0.9864,
      "step": 1420
    },
    {
      "epoch": 0.4085714285714286,
      "grad_norm": 1.0951884984970093,
      "learning_rate": 1.9455238095238095e-05,
      "loss": 0.4064,
      "step": 1430
    },
    {
      "epoch": 0.4114285714285714,
      "grad_norm": 14.153560638427734,
      "learning_rate": 1.9451428571428573e-05,
      "loss": 0.5626,
      "step": 1440
    },
    {
      "epoch": 0.4142857142857143,
      "grad_norm": 0.7573922276496887,
      "learning_rate": 1.944761904761905e-05,
      "loss": 0.9202,
      "step": 1450
    },
    {
      "epoch": 0.41714285714285715,
      "grad_norm": 14.431132316589355,
      "learning_rate": 1.9443809523809525e-05,
      "loss": 0.5344,
      "step": 1460
    },
    {
      "epoch": 0.42,
      "grad_norm": 14.907238960266113,
      "learning_rate": 1.944e-05,
      "loss": 0.3777,
      "step": 1470
    },
    {
      "epoch": 0.4228571428571429,
      "grad_norm": 12.831520080566406,
      "learning_rate": 1.9436190476190476e-05,
      "loss": 0.7557,
      "step": 1480
    },
    {
      "epoch": 0.4257142857142857,
      "grad_norm": 14.909416198730469,
      "learning_rate": 1.9432380952380955e-05,
      "loss": 0.5073,
      "step": 1490
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 12.202527046203613,
      "learning_rate": 1.942857142857143e-05,
      "loss": 0.5062,
      "step": 1500
    },
    {
      "epoch": 0.43142857142857144,
      "grad_norm": 18.9754581451416,
      "learning_rate": 1.9424761904761906e-05,
      "loss": 0.4985,
      "step": 1510
    },
    {
      "epoch": 0.4342857142857143,
      "grad_norm": 13.666136741638184,
      "learning_rate": 1.942095238095238e-05,
      "loss": 0.4895,
      "step": 1520
    },
    {
      "epoch": 0.43714285714285717,
      "grad_norm": 12.161497116088867,
      "learning_rate": 1.941714285714286e-05,
      "loss": 0.8252,
      "step": 1530
    },
    {
      "epoch": 0.44,
      "grad_norm": 0.6251863837242126,
      "learning_rate": 1.9413333333333336e-05,
      "loss": 0.3354,
      "step": 1540
    },
    {
      "epoch": 0.44285714285714284,
      "grad_norm": 17.174747467041016,
      "learning_rate": 1.940952380952381e-05,
      "loss": 0.6311,
      "step": 1550
    },
    {
      "epoch": 0.44571428571428573,
      "grad_norm": 0.4846016466617584,
      "learning_rate": 1.9405714285714287e-05,
      "loss": 0.5044,
      "step": 1560
    },
    {
      "epoch": 0.44857142857142857,
      "grad_norm": 0.6592255234718323,
      "learning_rate": 1.9401904761904763e-05,
      "loss": 0.5378,
      "step": 1570
    },
    {
      "epoch": 0.4514285714285714,
      "grad_norm": 2.104314088821411,
      "learning_rate": 1.9398095238095242e-05,
      "loss": 0.4665,
      "step": 1580
    },
    {
      "epoch": 0.4542857142857143,
      "grad_norm": 1.3496664762496948,
      "learning_rate": 1.9394285714285717e-05,
      "loss": 0.486,
      "step": 1590
    },
    {
      "epoch": 0.45714285714285713,
      "grad_norm": 13.032271385192871,
      "learning_rate": 1.9390476190476193e-05,
      "loss": 0.3987,
      "step": 1600
    },
    {
      "epoch": 0.46,
      "grad_norm": 0.7709241509437561,
      "learning_rate": 1.938666666666667e-05,
      "loss": 0.773,
      "step": 1610
    },
    {
      "epoch": 0.46285714285714286,
      "grad_norm": 0.667007565498352,
      "learning_rate": 1.9382857142857144e-05,
      "loss": 0.4802,
      "step": 1620
    },
    {
      "epoch": 0.4657142857142857,
      "grad_norm": 12.024800300598145,
      "learning_rate": 1.937904761904762e-05,
      "loss": 0.7217,
      "step": 1630
    },
    {
      "epoch": 0.4685714285714286,
      "grad_norm": 12.08418083190918,
      "learning_rate": 1.9375238095238095e-05,
      "loss": 0.4874,
      "step": 1640
    },
    {
      "epoch": 0.4714285714285714,
      "grad_norm": 0.2276018112897873,
      "learning_rate": 1.937142857142857e-05,
      "loss": 0.2345,
      "step": 1650
    },
    {
      "epoch": 0.4742857142857143,
      "grad_norm": 0.23462411761283875,
      "learning_rate": 1.936761904761905e-05,
      "loss": 0.4389,
      "step": 1660
    },
    {
      "epoch": 0.47714285714285715,
      "grad_norm": 1.013572096824646,
      "learning_rate": 1.9363809523809525e-05,
      "loss": 0.8213,
      "step": 1670
    },
    {
      "epoch": 0.48,
      "grad_norm": 5.123472213745117,
      "learning_rate": 1.936e-05,
      "loss": 0.3318,
      "step": 1680
    },
    {
      "epoch": 0.4828571428571429,
      "grad_norm": 16.627742767333984,
      "learning_rate": 1.9356190476190477e-05,
      "loss": 0.3798,
      "step": 1690
    },
    {
      "epoch": 0.4857142857142857,
      "grad_norm": 0.6028226613998413,
      "learning_rate": 1.9352380952380952e-05,
      "loss": 0.8747,
      "step": 1700
    },
    {
      "epoch": 0.48857142857142855,
      "grad_norm": 17.245197296142578,
      "learning_rate": 1.934857142857143e-05,
      "loss": 0.7283,
      "step": 1710
    },
    {
      "epoch": 0.49142857142857144,
      "grad_norm": 1.910434603691101,
      "learning_rate": 1.9344761904761907e-05,
      "loss": 0.343,
      "step": 1720
    },
    {
      "epoch": 0.4942857142857143,
      "grad_norm": 13.430169105529785,
      "learning_rate": 1.9340952380952382e-05,
      "loss": 0.4335,
      "step": 1730
    },
    {
      "epoch": 0.49714285714285716,
      "grad_norm": 0.3008939325809479,
      "learning_rate": 1.9337142857142858e-05,
      "loss": 0.496,
      "step": 1740
    },
    {
      "epoch": 0.5,
      "grad_norm": 15.147048950195312,
      "learning_rate": 1.9333333333333333e-05,
      "loss": 0.9154,
      "step": 1750
    },
    {
      "epoch": 0.5028571428571429,
      "grad_norm": 13.899116516113281,
      "learning_rate": 1.9329523809523812e-05,
      "loss": 0.6176,
      "step": 1760
    },
    {
      "epoch": 0.5057142857142857,
      "grad_norm": 24.988950729370117,
      "learning_rate": 1.9325714285714288e-05,
      "loss": 0.3688,
      "step": 1770
    },
    {
      "epoch": 0.5085714285714286,
      "grad_norm": 13.086713790893555,
      "learning_rate": 1.9321904761904764e-05,
      "loss": 0.7773,
      "step": 1780
    },
    {
      "epoch": 0.5114285714285715,
      "grad_norm": 1.4201890230178833,
      "learning_rate": 1.931809523809524e-05,
      "loss": 0.4669,
      "step": 1790
    },
    {
      "epoch": 0.5142857142857142,
      "grad_norm": 13.162192344665527,
      "learning_rate": 1.9314285714285718e-05,
      "loss": 0.7476,
      "step": 1800
    },
    {
      "epoch": 0.5171428571428571,
      "grad_norm": 1.4403637647628784,
      "learning_rate": 1.9310476190476194e-05,
      "loss": 0.7191,
      "step": 1810
    },
    {
      "epoch": 0.52,
      "grad_norm": 1.2217228412628174,
      "learning_rate": 1.930666666666667e-05,
      "loss": 0.4619,
      "step": 1820
    },
    {
      "epoch": 0.5228571428571429,
      "grad_norm": 14.359800338745117,
      "learning_rate": 1.9302857142857145e-05,
      "loss": 0.1465,
      "step": 1830
    },
    {
      "epoch": 0.5257142857142857,
      "grad_norm": 26.364551544189453,
      "learning_rate": 1.929904761904762e-05,
      "loss": 1.1416,
      "step": 1840
    },
    {
      "epoch": 0.5285714285714286,
      "grad_norm": 0.47900354862213135,
      "learning_rate": 1.9295238095238096e-05,
      "loss": 0.2106,
      "step": 1850
    },
    {
      "epoch": 0.5314285714285715,
      "grad_norm": 0.32970932126045227,
      "learning_rate": 1.929142857142857e-05,
      "loss": 0.2559,
      "step": 1860
    },
    {
      "epoch": 0.5342857142857143,
      "grad_norm": 0.16815580427646637,
      "learning_rate": 1.9287619047619047e-05,
      "loss": 0.1995,
      "step": 1870
    },
    {
      "epoch": 0.5371428571428571,
      "grad_norm": 0.1328059434890747,
      "learning_rate": 1.9283809523809526e-05,
      "loss": 0.29,
      "step": 1880
    },
    {
      "epoch": 0.54,
      "grad_norm": 14.312291145324707,
      "learning_rate": 1.9280000000000002e-05,
      "loss": 1.3866,
      "step": 1890
    },
    {
      "epoch": 0.5428571428571428,
      "grad_norm": 12.036354064941406,
      "learning_rate": 1.9276190476190477e-05,
      "loss": 1.1801,
      "step": 1900
    },
    {
      "epoch": 0.5457142857142857,
      "grad_norm": 4.937571048736572,
      "learning_rate": 1.9272380952380953e-05,
      "loss": 0.3152,
      "step": 1910
    },
    {
      "epoch": 0.5485714285714286,
      "grad_norm": 12.765894889831543,
      "learning_rate": 1.926857142857143e-05,
      "loss": 0.4046,
      "step": 1920
    },
    {
      "epoch": 0.5514285714285714,
      "grad_norm": 0.9195579886436462,
      "learning_rate": 1.9264761904761907e-05,
      "loss": 0.0818,
      "step": 1930
    },
    {
      "epoch": 0.5542857142857143,
      "grad_norm": 1.474678874015808,
      "learning_rate": 1.9260952380952383e-05,
      "loss": 0.6513,
      "step": 1940
    },
    {
      "epoch": 0.5571428571428572,
      "grad_norm": 14.265393257141113,
      "learning_rate": 1.925714285714286e-05,
      "loss": 0.7523,
      "step": 1950
    },
    {
      "epoch": 0.56,
      "grad_norm": 17.929758071899414,
      "learning_rate": 1.9253333333333334e-05,
      "loss": 0.4718,
      "step": 1960
    },
    {
      "epoch": 0.5628571428571428,
      "grad_norm": 0.28729549050331116,
      "learning_rate": 1.924952380952381e-05,
      "loss": 0.149,
      "step": 1970
    },
    {
      "epoch": 0.5657142857142857,
      "grad_norm": 0.27393364906311035,
      "learning_rate": 1.924571428571429e-05,
      "loss": 0.7246,
      "step": 1980
    },
    {
      "epoch": 0.5685714285714286,
      "grad_norm": 0.4046177566051483,
      "learning_rate": 1.9241904761904764e-05,
      "loss": 0.2472,
      "step": 1990
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 12.728572845458984,
      "learning_rate": 1.923809523809524e-05,
      "loss": 0.5659,
      "step": 2000
    },
    {
      "epoch": 0.5742857142857143,
      "grad_norm": 12.60609245300293,
      "learning_rate": 1.9234285714285716e-05,
      "loss": 0.3197,
      "step": 2010
    },
    {
      "epoch": 0.5771428571428572,
      "grad_norm": 0.9713100790977478,
      "learning_rate": 1.923047619047619e-05,
      "loss": 0.7845,
      "step": 2020
    },
    {
      "epoch": 0.58,
      "grad_norm": 0.7891978025436401,
      "learning_rate": 1.922666666666667e-05,
      "loss": 0.2754,
      "step": 2030
    },
    {
      "epoch": 0.5828571428571429,
      "grad_norm": 26.311498641967773,
      "learning_rate": 1.9222857142857146e-05,
      "loss": 0.777,
      "step": 2040
    },
    {
      "epoch": 0.5857142857142857,
      "grad_norm": 14.4113187789917,
      "learning_rate": 1.921904761904762e-05,
      "loss": 0.5535,
      "step": 2050
    },
    {
      "epoch": 0.5885714285714285,
      "grad_norm": 0.8002667427062988,
      "learning_rate": 1.9215238095238097e-05,
      "loss": 0.391,
      "step": 2060
    },
    {
      "epoch": 0.5914285714285714,
      "grad_norm": 11.97769546508789,
      "learning_rate": 1.9211428571428572e-05,
      "loss": 0.5087,
      "step": 2070
    },
    {
      "epoch": 0.5942857142857143,
      "grad_norm": 0.36305588483810425,
      "learning_rate": 1.9207619047619048e-05,
      "loss": 0.2928,
      "step": 2080
    },
    {
      "epoch": 0.5971428571428572,
      "grad_norm": 0.6320545673370361,
      "learning_rate": 1.9203809523809524e-05,
      "loss": 0.5725,
      "step": 2090
    },
    {
      "epoch": 0.6,
      "grad_norm": 3.9663209915161133,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.614,
      "step": 2100
    },
    {
      "epoch": 0.6028571428571429,
      "grad_norm": 12.514128684997559,
      "learning_rate": 1.9196190476190478e-05,
      "loss": 0.4543,
      "step": 2110
    },
    {
      "epoch": 0.6057142857142858,
      "grad_norm": 17.2171688079834,
      "learning_rate": 1.9192380952380954e-05,
      "loss": 0.3569,
      "step": 2120
    },
    {
      "epoch": 0.6085714285714285,
      "grad_norm": 0.3522151708602905,
      "learning_rate": 1.918857142857143e-05,
      "loss": 0.5309,
      "step": 2130
    },
    {
      "epoch": 0.6114285714285714,
      "grad_norm": 11.674086570739746,
      "learning_rate": 1.9184761904761905e-05,
      "loss": 0.5088,
      "step": 2140
    },
    {
      "epoch": 0.6142857142857143,
      "grad_norm": 14.452776908874512,
      "learning_rate": 1.9180952380952384e-05,
      "loss": 0.6284,
      "step": 2150
    },
    {
      "epoch": 0.6171428571428571,
      "grad_norm": 20.812915802001953,
      "learning_rate": 1.917714285714286e-05,
      "loss": 0.3225,
      "step": 2160
    },
    {
      "epoch": 0.62,
      "grad_norm": 0.17090290784835815,
      "learning_rate": 1.9173333333333335e-05,
      "loss": 0.4307,
      "step": 2170
    },
    {
      "epoch": 0.6228571428571429,
      "grad_norm": 2.398603916168213,
      "learning_rate": 1.916952380952381e-05,
      "loss": 0.5094,
      "step": 2180
    },
    {
      "epoch": 0.6257142857142857,
      "grad_norm": 0.4256787896156311,
      "learning_rate": 1.9165714285714286e-05,
      "loss": 0.6336,
      "step": 2190
    },
    {
      "epoch": 0.6285714285714286,
      "grad_norm": 0.44298774003982544,
      "learning_rate": 1.9161904761904765e-05,
      "loss": 0.6093,
      "step": 2200
    },
    {
      "epoch": 0.6314285714285715,
      "grad_norm": 0.3103659451007843,
      "learning_rate": 1.915809523809524e-05,
      "loss": 0.3628,
      "step": 2210
    },
    {
      "epoch": 0.6342857142857142,
      "grad_norm": 14.428117752075195,
      "learning_rate": 1.9154285714285716e-05,
      "loss": 0.6469,
      "step": 2220
    },
    {
      "epoch": 0.6371428571428571,
      "grad_norm": 0.44891417026519775,
      "learning_rate": 1.9150476190476192e-05,
      "loss": 0.8166,
      "step": 2230
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.6803920865058899,
      "learning_rate": 1.9146666666666667e-05,
      "loss": 0.3317,
      "step": 2240
    },
    {
      "epoch": 0.6428571428571429,
      "grad_norm": 19.529529571533203,
      "learning_rate": 1.9142857142857146e-05,
      "loss": 0.3934,
      "step": 2250
    },
    {
      "epoch": 0.6457142857142857,
      "grad_norm": 22.950817108154297,
      "learning_rate": 1.9139047619047622e-05,
      "loss": 0.4571,
      "step": 2260
    },
    {
      "epoch": 0.6485714285714286,
      "grad_norm": 5.641322612762451,
      "learning_rate": 1.9135238095238098e-05,
      "loss": 0.4022,
      "step": 2270
    },
    {
      "epoch": 0.6514285714285715,
      "grad_norm": 1.1834385395050049,
      "learning_rate": 1.9131428571428573e-05,
      "loss": 0.4104,
      "step": 2280
    },
    {
      "epoch": 0.6542857142857142,
      "grad_norm": 3.5984456539154053,
      "learning_rate": 1.912761904761905e-05,
      "loss": 0.517,
      "step": 2290
    },
    {
      "epoch": 0.6571428571428571,
      "grad_norm": 2.104898691177368,
      "learning_rate": 1.9123809523809524e-05,
      "loss": 1.0059,
      "step": 2300
    },
    {
      "epoch": 0.66,
      "grad_norm": 2.4302210807800293,
      "learning_rate": 1.912e-05,
      "loss": 0.2747,
      "step": 2310
    },
    {
      "epoch": 0.6628571428571428,
      "grad_norm": 19.253171920776367,
      "learning_rate": 1.9116190476190475e-05,
      "loss": 0.4919,
      "step": 2320
    },
    {
      "epoch": 0.6657142857142857,
      "grad_norm": 0.2716240882873535,
      "learning_rate": 1.9112380952380954e-05,
      "loss": 0.3284,
      "step": 2330
    },
    {
      "epoch": 0.6685714285714286,
      "grad_norm": 20.613075256347656,
      "learning_rate": 1.910857142857143e-05,
      "loss": 0.4044,
      "step": 2340
    },
    {
      "epoch": 0.6714285714285714,
      "grad_norm": 2.9309840202331543,
      "learning_rate": 1.9104761904761906e-05,
      "loss": 0.2781,
      "step": 2350
    },
    {
      "epoch": 0.6742857142857143,
      "grad_norm": 0.8998394012451172,
      "learning_rate": 1.910095238095238e-05,
      "loss": 0.5878,
      "step": 2360
    },
    {
      "epoch": 0.6771428571428572,
      "grad_norm": 1.3135820627212524,
      "learning_rate": 1.909714285714286e-05,
      "loss": 0.6365,
      "step": 2370
    },
    {
      "epoch": 0.68,
      "grad_norm": 15.644879341125488,
      "learning_rate": 1.9093333333333336e-05,
      "loss": 0.68,
      "step": 2380
    },
    {
      "epoch": 0.6828571428571428,
      "grad_norm": 0.9993297457695007,
      "learning_rate": 1.908952380952381e-05,
      "loss": 0.51,
      "step": 2390
    },
    {
      "epoch": 0.6857142857142857,
      "grad_norm": 12.31161117553711,
      "learning_rate": 1.9085714285714287e-05,
      "loss": 0.7888,
      "step": 2400
    },
    {
      "epoch": 0.6885714285714286,
      "grad_norm": 1.1224299669265747,
      "learning_rate": 1.9081904761904762e-05,
      "loss": 0.5707,
      "step": 2410
    },
    {
      "epoch": 0.6914285714285714,
      "grad_norm": 13.901148796081543,
      "learning_rate": 1.907809523809524e-05,
      "loss": 0.4714,
      "step": 2420
    },
    {
      "epoch": 0.6942857142857143,
      "grad_norm": 24.98563003540039,
      "learning_rate": 1.9074285714285717e-05,
      "loss": 0.6746,
      "step": 2430
    },
    {
      "epoch": 0.6971428571428572,
      "grad_norm": 12.901192665100098,
      "learning_rate": 1.9070476190476193e-05,
      "loss": 0.315,
      "step": 2440
    },
    {
      "epoch": 0.7,
      "grad_norm": 0.3395313322544098,
      "learning_rate": 1.9066666666666668e-05,
      "loss": 0.2767,
      "step": 2450
    },
    {
      "epoch": 0.7028571428571428,
      "grad_norm": 1.520367980003357,
      "learning_rate": 1.9062857142857144e-05,
      "loss": 0.7293,
      "step": 2460
    },
    {
      "epoch": 0.7057142857142857,
      "grad_norm": 2.0781643390655518,
      "learning_rate": 1.9059047619047623e-05,
      "loss": 0.4043,
      "step": 2470
    },
    {
      "epoch": 0.7085714285714285,
      "grad_norm": 5.797769069671631,
      "learning_rate": 1.90552380952381e-05,
      "loss": 0.662,
      "step": 2480
    },
    {
      "epoch": 0.7114285714285714,
      "grad_norm": 2.5621001720428467,
      "learning_rate": 1.9051428571428574e-05,
      "loss": 0.5944,
      "step": 2490
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 13.210180282592773,
      "learning_rate": 1.904761904761905e-05,
      "loss": 0.5392,
      "step": 2500
    },
    {
      "epoch": 0.7171428571428572,
      "grad_norm": 14.057727813720703,
      "learning_rate": 1.9043809523809525e-05,
      "loss": 0.663,
      "step": 2510
    },
    {
      "epoch": 0.72,
      "grad_norm": 9.883805274963379,
      "learning_rate": 1.904e-05,
      "loss": 0.4061,
      "step": 2520
    },
    {
      "epoch": 0.7228571428571429,
      "grad_norm": 27.76972007751465,
      "learning_rate": 1.9036190476190476e-05,
      "loss": 0.3926,
      "step": 2530
    },
    {
      "epoch": 0.7257142857142858,
      "grad_norm": 21.86349105834961,
      "learning_rate": 1.9032380952380952e-05,
      "loss": 0.3357,
      "step": 2540
    },
    {
      "epoch": 0.7285714285714285,
      "grad_norm": 42.41703796386719,
      "learning_rate": 1.902857142857143e-05,
      "loss": 0.8037,
      "step": 2550
    },
    {
      "epoch": 0.7314285714285714,
      "grad_norm": 0.08896353840827942,
      "learning_rate": 1.9024761904761906e-05,
      "loss": 0.2995,
      "step": 2560
    },
    {
      "epoch": 0.7342857142857143,
      "grad_norm": 0.10992959141731262,
      "learning_rate": 1.9020952380952382e-05,
      "loss": 0.8374,
      "step": 2570
    },
    {
      "epoch": 0.7371428571428571,
      "grad_norm": 14.955741882324219,
      "learning_rate": 1.9017142857142858e-05,
      "loss": 0.8172,
      "step": 2580
    },
    {
      "epoch": 0.74,
      "grad_norm": 29.26750373840332,
      "learning_rate": 1.9013333333333333e-05,
      "loss": 0.7231,
      "step": 2590
    },
    {
      "epoch": 0.7428571428571429,
      "grad_norm": 1.042108416557312,
      "learning_rate": 1.9009523809523812e-05,
      "loss": 0.1497,
      "step": 2600
    },
    {
      "epoch": 0.7457142857142857,
      "grad_norm": 11.923301696777344,
      "learning_rate": 1.9005714285714288e-05,
      "loss": 0.3468,
      "step": 2610
    },
    {
      "epoch": 0.7485714285714286,
      "grad_norm": 21.7850341796875,
      "learning_rate": 1.9001904761904763e-05,
      "loss": 0.6687,
      "step": 2620
    },
    {
      "epoch": 0.7514285714285714,
      "grad_norm": 0.2409602850675583,
      "learning_rate": 1.899809523809524e-05,
      "loss": 0.274,
      "step": 2630
    },
    {
      "epoch": 0.7542857142857143,
      "grad_norm": 0.14368899166584015,
      "learning_rate": 1.8994285714285718e-05,
      "loss": 0.3791,
      "step": 2640
    },
    {
      "epoch": 0.7571428571428571,
      "grad_norm": 0.544235110282898,
      "learning_rate": 1.8990476190476193e-05,
      "loss": 0.9361,
      "step": 2650
    },
    {
      "epoch": 0.76,
      "grad_norm": 14.488361358642578,
      "learning_rate": 1.898666666666667e-05,
      "loss": 0.6271,
      "step": 2660
    },
    {
      "epoch": 0.7628571428571429,
      "grad_norm": 12.030635833740234,
      "learning_rate": 1.8982857142857145e-05,
      "loss": 0.5344,
      "step": 2670
    },
    {
      "epoch": 0.7657142857142857,
      "grad_norm": 0.4773743152618408,
      "learning_rate": 1.897904761904762e-05,
      "loss": 0.4437,
      "step": 2680
    },
    {
      "epoch": 0.7685714285714286,
      "grad_norm": 14.225260734558105,
      "learning_rate": 1.89752380952381e-05,
      "loss": 0.6648,
      "step": 2690
    },
    {
      "epoch": 0.7714285714285715,
      "grad_norm": 5.589629173278809,
      "learning_rate": 1.8971428571428575e-05,
      "loss": 0.41,
      "step": 2700
    },
    {
      "epoch": 0.7742857142857142,
      "grad_norm": 0.6007431745529175,
      "learning_rate": 1.896761904761905e-05,
      "loss": 0.5021,
      "step": 2710
    },
    {
      "epoch": 0.7771428571428571,
      "grad_norm": 0.08400370925664902,
      "learning_rate": 1.8963809523809526e-05,
      "loss": 0.272,
      "step": 2720
    },
    {
      "epoch": 0.78,
      "grad_norm": 18.860265731811523,
      "learning_rate": 1.896e-05,
      "loss": 0.9583,
      "step": 2730
    },
    {
      "epoch": 0.7828571428571428,
      "grad_norm": 12.732344627380371,
      "learning_rate": 1.8956190476190477e-05,
      "loss": 0.5596,
      "step": 2740
    },
    {
      "epoch": 0.7857142857142857,
      "grad_norm": 11.165006637573242,
      "learning_rate": 1.8952380952380953e-05,
      "loss": 0.5882,
      "step": 2750
    },
    {
      "epoch": 0.7885714285714286,
      "grad_norm": 0.4069215655326843,
      "learning_rate": 1.8948571428571428e-05,
      "loss": 0.4881,
      "step": 2760
    },
    {
      "epoch": 0.7914285714285715,
      "grad_norm": 20.568973541259766,
      "learning_rate": 1.8944761904761907e-05,
      "loss": 0.307,
      "step": 2770
    },
    {
      "epoch": 0.7942857142857143,
      "grad_norm": 13.130693435668945,
      "learning_rate": 1.8940952380952383e-05,
      "loss": 0.4488,
      "step": 2780
    },
    {
      "epoch": 0.7971428571428572,
      "grad_norm": 0.5171682238578796,
      "learning_rate": 1.893714285714286e-05,
      "loss": 0.8919,
      "step": 2790
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.5539140105247498,
      "learning_rate": 1.8933333333333334e-05,
      "loss": 0.6309,
      "step": 2800
    },
    {
      "epoch": 0.8028571428571428,
      "grad_norm": 14.34811782836914,
      "learning_rate": 1.892952380952381e-05,
      "loss": 0.4293,
      "step": 2810
    },
    {
      "epoch": 0.8057142857142857,
      "grad_norm": 0.7498053908348083,
      "learning_rate": 1.892571428571429e-05,
      "loss": 0.5709,
      "step": 2820
    },
    {
      "epoch": 0.8085714285714286,
      "grad_norm": 0.3085012137889862,
      "learning_rate": 1.8921904761904764e-05,
      "loss": 0.3612,
      "step": 2830
    },
    {
      "epoch": 0.8114285714285714,
      "grad_norm": 74.48991394042969,
      "learning_rate": 1.891809523809524e-05,
      "loss": 0.6424,
      "step": 2840
    },
    {
      "epoch": 0.8142857142857143,
      "grad_norm": 0.6228105425834656,
      "learning_rate": 1.8914285714285715e-05,
      "loss": 0.5233,
      "step": 2850
    },
    {
      "epoch": 0.8171428571428572,
      "grad_norm": 13.252086639404297,
      "learning_rate": 1.891047619047619e-05,
      "loss": 0.5913,
      "step": 2860
    },
    {
      "epoch": 0.82,
      "grad_norm": 19.57339859008789,
      "learning_rate": 1.890666666666667e-05,
      "loss": 0.4991,
      "step": 2870
    },
    {
      "epoch": 0.8228571428571428,
      "grad_norm": 1.6013250350952148,
      "learning_rate": 1.8902857142857145e-05,
      "loss": 0.6441,
      "step": 2880
    },
    {
      "epoch": 0.8257142857142857,
      "grad_norm": 5.972817420959473,
      "learning_rate": 1.889904761904762e-05,
      "loss": 0.3266,
      "step": 2890
    },
    {
      "epoch": 0.8285714285714286,
      "grad_norm": 0.13962513208389282,
      "learning_rate": 1.8895238095238096e-05,
      "loss": 0.0578,
      "step": 2900
    },
    {
      "epoch": 0.8314285714285714,
      "grad_norm": 39.93212127685547,
      "learning_rate": 1.8891428571428575e-05,
      "loss": 0.6678,
      "step": 2910
    },
    {
      "epoch": 0.8342857142857143,
      "grad_norm": 0.2027939110994339,
      "learning_rate": 1.888761904761905e-05,
      "loss": 0.49,
      "step": 2920
    },
    {
      "epoch": 0.8371428571428572,
      "grad_norm": 9.431883811950684,
      "learning_rate": 1.8883809523809523e-05,
      "loss": 0.294,
      "step": 2930
    },
    {
      "epoch": 0.84,
      "grad_norm": 3.7847204208374023,
      "learning_rate": 1.8880000000000002e-05,
      "loss": 0.234,
      "step": 2940
    },
    {
      "epoch": 0.8428571428571429,
      "grad_norm": 0.46343666315078735,
      "learning_rate": 1.8876190476190478e-05,
      "loss": 0.5997,
      "step": 2950
    },
    {
      "epoch": 0.8457142857142858,
      "grad_norm": 15.360284805297852,
      "learning_rate": 1.8872380952380953e-05,
      "loss": 0.6584,
      "step": 2960
    },
    {
      "epoch": 0.8485714285714285,
      "grad_norm": 25.85516357421875,
      "learning_rate": 1.886857142857143e-05,
      "loss": 0.3149,
      "step": 2970
    },
    {
      "epoch": 0.8514285714285714,
      "grad_norm": 0.47538432478904724,
      "learning_rate": 1.8864761904761905e-05,
      "loss": 0.1843,
      "step": 2980
    },
    {
      "epoch": 0.8542857142857143,
      "grad_norm": 0.20352958142757416,
      "learning_rate": 1.8860952380952383e-05,
      "loss": 0.4232,
      "step": 2990
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.2839951813220978,
      "learning_rate": 1.885714285714286e-05,
      "loss": 0.4444,
      "step": 3000
    },
    {
      "epoch": 0.86,
      "grad_norm": 0.7131670713424683,
      "learning_rate": 1.8853333333333335e-05,
      "loss": 0.4099,
      "step": 3010
    },
    {
      "epoch": 0.8628571428571429,
      "grad_norm": 0.4154931902885437,
      "learning_rate": 1.884952380952381e-05,
      "loss": 0.4944,
      "step": 3020
    },
    {
      "epoch": 0.8657142857142858,
      "grad_norm": 0.45338210463523865,
      "learning_rate": 1.8845714285714286e-05,
      "loss": 0.8523,
      "step": 3030
    },
    {
      "epoch": 0.8685714285714285,
      "grad_norm": 19.330562591552734,
      "learning_rate": 1.8841904761904765e-05,
      "loss": 0.2553,
      "step": 3040
    },
    {
      "epoch": 0.8714285714285714,
      "grad_norm": 0.1904720813035965,
      "learning_rate": 1.883809523809524e-05,
      "loss": 0.254,
      "step": 3050
    },
    {
      "epoch": 0.8742857142857143,
      "grad_norm": 138.9644775390625,
      "learning_rate": 1.8834285714285716e-05,
      "loss": 0.3167,
      "step": 3060
    },
    {
      "epoch": 0.8771428571428571,
      "grad_norm": 0.8208797574043274,
      "learning_rate": 1.883047619047619e-05,
      "loss": 0.2397,
      "step": 3070
    },
    {
      "epoch": 0.88,
      "grad_norm": 59.03577423095703,
      "learning_rate": 1.8826666666666667e-05,
      "loss": 0.6047,
      "step": 3080
    },
    {
      "epoch": 0.8828571428571429,
      "grad_norm": 0.5758447647094727,
      "learning_rate": 1.8822857142857146e-05,
      "loss": 0.4634,
      "step": 3090
    },
    {
      "epoch": 0.8857142857142857,
      "grad_norm": 13.9086275100708,
      "learning_rate": 1.881904761904762e-05,
      "loss": 0.3442,
      "step": 3100
    },
    {
      "epoch": 0.8885714285714286,
      "grad_norm": 5.877348899841309,
      "learning_rate": 1.8815238095238097e-05,
      "loss": 0.4269,
      "step": 3110
    },
    {
      "epoch": 0.8914285714285715,
      "grad_norm": 0.5823636054992676,
      "learning_rate": 1.8811428571428573e-05,
      "loss": 0.2588,
      "step": 3120
    },
    {
      "epoch": 0.8942857142857142,
      "grad_norm": 12.6455717086792,
      "learning_rate": 1.8807619047619052e-05,
      "loss": 0.5428,
      "step": 3130
    },
    {
      "epoch": 0.8971428571428571,
      "grad_norm": 0.3643287718296051,
      "learning_rate": 1.8803809523809527e-05,
      "loss": 0.9277,
      "step": 3140
    },
    {
      "epoch": 0.9,
      "grad_norm": 1.1940127611160278,
      "learning_rate": 1.88e-05,
      "loss": 0.4354,
      "step": 3150
    },
    {
      "epoch": 0.9028571428571428,
      "grad_norm": 2.4531712532043457,
      "learning_rate": 1.8796190476190475e-05,
      "loss": 0.483,
      "step": 3160
    },
    {
      "epoch": 0.9057142857142857,
      "grad_norm": 0.3268527686595917,
      "learning_rate": 1.8792380952380954e-05,
      "loss": 0.6774,
      "step": 3170
    },
    {
      "epoch": 0.9085714285714286,
      "grad_norm": 31.764694213867188,
      "learning_rate": 1.878857142857143e-05,
      "loss": 0.4804,
      "step": 3180
    },
    {
      "epoch": 0.9114285714285715,
      "grad_norm": 0.46528884768486023,
      "learning_rate": 1.8784761904761905e-05,
      "loss": 0.5342,
      "step": 3190
    },
    {
      "epoch": 0.9142857142857143,
      "grad_norm": 52.96959686279297,
      "learning_rate": 1.878095238095238e-05,
      "loss": 0.6232,
      "step": 3200
    },
    {
      "epoch": 0.9171428571428571,
      "grad_norm": 0.2468448281288147,
      "learning_rate": 1.877714285714286e-05,
      "loss": 0.6576,
      "step": 3210
    },
    {
      "epoch": 0.92,
      "grad_norm": 1.6994539499282837,
      "learning_rate": 1.8773333333333335e-05,
      "loss": 0.3252,
      "step": 3220
    },
    {
      "epoch": 0.9228571428571428,
      "grad_norm": 15.290642738342285,
      "learning_rate": 1.876952380952381e-05,
      "loss": 0.5837,
      "step": 3230
    },
    {
      "epoch": 0.9257142857142857,
      "grad_norm": 18.041250228881836,
      "learning_rate": 1.8765714285714287e-05,
      "loss": 0.4212,
      "step": 3240
    },
    {
      "epoch": 0.9285714285714286,
      "grad_norm": 0.28226932883262634,
      "learning_rate": 1.8761904761904762e-05,
      "loss": 0.4706,
      "step": 3250
    },
    {
      "epoch": 0.9314285714285714,
      "grad_norm": 0.26169902086257935,
      "learning_rate": 1.875809523809524e-05,
      "loss": 0.4008,
      "step": 3260
    },
    {
      "epoch": 0.9342857142857143,
      "grad_norm": 0.5884233117103577,
      "learning_rate": 1.8754285714285717e-05,
      "loss": 0.3231,
      "step": 3270
    },
    {
      "epoch": 0.9371428571428572,
      "grad_norm": 0.2958959937095642,
      "learning_rate": 1.8750476190476192e-05,
      "loss": 0.4239,
      "step": 3280
    },
    {
      "epoch": 0.94,
      "grad_norm": 13.011128425598145,
      "learning_rate": 1.8746666666666668e-05,
      "loss": 0.5631,
      "step": 3290
    },
    {
      "epoch": 0.9428571428571428,
      "grad_norm": 0.2546530067920685,
      "learning_rate": 1.8742857142857143e-05,
      "loss": 0.425,
      "step": 3300
    },
    {
      "epoch": 0.9457142857142857,
      "grad_norm": 0.1695709228515625,
      "learning_rate": 1.8739047619047622e-05,
      "loss": 0.2205,
      "step": 3310
    },
    {
      "epoch": 0.9485714285714286,
      "grad_norm": 0.25033652782440186,
      "learning_rate": 1.8735238095238098e-05,
      "loss": 1.0106,
      "step": 3320
    },
    {
      "epoch": 0.9514285714285714,
      "grad_norm": 0.6256197690963745,
      "learning_rate": 1.8731428571428574e-05,
      "loss": 0.6374,
      "step": 3330
    },
    {
      "epoch": 0.9542857142857143,
      "grad_norm": 0.43018704652786255,
      "learning_rate": 1.872761904761905e-05,
      "loss": 0.2159,
      "step": 3340
    },
    {
      "epoch": 0.9571428571428572,
      "grad_norm": 12.217121124267578,
      "learning_rate": 1.8723809523809525e-05,
      "loss": 0.4361,
      "step": 3350
    },
    {
      "epoch": 0.96,
      "grad_norm": 1.5563842058181763,
      "learning_rate": 1.8720000000000004e-05,
      "loss": 0.5388,
      "step": 3360
    },
    {
      "epoch": 0.9628571428571429,
      "grad_norm": 0.38249659538269043,
      "learning_rate": 1.8716190476190476e-05,
      "loss": 0.3225,
      "step": 3370
    },
    {
      "epoch": 0.9657142857142857,
      "grad_norm": 14.134017944335938,
      "learning_rate": 1.871238095238095e-05,
      "loss": 0.5811,
      "step": 3380
    },
    {
      "epoch": 0.9685714285714285,
      "grad_norm": 6.190420150756836,
      "learning_rate": 1.870857142857143e-05,
      "loss": 0.2167,
      "step": 3390
    },
    {
      "epoch": 0.9714285714285714,
      "grad_norm": 26.479005813598633,
      "learning_rate": 1.8704761904761906e-05,
      "loss": 0.5687,
      "step": 3400
    },
    {
      "epoch": 0.9742857142857143,
      "grad_norm": 0.14913764595985413,
      "learning_rate": 1.870095238095238e-05,
      "loss": 0.5466,
      "step": 3410
    },
    {
      "epoch": 0.9771428571428571,
      "grad_norm": 0.4878743886947632,
      "learning_rate": 1.8697142857142857e-05,
      "loss": 0.814,
      "step": 3420
    },
    {
      "epoch": 0.98,
      "grad_norm": 1.0991538763046265,
      "learning_rate": 1.8693333333333333e-05,
      "loss": 0.513,
      "step": 3430
    },
    {
      "epoch": 0.9828571428571429,
      "grad_norm": 22.55204963684082,
      "learning_rate": 1.8689523809523812e-05,
      "loss": 0.1716,
      "step": 3440
    },
    {
      "epoch": 0.9857142857142858,
      "grad_norm": 0.14519116282463074,
      "learning_rate": 1.8685714285714287e-05,
      "loss": 0.3384,
      "step": 3450
    },
    {
      "epoch": 0.9885714285714285,
      "grad_norm": 0.11661075055599213,
      "learning_rate": 1.8681904761904763e-05,
      "loss": 0.5462,
      "step": 3460
    },
    {
      "epoch": 0.9914285714285714,
      "grad_norm": 18.055574417114258,
      "learning_rate": 1.867809523809524e-05,
      "loss": 0.7273,
      "step": 3470
    },
    {
      "epoch": 0.9942857142857143,
      "grad_norm": 27.05939483642578,
      "learning_rate": 1.8674285714285717e-05,
      "loss": 0.9273,
      "step": 3480
    },
    {
      "epoch": 0.9971428571428571,
      "grad_norm": 1.1399983167648315,
      "learning_rate": 1.8670476190476193e-05,
      "loss": 0.2584,
      "step": 3490
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.2887977063655853,
      "learning_rate": 1.866666666666667e-05,
      "loss": 0.4786,
      "step": 3500
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8924731182795699,
      "eval_f1": 0.36507936507936506,
      "eval_loss": 0.4405713677406311,
      "eval_precision": 0.7666666666666667,
      "eval_recall": 0.23958333333333334,
      "eval_runtime": 45.9265,
      "eval_samples_per_second": 65.322,
      "eval_steps_per_second": 2.047,
      "step": 3500
    },
    {
      "epoch": 1.002857142857143,
      "grad_norm": 0.26389020681381226,
      "learning_rate": 1.8662857142857144e-05,
      "loss": 0.4702,
      "step": 3510
    },
    {
      "epoch": 1.0057142857142858,
      "grad_norm": 14.458706855773926,
      "learning_rate": 1.865904761904762e-05,
      "loss": 0.295,
      "step": 3520
    },
    {
      "epoch": 1.0085714285714287,
      "grad_norm": 25.83440399169922,
      "learning_rate": 1.86552380952381e-05,
      "loss": 0.992,
      "step": 3530
    },
    {
      "epoch": 1.0114285714285713,
      "grad_norm": 17.506507873535156,
      "learning_rate": 1.8651428571428574e-05,
      "loss": 0.4063,
      "step": 3540
    },
    {
      "epoch": 1.0142857142857142,
      "grad_norm": 0.6936419010162354,
      "learning_rate": 1.864761904761905e-05,
      "loss": 0.144,
      "step": 3550
    },
    {
      "epoch": 1.0171428571428571,
      "grad_norm": 0.25555840134620667,
      "learning_rate": 1.8643809523809526e-05,
      "loss": 0.3691,
      "step": 3560
    },
    {
      "epoch": 1.02,
      "grad_norm": 24.35759925842285,
      "learning_rate": 1.864e-05,
      "loss": 0.7579,
      "step": 3570
    },
    {
      "epoch": 1.022857142857143,
      "grad_norm": 19.1872501373291,
      "learning_rate": 1.8636190476190477e-05,
      "loss": 0.4477,
      "step": 3580
    },
    {
      "epoch": 1.0257142857142858,
      "grad_norm": 0.15545660257339478,
      "learning_rate": 1.8632380952380952e-05,
      "loss": 0.3406,
      "step": 3590
    },
    {
      "epoch": 1.0285714285714285,
      "grad_norm": 0.31949475407600403,
      "learning_rate": 1.8628571428571428e-05,
      "loss": 0.4641,
      "step": 3600
    },
    {
      "epoch": 1.0314285714285714,
      "grad_norm": 26.373153686523438,
      "learning_rate": 1.8624761904761907e-05,
      "loss": 0.7198,
      "step": 3610
    },
    {
      "epoch": 1.0342857142857143,
      "grad_norm": 3.367077112197876,
      "learning_rate": 1.8620952380952382e-05,
      "loss": 0.3336,
      "step": 3620
    },
    {
      "epoch": 1.0371428571428571,
      "grad_norm": 13.672977447509766,
      "learning_rate": 1.8617142857142858e-05,
      "loss": 0.7716,
      "step": 3630
    },
    {
      "epoch": 1.04,
      "grad_norm": 19.147022247314453,
      "learning_rate": 1.8613333333333334e-05,
      "loss": 0.4069,
      "step": 3640
    },
    {
      "epoch": 1.042857142857143,
      "grad_norm": 0.8722532391548157,
      "learning_rate": 1.860952380952381e-05,
      "loss": 0.2783,
      "step": 3650
    },
    {
      "epoch": 1.0457142857142858,
      "grad_norm": 27.095901489257812,
      "learning_rate": 1.8605714285714288e-05,
      "loss": 0.3782,
      "step": 3660
    },
    {
      "epoch": 1.0485714285714285,
      "grad_norm": 0.0754387304186821,
      "learning_rate": 1.8601904761904764e-05,
      "loss": 0.3164,
      "step": 3670
    },
    {
      "epoch": 1.0514285714285714,
      "grad_norm": 14.415865898132324,
      "learning_rate": 1.859809523809524e-05,
      "loss": 0.16,
      "step": 3680
    },
    {
      "epoch": 1.0542857142857143,
      "grad_norm": 16.613483428955078,
      "learning_rate": 1.8594285714285715e-05,
      "loss": 0.4738,
      "step": 3690
    },
    {
      "epoch": 1.0571428571428572,
      "grad_norm": 0.30655544996261597,
      "learning_rate": 1.859047619047619e-05,
      "loss": 0.2322,
      "step": 3700
    },
    {
      "epoch": 1.06,
      "grad_norm": 23.832481384277344,
      "learning_rate": 1.858666666666667e-05,
      "loss": 0.3671,
      "step": 3710
    },
    {
      "epoch": 1.062857142857143,
      "grad_norm": 24.82067108154297,
      "learning_rate": 1.8582857142857145e-05,
      "loss": 0.69,
      "step": 3720
    },
    {
      "epoch": 1.0657142857142856,
      "grad_norm": 12.17879581451416,
      "learning_rate": 1.857904761904762e-05,
      "loss": 0.2292,
      "step": 3730
    },
    {
      "epoch": 1.0685714285714285,
      "grad_norm": 0.4501093924045563,
      "learning_rate": 1.8575238095238096e-05,
      "loss": 0.6294,
      "step": 3740
    },
    {
      "epoch": 1.0714285714285714,
      "grad_norm": 0.41983211040496826,
      "learning_rate": 1.8571428571428575e-05,
      "loss": 0.3753,
      "step": 3750
    },
    {
      "epoch": 1.0742857142857143,
      "grad_norm": 0.41458746790885925,
      "learning_rate": 1.856761904761905e-05,
      "loss": 0.507,
      "step": 3760
    },
    {
      "epoch": 1.0771428571428572,
      "grad_norm": 41.7114143371582,
      "learning_rate": 1.8563809523809526e-05,
      "loss": 0.3383,
      "step": 3770
    },
    {
      "epoch": 1.08,
      "grad_norm": 0.36638131737709045,
      "learning_rate": 1.8560000000000002e-05,
      "loss": 0.0251,
      "step": 3780
    },
    {
      "epoch": 1.0828571428571427,
      "grad_norm": 0.1535482555627823,
      "learning_rate": 1.8556190476190477e-05,
      "loss": 0.6088,
      "step": 3790
    },
    {
      "epoch": 1.0857142857142856,
      "grad_norm": 0.45119380950927734,
      "learning_rate": 1.8552380952380953e-05,
      "loss": 0.5389,
      "step": 3800
    },
    {
      "epoch": 1.0885714285714285,
      "grad_norm": 3.567033529281616,
      "learning_rate": 1.854857142857143e-05,
      "loss": 0.4416,
      "step": 3810
    },
    {
      "epoch": 1.0914285714285714,
      "grad_norm": 0.30730071663856506,
      "learning_rate": 1.8544761904761904e-05,
      "loss": 0.2986,
      "step": 3820
    },
    {
      "epoch": 1.0942857142857143,
      "grad_norm": 14.565278053283691,
      "learning_rate": 1.8540952380952383e-05,
      "loss": 0.5059,
      "step": 3830
    },
    {
      "epoch": 1.0971428571428572,
      "grad_norm": 0.32251837849617004,
      "learning_rate": 1.853714285714286e-05,
      "loss": 0.2128,
      "step": 3840
    },
    {
      "epoch": 1.1,
      "grad_norm": 20.68483543395996,
      "learning_rate": 1.8533333333333334e-05,
      "loss": 0.2725,
      "step": 3850
    },
    {
      "epoch": 1.1028571428571428,
      "grad_norm": 1.9041024446487427,
      "learning_rate": 1.852952380952381e-05,
      "loss": 0.2935,
      "step": 3860
    },
    {
      "epoch": 1.1057142857142856,
      "grad_norm": 55.90131378173828,
      "learning_rate": 1.8525714285714285e-05,
      "loss": 0.6729,
      "step": 3870
    },
    {
      "epoch": 1.1085714285714285,
      "grad_norm": 20.027307510375977,
      "learning_rate": 1.8521904761904764e-05,
      "loss": 0.245,
      "step": 3880
    },
    {
      "epoch": 1.1114285714285714,
      "grad_norm": 0.940294086933136,
      "learning_rate": 1.851809523809524e-05,
      "loss": 0.4023,
      "step": 3890
    },
    {
      "epoch": 1.1142857142857143,
      "grad_norm": 0.2110672891139984,
      "learning_rate": 1.8514285714285716e-05,
      "loss": 0.4631,
      "step": 3900
    },
    {
      "epoch": 1.1171428571428572,
      "grad_norm": 52.88468551635742,
      "learning_rate": 1.851047619047619e-05,
      "loss": 0.8261,
      "step": 3910
    },
    {
      "epoch": 1.12,
      "grad_norm": 17.299776077270508,
      "learning_rate": 1.8506666666666667e-05,
      "loss": 0.5576,
      "step": 3920
    },
    {
      "epoch": 1.1228571428571428,
      "grad_norm": 0.26919081807136536,
      "learning_rate": 1.8502857142857146e-05,
      "loss": 0.1281,
      "step": 3930
    },
    {
      "epoch": 1.1257142857142857,
      "grad_norm": 14.920262336730957,
      "learning_rate": 1.849904761904762e-05,
      "loss": 0.2787,
      "step": 3940
    },
    {
      "epoch": 1.1285714285714286,
      "grad_norm": 0.13731051981449127,
      "learning_rate": 1.8495238095238097e-05,
      "loss": 0.4104,
      "step": 3950
    },
    {
      "epoch": 1.1314285714285715,
      "grad_norm": 0.23717565834522247,
      "learning_rate": 1.8491428571428573e-05,
      "loss": 0.5491,
      "step": 3960
    },
    {
      "epoch": 1.1342857142857143,
      "grad_norm": 0.3175877332687378,
      "learning_rate": 1.848761904761905e-05,
      "loss": 0.5487,
      "step": 3970
    },
    {
      "epoch": 1.1371428571428572,
      "grad_norm": 17.133508682250977,
      "learning_rate": 1.8483809523809527e-05,
      "loss": 0.2914,
      "step": 3980
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 28.89575958251953,
      "learning_rate": 1.8480000000000003e-05,
      "loss": 0.4525,
      "step": 3990
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 0.351237952709198,
      "learning_rate": 1.8476190476190478e-05,
      "loss": 0.4405,
      "step": 4000
    },
    {
      "epoch": 1.1457142857142857,
      "grad_norm": 13.57690143585205,
      "learning_rate": 1.8472380952380954e-05,
      "loss": 0.3825,
      "step": 4010
    },
    {
      "epoch": 1.1485714285714286,
      "grad_norm": 12.939742088317871,
      "learning_rate": 1.846857142857143e-05,
      "loss": 0.2575,
      "step": 4020
    },
    {
      "epoch": 1.1514285714285715,
      "grad_norm": 0.21233521401882172,
      "learning_rate": 1.8464761904761905e-05,
      "loss": 0.306,
      "step": 4030
    },
    {
      "epoch": 1.1542857142857144,
      "grad_norm": 0.3825798034667969,
      "learning_rate": 1.846095238095238e-05,
      "loss": 0.4914,
      "step": 4040
    },
    {
      "epoch": 1.157142857142857,
      "grad_norm": 0.252938449382782,
      "learning_rate": 1.845714285714286e-05,
      "loss": 0.3708,
      "step": 4050
    },
    {
      "epoch": 1.16,
      "grad_norm": 0.22784622013568878,
      "learning_rate": 1.8453333333333335e-05,
      "loss": 0.4282,
      "step": 4060
    },
    {
      "epoch": 1.1628571428571428,
      "grad_norm": 0.22171086072921753,
      "learning_rate": 1.844952380952381e-05,
      "loss": 0.1408,
      "step": 4070
    },
    {
      "epoch": 1.1657142857142857,
      "grad_norm": 0.24991610646247864,
      "learning_rate": 1.8445714285714286e-05,
      "loss": 0.3722,
      "step": 4080
    },
    {
      "epoch": 1.1685714285714286,
      "grad_norm": 0.18579578399658203,
      "learning_rate": 1.8441904761904762e-05,
      "loss": 0.4886,
      "step": 4090
    },
    {
      "epoch": 1.1714285714285715,
      "grad_norm": 0.1384504735469818,
      "learning_rate": 1.843809523809524e-05,
      "loss": 0.1279,
      "step": 4100
    },
    {
      "epoch": 1.1742857142857144,
      "grad_norm": 0.20575204491615295,
      "learning_rate": 1.8434285714285716e-05,
      "loss": 0.2848,
      "step": 4110
    },
    {
      "epoch": 1.177142857142857,
      "grad_norm": 0.0760747492313385,
      "learning_rate": 1.8430476190476192e-05,
      "loss": 0.4354,
      "step": 4120
    },
    {
      "epoch": 1.18,
      "grad_norm": 17.472454071044922,
      "learning_rate": 1.8426666666666668e-05,
      "loss": 0.6114,
      "step": 4130
    },
    {
      "epoch": 1.1828571428571428,
      "grad_norm": 14.919843673706055,
      "learning_rate": 1.8422857142857143e-05,
      "loss": 0.1935,
      "step": 4140
    },
    {
      "epoch": 1.1857142857142857,
      "grad_norm": 0.10586513578891754,
      "learning_rate": 1.8419047619047622e-05,
      "loss": 0.1617,
      "step": 4150
    },
    {
      "epoch": 1.1885714285714286,
      "grad_norm": 15.712157249450684,
      "learning_rate": 1.8415238095238098e-05,
      "loss": 0.4517,
      "step": 4160
    },
    {
      "epoch": 1.1914285714285715,
      "grad_norm": 0.15405899286270142,
      "learning_rate": 1.8411428571428573e-05,
      "loss": 0.3506,
      "step": 4170
    },
    {
      "epoch": 1.1942857142857144,
      "grad_norm": 0.6921061873435974,
      "learning_rate": 1.840761904761905e-05,
      "loss": 0.4147,
      "step": 4180
    },
    {
      "epoch": 1.197142857142857,
      "grad_norm": 17.677188873291016,
      "learning_rate": 1.8403809523809524e-05,
      "loss": 0.4698,
      "step": 4190
    },
    {
      "epoch": 1.2,
      "grad_norm": 9.322260856628418,
      "learning_rate": 1.8400000000000003e-05,
      "loss": 0.3898,
      "step": 4200
    },
    {
      "epoch": 1.2028571428571428,
      "grad_norm": 10.622913360595703,
      "learning_rate": 1.839619047619048e-05,
      "loss": 0.4353,
      "step": 4210
    },
    {
      "epoch": 1.2057142857142857,
      "grad_norm": 0.46684765815734863,
      "learning_rate": 1.8392380952380955e-05,
      "loss": 0.5253,
      "step": 4220
    },
    {
      "epoch": 1.2085714285714286,
      "grad_norm": 0.4305132329463959,
      "learning_rate": 1.838857142857143e-05,
      "loss": 0.4029,
      "step": 4230
    },
    {
      "epoch": 1.2114285714285715,
      "grad_norm": 0.9759644269943237,
      "learning_rate": 1.8384761904761906e-05,
      "loss": 0.2165,
      "step": 4240
    },
    {
      "epoch": 1.2142857142857142,
      "grad_norm": 0.14703211188316345,
      "learning_rate": 1.838095238095238e-05,
      "loss": 0.4614,
      "step": 4250
    },
    {
      "epoch": 1.217142857142857,
      "grad_norm": 30.415699005126953,
      "learning_rate": 1.8377142857142857e-05,
      "loss": 0.5153,
      "step": 4260
    },
    {
      "epoch": 1.22,
      "grad_norm": 13.201057434082031,
      "learning_rate": 1.8373333333333332e-05,
      "loss": 0.5333,
      "step": 4270
    },
    {
      "epoch": 1.2228571428571429,
      "grad_norm": 25.351783752441406,
      "learning_rate": 1.836952380952381e-05,
      "loss": 0.7239,
      "step": 4280
    },
    {
      "epoch": 1.2257142857142858,
      "grad_norm": 0.4074253439903259,
      "learning_rate": 1.8365714285714287e-05,
      "loss": 0.2271,
      "step": 4290
    },
    {
      "epoch": 1.2285714285714286,
      "grad_norm": 0.18704304099082947,
      "learning_rate": 1.8361904761904763e-05,
      "loss": 0.3198,
      "step": 4300
    },
    {
      "epoch": 1.2314285714285713,
      "grad_norm": 15.495862007141113,
      "learning_rate": 1.8358095238095238e-05,
      "loss": 0.3822,
      "step": 4310
    },
    {
      "epoch": 1.2342857142857142,
      "grad_norm": 35.94046401977539,
      "learning_rate": 1.8354285714285717e-05,
      "loss": 0.3639,
      "step": 4320
    },
    {
      "epoch": 1.237142857142857,
      "grad_norm": 2.704247236251831,
      "learning_rate": 1.8350476190476193e-05,
      "loss": 0.642,
      "step": 4330
    },
    {
      "epoch": 1.24,
      "grad_norm": 20.14732551574707,
      "learning_rate": 1.834666666666667e-05,
      "loss": 0.3217,
      "step": 4340
    },
    {
      "epoch": 1.2428571428571429,
      "grad_norm": 0.6601532697677612,
      "learning_rate": 1.8342857142857144e-05,
      "loss": 0.3232,
      "step": 4350
    },
    {
      "epoch": 1.2457142857142858,
      "grad_norm": 13.640856742858887,
      "learning_rate": 1.833904761904762e-05,
      "loss": 0.4372,
      "step": 4360
    },
    {
      "epoch": 1.2485714285714287,
      "grad_norm": 0.8208368420600891,
      "learning_rate": 1.83352380952381e-05,
      "loss": 0.5893,
      "step": 4370
    },
    {
      "epoch": 1.2514285714285713,
      "grad_norm": 0.33989137411117554,
      "learning_rate": 1.8331428571428574e-05,
      "loss": 0.2312,
      "step": 4380
    },
    {
      "epoch": 1.2542857142857142,
      "grad_norm": 12.255657196044922,
      "learning_rate": 1.832761904761905e-05,
      "loss": 0.2437,
      "step": 4390
    },
    {
      "epoch": 1.2571428571428571,
      "grad_norm": 0.1931907832622528,
      "learning_rate": 1.8323809523809525e-05,
      "loss": 0.8832,
      "step": 4400
    },
    {
      "epoch": 1.26,
      "grad_norm": 0.4179114103317261,
      "learning_rate": 1.832e-05,
      "loss": 0.6655,
      "step": 4410
    },
    {
      "epoch": 1.262857142857143,
      "grad_norm": 19.69399642944336,
      "learning_rate": 1.831619047619048e-05,
      "loss": 0.3617,
      "step": 4420
    },
    {
      "epoch": 1.2657142857142858,
      "grad_norm": 17.978031158447266,
      "learning_rate": 1.8312380952380955e-05,
      "loss": 0.499,
      "step": 4430
    },
    {
      "epoch": 1.2685714285714287,
      "grad_norm": 0.32791250944137573,
      "learning_rate": 1.830857142857143e-05,
      "loss": 0.119,
      "step": 4440
    },
    {
      "epoch": 1.2714285714285714,
      "grad_norm": 0.31526362895965576,
      "learning_rate": 1.8304761904761906e-05,
      "loss": 0.6631,
      "step": 4450
    },
    {
      "epoch": 1.2742857142857142,
      "grad_norm": 0.4231887757778168,
      "learning_rate": 1.8300952380952382e-05,
      "loss": 0.4533,
      "step": 4460
    },
    {
      "epoch": 1.2771428571428571,
      "grad_norm": 1.180917501449585,
      "learning_rate": 1.8297142857142858e-05,
      "loss": 0.3761,
      "step": 4470
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.5203018188476562,
      "learning_rate": 1.8293333333333333e-05,
      "loss": 0.2351,
      "step": 4480
    },
    {
      "epoch": 1.282857142857143,
      "grad_norm": 14.583853721618652,
      "learning_rate": 1.828952380952381e-05,
      "loss": 0.3511,
      "step": 4490
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 6.935585975646973,
      "learning_rate": 1.8285714285714288e-05,
      "loss": 0.1595,
      "step": 4500
    },
    {
      "epoch": 1.2885714285714287,
      "grad_norm": 0.34217649698257446,
      "learning_rate": 1.8281904761904763e-05,
      "loss": 0.7464,
      "step": 4510
    },
    {
      "epoch": 1.2914285714285714,
      "grad_norm": 0.1884804368019104,
      "learning_rate": 1.827809523809524e-05,
      "loss": 0.2349,
      "step": 4520
    },
    {
      "epoch": 1.2942857142857143,
      "grad_norm": 18.24878692626953,
      "learning_rate": 1.8274285714285715e-05,
      "loss": 0.1726,
      "step": 4530
    },
    {
      "epoch": 1.2971428571428572,
      "grad_norm": 0.15104420483112335,
      "learning_rate": 1.827047619047619e-05,
      "loss": 0.487,
      "step": 4540
    },
    {
      "epoch": 1.3,
      "grad_norm": 0.3373062014579773,
      "learning_rate": 1.826666666666667e-05,
      "loss": 0.6185,
      "step": 4550
    },
    {
      "epoch": 1.302857142857143,
      "grad_norm": 0.26005819439888,
      "learning_rate": 1.8262857142857145e-05,
      "loss": 0.3985,
      "step": 4560
    },
    {
      "epoch": 1.3057142857142856,
      "grad_norm": 0.18500962853431702,
      "learning_rate": 1.825904761904762e-05,
      "loss": 0.3827,
      "step": 4570
    },
    {
      "epoch": 1.3085714285714285,
      "grad_norm": 13.973786354064941,
      "learning_rate": 1.8255238095238096e-05,
      "loss": 0.6222,
      "step": 4580
    },
    {
      "epoch": 1.3114285714285714,
      "grad_norm": 0.9885916113853455,
      "learning_rate": 1.8251428571428575e-05,
      "loss": 0.1454,
      "step": 4590
    },
    {
      "epoch": 1.3142857142857143,
      "grad_norm": 8.86458683013916,
      "learning_rate": 1.824761904761905e-05,
      "loss": 0.2556,
      "step": 4600
    },
    {
      "epoch": 1.3171428571428572,
      "grad_norm": 0.08911831676959991,
      "learning_rate": 1.8243809523809526e-05,
      "loss": 0.3154,
      "step": 4610
    },
    {
      "epoch": 1.32,
      "grad_norm": 1.499031662940979,
      "learning_rate": 1.824e-05,
      "loss": 0.3525,
      "step": 4620
    },
    {
      "epoch": 1.322857142857143,
      "grad_norm": 0.7668809294700623,
      "learning_rate": 1.8236190476190477e-05,
      "loss": 0.2645,
      "step": 4630
    },
    {
      "epoch": 1.3257142857142856,
      "grad_norm": 0.10216621309518814,
      "learning_rate": 1.8232380952380956e-05,
      "loss": 0.535,
      "step": 4640
    },
    {
      "epoch": 1.3285714285714285,
      "grad_norm": 0.06559067219495773,
      "learning_rate": 1.822857142857143e-05,
      "loss": 0.2493,
      "step": 4650
    },
    {
      "epoch": 1.3314285714285714,
      "grad_norm": 0.09248959273099899,
      "learning_rate": 1.8224761904761907e-05,
      "loss": 0.4062,
      "step": 4660
    },
    {
      "epoch": 1.3342857142857143,
      "grad_norm": 1.0036349296569824,
      "learning_rate": 1.8220952380952383e-05,
      "loss": 0.7446,
      "step": 4670
    },
    {
      "epoch": 1.3371428571428572,
      "grad_norm": 0.6102598905563354,
      "learning_rate": 1.821714285714286e-05,
      "loss": 0.3481,
      "step": 4680
    },
    {
      "epoch": 1.34,
      "grad_norm": 0.4613012671470642,
      "learning_rate": 1.8213333333333334e-05,
      "loss": 0.3072,
      "step": 4690
    },
    {
      "epoch": 1.342857142857143,
      "grad_norm": 0.3075997233390808,
      "learning_rate": 1.820952380952381e-05,
      "loss": 0.2337,
      "step": 4700
    },
    {
      "epoch": 1.3457142857142856,
      "grad_norm": 0.10510478168725967,
      "learning_rate": 1.8205714285714285e-05,
      "loss": 0.3016,
      "step": 4710
    },
    {
      "epoch": 1.3485714285714285,
      "grad_norm": 13.850586891174316,
      "learning_rate": 1.8201904761904764e-05,
      "loss": 0.2854,
      "step": 4720
    },
    {
      "epoch": 1.3514285714285714,
      "grad_norm": 114.28234100341797,
      "learning_rate": 1.819809523809524e-05,
      "loss": 0.794,
      "step": 4730
    },
    {
      "epoch": 1.3542857142857143,
      "grad_norm": 0.12390287965536118,
      "learning_rate": 1.8194285714285715e-05,
      "loss": 0.6521,
      "step": 4740
    },
    {
      "epoch": 1.3571428571428572,
      "grad_norm": 0.4291640818119049,
      "learning_rate": 1.819047619047619e-05,
      "loss": 0.2315,
      "step": 4750
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 13.47060775756836,
      "learning_rate": 1.8186666666666666e-05,
      "loss": 0.274,
      "step": 4760
    },
    {
      "epoch": 1.362857142857143,
      "grad_norm": 0.2866239547729492,
      "learning_rate": 1.8182857142857145e-05,
      "loss": 0.3607,
      "step": 4770
    },
    {
      "epoch": 1.3657142857142857,
      "grad_norm": 0.2429809421300888,
      "learning_rate": 1.817904761904762e-05,
      "loss": 0.2839,
      "step": 4780
    },
    {
      "epoch": 1.3685714285714285,
      "grad_norm": 1.2418535947799683,
      "learning_rate": 1.8175238095238097e-05,
      "loss": 0.2319,
      "step": 4790
    },
    {
      "epoch": 1.3714285714285714,
      "grad_norm": 0.3911818563938141,
      "learning_rate": 1.8171428571428572e-05,
      "loss": 0.5881,
      "step": 4800
    },
    {
      "epoch": 1.3742857142857143,
      "grad_norm": 0.20153634250164032,
      "learning_rate": 1.816761904761905e-05,
      "loss": 0.2124,
      "step": 4810
    },
    {
      "epoch": 1.3771428571428572,
      "grad_norm": 0.15434689819812775,
      "learning_rate": 1.8163809523809527e-05,
      "loss": 0.3412,
      "step": 4820
    },
    {
      "epoch": 1.38,
      "grad_norm": 0.3310449421405792,
      "learning_rate": 1.8160000000000002e-05,
      "loss": 0.3319,
      "step": 4830
    },
    {
      "epoch": 1.3828571428571428,
      "grad_norm": 0.11325496435165405,
      "learning_rate": 1.8156190476190478e-05,
      "loss": 0.7149,
      "step": 4840
    },
    {
      "epoch": 1.3857142857142857,
      "grad_norm": 0.35459235310554504,
      "learning_rate": 1.8152380952380953e-05,
      "loss": 0.3595,
      "step": 4850
    },
    {
      "epoch": 1.3885714285714286,
      "grad_norm": 0.4106539785861969,
      "learning_rate": 1.8148571428571432e-05,
      "loss": 0.4073,
      "step": 4860
    },
    {
      "epoch": 1.3914285714285715,
      "grad_norm": 12.326053619384766,
      "learning_rate": 1.8144761904761908e-05,
      "loss": 0.7907,
      "step": 4870
    },
    {
      "epoch": 1.3942857142857144,
      "grad_norm": 0.45309609174728394,
      "learning_rate": 1.8140952380952384e-05,
      "loss": 0.0126,
      "step": 4880
    },
    {
      "epoch": 1.3971428571428572,
      "grad_norm": 0.33240458369255066,
      "learning_rate": 1.813714285714286e-05,
      "loss": 0.2149,
      "step": 4890
    },
    {
      "epoch": 1.4,
      "grad_norm": 16.302946090698242,
      "learning_rate": 1.8133333333333335e-05,
      "loss": 0.3163,
      "step": 4900
    },
    {
      "epoch": 1.4028571428571428,
      "grad_norm": 0.13291381299495697,
      "learning_rate": 1.812952380952381e-05,
      "loss": 0.1124,
      "step": 4910
    },
    {
      "epoch": 1.4057142857142857,
      "grad_norm": 0.09011776745319366,
      "learning_rate": 1.8125714285714286e-05,
      "loss": 0.3963,
      "step": 4920
    },
    {
      "epoch": 1.4085714285714286,
      "grad_norm": 18.282581329345703,
      "learning_rate": 1.812190476190476e-05,
      "loss": 0.5793,
      "step": 4930
    },
    {
      "epoch": 1.4114285714285715,
      "grad_norm": 0.6850345730781555,
      "learning_rate": 1.811809523809524e-05,
      "loss": 0.1726,
      "step": 4940
    },
    {
      "epoch": 1.4142857142857144,
      "grad_norm": 30.259201049804688,
      "learning_rate": 1.8114285714285716e-05,
      "loss": 0.4342,
      "step": 4950
    },
    {
      "epoch": 1.4171428571428573,
      "grad_norm": 0.1650380790233612,
      "learning_rate": 1.811047619047619e-05,
      "loss": 0.4455,
      "step": 4960
    },
    {
      "epoch": 1.42,
      "grad_norm": 4.451655864715576,
      "learning_rate": 1.8106666666666667e-05,
      "loss": 0.6825,
      "step": 4970
    },
    {
      "epoch": 1.4228571428571428,
      "grad_norm": 14.132301330566406,
      "learning_rate": 1.8102857142857143e-05,
      "loss": 0.6834,
      "step": 4980
    },
    {
      "epoch": 1.4257142857142857,
      "grad_norm": 13.713637351989746,
      "learning_rate": 1.8099047619047622e-05,
      "loss": 0.4468,
      "step": 4990
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 0.0903613343834877,
      "learning_rate": 1.8095238095238097e-05,
      "loss": 0.2525,
      "step": 5000
    },
    {
      "epoch": 1.4314285714285715,
      "grad_norm": 0.515978991985321,
      "learning_rate": 1.8091428571428573e-05,
      "loss": 0.1782,
      "step": 5010
    },
    {
      "epoch": 1.4342857142857142,
      "grad_norm": 0.41607511043548584,
      "learning_rate": 1.808761904761905e-05,
      "loss": 0.2913,
      "step": 5020
    },
    {
      "epoch": 1.4371428571428573,
      "grad_norm": 0.11232530325651169,
      "learning_rate": 1.8083809523809524e-05,
      "loss": 0.336,
      "step": 5030
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.04882444813847542,
      "learning_rate": 1.8080000000000003e-05,
      "loss": 0.2469,
      "step": 5040
    },
    {
      "epoch": 1.4428571428571428,
      "grad_norm": 0.1450503021478653,
      "learning_rate": 1.807619047619048e-05,
      "loss": 0.4144,
      "step": 5050
    },
    {
      "epoch": 1.4457142857142857,
      "grad_norm": 1.4811445474624634,
      "learning_rate": 1.8072380952380954e-05,
      "loss": 0.1795,
      "step": 5060
    },
    {
      "epoch": 1.4485714285714286,
      "grad_norm": 0.04738010838627815,
      "learning_rate": 1.806857142857143e-05,
      "loss": 1.144,
      "step": 5070
    },
    {
      "epoch": 1.4514285714285715,
      "grad_norm": 14.933079719543457,
      "learning_rate": 1.806476190476191e-05,
      "loss": 0.3016,
      "step": 5080
    },
    {
      "epoch": 1.4542857142857142,
      "grad_norm": 66.02151489257812,
      "learning_rate": 1.8060952380952384e-05,
      "loss": 0.2624,
      "step": 5090
    },
    {
      "epoch": 1.457142857142857,
      "grad_norm": 32.94596481323242,
      "learning_rate": 1.8057142857142857e-05,
      "loss": 0.2885,
      "step": 5100
    },
    {
      "epoch": 1.46,
      "grad_norm": 0.6654701828956604,
      "learning_rate": 1.8053333333333332e-05,
      "loss": 0.4608,
      "step": 5110
    },
    {
      "epoch": 1.4628571428571429,
      "grad_norm": 14.617047309875488,
      "learning_rate": 1.804952380952381e-05,
      "loss": 0.3673,
      "step": 5120
    },
    {
      "epoch": 1.4657142857142857,
      "grad_norm": 0.6662327647209167,
      "learning_rate": 1.8045714285714287e-05,
      "loss": 0.4357,
      "step": 5130
    },
    {
      "epoch": 1.4685714285714286,
      "grad_norm": 0.8201406598091125,
      "learning_rate": 1.8041904761904762e-05,
      "loss": 0.339,
      "step": 5140
    },
    {
      "epoch": 1.4714285714285715,
      "grad_norm": 10.543813705444336,
      "learning_rate": 1.8038095238095238e-05,
      "loss": 0.3381,
      "step": 5150
    },
    {
      "epoch": 1.4742857142857142,
      "grad_norm": 0.34032127261161804,
      "learning_rate": 1.8034285714285717e-05,
      "loss": 0.7149,
      "step": 5160
    },
    {
      "epoch": 1.477142857142857,
      "grad_norm": 1.0017987489700317,
      "learning_rate": 1.8030476190476192e-05,
      "loss": 0.2722,
      "step": 5170
    },
    {
      "epoch": 1.48,
      "grad_norm": 0.6132124662399292,
      "learning_rate": 1.8026666666666668e-05,
      "loss": 0.4444,
      "step": 5180
    },
    {
      "epoch": 1.4828571428571429,
      "grad_norm": 0.8116687536239624,
      "learning_rate": 1.8022857142857144e-05,
      "loss": 0.4042,
      "step": 5190
    },
    {
      "epoch": 1.4857142857142858,
      "grad_norm": 0.3682577610015869,
      "learning_rate": 1.801904761904762e-05,
      "loss": 0.277,
      "step": 5200
    },
    {
      "epoch": 1.4885714285714284,
      "grad_norm": 0.30328476428985596,
      "learning_rate": 1.8015238095238098e-05,
      "loss": 0.5886,
      "step": 5210
    },
    {
      "epoch": 1.4914285714285715,
      "grad_norm": 0.7572634816169739,
      "learning_rate": 1.8011428571428574e-05,
      "loss": 0.4499,
      "step": 5220
    },
    {
      "epoch": 1.4942857142857142,
      "grad_norm": 26.90070343017578,
      "learning_rate": 1.800761904761905e-05,
      "loss": 0.1697,
      "step": 5230
    },
    {
      "epoch": 1.497142857142857,
      "grad_norm": 1.0339741706848145,
      "learning_rate": 1.8003809523809525e-05,
      "loss": 0.083,
      "step": 5240
    },
    {
      "epoch": 1.5,
      "grad_norm": 16.89773178100586,
      "learning_rate": 1.8e-05,
      "loss": 0.3152,
      "step": 5250
    },
    {
      "epoch": 1.502857142857143,
      "grad_norm": 0.14381159842014313,
      "learning_rate": 1.799619047619048e-05,
      "loss": 0.7589,
      "step": 5260
    },
    {
      "epoch": 1.5057142857142858,
      "grad_norm": 0.18396008014678955,
      "learning_rate": 1.7992380952380955e-05,
      "loss": 0.2778,
      "step": 5270
    },
    {
      "epoch": 1.5085714285714285,
      "grad_norm": 1.5805166959762573,
      "learning_rate": 1.798857142857143e-05,
      "loss": 0.4376,
      "step": 5280
    },
    {
      "epoch": 1.5114285714285716,
      "grad_norm": 0.2228095531463623,
      "learning_rate": 1.7984761904761906e-05,
      "loss": 0.2934,
      "step": 5290
    },
    {
      "epoch": 1.5142857142857142,
      "grad_norm": 1.0040501356124878,
      "learning_rate": 1.7980952380952382e-05,
      "loss": 0.4117,
      "step": 5300
    },
    {
      "epoch": 1.5171428571428571,
      "grad_norm": 0.2542667090892792,
      "learning_rate": 1.797714285714286e-05,
      "loss": 0.5034,
      "step": 5310
    },
    {
      "epoch": 1.52,
      "grad_norm": 12.130512237548828,
      "learning_rate": 1.7973333333333333e-05,
      "loss": 0.5373,
      "step": 5320
    },
    {
      "epoch": 1.522857142857143,
      "grad_norm": 1.0364283323287964,
      "learning_rate": 1.796952380952381e-05,
      "loss": 0.2769,
      "step": 5330
    },
    {
      "epoch": 1.5257142857142858,
      "grad_norm": 0.4092901945114136,
      "learning_rate": 1.7965714285714287e-05,
      "loss": 0.2208,
      "step": 5340
    },
    {
      "epoch": 1.5285714285714285,
      "grad_norm": 1.391255497932434,
      "learning_rate": 1.7961904761904763e-05,
      "loss": 0.8374,
      "step": 5350
    },
    {
      "epoch": 1.5314285714285716,
      "grad_norm": 0.9078632593154907,
      "learning_rate": 1.795809523809524e-05,
      "loss": 0.3198,
      "step": 5360
    },
    {
      "epoch": 1.5342857142857143,
      "grad_norm": 20.514251708984375,
      "learning_rate": 1.7954285714285714e-05,
      "loss": 0.4081,
      "step": 5370
    },
    {
      "epoch": 1.5371428571428571,
      "grad_norm": 0.5625103712081909,
      "learning_rate": 1.7950476190476193e-05,
      "loss": 0.3147,
      "step": 5380
    },
    {
      "epoch": 1.54,
      "grad_norm": 0.12209626287221909,
      "learning_rate": 1.794666666666667e-05,
      "loss": 0.3301,
      "step": 5390
    },
    {
      "epoch": 1.5428571428571427,
      "grad_norm": 0.27555111050605774,
      "learning_rate": 1.7942857142857144e-05,
      "loss": 0.4549,
      "step": 5400
    },
    {
      "epoch": 1.5457142857142858,
      "grad_norm": 12.07695198059082,
      "learning_rate": 1.793904761904762e-05,
      "loss": 0.3133,
      "step": 5410
    },
    {
      "epoch": 1.5485714285714285,
      "grad_norm": 0.12104002386331558,
      "learning_rate": 1.7935238095238096e-05,
      "loss": 0.8197,
      "step": 5420
    },
    {
      "epoch": 1.5514285714285714,
      "grad_norm": 0.27324721217155457,
      "learning_rate": 1.7931428571428574e-05,
      "loss": 0.3287,
      "step": 5430
    },
    {
      "epoch": 1.5542857142857143,
      "grad_norm": 0.12395475059747696,
      "learning_rate": 1.792761904761905e-05,
      "loss": 0.1572,
      "step": 5440
    },
    {
      "epoch": 1.5571428571428572,
      "grad_norm": 0.11679255962371826,
      "learning_rate": 1.7923809523809526e-05,
      "loss": 0.1554,
      "step": 5450
    },
    {
      "epoch": 1.56,
      "grad_norm": 0.25974422693252563,
      "learning_rate": 1.792e-05,
      "loss": 0.4064,
      "step": 5460
    },
    {
      "epoch": 1.5628571428571427,
      "grad_norm": 0.10441318899393082,
      "learning_rate": 1.7916190476190477e-05,
      "loss": 0.2664,
      "step": 5470
    },
    {
      "epoch": 1.5657142857142858,
      "grad_norm": 14.940489768981934,
      "learning_rate": 1.7912380952380956e-05,
      "loss": 0.6091,
      "step": 5480
    },
    {
      "epoch": 1.5685714285714285,
      "grad_norm": 0.40252748131752014,
      "learning_rate": 1.790857142857143e-05,
      "loss": 0.5692,
      "step": 5490
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.3357144296169281,
      "learning_rate": 1.7904761904761907e-05,
      "loss": 0.4152,
      "step": 5500
    },
    {
      "epoch": 1.5742857142857143,
      "grad_norm": 11.646759033203125,
      "learning_rate": 1.7900952380952383e-05,
      "loss": 0.5429,
      "step": 5510
    },
    {
      "epoch": 1.5771428571428572,
      "grad_norm": 12.337810516357422,
      "learning_rate": 1.7897142857142858e-05,
      "loss": 0.4954,
      "step": 5520
    },
    {
      "epoch": 1.58,
      "grad_norm": 0.17201535403728485,
      "learning_rate": 1.7893333333333337e-05,
      "loss": 0.1198,
      "step": 5530
    },
    {
      "epoch": 1.5828571428571427,
      "grad_norm": 0.13227631151676178,
      "learning_rate": 1.788952380952381e-05,
      "loss": 0.209,
      "step": 5540
    },
    {
      "epoch": 1.5857142857142859,
      "grad_norm": 14.709573745727539,
      "learning_rate": 1.7885714285714285e-05,
      "loss": 0.6757,
      "step": 5550
    },
    {
      "epoch": 1.5885714285714285,
      "grad_norm": 0.2089645266532898,
      "learning_rate": 1.7881904761904764e-05,
      "loss": 0.21,
      "step": 5560
    },
    {
      "epoch": 1.5914285714285714,
      "grad_norm": 0.25162360072135925,
      "learning_rate": 1.787809523809524e-05,
      "loss": 0.4907,
      "step": 5570
    },
    {
      "epoch": 1.5942857142857143,
      "grad_norm": 0.2189425826072693,
      "learning_rate": 1.7874285714285715e-05,
      "loss": 0.281,
      "step": 5580
    },
    {
      "epoch": 1.5971428571428572,
      "grad_norm": 12.366610527038574,
      "learning_rate": 1.787047619047619e-05,
      "loss": 0.418,
      "step": 5590
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.31896206736564636,
      "learning_rate": 1.7866666666666666e-05,
      "loss": 0.3449,
      "step": 5600
    },
    {
      "epoch": 1.6028571428571428,
      "grad_norm": 104.23517608642578,
      "learning_rate": 1.7862857142857145e-05,
      "loss": 0.6218,
      "step": 5610
    },
    {
      "epoch": 1.6057142857142859,
      "grad_norm": 0.43968507647514343,
      "learning_rate": 1.785904761904762e-05,
      "loss": 0.2299,
      "step": 5620
    },
    {
      "epoch": 1.6085714285714285,
      "grad_norm": 0.16014045476913452,
      "learning_rate": 1.7855238095238096e-05,
      "loss": 0.4155,
      "step": 5630
    },
    {
      "epoch": 1.6114285714285714,
      "grad_norm": 1.8911117315292358,
      "learning_rate": 1.7851428571428572e-05,
      "loss": 0.4112,
      "step": 5640
    },
    {
      "epoch": 1.6142857142857143,
      "grad_norm": 0.247153177857399,
      "learning_rate": 1.784761904761905e-05,
      "loss": 0.2339,
      "step": 5650
    },
    {
      "epoch": 1.617142857142857,
      "grad_norm": 0.43258142471313477,
      "learning_rate": 1.7843809523809526e-05,
      "loss": 0.426,
      "step": 5660
    },
    {
      "epoch": 1.62,
      "grad_norm": 11.430171012878418,
      "learning_rate": 1.7840000000000002e-05,
      "loss": 0.458,
      "step": 5670
    },
    {
      "epoch": 1.6228571428571428,
      "grad_norm": 12.747138977050781,
      "learning_rate": 1.7836190476190478e-05,
      "loss": 0.2338,
      "step": 5680
    },
    {
      "epoch": 1.6257142857142857,
      "grad_norm": 0.09740903228521347,
      "learning_rate": 1.7832380952380953e-05,
      "loss": 0.4439,
      "step": 5690
    },
    {
      "epoch": 1.6285714285714286,
      "grad_norm": 0.9510939717292786,
      "learning_rate": 1.7828571428571432e-05,
      "loss": 0.5021,
      "step": 5700
    },
    {
      "epoch": 1.6314285714285715,
      "grad_norm": 0.0851241946220398,
      "learning_rate": 1.7824761904761908e-05,
      "loss": 0.3483,
      "step": 5710
    },
    {
      "epoch": 1.6342857142857143,
      "grad_norm": 20.72119140625,
      "learning_rate": 1.7820952380952383e-05,
      "loss": 0.7148,
      "step": 5720
    },
    {
      "epoch": 1.637142857142857,
      "grad_norm": 14.319897651672363,
      "learning_rate": 1.781714285714286e-05,
      "loss": 0.1266,
      "step": 5730
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 0.08252348005771637,
      "learning_rate": 1.7813333333333334e-05,
      "loss": 0.2728,
      "step": 5740
    },
    {
      "epoch": 1.6428571428571428,
      "grad_norm": 0.2833843529224396,
      "learning_rate": 1.780952380952381e-05,
      "loss": 0.5179,
      "step": 5750
    },
    {
      "epoch": 1.6457142857142857,
      "grad_norm": 0.09551817178726196,
      "learning_rate": 1.7805714285714286e-05,
      "loss": 0.0076,
      "step": 5760
    },
    {
      "epoch": 1.6485714285714286,
      "grad_norm": 29.821928024291992,
      "learning_rate": 1.780190476190476e-05,
      "loss": 0.5606,
      "step": 5770
    },
    {
      "epoch": 1.6514285714285715,
      "grad_norm": 14.752562522888184,
      "learning_rate": 1.779809523809524e-05,
      "loss": 0.1544,
      "step": 5780
    },
    {
      "epoch": 1.6542857142857144,
      "grad_norm": 17.284198760986328,
      "learning_rate": 1.7794285714285716e-05,
      "loss": 0.9519,
      "step": 5790
    },
    {
      "epoch": 1.657142857142857,
      "grad_norm": 13.887701034545898,
      "learning_rate": 1.779047619047619e-05,
      "loss": 0.4049,
      "step": 5800
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 0.08081265538930893,
      "learning_rate": 1.7786666666666667e-05,
      "loss": 0.1998,
      "step": 5810
    },
    {
      "epoch": 1.6628571428571428,
      "grad_norm": 0.05398988723754883,
      "learning_rate": 1.7782857142857142e-05,
      "loss": 0.0846,
      "step": 5820
    },
    {
      "epoch": 1.6657142857142857,
      "grad_norm": 0.06347411125898361,
      "learning_rate": 1.777904761904762e-05,
      "loss": 0.4226,
      "step": 5830
    },
    {
      "epoch": 1.6685714285714286,
      "grad_norm": 13.04503345489502,
      "learning_rate": 1.7775238095238097e-05,
      "loss": 0.6015,
      "step": 5840
    },
    {
      "epoch": 1.6714285714285713,
      "grad_norm": 0.11113961786031723,
      "learning_rate": 1.7771428571428573e-05,
      "loss": 0.1523,
      "step": 5850
    },
    {
      "epoch": 1.6742857142857144,
      "grad_norm": 0.780779242515564,
      "learning_rate": 1.7767619047619048e-05,
      "loss": 0.4054,
      "step": 5860
    },
    {
      "epoch": 1.677142857142857,
      "grad_norm": 0.3912104070186615,
      "learning_rate": 1.7763809523809524e-05,
      "loss": 0.2213,
      "step": 5870
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.11486997455358505,
      "learning_rate": 1.7760000000000003e-05,
      "loss": 0.6692,
      "step": 5880
    },
    {
      "epoch": 1.6828571428571428,
      "grad_norm": 25.024511337280273,
      "learning_rate": 1.775619047619048e-05,
      "loss": 0.5863,
      "step": 5890
    },
    {
      "epoch": 1.6857142857142857,
      "grad_norm": 0.157716304063797,
      "learning_rate": 1.7752380952380954e-05,
      "loss": 0.2627,
      "step": 5900
    },
    {
      "epoch": 1.6885714285714286,
      "grad_norm": 6.278134346008301,
      "learning_rate": 1.774857142857143e-05,
      "loss": 0.1936,
      "step": 5910
    },
    {
      "epoch": 1.6914285714285713,
      "grad_norm": 0.16789111495018005,
      "learning_rate": 1.774476190476191e-05,
      "loss": 0.2914,
      "step": 5920
    },
    {
      "epoch": 1.6942857142857144,
      "grad_norm": 11.547998428344727,
      "learning_rate": 1.7740952380952384e-05,
      "loss": 0.7892,
      "step": 5930
    },
    {
      "epoch": 1.697142857142857,
      "grad_norm": 133.55641174316406,
      "learning_rate": 1.773714285714286e-05,
      "loss": 0.4107,
      "step": 5940
    },
    {
      "epoch": 1.7,
      "grad_norm": 0.7397486567497253,
      "learning_rate": 1.7733333333333335e-05,
      "loss": 0.4515,
      "step": 5950
    },
    {
      "epoch": 1.7028571428571428,
      "grad_norm": 13.030305862426758,
      "learning_rate": 1.772952380952381e-05,
      "loss": 0.4964,
      "step": 5960
    },
    {
      "epoch": 1.7057142857142857,
      "grad_norm": 22.05052947998047,
      "learning_rate": 1.7725714285714286e-05,
      "loss": 0.3623,
      "step": 5970
    },
    {
      "epoch": 1.7085714285714286,
      "grad_norm": 3.405240297317505,
      "learning_rate": 1.7721904761904762e-05,
      "loss": 0.3802,
      "step": 5980
    },
    {
      "epoch": 1.7114285714285713,
      "grad_norm": 9.066624641418457,
      "learning_rate": 1.7718095238095238e-05,
      "loss": 0.4584,
      "step": 5990
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 14.127467155456543,
      "learning_rate": 1.7714285714285717e-05,
      "loss": 0.2346,
      "step": 6000
    },
    {
      "epoch": 1.717142857142857,
      "grad_norm": 0.6290062069892883,
      "learning_rate": 1.7710476190476192e-05,
      "loss": 0.2544,
      "step": 6010
    },
    {
      "epoch": 1.72,
      "grad_norm": 17.49376678466797,
      "learning_rate": 1.7706666666666668e-05,
      "loss": 0.4208,
      "step": 6020
    },
    {
      "epoch": 1.7228571428571429,
      "grad_norm": 0.8457037806510925,
      "learning_rate": 1.7702857142857143e-05,
      "loss": 0.42,
      "step": 6030
    },
    {
      "epoch": 1.7257142857142858,
      "grad_norm": 0.7704611420631409,
      "learning_rate": 1.769904761904762e-05,
      "loss": 0.4882,
      "step": 6040
    },
    {
      "epoch": 1.7285714285714286,
      "grad_norm": 0.19721490144729614,
      "learning_rate": 1.7695238095238098e-05,
      "loss": 0.1059,
      "step": 6050
    },
    {
      "epoch": 1.7314285714285713,
      "grad_norm": 0.09407360106706619,
      "learning_rate": 1.7691428571428573e-05,
      "loss": 0.3826,
      "step": 6060
    },
    {
      "epoch": 1.7342857142857144,
      "grad_norm": 0.3548150956630707,
      "learning_rate": 1.768761904761905e-05,
      "loss": 0.8209,
      "step": 6070
    },
    {
      "epoch": 1.737142857142857,
      "grad_norm": 0.9788267016410828,
      "learning_rate": 1.7683809523809525e-05,
      "loss": 0.5066,
      "step": 6080
    },
    {
      "epoch": 1.74,
      "grad_norm": 0.2656415104866028,
      "learning_rate": 1.768e-05,
      "loss": 0.2853,
      "step": 6090
    },
    {
      "epoch": 1.7428571428571429,
      "grad_norm": 0.18396256864070892,
      "learning_rate": 1.767619047619048e-05,
      "loss": 0.1153,
      "step": 6100
    },
    {
      "epoch": 1.7457142857142856,
      "grad_norm": 0.2827898859977722,
      "learning_rate": 1.7672380952380955e-05,
      "loss": 0.2435,
      "step": 6110
    },
    {
      "epoch": 1.7485714285714287,
      "grad_norm": 12.2979097366333,
      "learning_rate": 1.766857142857143e-05,
      "loss": 0.166,
      "step": 6120
    },
    {
      "epoch": 1.7514285714285713,
      "grad_norm": 0.20623239874839783,
      "learning_rate": 1.7664761904761906e-05,
      "loss": 0.4543,
      "step": 6130
    },
    {
      "epoch": 1.7542857142857144,
      "grad_norm": 12.366190910339355,
      "learning_rate": 1.766095238095238e-05,
      "loss": 0.7406,
      "step": 6140
    },
    {
      "epoch": 1.7571428571428571,
      "grad_norm": 11.987811088562012,
      "learning_rate": 1.765714285714286e-05,
      "loss": 0.7573,
      "step": 6150
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.31473174691200256,
      "learning_rate": 1.7653333333333336e-05,
      "loss": 0.4146,
      "step": 6160
    },
    {
      "epoch": 1.762857142857143,
      "grad_norm": 0.5250070691108704,
      "learning_rate": 1.764952380952381e-05,
      "loss": 0.2611,
      "step": 6170
    },
    {
      "epoch": 1.7657142857142856,
      "grad_norm": 0.21318575739860535,
      "learning_rate": 1.7645714285714287e-05,
      "loss": 0.0064,
      "step": 6180
    },
    {
      "epoch": 1.7685714285714287,
      "grad_norm": 0.10522930324077606,
      "learning_rate": 1.7641904761904763e-05,
      "loss": 0.4423,
      "step": 6190
    },
    {
      "epoch": 1.7714285714285714,
      "grad_norm": 0.0775262713432312,
      "learning_rate": 1.7638095238095238e-05,
      "loss": 1.0645,
      "step": 6200
    },
    {
      "epoch": 1.7742857142857142,
      "grad_norm": 0.444614440202713,
      "learning_rate": 1.7634285714285714e-05,
      "loss": 0.8809,
      "step": 6210
    },
    {
      "epoch": 1.7771428571428571,
      "grad_norm": 11.505267143249512,
      "learning_rate": 1.7630476190476193e-05,
      "loss": 0.6175,
      "step": 6220
    },
    {
      "epoch": 1.78,
      "grad_norm": 22.320556640625,
      "learning_rate": 1.762666666666667e-05,
      "loss": 0.2226,
      "step": 6230
    },
    {
      "epoch": 1.782857142857143,
      "grad_norm": 0.7836236357688904,
      "learning_rate": 1.7622857142857144e-05,
      "loss": 0.2079,
      "step": 6240
    },
    {
      "epoch": 1.7857142857142856,
      "grad_norm": 11.655888557434082,
      "learning_rate": 1.761904761904762e-05,
      "loss": 0.3848,
      "step": 6250
    },
    {
      "epoch": 1.7885714285714287,
      "grad_norm": 0.0777185931801796,
      "learning_rate": 1.7615238095238095e-05,
      "loss": 0.2189,
      "step": 6260
    },
    {
      "epoch": 1.7914285714285714,
      "grad_norm": 0.4733625650405884,
      "learning_rate": 1.7611428571428574e-05,
      "loss": 0.1867,
      "step": 6270
    },
    {
      "epoch": 1.7942857142857143,
      "grad_norm": 14.218162536621094,
      "learning_rate": 1.760761904761905e-05,
      "loss": 0.4308,
      "step": 6280
    },
    {
      "epoch": 1.7971428571428572,
      "grad_norm": 25.86809539794922,
      "learning_rate": 1.7603809523809525e-05,
      "loss": 0.6611,
      "step": 6290
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.5221202969551086,
      "learning_rate": 1.76e-05,
      "loss": 0.1059,
      "step": 6300
    },
    {
      "epoch": 1.802857142857143,
      "grad_norm": 0.2563519775867462,
      "learning_rate": 1.7596190476190476e-05,
      "loss": 0.0094,
      "step": 6310
    },
    {
      "epoch": 1.8057142857142856,
      "grad_norm": 13.16480541229248,
      "learning_rate": 1.7592380952380955e-05,
      "loss": 0.7137,
      "step": 6320
    },
    {
      "epoch": 1.8085714285714287,
      "grad_norm": 14.102355003356934,
      "learning_rate": 1.758857142857143e-05,
      "loss": 0.2839,
      "step": 6330
    },
    {
      "epoch": 1.8114285714285714,
      "grad_norm": 0.16212250292301178,
      "learning_rate": 1.7584761904761907e-05,
      "loss": 0.2478,
      "step": 6340
    },
    {
      "epoch": 1.8142857142857143,
      "grad_norm": 0.14994342625141144,
      "learning_rate": 1.7580952380952382e-05,
      "loss": 0.3194,
      "step": 6350
    },
    {
      "epoch": 1.8171428571428572,
      "grad_norm": 14.439864158630371,
      "learning_rate": 1.7577142857142858e-05,
      "loss": 0.2459,
      "step": 6360
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 23.931032180786133,
      "learning_rate": 1.7573333333333337e-05,
      "loss": 0.5089,
      "step": 6370
    },
    {
      "epoch": 1.822857142857143,
      "grad_norm": 0.0929444283246994,
      "learning_rate": 1.7569523809523812e-05,
      "loss": 0.342,
      "step": 6380
    },
    {
      "epoch": 1.8257142857142856,
      "grad_norm": 0.24358105659484863,
      "learning_rate": 1.7565714285714288e-05,
      "loss": 0.0052,
      "step": 6390
    },
    {
      "epoch": 1.8285714285714287,
      "grad_norm": 0.1901489645242691,
      "learning_rate": 1.7561904761904763e-05,
      "loss": 0.4641,
      "step": 6400
    },
    {
      "epoch": 1.8314285714285714,
      "grad_norm": 0.3331841826438904,
      "learning_rate": 1.755809523809524e-05,
      "loss": 0.1848,
      "step": 6410
    },
    {
      "epoch": 1.8342857142857143,
      "grad_norm": 0.056475382298231125,
      "learning_rate": 1.7554285714285715e-05,
      "loss": 0.472,
      "step": 6420
    },
    {
      "epoch": 1.8371428571428572,
      "grad_norm": 0.07223807275295258,
      "learning_rate": 1.755047619047619e-05,
      "loss": 0.5781,
      "step": 6430
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.16539216041564941,
      "learning_rate": 1.7546666666666666e-05,
      "loss": 0.1154,
      "step": 6440
    },
    {
      "epoch": 1.842857142857143,
      "grad_norm": 0.38414037227630615,
      "learning_rate": 1.7542857142857145e-05,
      "loss": 0.1983,
      "step": 6450
    },
    {
      "epoch": 1.8457142857142856,
      "grad_norm": 0.12427283078432083,
      "learning_rate": 1.753904761904762e-05,
      "loss": 0.3574,
      "step": 6460
    },
    {
      "epoch": 1.8485714285714285,
      "grad_norm": 0.10171855241060257,
      "learning_rate": 1.7535238095238096e-05,
      "loss": 0.1044,
      "step": 6470
    },
    {
      "epoch": 1.8514285714285714,
      "grad_norm": 27.09733772277832,
      "learning_rate": 1.753142857142857e-05,
      "loss": 0.4722,
      "step": 6480
    },
    {
      "epoch": 1.8542857142857143,
      "grad_norm": 24.992002487182617,
      "learning_rate": 1.752761904761905e-05,
      "loss": 0.4146,
      "step": 6490
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 1.2237571477890015,
      "learning_rate": 1.7523809523809526e-05,
      "loss": 0.2367,
      "step": 6500
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 13.683640480041504,
      "learning_rate": 1.752e-05,
      "loss": 0.2843,
      "step": 6510
    },
    {
      "epoch": 1.862857142857143,
      "grad_norm": 0.12187181413173676,
      "learning_rate": 1.7516190476190477e-05,
      "loss": 0.2166,
      "step": 6520
    },
    {
      "epoch": 1.8657142857142857,
      "grad_norm": 0.640123724937439,
      "learning_rate": 1.7512380952380953e-05,
      "loss": 0.7128,
      "step": 6530
    },
    {
      "epoch": 1.8685714285714285,
      "grad_norm": 29.849388122558594,
      "learning_rate": 1.7508571428571432e-05,
      "loss": 0.6966,
      "step": 6540
    },
    {
      "epoch": 1.8714285714285714,
      "grad_norm": 0.9772548079490662,
      "learning_rate": 1.7504761904761907e-05,
      "loss": 0.6036,
      "step": 6550
    },
    {
      "epoch": 1.8742857142857143,
      "grad_norm": 0.11577065289020538,
      "learning_rate": 1.7500952380952383e-05,
      "loss": 0.4522,
      "step": 6560
    },
    {
      "epoch": 1.8771428571428572,
      "grad_norm": 1.2583718299865723,
      "learning_rate": 1.749714285714286e-05,
      "loss": 0.2562,
      "step": 6570
    },
    {
      "epoch": 1.88,
      "grad_norm": 0.0497463196516037,
      "learning_rate": 1.7493333333333334e-05,
      "loss": 0.1132,
      "step": 6580
    },
    {
      "epoch": 1.882857142857143,
      "grad_norm": 0.03298433497548103,
      "learning_rate": 1.7489523809523813e-05,
      "loss": 0.1168,
      "step": 6590
    },
    {
      "epoch": 1.8857142857142857,
      "grad_norm": 0.031727854162454605,
      "learning_rate": 1.748571428571429e-05,
      "loss": 0.4303,
      "step": 6600
    },
    {
      "epoch": 1.8885714285714286,
      "grad_norm": 0.0681150034070015,
      "learning_rate": 1.7481904761904764e-05,
      "loss": 0.7672,
      "step": 6610
    },
    {
      "epoch": 1.8914285714285715,
      "grad_norm": 0.46110275387763977,
      "learning_rate": 1.747809523809524e-05,
      "loss": 0.2514,
      "step": 6620
    },
    {
      "epoch": 1.8942857142857141,
      "grad_norm": 0.1518845558166504,
      "learning_rate": 1.7474285714285715e-05,
      "loss": 0.1792,
      "step": 6630
    },
    {
      "epoch": 1.8971428571428572,
      "grad_norm": 1.4992849826812744,
      "learning_rate": 1.747047619047619e-05,
      "loss": 0.2214,
      "step": 6640
    },
    {
      "epoch": 1.9,
      "grad_norm": 0.10745445638895035,
      "learning_rate": 1.7466666666666667e-05,
      "loss": 0.5072,
      "step": 6650
    },
    {
      "epoch": 1.9028571428571428,
      "grad_norm": 0.15742531418800354,
      "learning_rate": 1.7462857142857142e-05,
      "loss": 0.2656,
      "step": 6660
    },
    {
      "epoch": 1.9057142857142857,
      "grad_norm": 0.09688762575387955,
      "learning_rate": 1.745904761904762e-05,
      "loss": 0.2671,
      "step": 6670
    },
    {
      "epoch": 1.9085714285714286,
      "grad_norm": 0.1605639010667801,
      "learning_rate": 1.7455238095238097e-05,
      "loss": 0.4194,
      "step": 6680
    },
    {
      "epoch": 1.9114285714285715,
      "grad_norm": 15.433431625366211,
      "learning_rate": 1.7451428571428572e-05,
      "loss": 0.528,
      "step": 6690
    },
    {
      "epoch": 1.9142857142857141,
      "grad_norm": 0.23112955689430237,
      "learning_rate": 1.7447619047619048e-05,
      "loss": 0.1081,
      "step": 6700
    },
    {
      "epoch": 1.9171428571428573,
      "grad_norm": 0.6895323991775513,
      "learning_rate": 1.7443809523809523e-05,
      "loss": 0.6023,
      "step": 6710
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.21633368730545044,
      "learning_rate": 1.7440000000000002e-05,
      "loss": 0.2746,
      "step": 6720
    },
    {
      "epoch": 1.9228571428571428,
      "grad_norm": 0.8964827656745911,
      "learning_rate": 1.7436190476190478e-05,
      "loss": 0.3274,
      "step": 6730
    },
    {
      "epoch": 1.9257142857142857,
      "grad_norm": 0.29814818501472473,
      "learning_rate": 1.7432380952380954e-05,
      "loss": 0.5502,
      "step": 6740
    },
    {
      "epoch": 1.9285714285714286,
      "grad_norm": 0.4678395688533783,
      "learning_rate": 1.742857142857143e-05,
      "loss": 0.3252,
      "step": 6750
    },
    {
      "epoch": 1.9314285714285715,
      "grad_norm": 0.5017223358154297,
      "learning_rate": 1.7424761904761908e-05,
      "loss": 0.4445,
      "step": 6760
    },
    {
      "epoch": 1.9342857142857142,
      "grad_norm": 0.9778099060058594,
      "learning_rate": 1.7420952380952384e-05,
      "loss": 0.4443,
      "step": 6770
    },
    {
      "epoch": 1.9371428571428573,
      "grad_norm": 0.8631781339645386,
      "learning_rate": 1.741714285714286e-05,
      "loss": 0.17,
      "step": 6780
    },
    {
      "epoch": 1.94,
      "grad_norm": 0.05306965857744217,
      "learning_rate": 1.7413333333333335e-05,
      "loss": 0.362,
      "step": 6790
    },
    {
      "epoch": 1.9428571428571428,
      "grad_norm": 0.034674931317567825,
      "learning_rate": 1.740952380952381e-05,
      "loss": 0.5715,
      "step": 6800
    },
    {
      "epoch": 1.9457142857142857,
      "grad_norm": 0.5541523098945618,
      "learning_rate": 1.740571428571429e-05,
      "loss": 0.5392,
      "step": 6810
    },
    {
      "epoch": 1.9485714285714286,
      "grad_norm": 19.131681442260742,
      "learning_rate": 1.7401904761904765e-05,
      "loss": 0.1967,
      "step": 6820
    },
    {
      "epoch": 1.9514285714285715,
      "grad_norm": 0.05847250670194626,
      "learning_rate": 1.739809523809524e-05,
      "loss": 0.2578,
      "step": 6830
    },
    {
      "epoch": 1.9542857142857142,
      "grad_norm": 11.379075050354004,
      "learning_rate": 1.7394285714285716e-05,
      "loss": 0.8653,
      "step": 6840
    },
    {
      "epoch": 1.9571428571428573,
      "grad_norm": 0.5560583472251892,
      "learning_rate": 1.7390476190476192e-05,
      "loss": 0.2561,
      "step": 6850
    },
    {
      "epoch": 1.96,
      "grad_norm": 0.21958252787590027,
      "learning_rate": 1.7386666666666667e-05,
      "loss": 0.2495,
      "step": 6860
    },
    {
      "epoch": 1.9628571428571429,
      "grad_norm": 0.20695249736309052,
      "learning_rate": 1.7382857142857143e-05,
      "loss": 0.5066,
      "step": 6870
    },
    {
      "epoch": 1.9657142857142857,
      "grad_norm": 10.437541961669922,
      "learning_rate": 1.737904761904762e-05,
      "loss": 0.3728,
      "step": 6880
    },
    {
      "epoch": 1.9685714285714284,
      "grad_norm": 0.3744020462036133,
      "learning_rate": 1.7375238095238097e-05,
      "loss": 0.3185,
      "step": 6890
    },
    {
      "epoch": 1.9714285714285715,
      "grad_norm": 13.018670082092285,
      "learning_rate": 1.7371428571428573e-05,
      "loss": 0.8609,
      "step": 6900
    },
    {
      "epoch": 1.9742857142857142,
      "grad_norm": 0.32358303666114807,
      "learning_rate": 1.736761904761905e-05,
      "loss": 0.0134,
      "step": 6910
    },
    {
      "epoch": 1.977142857142857,
      "grad_norm": 12.054800987243652,
      "learning_rate": 1.7363809523809524e-05,
      "loss": 0.3471,
      "step": 6920
    },
    {
      "epoch": 1.98,
      "grad_norm": 0.12198082357645035,
      "learning_rate": 1.736e-05,
      "loss": 0.7257,
      "step": 6930
    },
    {
      "epoch": 1.9828571428571429,
      "grad_norm": 0.15640200674533844,
      "learning_rate": 1.735619047619048e-05,
      "loss": 0.3255,
      "step": 6940
    },
    {
      "epoch": 1.9857142857142858,
      "grad_norm": 0.09896942228078842,
      "learning_rate": 1.7352380952380954e-05,
      "loss": 0.1954,
      "step": 6950
    },
    {
      "epoch": 1.9885714285714284,
      "grad_norm": 18.718053817749023,
      "learning_rate": 1.734857142857143e-05,
      "loss": 0.5737,
      "step": 6960
    },
    {
      "epoch": 1.9914285714285715,
      "grad_norm": 0.7136094570159912,
      "learning_rate": 1.7344761904761906e-05,
      "loss": 0.1405,
      "step": 6970
    },
    {
      "epoch": 1.9942857142857142,
      "grad_norm": 0.04102465137839317,
      "learning_rate": 1.734095238095238e-05,
      "loss": 0.188,
      "step": 6980
    },
    {
      "epoch": 1.997142857142857,
      "grad_norm": 21.334611892700195,
      "learning_rate": 1.733714285714286e-05,
      "loss": 0.6139,
      "step": 6990
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.14280180633068085,
      "learning_rate": 1.7333333333333336e-05,
      "loss": 0.335,
      "step": 7000
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9166666666666666,
      "eval_f1": 0.6374269005847953,
      "eval_loss": 0.38612595200538635,
      "eval_precision": 0.7266666666666667,
      "eval_recall": 0.5677083333333334,
      "eval_runtime": 46.5534,
      "eval_samples_per_second": 64.442,
      "eval_steps_per_second": 2.019,
      "step": 7000
    },
    {
      "epoch": 2.0028571428571427,
      "grad_norm": 0.07577643543481827,
      "learning_rate": 1.732952380952381e-05,
      "loss": 0.2835,
      "step": 7010
    },
    {
      "epoch": 2.005714285714286,
      "grad_norm": 0.46197015047073364,
      "learning_rate": 1.7325714285714287e-05,
      "loss": 0.2047,
      "step": 7020
    },
    {
      "epoch": 2.0085714285714285,
      "grad_norm": 0.3887050449848175,
      "learning_rate": 1.7321904761904766e-05,
      "loss": 0.1011,
      "step": 7030
    },
    {
      "epoch": 2.0114285714285716,
      "grad_norm": 0.42534375190734863,
      "learning_rate": 1.731809523809524e-05,
      "loss": 0.0911,
      "step": 7040
    },
    {
      "epoch": 2.0142857142857142,
      "grad_norm": 0.04010165110230446,
      "learning_rate": 1.7314285714285717e-05,
      "loss": 0.3804,
      "step": 7050
    },
    {
      "epoch": 2.0171428571428573,
      "grad_norm": 0.04291130602359772,
      "learning_rate": 1.7310476190476193e-05,
      "loss": 0.5377,
      "step": 7060
    },
    {
      "epoch": 2.02,
      "grad_norm": 1.2075985670089722,
      "learning_rate": 1.7306666666666668e-05,
      "loss": 0.1043,
      "step": 7070
    },
    {
      "epoch": 2.0228571428571427,
      "grad_norm": 13.06177043914795,
      "learning_rate": 1.7302857142857144e-05,
      "loss": 0.3647,
      "step": 7080
    },
    {
      "epoch": 2.025714285714286,
      "grad_norm": 11.416580200195312,
      "learning_rate": 1.729904761904762e-05,
      "loss": 0.2064,
      "step": 7090
    },
    {
      "epoch": 2.0285714285714285,
      "grad_norm": 0.18859995901584625,
      "learning_rate": 1.7295238095238095e-05,
      "loss": 0.3293,
      "step": 7100
    },
    {
      "epoch": 2.0314285714285716,
      "grad_norm": 15.728053092956543,
      "learning_rate": 1.7291428571428574e-05,
      "loss": 0.2257,
      "step": 7110
    },
    {
      "epoch": 2.0342857142857143,
      "grad_norm": 0.20339557528495789,
      "learning_rate": 1.728761904761905e-05,
      "loss": 0.2018,
      "step": 7120
    },
    {
      "epoch": 2.0371428571428574,
      "grad_norm": 0.25518590211868286,
      "learning_rate": 1.7283809523809525e-05,
      "loss": 0.1559,
      "step": 7130
    },
    {
      "epoch": 2.04,
      "grad_norm": 33.32060241699219,
      "learning_rate": 1.728e-05,
      "loss": 0.5804,
      "step": 7140
    },
    {
      "epoch": 2.0428571428571427,
      "grad_norm": 0.1235639750957489,
      "learning_rate": 1.7276190476190476e-05,
      "loss": 0.4065,
      "step": 7150
    },
    {
      "epoch": 2.045714285714286,
      "grad_norm": 27.79957389831543,
      "learning_rate": 1.7272380952380955e-05,
      "loss": 0.416,
      "step": 7160
    },
    {
      "epoch": 2.0485714285714285,
      "grad_norm": 0.3138163089752197,
      "learning_rate": 1.726857142857143e-05,
      "loss": 0.2326,
      "step": 7170
    },
    {
      "epoch": 2.0514285714285716,
      "grad_norm": 0.2296753078699112,
      "learning_rate": 1.7264761904761906e-05,
      "loss": 0.2574,
      "step": 7180
    },
    {
      "epoch": 2.0542857142857143,
      "grad_norm": 0.32616472244262695,
      "learning_rate": 1.7260952380952382e-05,
      "loss": 0.4115,
      "step": 7190
    },
    {
      "epoch": 2.057142857142857,
      "grad_norm": 0.13770940899848938,
      "learning_rate": 1.7257142857142857e-05,
      "loss": 0.7103,
      "step": 7200
    },
    {
      "epoch": 2.06,
      "grad_norm": 0.16261684894561768,
      "learning_rate": 1.7253333333333336e-05,
      "loss": 0.1064,
      "step": 7210
    },
    {
      "epoch": 2.0628571428571427,
      "grad_norm": 0.3110434114933014,
      "learning_rate": 1.7249523809523812e-05,
      "loss": 0.3433,
      "step": 7220
    },
    {
      "epoch": 2.065714285714286,
      "grad_norm": 0.13676153123378754,
      "learning_rate": 1.7245714285714288e-05,
      "loss": 0.2632,
      "step": 7230
    },
    {
      "epoch": 2.0685714285714285,
      "grad_norm": 12.237085342407227,
      "learning_rate": 1.7241904761904763e-05,
      "loss": 0.5983,
      "step": 7240
    },
    {
      "epoch": 2.0714285714285716,
      "grad_norm": 11.819391250610352,
      "learning_rate": 1.723809523809524e-05,
      "loss": 0.3855,
      "step": 7250
    },
    {
      "epoch": 2.0742857142857143,
      "grad_norm": 0.15384869277477264,
      "learning_rate": 1.7234285714285718e-05,
      "loss": 0.3404,
      "step": 7260
    },
    {
      "epoch": 2.077142857142857,
      "grad_norm": 0.55437833070755,
      "learning_rate": 1.723047619047619e-05,
      "loss": 0.1725,
      "step": 7270
    },
    {
      "epoch": 2.08,
      "grad_norm": 13.62921142578125,
      "learning_rate": 1.7226666666666665e-05,
      "loss": 0.5559,
      "step": 7280
    },
    {
      "epoch": 2.0828571428571427,
      "grad_norm": 17.740398406982422,
      "learning_rate": 1.7222857142857144e-05,
      "loss": 0.1265,
      "step": 7290
    },
    {
      "epoch": 2.085714285714286,
      "grad_norm": 42.82889175415039,
      "learning_rate": 1.721904761904762e-05,
      "loss": 0.2849,
      "step": 7300
    },
    {
      "epoch": 2.0885714285714285,
      "grad_norm": 12.120363235473633,
      "learning_rate": 1.7215238095238096e-05,
      "loss": 0.368,
      "step": 7310
    },
    {
      "epoch": 2.0914285714285716,
      "grad_norm": 25.6570987701416,
      "learning_rate": 1.721142857142857e-05,
      "loss": 0.4606,
      "step": 7320
    },
    {
      "epoch": 2.0942857142857143,
      "grad_norm": 16.719112396240234,
      "learning_rate": 1.720761904761905e-05,
      "loss": 0.409,
      "step": 7330
    },
    {
      "epoch": 2.097142857142857,
      "grad_norm": 1.5026551485061646,
      "learning_rate": 1.7203809523809526e-05,
      "loss": 0.0926,
      "step": 7340
    },
    {
      "epoch": 2.1,
      "grad_norm": 15.372236251831055,
      "learning_rate": 1.72e-05,
      "loss": 0.2762,
      "step": 7350
    },
    {
      "epoch": 2.1028571428571428,
      "grad_norm": 0.025419145822525024,
      "learning_rate": 1.7196190476190477e-05,
      "loss": 0.139,
      "step": 7360
    },
    {
      "epoch": 2.105714285714286,
      "grad_norm": 16.403549194335938,
      "learning_rate": 1.7192380952380953e-05,
      "loss": 0.4081,
      "step": 7370
    },
    {
      "epoch": 2.1085714285714285,
      "grad_norm": 28.546785354614258,
      "learning_rate": 1.718857142857143e-05,
      "loss": 0.1529,
      "step": 7380
    },
    {
      "epoch": 2.111428571428571,
      "grad_norm": 0.0177492443472147,
      "learning_rate": 1.7184761904761907e-05,
      "loss": 0.1617,
      "step": 7390
    },
    {
      "epoch": 2.1142857142857143,
      "grad_norm": 0.015106471255421638,
      "learning_rate": 1.7180952380952383e-05,
      "loss": 0.4058,
      "step": 7400
    },
    {
      "epoch": 2.117142857142857,
      "grad_norm": 0.24305054545402527,
      "learning_rate": 1.7177142857142858e-05,
      "loss": 0.291,
      "step": 7410
    },
    {
      "epoch": 2.12,
      "grad_norm": 30.223093032836914,
      "learning_rate": 1.7173333333333334e-05,
      "loss": 0.6608,
      "step": 7420
    },
    {
      "epoch": 2.1228571428571428,
      "grad_norm": 2.1547293663024902,
      "learning_rate": 1.7169523809523813e-05,
      "loss": 0.4011,
      "step": 7430
    },
    {
      "epoch": 2.125714285714286,
      "grad_norm": 33.936038970947266,
      "learning_rate": 1.716571428571429e-05,
      "loss": 0.3101,
      "step": 7440
    },
    {
      "epoch": 2.1285714285714286,
      "grad_norm": 14.682075500488281,
      "learning_rate": 1.7161904761904764e-05,
      "loss": 0.4063,
      "step": 7450
    },
    {
      "epoch": 2.1314285714285712,
      "grad_norm": 13.043326377868652,
      "learning_rate": 1.715809523809524e-05,
      "loss": 0.4079,
      "step": 7460
    },
    {
      "epoch": 2.1342857142857143,
      "grad_norm": 0.10580921918153763,
      "learning_rate": 1.7154285714285715e-05,
      "loss": 0.5172,
      "step": 7470
    },
    {
      "epoch": 2.137142857142857,
      "grad_norm": 0.11467812210321426,
      "learning_rate": 1.7150476190476194e-05,
      "loss": 0.2652,
      "step": 7480
    },
    {
      "epoch": 2.14,
      "grad_norm": 0.43348032236099243,
      "learning_rate": 1.7146666666666666e-05,
      "loss": 0.1064,
      "step": 7490
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 0.05509944632649422,
      "learning_rate": 1.7142857142857142e-05,
      "loss": 0.1051,
      "step": 7500
    },
    {
      "epoch": 2.145714285714286,
      "grad_norm": 0.07203483581542969,
      "learning_rate": 1.713904761904762e-05,
      "loss": 0.4344,
      "step": 7510
    },
    {
      "epoch": 2.1485714285714286,
      "grad_norm": 0.06055070832371712,
      "learning_rate": 1.7135238095238096e-05,
      "loss": 0.1244,
      "step": 7520
    },
    {
      "epoch": 2.1514285714285712,
      "grad_norm": 0.36233294010162354,
      "learning_rate": 1.7131428571428572e-05,
      "loss": 0.38,
      "step": 7530
    },
    {
      "epoch": 2.1542857142857144,
      "grad_norm": 33.339962005615234,
      "learning_rate": 1.7127619047619048e-05,
      "loss": 0.8485,
      "step": 7540
    },
    {
      "epoch": 2.157142857142857,
      "grad_norm": 13.859565734863281,
      "learning_rate": 1.7123809523809523e-05,
      "loss": 0.3748,
      "step": 7550
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.8057398200035095,
      "learning_rate": 1.7120000000000002e-05,
      "loss": 0.2295,
      "step": 7560
    },
    {
      "epoch": 2.162857142857143,
      "grad_norm": 12.21774673461914,
      "learning_rate": 1.7116190476190478e-05,
      "loss": 0.6245,
      "step": 7570
    },
    {
      "epoch": 2.1657142857142855,
      "grad_norm": 0.7661145329475403,
      "learning_rate": 1.7112380952380953e-05,
      "loss": 0.3746,
      "step": 7580
    },
    {
      "epoch": 2.1685714285714286,
      "grad_norm": 12.087812423706055,
      "learning_rate": 1.710857142857143e-05,
      "loss": 0.302,
      "step": 7590
    },
    {
      "epoch": 2.1714285714285713,
      "grad_norm": 10.667940139770508,
      "learning_rate": 1.7104761904761908e-05,
      "loss": 0.2235,
      "step": 7600
    },
    {
      "epoch": 2.1742857142857144,
      "grad_norm": 13.420465469360352,
      "learning_rate": 1.7100952380952383e-05,
      "loss": 0.4662,
      "step": 7610
    },
    {
      "epoch": 2.177142857142857,
      "grad_norm": 0.4355921149253845,
      "learning_rate": 1.709714285714286e-05,
      "loss": 0.2583,
      "step": 7620
    },
    {
      "epoch": 2.18,
      "grad_norm": 0.05294745787978172,
      "learning_rate": 1.7093333333333335e-05,
      "loss": 0.0421,
      "step": 7630
    },
    {
      "epoch": 2.182857142857143,
      "grad_norm": 14.05019760131836,
      "learning_rate": 1.708952380952381e-05,
      "loss": 0.5467,
      "step": 7640
    },
    {
      "epoch": 2.185714285714286,
      "grad_norm": 11.742650032043457,
      "learning_rate": 1.708571428571429e-05,
      "loss": 0.1598,
      "step": 7650
    },
    {
      "epoch": 2.1885714285714286,
      "grad_norm": 13.269451141357422,
      "learning_rate": 1.7081904761904765e-05,
      "loss": 0.6182,
      "step": 7660
    },
    {
      "epoch": 2.1914285714285713,
      "grad_norm": 0.5372669696807861,
      "learning_rate": 1.707809523809524e-05,
      "loss": 0.2076,
      "step": 7670
    },
    {
      "epoch": 2.1942857142857144,
      "grad_norm": 0.4351658225059509,
      "learning_rate": 1.7074285714285716e-05,
      "loss": 0.2249,
      "step": 7680
    },
    {
      "epoch": 2.197142857142857,
      "grad_norm": 20.597061157226562,
      "learning_rate": 1.707047619047619e-05,
      "loss": 0.3598,
      "step": 7690
    },
    {
      "epoch": 2.2,
      "grad_norm": 0.4741407632827759,
      "learning_rate": 1.706666666666667e-05,
      "loss": 0.0874,
      "step": 7700
    },
    {
      "epoch": 2.202857142857143,
      "grad_norm": 0.1328974962234497,
      "learning_rate": 1.7062857142857143e-05,
      "loss": 0.2914,
      "step": 7710
    },
    {
      "epoch": 2.2057142857142855,
      "grad_norm": 25.828760147094727,
      "learning_rate": 1.7059047619047618e-05,
      "loss": 0.2501,
      "step": 7720
    },
    {
      "epoch": 2.2085714285714286,
      "grad_norm": 0.13334108889102936,
      "learning_rate": 1.7055238095238097e-05,
      "loss": 0.2331,
      "step": 7730
    },
    {
      "epoch": 2.2114285714285713,
      "grad_norm": 18.03595542907715,
      "learning_rate": 1.7051428571428573e-05,
      "loss": 0.3792,
      "step": 7740
    },
    {
      "epoch": 2.2142857142857144,
      "grad_norm": 0.03406266123056412,
      "learning_rate": 1.704761904761905e-05,
      "loss": 0.2902,
      "step": 7750
    },
    {
      "epoch": 2.217142857142857,
      "grad_norm": 0.4672272801399231,
      "learning_rate": 1.7043809523809524e-05,
      "loss": 0.3876,
      "step": 7760
    },
    {
      "epoch": 2.22,
      "grad_norm": 75.79849243164062,
      "learning_rate": 1.704e-05,
      "loss": 0.1921,
      "step": 7770
    },
    {
      "epoch": 2.222857142857143,
      "grad_norm": 0.06474433094263077,
      "learning_rate": 1.703619047619048e-05,
      "loss": 0.578,
      "step": 7780
    },
    {
      "epoch": 2.2257142857142855,
      "grad_norm": 0.07913728058338165,
      "learning_rate": 1.7032380952380954e-05,
      "loss": 0.1751,
      "step": 7790
    },
    {
      "epoch": 2.2285714285714286,
      "grad_norm": 0.0503397136926651,
      "learning_rate": 1.702857142857143e-05,
      "loss": 0.1051,
      "step": 7800
    },
    {
      "epoch": 2.2314285714285713,
      "grad_norm": 78.6143569946289,
      "learning_rate": 1.7024761904761905e-05,
      "loss": 0.325,
      "step": 7810
    },
    {
      "epoch": 2.2342857142857144,
      "grad_norm": 0.12408234924077988,
      "learning_rate": 1.702095238095238e-05,
      "loss": 0.614,
      "step": 7820
    },
    {
      "epoch": 2.237142857142857,
      "grad_norm": 19.070152282714844,
      "learning_rate": 1.701714285714286e-05,
      "loss": 0.3423,
      "step": 7830
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.10833902657032013,
      "learning_rate": 1.7013333333333335e-05,
      "loss": 0.9572,
      "step": 7840
    },
    {
      "epoch": 2.242857142857143,
      "grad_norm": 0.3221886157989502,
      "learning_rate": 1.700952380952381e-05,
      "loss": 0.256,
      "step": 7850
    },
    {
      "epoch": 2.2457142857142856,
      "grad_norm": 0.14042344689369202,
      "learning_rate": 1.7005714285714286e-05,
      "loss": 0.2706,
      "step": 7860
    },
    {
      "epoch": 2.2485714285714287,
      "grad_norm": 15.001609802246094,
      "learning_rate": 1.7001904761904765e-05,
      "loss": 0.7893,
      "step": 7870
    },
    {
      "epoch": 2.2514285714285713,
      "grad_norm": 3.391202211380005,
      "learning_rate": 1.699809523809524e-05,
      "loss": 0.4066,
      "step": 7880
    },
    {
      "epoch": 2.2542857142857144,
      "grad_norm": 17.738876342773438,
      "learning_rate": 1.6994285714285717e-05,
      "loss": 0.3947,
      "step": 7890
    },
    {
      "epoch": 2.257142857142857,
      "grad_norm": 0.17495837807655334,
      "learning_rate": 1.6990476190476192e-05,
      "loss": 0.442,
      "step": 7900
    },
    {
      "epoch": 2.26,
      "grad_norm": 72.41730499267578,
      "learning_rate": 1.6986666666666668e-05,
      "loss": 0.4026,
      "step": 7910
    },
    {
      "epoch": 2.262857142857143,
      "grad_norm": 17.97123146057129,
      "learning_rate": 1.6982857142857143e-05,
      "loss": 0.2012,
      "step": 7920
    },
    {
      "epoch": 2.2657142857142856,
      "grad_norm": 0.19610829651355743,
      "learning_rate": 1.697904761904762e-05,
      "loss": 0.3587,
      "step": 7930
    },
    {
      "epoch": 2.2685714285714287,
      "grad_norm": 15.295370101928711,
      "learning_rate": 1.6975238095238095e-05,
      "loss": 0.502,
      "step": 7940
    },
    {
      "epoch": 2.2714285714285714,
      "grad_norm": 0.17776665091514587,
      "learning_rate": 1.6971428571428574e-05,
      "loss": 0.4567,
      "step": 7950
    },
    {
      "epoch": 2.2742857142857145,
      "grad_norm": 1.4169403314590454,
      "learning_rate": 1.696761904761905e-05,
      "loss": 0.3407,
      "step": 7960
    },
    {
      "epoch": 2.277142857142857,
      "grad_norm": 0.6018010973930359,
      "learning_rate": 1.6963809523809525e-05,
      "loss": 0.0119,
      "step": 7970
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 13.249338150024414,
      "learning_rate": 1.696e-05,
      "loss": 0.4189,
      "step": 7980
    },
    {
      "epoch": 2.282857142857143,
      "grad_norm": 13.08094310760498,
      "learning_rate": 1.6956190476190476e-05,
      "loss": 0.641,
      "step": 7990
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 14.269290924072266,
      "learning_rate": 1.6952380952380955e-05,
      "loss": 0.439,
      "step": 8000
    },
    {
      "epoch": 2.2885714285714287,
      "grad_norm": 23.05204963684082,
      "learning_rate": 1.694857142857143e-05,
      "loss": 0.3809,
      "step": 8010
    },
    {
      "epoch": 2.2914285714285714,
      "grad_norm": 0.14780151844024658,
      "learning_rate": 1.6944761904761906e-05,
      "loss": 0.1925,
      "step": 8020
    },
    {
      "epoch": 2.2942857142857145,
      "grad_norm": 0.1318478137254715,
      "learning_rate": 1.694095238095238e-05,
      "loss": 0.0051,
      "step": 8030
    },
    {
      "epoch": 2.297142857142857,
      "grad_norm": 19.241771697998047,
      "learning_rate": 1.6937142857142857e-05,
      "loss": 0.2713,
      "step": 8040
    },
    {
      "epoch": 2.3,
      "grad_norm": 17.500520706176758,
      "learning_rate": 1.6933333333333336e-05,
      "loss": 0.5065,
      "step": 8050
    },
    {
      "epoch": 2.302857142857143,
      "grad_norm": 19.36070442199707,
      "learning_rate": 1.692952380952381e-05,
      "loss": 0.212,
      "step": 8060
    },
    {
      "epoch": 2.3057142857142856,
      "grad_norm": 0.16027076542377472,
      "learning_rate": 1.6925714285714287e-05,
      "loss": 0.1543,
      "step": 8070
    },
    {
      "epoch": 2.3085714285714287,
      "grad_norm": 0.033227622509002686,
      "learning_rate": 1.6921904761904763e-05,
      "loss": 0.156,
      "step": 8080
    },
    {
      "epoch": 2.3114285714285714,
      "grad_norm": 0.17030225694179535,
      "learning_rate": 1.6918095238095242e-05,
      "loss": 0.0069,
      "step": 8090
    },
    {
      "epoch": 2.314285714285714,
      "grad_norm": 0.09235981851816177,
      "learning_rate": 1.6914285714285717e-05,
      "loss": 0.5949,
      "step": 8100
    },
    {
      "epoch": 2.317142857142857,
      "grad_norm": 0.15021471679210663,
      "learning_rate": 1.6910476190476193e-05,
      "loss": 0.0283,
      "step": 8110
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.04937821999192238,
      "learning_rate": 1.690666666666667e-05,
      "loss": 0.0922,
      "step": 8120
    },
    {
      "epoch": 2.322857142857143,
      "grad_norm": 0.03485127165913582,
      "learning_rate": 1.6902857142857144e-05,
      "loss": 0.4035,
      "step": 8130
    },
    {
      "epoch": 2.3257142857142856,
      "grad_norm": 0.15649347007274628,
      "learning_rate": 1.689904761904762e-05,
      "loss": 0.5269,
      "step": 8140
    },
    {
      "epoch": 2.3285714285714287,
      "grad_norm": 18.819379806518555,
      "learning_rate": 1.6895238095238095e-05,
      "loss": 0.3605,
      "step": 8150
    },
    {
      "epoch": 2.3314285714285714,
      "grad_norm": 15.038473129272461,
      "learning_rate": 1.689142857142857e-05,
      "loss": 0.4962,
      "step": 8160
    },
    {
      "epoch": 2.3342857142857145,
      "grad_norm": 13.541397094726562,
      "learning_rate": 1.688761904761905e-05,
      "loss": 0.5482,
      "step": 8170
    },
    {
      "epoch": 2.337142857142857,
      "grad_norm": 0.3424672484397888,
      "learning_rate": 1.6883809523809525e-05,
      "loss": 0.1105,
      "step": 8180
    },
    {
      "epoch": 2.34,
      "grad_norm": 112.43798828125,
      "learning_rate": 1.688e-05,
      "loss": 0.3611,
      "step": 8190
    },
    {
      "epoch": 2.342857142857143,
      "grad_norm": 0.08905898779630661,
      "learning_rate": 1.6876190476190477e-05,
      "loss": 0.2627,
      "step": 8200
    },
    {
      "epoch": 2.3457142857142856,
      "grad_norm": 0.14157690107822418,
      "learning_rate": 1.6872380952380952e-05,
      "loss": 0.3787,
      "step": 8210
    },
    {
      "epoch": 2.3485714285714288,
      "grad_norm": 0.8954825401306152,
      "learning_rate": 1.686857142857143e-05,
      "loss": 0.8846,
      "step": 8220
    },
    {
      "epoch": 2.3514285714285714,
      "grad_norm": 0.530419111251831,
      "learning_rate": 1.6864761904761907e-05,
      "loss": 0.3728,
      "step": 8230
    },
    {
      "epoch": 2.354285714285714,
      "grad_norm": 0.3081146776676178,
      "learning_rate": 1.6860952380952382e-05,
      "loss": 0.1433,
      "step": 8240
    },
    {
      "epoch": 2.357142857142857,
      "grad_norm": 14.150151252746582,
      "learning_rate": 1.6857142857142858e-05,
      "loss": 0.3356,
      "step": 8250
    },
    {
      "epoch": 2.36,
      "grad_norm": 0.04070637747645378,
      "learning_rate": 1.6853333333333333e-05,
      "loss": 0.1154,
      "step": 8260
    },
    {
      "epoch": 2.362857142857143,
      "grad_norm": 0.0865066722035408,
      "learning_rate": 1.6849523809523812e-05,
      "loss": 0.2503,
      "step": 8270
    },
    {
      "epoch": 2.3657142857142857,
      "grad_norm": 20.473295211791992,
      "learning_rate": 1.6845714285714288e-05,
      "loss": 0.4532,
      "step": 8280
    },
    {
      "epoch": 2.3685714285714283,
      "grad_norm": 2.9948577880859375,
      "learning_rate": 1.6841904761904764e-05,
      "loss": 0.1036,
      "step": 8290
    },
    {
      "epoch": 2.3714285714285714,
      "grad_norm": 0.14447611570358276,
      "learning_rate": 1.683809523809524e-05,
      "loss": 0.3397,
      "step": 8300
    },
    {
      "epoch": 2.374285714285714,
      "grad_norm": 0.045008644461631775,
      "learning_rate": 1.6834285714285715e-05,
      "loss": 0.2256,
      "step": 8310
    },
    {
      "epoch": 2.3771428571428572,
      "grad_norm": 0.13481080532073975,
      "learning_rate": 1.6830476190476194e-05,
      "loss": 0.3416,
      "step": 8320
    },
    {
      "epoch": 2.38,
      "grad_norm": 0.1665487289428711,
      "learning_rate": 1.682666666666667e-05,
      "loss": 0.1657,
      "step": 8330
    },
    {
      "epoch": 2.382857142857143,
      "grad_norm": 0.06812925636768341,
      "learning_rate": 1.6822857142857145e-05,
      "loss": 0.4914,
      "step": 8340
    },
    {
      "epoch": 2.3857142857142857,
      "grad_norm": 0.07510068267583847,
      "learning_rate": 1.681904761904762e-05,
      "loss": 0.1839,
      "step": 8350
    },
    {
      "epoch": 2.388571428571429,
      "grad_norm": 0.04769423231482506,
      "learning_rate": 1.6815238095238096e-05,
      "loss": 0.1031,
      "step": 8360
    },
    {
      "epoch": 2.3914285714285715,
      "grad_norm": 0.059291329234838486,
      "learning_rate": 1.681142857142857e-05,
      "loss": 0.3068,
      "step": 8370
    },
    {
      "epoch": 2.394285714285714,
      "grad_norm": 20.23959732055664,
      "learning_rate": 1.6807619047619047e-05,
      "loss": 0.6027,
      "step": 8380
    },
    {
      "epoch": 2.3971428571428572,
      "grad_norm": 1.457655429840088,
      "learning_rate": 1.6803809523809523e-05,
      "loss": 0.3558,
      "step": 8390
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.16576184332370758,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.2735,
      "step": 8400
    },
    {
      "epoch": 2.402857142857143,
      "grad_norm": 0.11053651571273804,
      "learning_rate": 1.6796190476190477e-05,
      "loss": 0.1412,
      "step": 8410
    },
    {
      "epoch": 2.4057142857142857,
      "grad_norm": 0.10268211364746094,
      "learning_rate": 1.6792380952380953e-05,
      "loss": 0.0785,
      "step": 8420
    },
    {
      "epoch": 2.4085714285714284,
      "grad_norm": 0.0336918868124485,
      "learning_rate": 1.678857142857143e-05,
      "loss": 0.1374,
      "step": 8430
    },
    {
      "epoch": 2.4114285714285715,
      "grad_norm": 0.041719358414411545,
      "learning_rate": 1.6784761904761907e-05,
      "loss": 0.6361,
      "step": 8440
    },
    {
      "epoch": 2.414285714285714,
      "grad_norm": 0.05211754888296127,
      "learning_rate": 1.6780952380952383e-05,
      "loss": 0.0048,
      "step": 8450
    },
    {
      "epoch": 2.4171428571428573,
      "grad_norm": 0.053652770817279816,
      "learning_rate": 1.677714285714286e-05,
      "loss": 0.3267,
      "step": 8460
    },
    {
      "epoch": 2.42,
      "grad_norm": 0.033407386392354965,
      "learning_rate": 1.6773333333333334e-05,
      "loss": 0.3164,
      "step": 8470
    },
    {
      "epoch": 2.422857142857143,
      "grad_norm": 243.79598999023438,
      "learning_rate": 1.676952380952381e-05,
      "loss": 0.2391,
      "step": 8480
    },
    {
      "epoch": 2.4257142857142857,
      "grad_norm": 2.955781936645508,
      "learning_rate": 1.676571428571429e-05,
      "loss": 0.1251,
      "step": 8490
    },
    {
      "epoch": 2.4285714285714284,
      "grad_norm": 0.28027135133743286,
      "learning_rate": 1.6761904761904764e-05,
      "loss": 0.4242,
      "step": 8500
    },
    {
      "epoch": 2.4314285714285715,
      "grad_norm": 0.48467591404914856,
      "learning_rate": 1.675809523809524e-05,
      "loss": 0.2303,
      "step": 8510
    },
    {
      "epoch": 2.434285714285714,
      "grad_norm": 0.03929727151989937,
      "learning_rate": 1.6754285714285716e-05,
      "loss": 0.3335,
      "step": 8520
    },
    {
      "epoch": 2.4371428571428573,
      "grad_norm": 1.1702944040298462,
      "learning_rate": 1.675047619047619e-05,
      "loss": 0.8599,
      "step": 8530
    },
    {
      "epoch": 2.44,
      "grad_norm": 0.08324665576219559,
      "learning_rate": 1.674666666666667e-05,
      "loss": 0.2193,
      "step": 8540
    },
    {
      "epoch": 2.442857142857143,
      "grad_norm": 0.4694598317146301,
      "learning_rate": 1.6742857142857146e-05,
      "loss": 0.0118,
      "step": 8550
    },
    {
      "epoch": 2.4457142857142857,
      "grad_norm": 0.05430860072374344,
      "learning_rate": 1.673904761904762e-05,
      "loss": 0.1119,
      "step": 8560
    },
    {
      "epoch": 2.4485714285714284,
      "grad_norm": 0.039355117827653885,
      "learning_rate": 1.6735238095238097e-05,
      "loss": 0.116,
      "step": 8570
    },
    {
      "epoch": 2.4514285714285715,
      "grad_norm": 0.5674318671226501,
      "learning_rate": 1.6731428571428572e-05,
      "loss": 0.4114,
      "step": 8580
    },
    {
      "epoch": 2.454285714285714,
      "grad_norm": 0.16979394853115082,
      "learning_rate": 1.6727619047619048e-05,
      "loss": 0.178,
      "step": 8590
    },
    {
      "epoch": 2.4571428571428573,
      "grad_norm": 35.76247787475586,
      "learning_rate": 1.6723809523809524e-05,
      "loss": 0.4179,
      "step": 8600
    },
    {
      "epoch": 2.46,
      "grad_norm": 0.056076619774103165,
      "learning_rate": 1.672e-05,
      "loss": 0.1219,
      "step": 8610
    },
    {
      "epoch": 2.4628571428571426,
      "grad_norm": 0.1443074494600296,
      "learning_rate": 1.6716190476190478e-05,
      "loss": 0.1251,
      "step": 8620
    },
    {
      "epoch": 2.4657142857142857,
      "grad_norm": 0.03451039269566536,
      "learning_rate": 1.6712380952380954e-05,
      "loss": 0.0039,
      "step": 8630
    },
    {
      "epoch": 2.4685714285714284,
      "grad_norm": 0.04590747505426407,
      "learning_rate": 1.670857142857143e-05,
      "loss": 0.1733,
      "step": 8640
    },
    {
      "epoch": 2.4714285714285715,
      "grad_norm": 23.180145263671875,
      "learning_rate": 1.6704761904761905e-05,
      "loss": 0.4234,
      "step": 8650
    },
    {
      "epoch": 2.474285714285714,
      "grad_norm": 0.1370982974767685,
      "learning_rate": 1.670095238095238e-05,
      "loss": 0.0026,
      "step": 8660
    },
    {
      "epoch": 2.4771428571428573,
      "grad_norm": 0.12850867211818695,
      "learning_rate": 1.669714285714286e-05,
      "loss": 0.368,
      "step": 8670
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.17100532352924347,
      "learning_rate": 1.6693333333333335e-05,
      "loss": 0.3641,
      "step": 8680
    },
    {
      "epoch": 2.482857142857143,
      "grad_norm": 0.3700658977031708,
      "learning_rate": 1.668952380952381e-05,
      "loss": 0.2326,
      "step": 8690
    },
    {
      "epoch": 2.4857142857142858,
      "grad_norm": 60.130157470703125,
      "learning_rate": 1.6685714285714286e-05,
      "loss": 0.3126,
      "step": 8700
    },
    {
      "epoch": 2.4885714285714284,
      "grad_norm": 0.028300676494836807,
      "learning_rate": 1.6681904761904765e-05,
      "loss": 0.1579,
      "step": 8710
    },
    {
      "epoch": 2.4914285714285715,
      "grad_norm": 18.085620880126953,
      "learning_rate": 1.667809523809524e-05,
      "loss": 0.4634,
      "step": 8720
    },
    {
      "epoch": 2.494285714285714,
      "grad_norm": 0.2004883736371994,
      "learning_rate": 1.6674285714285716e-05,
      "loss": 0.2591,
      "step": 8730
    },
    {
      "epoch": 2.4971428571428573,
      "grad_norm": 13.904296875,
      "learning_rate": 1.6670476190476192e-05,
      "loss": 0.1137,
      "step": 8740
    },
    {
      "epoch": 2.5,
      "grad_norm": 12.516164779663086,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.4061,
      "step": 8750
    },
    {
      "epoch": 2.5028571428571427,
      "grad_norm": 0.09998492896556854,
      "learning_rate": 1.6662857142857146e-05,
      "loss": 0.5628,
      "step": 8760
    },
    {
      "epoch": 2.505714285714286,
      "grad_norm": 0.38768425583839417,
      "learning_rate": 1.6659047619047622e-05,
      "loss": 0.2206,
      "step": 8770
    },
    {
      "epoch": 2.5085714285714285,
      "grad_norm": 14.390820503234863,
      "learning_rate": 1.6655238095238098e-05,
      "loss": 0.4389,
      "step": 8780
    },
    {
      "epoch": 2.5114285714285716,
      "grad_norm": 0.17567011713981628,
      "learning_rate": 1.6651428571428573e-05,
      "loss": 0.5515,
      "step": 8790
    },
    {
      "epoch": 2.5142857142857142,
      "grad_norm": 14.421985626220703,
      "learning_rate": 1.664761904761905e-05,
      "loss": 0.5707,
      "step": 8800
    },
    {
      "epoch": 2.517142857142857,
      "grad_norm": 0.8524218797683716,
      "learning_rate": 1.6643809523809524e-05,
      "loss": 0.5427,
      "step": 8810
    },
    {
      "epoch": 2.52,
      "grad_norm": 0.6117038130760193,
      "learning_rate": 1.664e-05,
      "loss": 0.4703,
      "step": 8820
    },
    {
      "epoch": 2.522857142857143,
      "grad_norm": 0.5053251385688782,
      "learning_rate": 1.6636190476190476e-05,
      "loss": 0.4221,
      "step": 8830
    },
    {
      "epoch": 2.525714285714286,
      "grad_norm": 0.07232832163572311,
      "learning_rate": 1.6632380952380954e-05,
      "loss": 0.074,
      "step": 8840
    },
    {
      "epoch": 2.5285714285714285,
      "grad_norm": 11.544351577758789,
      "learning_rate": 1.662857142857143e-05,
      "loss": 0.4673,
      "step": 8850
    },
    {
      "epoch": 2.5314285714285716,
      "grad_norm": 0.0908011943101883,
      "learning_rate": 1.6624761904761906e-05,
      "loss": 0.4909,
      "step": 8860
    },
    {
      "epoch": 2.5342857142857143,
      "grad_norm": 0.29860466718673706,
      "learning_rate": 1.662095238095238e-05,
      "loss": 0.4335,
      "step": 8870
    },
    {
      "epoch": 2.5371428571428574,
      "grad_norm": 0.33486631512641907,
      "learning_rate": 1.6617142857142857e-05,
      "loss": 0.1702,
      "step": 8880
    },
    {
      "epoch": 2.54,
      "grad_norm": 0.17197227478027344,
      "learning_rate": 1.6613333333333336e-05,
      "loss": 0.529,
      "step": 8890
    },
    {
      "epoch": 2.5428571428571427,
      "grad_norm": 16.279945373535156,
      "learning_rate": 1.660952380952381e-05,
      "loss": 0.3587,
      "step": 8900
    },
    {
      "epoch": 2.545714285714286,
      "grad_norm": 0.3324393033981323,
      "learning_rate": 1.6605714285714287e-05,
      "loss": 0.0907,
      "step": 8910
    },
    {
      "epoch": 2.5485714285714285,
      "grad_norm": 0.2155003845691681,
      "learning_rate": 1.6601904761904763e-05,
      "loss": 0.116,
      "step": 8920
    },
    {
      "epoch": 2.5514285714285716,
      "grad_norm": 0.4193713068962097,
      "learning_rate": 1.659809523809524e-05,
      "loss": 0.3112,
      "step": 8930
    },
    {
      "epoch": 2.5542857142857143,
      "grad_norm": 0.2389821708202362,
      "learning_rate": 1.6594285714285717e-05,
      "loss": 0.2706,
      "step": 8940
    },
    {
      "epoch": 2.557142857142857,
      "grad_norm": 0.04738696292042732,
      "learning_rate": 1.6590476190476193e-05,
      "loss": 0.0091,
      "step": 8950
    },
    {
      "epoch": 2.56,
      "grad_norm": 13.928468704223633,
      "learning_rate": 1.6586666666666668e-05,
      "loss": 0.5138,
      "step": 8960
    },
    {
      "epoch": 2.5628571428571427,
      "grad_norm": 0.2672847509384155,
      "learning_rate": 1.6582857142857144e-05,
      "loss": 0.4942,
      "step": 8970
    },
    {
      "epoch": 2.565714285714286,
      "grad_norm": 0.2101696878671646,
      "learning_rate": 1.6579047619047623e-05,
      "loss": 0.1184,
      "step": 8980
    },
    {
      "epoch": 2.5685714285714285,
      "grad_norm": 0.034540947526693344,
      "learning_rate": 1.65752380952381e-05,
      "loss": 0.3571,
      "step": 8990
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 0.05322754383087158,
      "learning_rate": 1.6571428571428574e-05,
      "loss": 0.1001,
      "step": 9000
    },
    {
      "epoch": 2.5742857142857143,
      "grad_norm": 0.29421335458755493,
      "learning_rate": 1.656761904761905e-05,
      "loss": 0.4533,
      "step": 9010
    },
    {
      "epoch": 2.5771428571428574,
      "grad_norm": 0.037997063249349594,
      "learning_rate": 1.6563809523809525e-05,
      "loss": 0.3944,
      "step": 9020
    },
    {
      "epoch": 2.58,
      "grad_norm": 0.1873622089624405,
      "learning_rate": 1.656e-05,
      "loss": 0.5028,
      "step": 9030
    },
    {
      "epoch": 2.5828571428571427,
      "grad_norm": 0.31122729182243347,
      "learning_rate": 1.6556190476190476e-05,
      "loss": 0.3576,
      "step": 9040
    },
    {
      "epoch": 2.585714285714286,
      "grad_norm": 0.7361935377120972,
      "learning_rate": 1.6552380952380952e-05,
      "loss": 0.3414,
      "step": 9050
    },
    {
      "epoch": 2.5885714285714285,
      "grad_norm": 0.6753628849983215,
      "learning_rate": 1.654857142857143e-05,
      "loss": 0.2233,
      "step": 9060
    },
    {
      "epoch": 2.5914285714285716,
      "grad_norm": 0.02666420303285122,
      "learning_rate": 1.6544761904761906e-05,
      "loss": 0.0439,
      "step": 9070
    },
    {
      "epoch": 2.5942857142857143,
      "grad_norm": 0.029773376882076263,
      "learning_rate": 1.6540952380952382e-05,
      "loss": 0.218,
      "step": 9080
    },
    {
      "epoch": 2.597142857142857,
      "grad_norm": 11.795623779296875,
      "learning_rate": 1.6537142857142858e-05,
      "loss": 0.2558,
      "step": 9090
    },
    {
      "epoch": 2.6,
      "grad_norm": 34.72541427612305,
      "learning_rate": 1.6533333333333333e-05,
      "loss": 0.4166,
      "step": 9100
    },
    {
      "epoch": 2.6028571428571428,
      "grad_norm": 0.3620101809501648,
      "learning_rate": 1.6529523809523812e-05,
      "loss": 0.3395,
      "step": 9110
    },
    {
      "epoch": 2.605714285714286,
      "grad_norm": 0.34536212682724,
      "learning_rate": 1.6525714285714288e-05,
      "loss": 0.0035,
      "step": 9120
    },
    {
      "epoch": 2.6085714285714285,
      "grad_norm": 0.12091274559497833,
      "learning_rate": 1.6521904761904763e-05,
      "loss": 0.3656,
      "step": 9130
    },
    {
      "epoch": 2.611428571428571,
      "grad_norm": 2.3728256225585938,
      "learning_rate": 1.651809523809524e-05,
      "loss": 0.1618,
      "step": 9140
    },
    {
      "epoch": 2.6142857142857143,
      "grad_norm": 0.2192848026752472,
      "learning_rate": 1.6514285714285714e-05,
      "loss": 0.1961,
      "step": 9150
    },
    {
      "epoch": 2.617142857142857,
      "grad_norm": 0.016588596627116203,
      "learning_rate": 1.6510476190476193e-05,
      "loss": 0.1728,
      "step": 9160
    },
    {
      "epoch": 2.62,
      "grad_norm": 28.622697830200195,
      "learning_rate": 1.650666666666667e-05,
      "loss": 0.4263,
      "step": 9170
    },
    {
      "epoch": 2.6228571428571428,
      "grad_norm": 0.058982037007808685,
      "learning_rate": 1.6502857142857145e-05,
      "loss": 0.528,
      "step": 9180
    },
    {
      "epoch": 2.6257142857142854,
      "grad_norm": 0.08386027812957764,
      "learning_rate": 1.649904761904762e-05,
      "loss": 0.0024,
      "step": 9190
    },
    {
      "epoch": 2.6285714285714286,
      "grad_norm": 0.23319467902183533,
      "learning_rate": 1.64952380952381e-05,
      "loss": 0.5029,
      "step": 9200
    },
    {
      "epoch": 2.6314285714285717,
      "grad_norm": 0.36143308877944946,
      "learning_rate": 1.6491428571428575e-05,
      "loss": 0.3689,
      "step": 9210
    },
    {
      "epoch": 2.6342857142857143,
      "grad_norm": 0.05602482333779335,
      "learning_rate": 1.648761904761905e-05,
      "loss": 0.0987,
      "step": 9220
    },
    {
      "epoch": 2.637142857142857,
      "grad_norm": 0.039927657693624496,
      "learning_rate": 1.6483809523809522e-05,
      "loss": 0.3332,
      "step": 9230
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.07058066129684448,
      "learning_rate": 1.648e-05,
      "loss": 0.2649,
      "step": 9240
    },
    {
      "epoch": 2.642857142857143,
      "grad_norm": 17.968097686767578,
      "learning_rate": 1.6476190476190477e-05,
      "loss": 0.192,
      "step": 9250
    },
    {
      "epoch": 2.645714285714286,
      "grad_norm": 0.08973029255867004,
      "learning_rate": 1.6472380952380953e-05,
      "loss": 0.5848,
      "step": 9260
    },
    {
      "epoch": 2.6485714285714286,
      "grad_norm": 22.873937606811523,
      "learning_rate": 1.6468571428571428e-05,
      "loss": 0.4929,
      "step": 9270
    },
    {
      "epoch": 2.6514285714285712,
      "grad_norm": 0.5209797024726868,
      "learning_rate": 1.6464761904761907e-05,
      "loss": 0.2053,
      "step": 9280
    },
    {
      "epoch": 2.6542857142857144,
      "grad_norm": 0.043296657502651215,
      "learning_rate": 1.6460952380952383e-05,
      "loss": 0.0709,
      "step": 9290
    },
    {
      "epoch": 2.657142857142857,
      "grad_norm": 0.9489582777023315,
      "learning_rate": 1.645714285714286e-05,
      "loss": 0.2431,
      "step": 9300
    },
    {
      "epoch": 2.66,
      "grad_norm": 0.06398233026266098,
      "learning_rate": 1.6453333333333334e-05,
      "loss": 0.4105,
      "step": 9310
    },
    {
      "epoch": 2.662857142857143,
      "grad_norm": 0.6466265320777893,
      "learning_rate": 1.644952380952381e-05,
      "loss": 0.2072,
      "step": 9320
    },
    {
      "epoch": 2.6657142857142855,
      "grad_norm": 0.3653692305088043,
      "learning_rate": 1.644571428571429e-05,
      "loss": 0.1349,
      "step": 9330
    },
    {
      "epoch": 2.6685714285714286,
      "grad_norm": 13.286306381225586,
      "learning_rate": 1.6441904761904764e-05,
      "loss": 0.3325,
      "step": 9340
    },
    {
      "epoch": 2.6714285714285713,
      "grad_norm": 0.281412273645401,
      "learning_rate": 1.643809523809524e-05,
      "loss": 0.3036,
      "step": 9350
    },
    {
      "epoch": 2.6742857142857144,
      "grad_norm": 0.0458282046020031,
      "learning_rate": 1.6434285714285715e-05,
      "loss": 0.1005,
      "step": 9360
    },
    {
      "epoch": 2.677142857142857,
      "grad_norm": 0.06392817199230194,
      "learning_rate": 1.643047619047619e-05,
      "loss": 0.407,
      "step": 9370
    },
    {
      "epoch": 2.68,
      "grad_norm": 0.24053741991519928,
      "learning_rate": 1.642666666666667e-05,
      "loss": 0.2517,
      "step": 9380
    },
    {
      "epoch": 2.682857142857143,
      "grad_norm": 0.25011491775512695,
      "learning_rate": 1.6422857142857145e-05,
      "loss": 0.3958,
      "step": 9390
    },
    {
      "epoch": 2.685714285714286,
      "grad_norm": 0.23717719316482544,
      "learning_rate": 1.641904761904762e-05,
      "loss": 0.3654,
      "step": 9400
    },
    {
      "epoch": 2.6885714285714286,
      "grad_norm": 121.87469482421875,
      "learning_rate": 1.6415238095238097e-05,
      "loss": 0.3252,
      "step": 9410
    },
    {
      "epoch": 2.6914285714285713,
      "grad_norm": 34.67681884765625,
      "learning_rate": 1.6411428571428572e-05,
      "loss": 0.1272,
      "step": 9420
    },
    {
      "epoch": 2.6942857142857144,
      "grad_norm": 0.10657563805580139,
      "learning_rate": 1.640761904761905e-05,
      "loss": 0.3444,
      "step": 9430
    },
    {
      "epoch": 2.697142857142857,
      "grad_norm": 0.21346673369407654,
      "learning_rate": 1.6403809523809523e-05,
      "loss": 0.005,
      "step": 9440
    },
    {
      "epoch": 2.7,
      "grad_norm": 36.98814392089844,
      "learning_rate": 1.64e-05,
      "loss": 0.6875,
      "step": 9450
    },
    {
      "epoch": 2.702857142857143,
      "grad_norm": 0.05756556987762451,
      "learning_rate": 1.6396190476190478e-05,
      "loss": 0.0021,
      "step": 9460
    },
    {
      "epoch": 2.7057142857142855,
      "grad_norm": 88.82939910888672,
      "learning_rate": 1.6392380952380953e-05,
      "loss": 0.4409,
      "step": 9470
    },
    {
      "epoch": 2.7085714285714286,
      "grad_norm": 0.08293116092681885,
      "learning_rate": 1.638857142857143e-05,
      "loss": 0.0095,
      "step": 9480
    },
    {
      "epoch": 2.7114285714285713,
      "grad_norm": 0.027547119185328484,
      "learning_rate": 1.6384761904761905e-05,
      "loss": 0.3001,
      "step": 9490
    },
    {
      "epoch": 2.7142857142857144,
      "grad_norm": 0.05265966057777405,
      "learning_rate": 1.6380952380952384e-05,
      "loss": 0.4671,
      "step": 9500
    },
    {
      "epoch": 2.717142857142857,
      "grad_norm": 1.8610117435455322,
      "learning_rate": 1.637714285714286e-05,
      "loss": 0.0402,
      "step": 9510
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 18.286569595336914,
      "learning_rate": 1.6373333333333335e-05,
      "loss": 0.6524,
      "step": 9520
    },
    {
      "epoch": 2.722857142857143,
      "grad_norm": 17.51657485961914,
      "learning_rate": 1.636952380952381e-05,
      "loss": 0.3051,
      "step": 9530
    },
    {
      "epoch": 2.725714285714286,
      "grad_norm": 0.6780664920806885,
      "learning_rate": 1.6365714285714286e-05,
      "loss": 0.179,
      "step": 9540
    },
    {
      "epoch": 2.7285714285714286,
      "grad_norm": 0.12400323897600174,
      "learning_rate": 1.6361904761904765e-05,
      "loss": 0.0189,
      "step": 9550
    },
    {
      "epoch": 2.7314285714285713,
      "grad_norm": 100.11604309082031,
      "learning_rate": 1.635809523809524e-05,
      "loss": 0.4832,
      "step": 9560
    },
    {
      "epoch": 2.7342857142857144,
      "grad_norm": 0.17820754647254944,
      "learning_rate": 1.6354285714285716e-05,
      "loss": 0.3267,
      "step": 9570
    },
    {
      "epoch": 2.737142857142857,
      "grad_norm": 0.09718729555606842,
      "learning_rate": 1.635047619047619e-05,
      "loss": 0.1004,
      "step": 9580
    },
    {
      "epoch": 2.74,
      "grad_norm": 0.10553159564733505,
      "learning_rate": 1.6346666666666667e-05,
      "loss": 0.0029,
      "step": 9590
    },
    {
      "epoch": 2.742857142857143,
      "grad_norm": 0.15941262245178223,
      "learning_rate": 1.6342857142857146e-05,
      "loss": 0.1401,
      "step": 9600
    },
    {
      "epoch": 2.7457142857142856,
      "grad_norm": 0.07823644578456879,
      "learning_rate": 1.633904761904762e-05,
      "loss": 0.2659,
      "step": 9610
    },
    {
      "epoch": 2.7485714285714287,
      "grad_norm": 0.08504702895879745,
      "learning_rate": 1.6335238095238097e-05,
      "loss": 0.0007,
      "step": 9620
    },
    {
      "epoch": 2.7514285714285713,
      "grad_norm": 0.029817674309015274,
      "learning_rate": 1.6331428571428573e-05,
      "loss": 0.1506,
      "step": 9630
    },
    {
      "epoch": 2.7542857142857144,
      "grad_norm": 0.07583829015493393,
      "learning_rate": 1.632761904761905e-05,
      "loss": 0.2884,
      "step": 9640
    },
    {
      "epoch": 2.757142857142857,
      "grad_norm": 0.022837065160274506,
      "learning_rate": 1.6323809523809527e-05,
      "loss": 0.1466,
      "step": 9650
    },
    {
      "epoch": 2.76,
      "grad_norm": 0.07101421803236008,
      "learning_rate": 1.632e-05,
      "loss": 0.3599,
      "step": 9660
    },
    {
      "epoch": 2.762857142857143,
      "grad_norm": 13.895156860351562,
      "learning_rate": 1.6316190476190475e-05,
      "loss": 0.5086,
      "step": 9670
    },
    {
      "epoch": 2.7657142857142856,
      "grad_norm": 0.2267249971628189,
      "learning_rate": 1.6312380952380954e-05,
      "loss": 0.558,
      "step": 9680
    },
    {
      "epoch": 2.7685714285714287,
      "grad_norm": 0.5707152485847473,
      "learning_rate": 1.630857142857143e-05,
      "loss": 0.1559,
      "step": 9690
    },
    {
      "epoch": 2.7714285714285714,
      "grad_norm": 0.24626809358596802,
      "learning_rate": 1.6304761904761905e-05,
      "loss": 0.1902,
      "step": 9700
    },
    {
      "epoch": 2.774285714285714,
      "grad_norm": 0.12068349868059158,
      "learning_rate": 1.630095238095238e-05,
      "loss": 0.2359,
      "step": 9710
    },
    {
      "epoch": 2.777142857142857,
      "grad_norm": 0.21816177666187286,
      "learning_rate": 1.6297142857142856e-05,
      "loss": 0.3137,
      "step": 9720
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 24.91680145263672,
      "learning_rate": 1.6293333333333335e-05,
      "loss": 0.2086,
      "step": 9730
    },
    {
      "epoch": 2.782857142857143,
      "grad_norm": 57.79698181152344,
      "learning_rate": 1.628952380952381e-05,
      "loss": 0.3552,
      "step": 9740
    },
    {
      "epoch": 2.7857142857142856,
      "grad_norm": 19.443603515625,
      "learning_rate": 1.6285714285714287e-05,
      "loss": 0.5268,
      "step": 9750
    },
    {
      "epoch": 2.7885714285714287,
      "grad_norm": 0.11899891495704651,
      "learning_rate": 1.6281904761904762e-05,
      "loss": 0.2446,
      "step": 9760
    },
    {
      "epoch": 2.7914285714285714,
      "grad_norm": 0.19858919084072113,
      "learning_rate": 1.627809523809524e-05,
      "loss": 0.404,
      "step": 9770
    },
    {
      "epoch": 2.7942857142857145,
      "grad_norm": 15.518646240234375,
      "learning_rate": 1.6274285714285717e-05,
      "loss": 0.4966,
      "step": 9780
    },
    {
      "epoch": 2.797142857142857,
      "grad_norm": 0.10156721621751785,
      "learning_rate": 1.6270476190476192e-05,
      "loss": 0.0957,
      "step": 9790
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.39890995621681213,
      "learning_rate": 1.6266666666666668e-05,
      "loss": 0.1542,
      "step": 9800
    },
    {
      "epoch": 2.802857142857143,
      "grad_norm": 80.92201232910156,
      "learning_rate": 1.6262857142857143e-05,
      "loss": 0.2265,
      "step": 9810
    },
    {
      "epoch": 2.8057142857142856,
      "grad_norm": 0.051236893981695175,
      "learning_rate": 1.6259047619047622e-05,
      "loss": 0.6232,
      "step": 9820
    },
    {
      "epoch": 2.8085714285714287,
      "grad_norm": 1.1268030405044556,
      "learning_rate": 1.6255238095238098e-05,
      "loss": 0.0933,
      "step": 9830
    },
    {
      "epoch": 2.8114285714285714,
      "grad_norm": 0.02772742509841919,
      "learning_rate": 1.6251428571428574e-05,
      "loss": 0.1996,
      "step": 9840
    },
    {
      "epoch": 2.814285714285714,
      "grad_norm": 0.2034052014350891,
      "learning_rate": 1.624761904761905e-05,
      "loss": 0.0021,
      "step": 9850
    },
    {
      "epoch": 2.817142857142857,
      "grad_norm": 0.029782012104988098,
      "learning_rate": 1.6243809523809525e-05,
      "loss": 0.439,
      "step": 9860
    },
    {
      "epoch": 2.82,
      "grad_norm": 0.05488796532154083,
      "learning_rate": 1.6240000000000004e-05,
      "loss": 0.2448,
      "step": 9870
    },
    {
      "epoch": 2.822857142857143,
      "grad_norm": 0.14994381368160248,
      "learning_rate": 1.6236190476190476e-05,
      "loss": 0.4935,
      "step": 9880
    },
    {
      "epoch": 2.8257142857142856,
      "grad_norm": 0.27810409665107727,
      "learning_rate": 1.623238095238095e-05,
      "loss": 0.0443,
      "step": 9890
    },
    {
      "epoch": 2.8285714285714287,
      "grad_norm": 0.29463401436805725,
      "learning_rate": 1.622857142857143e-05,
      "loss": 0.1099,
      "step": 9900
    },
    {
      "epoch": 2.8314285714285714,
      "grad_norm": 0.12887421250343323,
      "learning_rate": 1.6224761904761906e-05,
      "loss": 0.1373,
      "step": 9910
    },
    {
      "epoch": 2.8342857142857145,
      "grad_norm": 0.07924306392669678,
      "learning_rate": 1.622095238095238e-05,
      "loss": 0.191,
      "step": 9920
    },
    {
      "epoch": 2.837142857142857,
      "grad_norm": 0.021969256922602654,
      "learning_rate": 1.6217142857142857e-05,
      "loss": 0.0015,
      "step": 9930
    },
    {
      "epoch": 2.84,
      "grad_norm": 26.857786178588867,
      "learning_rate": 1.6213333333333333e-05,
      "loss": 0.6,
      "step": 9940
    },
    {
      "epoch": 2.842857142857143,
      "grad_norm": 5.62101411819458,
      "learning_rate": 1.6209523809523812e-05,
      "loss": 0.3405,
      "step": 9950
    },
    {
      "epoch": 2.8457142857142856,
      "grad_norm": 0.13027115166187286,
      "learning_rate": 1.6205714285714287e-05,
      "loss": 0.1221,
      "step": 9960
    },
    {
      "epoch": 2.8485714285714288,
      "grad_norm": 15.51662826538086,
      "learning_rate": 1.6201904761904763e-05,
      "loss": 0.3051,
      "step": 9970
    },
    {
      "epoch": 2.8514285714285714,
      "grad_norm": 24.102590560913086,
      "learning_rate": 1.619809523809524e-05,
      "loss": 0.4164,
      "step": 9980
    },
    {
      "epoch": 2.854285714285714,
      "grad_norm": 30.897890090942383,
      "learning_rate": 1.6194285714285714e-05,
      "loss": 0.3834,
      "step": 9990
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 24.224868774414062,
      "learning_rate": 1.6190476190476193e-05,
      "loss": 0.246,
      "step": 10000
    },
    {
      "epoch": 2.86,
      "grad_norm": 0.24294732511043549,
      "learning_rate": 1.618666666666667e-05,
      "loss": 0.4319,
      "step": 10010
    },
    {
      "epoch": 2.862857142857143,
      "grad_norm": 0.4431028962135315,
      "learning_rate": 1.6182857142857144e-05,
      "loss": 0.1047,
      "step": 10020
    },
    {
      "epoch": 2.8657142857142857,
      "grad_norm": 0.07137303054332733,
      "learning_rate": 1.617904761904762e-05,
      "loss": 0.3525,
      "step": 10030
    },
    {
      "epoch": 2.8685714285714283,
      "grad_norm": 0.16249346733093262,
      "learning_rate": 1.61752380952381e-05,
      "loss": 0.1089,
      "step": 10040
    },
    {
      "epoch": 2.8714285714285714,
      "grad_norm": 24.58985137939453,
      "learning_rate": 1.6171428571428574e-05,
      "loss": 0.4507,
      "step": 10050
    },
    {
      "epoch": 2.8742857142857146,
      "grad_norm": 0.272920161485672,
      "learning_rate": 1.616761904761905e-05,
      "loss": 0.0038,
      "step": 10060
    },
    {
      "epoch": 2.8771428571428572,
      "grad_norm": 0.02166934683918953,
      "learning_rate": 1.6163809523809526e-05,
      "loss": 0.1089,
      "step": 10070
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.18209578096866608,
      "learning_rate": 1.616e-05,
      "loss": 0.2519,
      "step": 10080
    },
    {
      "epoch": 2.882857142857143,
      "grad_norm": 0.19496318697929382,
      "learning_rate": 1.6156190476190477e-05,
      "loss": 0.4149,
      "step": 10090
    },
    {
      "epoch": 2.8857142857142857,
      "grad_norm": 0.08967047929763794,
      "learning_rate": 1.6152380952380952e-05,
      "loss": 0.0144,
      "step": 10100
    },
    {
      "epoch": 2.888571428571429,
      "grad_norm": 0.17045600712299347,
      "learning_rate": 1.6148571428571428e-05,
      "loss": 0.2643,
      "step": 10110
    },
    {
      "epoch": 2.8914285714285715,
      "grad_norm": 1.4347089529037476,
      "learning_rate": 1.6144761904761907e-05,
      "loss": 0.2431,
      "step": 10120
    },
    {
      "epoch": 2.894285714285714,
      "grad_norm": 0.20366232097148895,
      "learning_rate": 1.6140952380952382e-05,
      "loss": 0.2369,
      "step": 10130
    },
    {
      "epoch": 2.8971428571428572,
      "grad_norm": 0.11717016994953156,
      "learning_rate": 1.6137142857142858e-05,
      "loss": 0.2376,
      "step": 10140
    },
    {
      "epoch": 2.9,
      "grad_norm": 1.2572475671768188,
      "learning_rate": 1.6133333333333334e-05,
      "loss": 0.0033,
      "step": 10150
    },
    {
      "epoch": 2.902857142857143,
      "grad_norm": 0.07415063679218292,
      "learning_rate": 1.612952380952381e-05,
      "loss": 0.3921,
      "step": 10160
    },
    {
      "epoch": 2.9057142857142857,
      "grad_norm": 0.018208816647529602,
      "learning_rate": 1.6125714285714288e-05,
      "loss": 0.0518,
      "step": 10170
    },
    {
      "epoch": 2.9085714285714284,
      "grad_norm": 0.022058235481381416,
      "learning_rate": 1.6121904761904764e-05,
      "loss": 0.2751,
      "step": 10180
    },
    {
      "epoch": 2.9114285714285715,
      "grad_norm": 0.05180145055055618,
      "learning_rate": 1.611809523809524e-05,
      "loss": 0.1112,
      "step": 10190
    },
    {
      "epoch": 2.914285714285714,
      "grad_norm": 0.1480637788772583,
      "learning_rate": 1.6114285714285715e-05,
      "loss": 0.39,
      "step": 10200
    },
    {
      "epoch": 2.9171428571428573,
      "grad_norm": 0.03830466419458389,
      "learning_rate": 1.611047619047619e-05,
      "loss": 0.3813,
      "step": 10210
    },
    {
      "epoch": 2.92,
      "grad_norm": 0.04633210971951485,
      "learning_rate": 1.610666666666667e-05,
      "loss": 0.0952,
      "step": 10220
    },
    {
      "epoch": 2.9228571428571426,
      "grad_norm": 0.011737602762877941,
      "learning_rate": 1.6102857142857145e-05,
      "loss": 0.1081,
      "step": 10230
    },
    {
      "epoch": 2.9257142857142857,
      "grad_norm": 16.892654418945312,
      "learning_rate": 1.609904761904762e-05,
      "loss": 0.6578,
      "step": 10240
    },
    {
      "epoch": 2.928571428571429,
      "grad_norm": 37.03798294067383,
      "learning_rate": 1.6095238095238096e-05,
      "loss": 0.4536,
      "step": 10250
    },
    {
      "epoch": 2.9314285714285715,
      "grad_norm": 0.2449953258037567,
      "learning_rate": 1.6091428571428572e-05,
      "loss": 0.2552,
      "step": 10260
    },
    {
      "epoch": 2.934285714285714,
      "grad_norm": 0.04516065865755081,
      "learning_rate": 1.608761904761905e-05,
      "loss": 0.1029,
      "step": 10270
    },
    {
      "epoch": 2.9371428571428573,
      "grad_norm": 0.17044925689697266,
      "learning_rate": 1.6083809523809526e-05,
      "loss": 0.0019,
      "step": 10280
    },
    {
      "epoch": 2.94,
      "grad_norm": 0.011225027032196522,
      "learning_rate": 1.6080000000000002e-05,
      "loss": 0.2229,
      "step": 10290
    },
    {
      "epoch": 2.942857142857143,
      "grad_norm": 0.014285637065768242,
      "learning_rate": 1.6076190476190477e-05,
      "loss": 0.4022,
      "step": 10300
    },
    {
      "epoch": 2.9457142857142857,
      "grad_norm": 0.19987595081329346,
      "learning_rate": 1.6072380952380953e-05,
      "loss": 0.0734,
      "step": 10310
    },
    {
      "epoch": 2.9485714285714284,
      "grad_norm": 0.07503639906644821,
      "learning_rate": 1.606857142857143e-05,
      "loss": 0.3592,
      "step": 10320
    },
    {
      "epoch": 2.9514285714285715,
      "grad_norm": 18.69179344177246,
      "learning_rate": 1.6064761904761904e-05,
      "loss": 0.3258,
      "step": 10330
    },
    {
      "epoch": 2.954285714285714,
      "grad_norm": 0.31073498725891113,
      "learning_rate": 1.6060952380952383e-05,
      "loss": 0.1083,
      "step": 10340
    },
    {
      "epoch": 2.9571428571428573,
      "grad_norm": 0.057290319353342056,
      "learning_rate": 1.605714285714286e-05,
      "loss": 0.5806,
      "step": 10350
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.12076911330223083,
      "learning_rate": 1.6053333333333334e-05,
      "loss": 0.2007,
      "step": 10360
    },
    {
      "epoch": 2.9628571428571426,
      "grad_norm": 0.5951887369155884,
      "learning_rate": 1.604952380952381e-05,
      "loss": 0.1639,
      "step": 10370
    },
    {
      "epoch": 2.9657142857142857,
      "grad_norm": 13.403386116027832,
      "learning_rate": 1.6045714285714286e-05,
      "loss": 0.2005,
      "step": 10380
    },
    {
      "epoch": 2.9685714285714284,
      "grad_norm": 0.19053567945957184,
      "learning_rate": 1.6041904761904764e-05,
      "loss": 0.4038,
      "step": 10390
    },
    {
      "epoch": 2.9714285714285715,
      "grad_norm": 0.8461757898330688,
      "learning_rate": 1.603809523809524e-05,
      "loss": 0.154,
      "step": 10400
    },
    {
      "epoch": 2.974285714285714,
      "grad_norm": 0.14790351688861847,
      "learning_rate": 1.6034285714285716e-05,
      "loss": 0.3571,
      "step": 10410
    },
    {
      "epoch": 2.977142857142857,
      "grad_norm": 0.10285276174545288,
      "learning_rate": 1.603047619047619e-05,
      "loss": 0.1177,
      "step": 10420
    },
    {
      "epoch": 2.98,
      "grad_norm": 22.71360969543457,
      "learning_rate": 1.6026666666666667e-05,
      "loss": 0.202,
      "step": 10430
    },
    {
      "epoch": 2.982857142857143,
      "grad_norm": 0.269621342420578,
      "learning_rate": 1.6022857142857146e-05,
      "loss": 0.0039,
      "step": 10440
    },
    {
      "epoch": 2.9857142857142858,
      "grad_norm": 0.724017858505249,
      "learning_rate": 1.601904761904762e-05,
      "loss": 0.5757,
      "step": 10450
    },
    {
      "epoch": 2.9885714285714284,
      "grad_norm": 0.18551179766654968,
      "learning_rate": 1.6015238095238097e-05,
      "loss": 0.3064,
      "step": 10460
    },
    {
      "epoch": 2.9914285714285715,
      "grad_norm": 0.32641515135765076,
      "learning_rate": 1.6011428571428573e-05,
      "loss": 0.2686,
      "step": 10470
    },
    {
      "epoch": 2.994285714285714,
      "grad_norm": 0.609052836894989,
      "learning_rate": 1.6007619047619048e-05,
      "loss": 0.5596,
      "step": 10480
    },
    {
      "epoch": 2.9971428571428573,
      "grad_norm": 53.079193115234375,
      "learning_rate": 1.6003809523809527e-05,
      "loss": 0.0103,
      "step": 10490
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.18496888875961304,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.1677,
      "step": 10500
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.928763440860215,
      "eval_f1": 0.6666666666666667,
      "eval_loss": 0.329950749874115,
      "eval_precision": 0.8412698412698413,
      "eval_recall": 0.5520833333333334,
      "eval_runtime": 49.0595,
      "eval_samples_per_second": 61.15,
      "eval_steps_per_second": 1.916,
      "step": 10500
    },
    {
      "epoch": 3.0028571428571427,
      "grad_norm": 0.4500125050544739,
      "learning_rate": 1.5996190476190478e-05,
      "loss": 0.0034,
      "step": 10510
    },
    {
      "epoch": 3.005714285714286,
      "grad_norm": 0.1570286750793457,
      "learning_rate": 1.5992380952380954e-05,
      "loss": 0.1767,
      "step": 10520
    },
    {
      "epoch": 3.0085714285714285,
      "grad_norm": 0.13012365996837616,
      "learning_rate": 1.598857142857143e-05,
      "loss": 0.1062,
      "step": 10530
    },
    {
      "epoch": 3.0114285714285716,
      "grad_norm": 0.01805659383535385,
      "learning_rate": 1.5984761904761905e-05,
      "loss": 0.0706,
      "step": 10540
    },
    {
      "epoch": 3.0142857142857142,
      "grad_norm": 0.017846258357167244,
      "learning_rate": 1.598095238095238e-05,
      "loss": 0.4536,
      "step": 10550
    },
    {
      "epoch": 3.0171428571428573,
      "grad_norm": 0.021745124831795692,
      "learning_rate": 1.5977142857142856e-05,
      "loss": 0.0021,
      "step": 10560
    },
    {
      "epoch": 3.02,
      "grad_norm": 4.262531757354736,
      "learning_rate": 1.5973333333333335e-05,
      "loss": 0.5137,
      "step": 10570
    },
    {
      "epoch": 3.0228571428571427,
      "grad_norm": 0.020050952211022377,
      "learning_rate": 1.596952380952381e-05,
      "loss": 0.1769,
      "step": 10580
    },
    {
      "epoch": 3.025714285714286,
      "grad_norm": 0.060461483895778656,
      "learning_rate": 1.5965714285714286e-05,
      "loss": 0.1254,
      "step": 10590
    },
    {
      "epoch": 3.0285714285714285,
      "grad_norm": 0.04398253187537193,
      "learning_rate": 1.5961904761904762e-05,
      "loss": 0.1546,
      "step": 10600
    },
    {
      "epoch": 3.0314285714285716,
      "grad_norm": 0.15280316770076752,
      "learning_rate": 1.595809523809524e-05,
      "loss": 0.0835,
      "step": 10610
    },
    {
      "epoch": 3.0342857142857143,
      "grad_norm": 0.02529379166662693,
      "learning_rate": 1.5954285714285716e-05,
      "loss": 0.3167,
      "step": 10620
    },
    {
      "epoch": 3.0371428571428574,
      "grad_norm": 0.02330375835299492,
      "learning_rate": 1.5950476190476192e-05,
      "loss": 0.1665,
      "step": 10630
    },
    {
      "epoch": 3.04,
      "grad_norm": 0.13717620074748993,
      "learning_rate": 1.5946666666666668e-05,
      "loss": 0.1108,
      "step": 10640
    },
    {
      "epoch": 3.0428571428571427,
      "grad_norm": 0.5597459077835083,
      "learning_rate": 1.5942857142857143e-05,
      "loss": 0.1449,
      "step": 10650
    },
    {
      "epoch": 3.045714285714286,
      "grad_norm": 0.24583640694618225,
      "learning_rate": 1.5939047619047622e-05,
      "loss": 0.1933,
      "step": 10660
    },
    {
      "epoch": 3.0485714285714285,
      "grad_norm": 0.026904208585619926,
      "learning_rate": 1.5935238095238098e-05,
      "loss": 0.1959,
      "step": 10670
    },
    {
      "epoch": 3.0514285714285716,
      "grad_norm": 0.02899029478430748,
      "learning_rate": 1.5931428571428573e-05,
      "loss": 0.0031,
      "step": 10680
    },
    {
      "epoch": 3.0542857142857143,
      "grad_norm": 38.6600341796875,
      "learning_rate": 1.592761904761905e-05,
      "loss": 0.4603,
      "step": 10690
    },
    {
      "epoch": 3.057142857142857,
      "grad_norm": 0.17908082902431488,
      "learning_rate": 1.5923809523809524e-05,
      "loss": 0.1894,
      "step": 10700
    },
    {
      "epoch": 3.06,
      "grad_norm": 0.060235992074012756,
      "learning_rate": 1.5920000000000003e-05,
      "loss": 0.1713,
      "step": 10710
    },
    {
      "epoch": 3.0628571428571427,
      "grad_norm": 0.09200900793075562,
      "learning_rate": 1.591619047619048e-05,
      "loss": 0.1586,
      "step": 10720
    },
    {
      "epoch": 3.065714285714286,
      "grad_norm": 0.035045575350522995,
      "learning_rate": 1.5912380952380955e-05,
      "loss": 0.0025,
      "step": 10730
    },
    {
      "epoch": 3.0685714285714285,
      "grad_norm": 0.018030237406492233,
      "learning_rate": 1.590857142857143e-05,
      "loss": 0.2855,
      "step": 10740
    },
    {
      "epoch": 3.0714285714285716,
      "grad_norm": 77.26187896728516,
      "learning_rate": 1.5904761904761906e-05,
      "loss": 0.5178,
      "step": 10750
    },
    {
      "epoch": 3.0742857142857143,
      "grad_norm": 3.395195484161377,
      "learning_rate": 1.590095238095238e-05,
      "loss": 0.0035,
      "step": 10760
    },
    {
      "epoch": 3.077142857142857,
      "grad_norm": 0.017472459003329277,
      "learning_rate": 1.5897142857142857e-05,
      "loss": 0.2403,
      "step": 10770
    },
    {
      "epoch": 3.08,
      "grad_norm": 13.014256477355957,
      "learning_rate": 1.5893333333333333e-05,
      "loss": 0.133,
      "step": 10780
    },
    {
      "epoch": 3.0828571428571427,
      "grad_norm": 0.060438722372055054,
      "learning_rate": 1.588952380952381e-05,
      "loss": 0.1118,
      "step": 10790
    },
    {
      "epoch": 3.085714285714286,
      "grad_norm": 0.11553756147623062,
      "learning_rate": 1.5885714285714287e-05,
      "loss": 0.1715,
      "step": 10800
    },
    {
      "epoch": 3.0885714285714285,
      "grad_norm": 0.13773523271083832,
      "learning_rate": 1.5881904761904763e-05,
      "loss": 0.0377,
      "step": 10810
    },
    {
      "epoch": 3.0914285714285716,
      "grad_norm": 0.016897300258278847,
      "learning_rate": 1.5878095238095238e-05,
      "loss": 0.0862,
      "step": 10820
    },
    {
      "epoch": 3.0942857142857143,
      "grad_norm": 58.85334014892578,
      "learning_rate": 1.5874285714285714e-05,
      "loss": 0.3503,
      "step": 10830
    },
    {
      "epoch": 3.097142857142857,
      "grad_norm": 0.11159134656190872,
      "learning_rate": 1.5870476190476193e-05,
      "loss": 0.1545,
      "step": 10840
    },
    {
      "epoch": 3.1,
      "grad_norm": 0.034239836037158966,
      "learning_rate": 1.586666666666667e-05,
      "loss": 0.0195,
      "step": 10850
    },
    {
      "epoch": 3.1028571428571428,
      "grad_norm": 0.009002117440104485,
      "learning_rate": 1.5862857142857144e-05,
      "loss": 0.0024,
      "step": 10860
    },
    {
      "epoch": 3.105714285714286,
      "grad_norm": 0.03030381351709366,
      "learning_rate": 1.585904761904762e-05,
      "loss": 0.5767,
      "step": 10870
    },
    {
      "epoch": 3.1085714285714285,
      "grad_norm": 0.01968677155673504,
      "learning_rate": 1.58552380952381e-05,
      "loss": 0.2129,
      "step": 10880
    },
    {
      "epoch": 3.111428571428571,
      "grad_norm": 0.050620708614587784,
      "learning_rate": 1.5851428571428574e-05,
      "loss": 0.0873,
      "step": 10890
    },
    {
      "epoch": 3.1142857142857143,
      "grad_norm": 0.01718757301568985,
      "learning_rate": 1.584761904761905e-05,
      "loss": 0.02,
      "step": 10900
    },
    {
      "epoch": 3.117142857142857,
      "grad_norm": 0.07475533336400986,
      "learning_rate": 1.5843809523809525e-05,
      "loss": 0.0029,
      "step": 10910
    },
    {
      "epoch": 3.12,
      "grad_norm": 0.6930198073387146,
      "learning_rate": 1.584e-05,
      "loss": 0.1189,
      "step": 10920
    },
    {
      "epoch": 3.1228571428571428,
      "grad_norm": 0.011511513963341713,
      "learning_rate": 1.583619047619048e-05,
      "loss": 0.2129,
      "step": 10930
    },
    {
      "epoch": 3.125714285714286,
      "grad_norm": 0.014309129677712917,
      "learning_rate": 1.5832380952380955e-05,
      "loss": 0.0016,
      "step": 10940
    },
    {
      "epoch": 3.1285714285714286,
      "grad_norm": 13.032285690307617,
      "learning_rate": 1.582857142857143e-05,
      "loss": 0.1495,
      "step": 10950
    },
    {
      "epoch": 3.1314285714285712,
      "grad_norm": 0.015593191608786583,
      "learning_rate": 1.5824761904761907e-05,
      "loss": 0.1958,
      "step": 10960
    },
    {
      "epoch": 3.1342857142857143,
      "grad_norm": 87.7929458618164,
      "learning_rate": 1.5820952380952382e-05,
      "loss": 0.0908,
      "step": 10970
    },
    {
      "epoch": 3.137142857142857,
      "grad_norm": 0.045215409249067307,
      "learning_rate": 1.5817142857142858e-05,
      "loss": 0.3446,
      "step": 10980
    },
    {
      "epoch": 3.14,
      "grad_norm": 0.07532883435487747,
      "learning_rate": 1.5813333333333333e-05,
      "loss": 0.5538,
      "step": 10990
    },
    {
      "epoch": 3.142857142857143,
      "grad_norm": 24.65769386291504,
      "learning_rate": 1.580952380952381e-05,
      "loss": 0.3064,
      "step": 11000
    },
    {
      "epoch": 3.145714285714286,
      "grad_norm": 0.02727513574063778,
      "learning_rate": 1.5805714285714288e-05,
      "loss": 0.0011,
      "step": 11010
    },
    {
      "epoch": 3.1485714285714286,
      "grad_norm": 134.9658966064453,
      "learning_rate": 1.5801904761904763e-05,
      "loss": 0.4315,
      "step": 11020
    },
    {
      "epoch": 3.1514285714285712,
      "grad_norm": 26.973175048828125,
      "learning_rate": 1.579809523809524e-05,
      "loss": 0.4444,
      "step": 11030
    },
    {
      "epoch": 3.1542857142857144,
      "grad_norm": 0.11072610318660736,
      "learning_rate": 1.5794285714285715e-05,
      "loss": 0.1323,
      "step": 11040
    },
    {
      "epoch": 3.157142857142857,
      "grad_norm": 0.3505330979824066,
      "learning_rate": 1.579047619047619e-05,
      "loss": 0.2268,
      "step": 11050
    },
    {
      "epoch": 3.16,
      "grad_norm": 0.04734306409955025,
      "learning_rate": 1.578666666666667e-05,
      "loss": 0.1406,
      "step": 11060
    },
    {
      "epoch": 3.162857142857143,
      "grad_norm": 6.164464950561523,
      "learning_rate": 1.5782857142857145e-05,
      "loss": 0.0031,
      "step": 11070
    },
    {
      "epoch": 3.1657142857142855,
      "grad_norm": 0.022550955414772034,
      "learning_rate": 1.577904761904762e-05,
      "loss": 0.098,
      "step": 11080
    },
    {
      "epoch": 3.1685714285714286,
      "grad_norm": 13.625679016113281,
      "learning_rate": 1.5775238095238096e-05,
      "loss": 0.1895,
      "step": 11090
    },
    {
      "epoch": 3.1714285714285713,
      "grad_norm": 0.10133317857980728,
      "learning_rate": 1.577142857142857e-05,
      "loss": 0.2017,
      "step": 11100
    },
    {
      "epoch": 3.1742857142857144,
      "grad_norm": 0.022765105590224266,
      "learning_rate": 1.576761904761905e-05,
      "loss": 0.1977,
      "step": 11110
    },
    {
      "epoch": 3.177142857142857,
      "grad_norm": 0.02428816445171833,
      "learning_rate": 1.5763809523809526e-05,
      "loss": 0.0027,
      "step": 11120
    },
    {
      "epoch": 3.18,
      "grad_norm": 0.09450438618659973,
      "learning_rate": 1.576e-05,
      "loss": 0.1914,
      "step": 11130
    },
    {
      "epoch": 3.182857142857143,
      "grad_norm": 0.06051420420408249,
      "learning_rate": 1.5756190476190477e-05,
      "loss": 0.0009,
      "step": 11140
    },
    {
      "epoch": 3.185714285714286,
      "grad_norm": 88.95976257324219,
      "learning_rate": 1.5752380952380956e-05,
      "loss": 0.3014,
      "step": 11150
    },
    {
      "epoch": 3.1885714285714286,
      "grad_norm": 0.045139335095882416,
      "learning_rate": 1.5748571428571432e-05,
      "loss": 0.1191,
      "step": 11160
    },
    {
      "epoch": 3.1914285714285713,
      "grad_norm": 0.025116214528679848,
      "learning_rate": 1.5744761904761907e-05,
      "loss": 0.151,
      "step": 11170
    },
    {
      "epoch": 3.1942857142857144,
      "grad_norm": 0.12442025542259216,
      "learning_rate": 1.5740952380952383e-05,
      "loss": 0.2017,
      "step": 11180
    },
    {
      "epoch": 3.197142857142857,
      "grad_norm": 0.024999534711241722,
      "learning_rate": 1.573714285714286e-05,
      "loss": 0.131,
      "step": 11190
    },
    {
      "epoch": 3.2,
      "grad_norm": 0.02240847423672676,
      "learning_rate": 1.5733333333333334e-05,
      "loss": 0.0011,
      "step": 11200
    },
    {
      "epoch": 3.202857142857143,
      "grad_norm": 0.05439796298742294,
      "learning_rate": 1.572952380952381e-05,
      "loss": 0.0012,
      "step": 11210
    },
    {
      "epoch": 3.2057142857142855,
      "grad_norm": 13.219088554382324,
      "learning_rate": 1.5725714285714285e-05,
      "loss": 0.431,
      "step": 11220
    },
    {
      "epoch": 3.2085714285714286,
      "grad_norm": 0.044036250561475754,
      "learning_rate": 1.5721904761904764e-05,
      "loss": 0.2925,
      "step": 11230
    },
    {
      "epoch": 3.2114285714285713,
      "grad_norm": 0.09007859975099564,
      "learning_rate": 1.571809523809524e-05,
      "loss": 0.0409,
      "step": 11240
    },
    {
      "epoch": 3.2142857142857144,
      "grad_norm": 0.029250936582684517,
      "learning_rate": 1.5714285714285715e-05,
      "loss": 0.3168,
      "step": 11250
    },
    {
      "epoch": 3.217142857142857,
      "grad_norm": 29.98186492919922,
      "learning_rate": 1.571047619047619e-05,
      "loss": 0.2013,
      "step": 11260
    },
    {
      "epoch": 3.22,
      "grad_norm": 0.2175658792257309,
      "learning_rate": 1.5706666666666666e-05,
      "loss": 0.1898,
      "step": 11270
    },
    {
      "epoch": 3.222857142857143,
      "grad_norm": 0.16074971854686737,
      "learning_rate": 1.5702857142857145e-05,
      "loss": 0.1979,
      "step": 11280
    },
    {
      "epoch": 3.2257142857142855,
      "grad_norm": 0.22958935797214508,
      "learning_rate": 1.569904761904762e-05,
      "loss": 0.0018,
      "step": 11290
    },
    {
      "epoch": 3.2285714285714286,
      "grad_norm": 0.058865658938884735,
      "learning_rate": 1.5695238095238097e-05,
      "loss": 0.0902,
      "step": 11300
    },
    {
      "epoch": 3.2314285714285713,
      "grad_norm": 0.1095389649271965,
      "learning_rate": 1.5691428571428572e-05,
      "loss": 0.0592,
      "step": 11310
    },
    {
      "epoch": 3.2342857142857144,
      "grad_norm": 0.5302996039390564,
      "learning_rate": 1.5687619047619048e-05,
      "loss": 0.0079,
      "step": 11320
    },
    {
      "epoch": 3.237142857142857,
      "grad_norm": 0.027151240035891533,
      "learning_rate": 1.5683809523809527e-05,
      "loss": 0.4562,
      "step": 11330
    },
    {
      "epoch": 3.24,
      "grad_norm": 0.022152597084641457,
      "learning_rate": 1.5680000000000002e-05,
      "loss": 0.1595,
      "step": 11340
    },
    {
      "epoch": 3.242857142857143,
      "grad_norm": 0.09903263300657272,
      "learning_rate": 1.5676190476190478e-05,
      "loss": 0.1335,
      "step": 11350
    },
    {
      "epoch": 3.2457142857142856,
      "grad_norm": 26.458133697509766,
      "learning_rate": 1.5672380952380954e-05,
      "loss": 0.365,
      "step": 11360
    },
    {
      "epoch": 3.2485714285714287,
      "grad_norm": 0.022694922983646393,
      "learning_rate": 1.566857142857143e-05,
      "loss": 0.2407,
      "step": 11370
    },
    {
      "epoch": 3.2514285714285713,
      "grad_norm": 0.22143785655498505,
      "learning_rate": 1.5664761904761908e-05,
      "loss": 0.2287,
      "step": 11380
    },
    {
      "epoch": 3.2542857142857144,
      "grad_norm": 0.0766882374882698,
      "learning_rate": 1.566095238095238e-05,
      "loss": 0.4774,
      "step": 11390
    },
    {
      "epoch": 3.257142857142857,
      "grad_norm": 0.05402998998761177,
      "learning_rate": 1.5657142857142856e-05,
      "loss": 0.2699,
      "step": 11400
    },
    {
      "epoch": 3.26,
      "grad_norm": 0.04455944895744324,
      "learning_rate": 1.5653333333333335e-05,
      "loss": 0.1794,
      "step": 11410
    },
    {
      "epoch": 3.262857142857143,
      "grad_norm": 7.415871620178223,
      "learning_rate": 1.564952380952381e-05,
      "loss": 0.155,
      "step": 11420
    },
    {
      "epoch": 3.2657142857142856,
      "grad_norm": 0.03631396219134331,
      "learning_rate": 1.5645714285714286e-05,
      "loss": 0.0118,
      "step": 11430
    },
    {
      "epoch": 3.2685714285714287,
      "grad_norm": 86.84920501708984,
      "learning_rate": 1.564190476190476e-05,
      "loss": 0.0856,
      "step": 11440
    },
    {
      "epoch": 3.2714285714285714,
      "grad_norm": 0.05017491430044174,
      "learning_rate": 1.563809523809524e-05,
      "loss": 0.8216,
      "step": 11450
    },
    {
      "epoch": 3.2742857142857145,
      "grad_norm": 0.1280176192522049,
      "learning_rate": 1.5634285714285716e-05,
      "loss": 0.139,
      "step": 11460
    },
    {
      "epoch": 3.277142857142857,
      "grad_norm": 0.13920661807060242,
      "learning_rate": 1.563047619047619e-05,
      "loss": 0.0049,
      "step": 11470
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 0.5778751969337463,
      "learning_rate": 1.5626666666666667e-05,
      "loss": 0.0023,
      "step": 11480
    },
    {
      "epoch": 3.282857142857143,
      "grad_norm": 0.044389285147190094,
      "learning_rate": 1.5622857142857143e-05,
      "loss": 0.1776,
      "step": 11490
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 0.05095602944493294,
      "learning_rate": 1.5619047619047622e-05,
      "loss": 0.358,
      "step": 11500
    },
    {
      "epoch": 3.2885714285714287,
      "grad_norm": 0.17315711081027985,
      "learning_rate": 1.5615238095238097e-05,
      "loss": 0.4157,
      "step": 11510
    },
    {
      "epoch": 3.2914285714285714,
      "grad_norm": 0.06931217014789581,
      "learning_rate": 1.5611428571428573e-05,
      "loss": 0.2204,
      "step": 11520
    },
    {
      "epoch": 3.2942857142857145,
      "grad_norm": 32.75654220581055,
      "learning_rate": 1.560761904761905e-05,
      "loss": 0.3846,
      "step": 11530
    },
    {
      "epoch": 3.297142857142857,
      "grad_norm": 17.56983757019043,
      "learning_rate": 1.5603809523809524e-05,
      "loss": 0.2366,
      "step": 11540
    },
    {
      "epoch": 3.3,
      "grad_norm": 0.07232517004013062,
      "learning_rate": 1.5600000000000003e-05,
      "loss": 0.0163,
      "step": 11550
    },
    {
      "epoch": 3.302857142857143,
      "grad_norm": 142.4844207763672,
      "learning_rate": 1.559619047619048e-05,
      "loss": 0.2994,
      "step": 11560
    },
    {
      "epoch": 3.3057142857142856,
      "grad_norm": 0.15936867892742157,
      "learning_rate": 1.5592380952380954e-05,
      "loss": 0.1215,
      "step": 11570
    },
    {
      "epoch": 3.3085714285714287,
      "grad_norm": 0.5138884782791138,
      "learning_rate": 1.558857142857143e-05,
      "loss": 0.0501,
      "step": 11580
    },
    {
      "epoch": 3.3114285714285714,
      "grad_norm": 0.05471745878458023,
      "learning_rate": 1.5584761904761905e-05,
      "loss": 0.3753,
      "step": 11590
    },
    {
      "epoch": 3.314285714285714,
      "grad_norm": 15.182493209838867,
      "learning_rate": 1.5580952380952384e-05,
      "loss": 0.5138,
      "step": 11600
    },
    {
      "epoch": 3.317142857142857,
      "grad_norm": 0.08526645600795746,
      "learning_rate": 1.5577142857142857e-05,
      "loss": 0.1113,
      "step": 11610
    },
    {
      "epoch": 3.32,
      "grad_norm": 0.3303208649158478,
      "learning_rate": 1.5573333333333332e-05,
      "loss": 0.3594,
      "step": 11620
    },
    {
      "epoch": 3.322857142857143,
      "grad_norm": 0.054106928408145905,
      "learning_rate": 1.556952380952381e-05,
      "loss": 0.1511,
      "step": 11630
    },
    {
      "epoch": 3.3257142857142856,
      "grad_norm": 11.815287590026855,
      "learning_rate": 1.5565714285714287e-05,
      "loss": 1.0484,
      "step": 11640
    },
    {
      "epoch": 3.3285714285714287,
      "grad_norm": 11.5996675491333,
      "learning_rate": 1.5561904761904762e-05,
      "loss": 0.7793,
      "step": 11650
    },
    {
      "epoch": 3.3314285714285714,
      "grad_norm": 6.375330924987793,
      "learning_rate": 1.5558095238095238e-05,
      "loss": 0.5601,
      "step": 11660
    },
    {
      "epoch": 3.3342857142857145,
      "grad_norm": 2.9729857444763184,
      "learning_rate": 1.5554285714285713e-05,
      "loss": 0.3398,
      "step": 11670
    },
    {
      "epoch": 3.337142857142857,
      "grad_norm": 13.684341430664062,
      "learning_rate": 1.5550476190476192e-05,
      "loss": 0.6711,
      "step": 11680
    },
    {
      "epoch": 3.34,
      "grad_norm": 0.7565246820449829,
      "learning_rate": 1.5546666666666668e-05,
      "loss": 0.4841,
      "step": 11690
    },
    {
      "epoch": 3.342857142857143,
      "grad_norm": 6.907683372497559,
      "learning_rate": 1.5542857142857144e-05,
      "loss": 0.8352,
      "step": 11700
    },
    {
      "epoch": 3.3457142857142856,
      "grad_norm": 0.5761725306510925,
      "learning_rate": 1.553904761904762e-05,
      "loss": 0.3657,
      "step": 11710
    },
    {
      "epoch": 3.3485714285714288,
      "grad_norm": 0.09208139032125473,
      "learning_rate": 1.5535238095238098e-05,
      "loss": 0.1494,
      "step": 11720
    },
    {
      "epoch": 3.3514285714285714,
      "grad_norm": 0.11198330670595169,
      "learning_rate": 1.5531428571428574e-05,
      "loss": 0.4609,
      "step": 11730
    },
    {
      "epoch": 3.354285714285714,
      "grad_norm": 0.3918999135494232,
      "learning_rate": 1.552761904761905e-05,
      "loss": 0.6656,
      "step": 11740
    },
    {
      "epoch": 3.357142857142857,
      "grad_norm": 1.4661200046539307,
      "learning_rate": 1.5523809523809525e-05,
      "loss": 0.8509,
      "step": 11750
    },
    {
      "epoch": 3.36,
      "grad_norm": 11.918452262878418,
      "learning_rate": 1.552e-05,
      "loss": 0.4992,
      "step": 11760
    },
    {
      "epoch": 3.362857142857143,
      "grad_norm": 0.7038776874542236,
      "learning_rate": 1.551619047619048e-05,
      "loss": 0.314,
      "step": 11770
    },
    {
      "epoch": 3.3657142857142857,
      "grad_norm": 0.42723992466926575,
      "learning_rate": 1.5512380952380955e-05,
      "loss": 0.5874,
      "step": 11780
    },
    {
      "epoch": 3.3685714285714283,
      "grad_norm": 0.578581690788269,
      "learning_rate": 1.550857142857143e-05,
      "loss": 0.5834,
      "step": 11790
    },
    {
      "epoch": 3.3714285714285714,
      "grad_norm": 0.8964341878890991,
      "learning_rate": 1.5504761904761906e-05,
      "loss": 1.0202,
      "step": 11800
    },
    {
      "epoch": 3.374285714285714,
      "grad_norm": 0.8880950212478638,
      "learning_rate": 1.5500952380952382e-05,
      "loss": 0.421,
      "step": 11810
    },
    {
      "epoch": 3.3771428571428572,
      "grad_norm": 0.7440069317817688,
      "learning_rate": 1.549714285714286e-05,
      "loss": 0.3163,
      "step": 11820
    },
    {
      "epoch": 3.38,
      "grad_norm": 0.7048748731613159,
      "learning_rate": 1.5493333333333333e-05,
      "loss": 0.6707,
      "step": 11830
    },
    {
      "epoch": 3.382857142857143,
      "grad_norm": 0.5611252784729004,
      "learning_rate": 1.548952380952381e-05,
      "loss": 0.4525,
      "step": 11840
    },
    {
      "epoch": 3.3857142857142857,
      "grad_norm": 0.5997125506401062,
      "learning_rate": 1.5485714285714287e-05,
      "loss": 0.5557,
      "step": 11850
    },
    {
      "epoch": 3.388571428571429,
      "grad_norm": 10.99991512298584,
      "learning_rate": 1.5481904761904763e-05,
      "loss": 0.6866,
      "step": 11860
    },
    {
      "epoch": 3.3914285714285715,
      "grad_norm": 0.5115445852279663,
      "learning_rate": 1.547809523809524e-05,
      "loss": 0.5856,
      "step": 11870
    },
    {
      "epoch": 3.394285714285714,
      "grad_norm": 0.4707340598106384,
      "learning_rate": 1.5474285714285714e-05,
      "loss": 0.4525,
      "step": 11880
    },
    {
      "epoch": 3.3971428571428572,
      "grad_norm": 34.786190032958984,
      "learning_rate": 1.547047619047619e-05,
      "loss": 0.7909,
      "step": 11890
    },
    {
      "epoch": 3.4,
      "grad_norm": 0.9733182787895203,
      "learning_rate": 1.546666666666667e-05,
      "loss": 0.6338,
      "step": 11900
    },
    {
      "epoch": 3.402857142857143,
      "grad_norm": 0.6338973641395569,
      "learning_rate": 1.5462857142857144e-05,
      "loss": 0.4207,
      "step": 11910
    },
    {
      "epoch": 3.4057142857142857,
      "grad_norm": 0.5369341373443604,
      "learning_rate": 1.545904761904762e-05,
      "loss": 0.6623,
      "step": 11920
    },
    {
      "epoch": 3.4085714285714284,
      "grad_norm": 23.790706634521484,
      "learning_rate": 1.5455238095238096e-05,
      "loss": 0.5946,
      "step": 11930
    },
    {
      "epoch": 3.4114285714285715,
      "grad_norm": 0.6470358371734619,
      "learning_rate": 1.545142857142857e-05,
      "loss": 0.6752,
      "step": 11940
    },
    {
      "epoch": 3.414285714285714,
      "grad_norm": 1.3574538230895996,
      "learning_rate": 1.544761904761905e-05,
      "loss": 1.0118,
      "step": 11950
    },
    {
      "epoch": 3.4171428571428573,
      "grad_norm": 1.4802175760269165,
      "learning_rate": 1.5443809523809526e-05,
      "loss": 0.3791,
      "step": 11960
    },
    {
      "epoch": 3.42,
      "grad_norm": 1.8330522775650024,
      "learning_rate": 1.544e-05,
      "loss": 0.6585,
      "step": 11970
    },
    {
      "epoch": 3.422857142857143,
      "grad_norm": 10.612512588500977,
      "learning_rate": 1.5436190476190477e-05,
      "loss": 0.3071,
      "step": 11980
    },
    {
      "epoch": 3.4257142857142857,
      "grad_norm": 0.9666556715965271,
      "learning_rate": 1.5432380952380956e-05,
      "loss": 0.6948,
      "step": 11990
    },
    {
      "epoch": 3.4285714285714284,
      "grad_norm": 1.1189717054367065,
      "learning_rate": 1.542857142857143e-05,
      "loss": 0.5055,
      "step": 12000
    },
    {
      "epoch": 3.4314285714285715,
      "grad_norm": 0.8080103397369385,
      "learning_rate": 1.5424761904761907e-05,
      "loss": 0.9429,
      "step": 12010
    },
    {
      "epoch": 3.434285714285714,
      "grad_norm": 10.800514221191406,
      "learning_rate": 1.5420952380952383e-05,
      "loss": 0.6903,
      "step": 12020
    },
    {
      "epoch": 3.4371428571428573,
      "grad_norm": 1.013459324836731,
      "learning_rate": 1.5417142857142858e-05,
      "loss": 0.3916,
      "step": 12030
    },
    {
      "epoch": 3.44,
      "grad_norm": 10.670403480529785,
      "learning_rate": 1.5413333333333337e-05,
      "loss": 0.3166,
      "step": 12040
    },
    {
      "epoch": 3.442857142857143,
      "grad_norm": 0.6005395650863647,
      "learning_rate": 1.540952380952381e-05,
      "loss": 0.3379,
      "step": 12050
    },
    {
      "epoch": 3.4457142857142857,
      "grad_norm": 12.618794441223145,
      "learning_rate": 1.5405714285714285e-05,
      "loss": 0.5555,
      "step": 12060
    },
    {
      "epoch": 3.4485714285714284,
      "grad_norm": 11.308425903320312,
      "learning_rate": 1.5401904761904764e-05,
      "loss": 0.4361,
      "step": 12070
    },
    {
      "epoch": 3.4514285714285715,
      "grad_norm": 11.178799629211426,
      "learning_rate": 1.539809523809524e-05,
      "loss": 0.5338,
      "step": 12080
    },
    {
      "epoch": 3.454285714285714,
      "grad_norm": 11.636293411254883,
      "learning_rate": 1.5394285714285715e-05,
      "loss": 0.6069,
      "step": 12090
    },
    {
      "epoch": 3.4571428571428573,
      "grad_norm": 10.8675537109375,
      "learning_rate": 1.539047619047619e-05,
      "loss": 0.7059,
      "step": 12100
    },
    {
      "epoch": 3.46,
      "grad_norm": 1.6137787103652954,
      "learning_rate": 1.5386666666666666e-05,
      "loss": 0.3924,
      "step": 12110
    },
    {
      "epoch": 3.4628571428571426,
      "grad_norm": 1.1461886167526245,
      "learning_rate": 1.5382857142857145e-05,
      "loss": 0.5681,
      "step": 12120
    },
    {
      "epoch": 3.4657142857142857,
      "grad_norm": 10.779738426208496,
      "learning_rate": 1.537904761904762e-05,
      "loss": 0.6871,
      "step": 12130
    },
    {
      "epoch": 3.4685714285714284,
      "grad_norm": 1.108292818069458,
      "learning_rate": 1.5375238095238096e-05,
      "loss": 0.3983,
      "step": 12140
    },
    {
      "epoch": 3.4714285714285715,
      "grad_norm": 0.6481146216392517,
      "learning_rate": 1.5371428571428572e-05,
      "loss": 0.4303,
      "step": 12150
    },
    {
      "epoch": 3.474285714285714,
      "grad_norm": 12.118477821350098,
      "learning_rate": 1.5367619047619047e-05,
      "loss": 0.6589,
      "step": 12160
    },
    {
      "epoch": 3.4771428571428573,
      "grad_norm": 13.19941520690918,
      "learning_rate": 1.5363809523809526e-05,
      "loss": 0.5355,
      "step": 12170
    },
    {
      "epoch": 3.48,
      "grad_norm": 11.257157325744629,
      "learning_rate": 1.5360000000000002e-05,
      "loss": 0.5119,
      "step": 12180
    },
    {
      "epoch": 3.482857142857143,
      "grad_norm": 1.0588525533676147,
      "learning_rate": 1.5356190476190478e-05,
      "loss": 0.7007,
      "step": 12190
    },
    {
      "epoch": 3.4857142857142858,
      "grad_norm": 0.6188631653785706,
      "learning_rate": 1.5352380952380953e-05,
      "loss": 0.5462,
      "step": 12200
    },
    {
      "epoch": 3.4885714285714284,
      "grad_norm": 22.991525650024414,
      "learning_rate": 1.534857142857143e-05,
      "loss": 0.672,
      "step": 12210
    },
    {
      "epoch": 3.4914285714285715,
      "grad_norm": 23.253175735473633,
      "learning_rate": 1.5344761904761908e-05,
      "loss": 0.6511,
      "step": 12220
    },
    {
      "epoch": 3.494285714285714,
      "grad_norm": 0.7264874577522278,
      "learning_rate": 1.5340952380952383e-05,
      "loss": 0.6419,
      "step": 12230
    },
    {
      "epoch": 3.4971428571428573,
      "grad_norm": 12.340025901794434,
      "learning_rate": 1.533714285714286e-05,
      "loss": 0.6568,
      "step": 12240
    },
    {
      "epoch": 3.5,
      "grad_norm": 0.6517765522003174,
      "learning_rate": 1.5333333333333334e-05,
      "loss": 0.6541,
      "step": 12250
    },
    {
      "epoch": 3.5028571428571427,
      "grad_norm": 11.601212501525879,
      "learning_rate": 1.532952380952381e-05,
      "loss": 0.5529,
      "step": 12260
    },
    {
      "epoch": 3.505714285714286,
      "grad_norm": 12.322246551513672,
      "learning_rate": 1.5325714285714286e-05,
      "loss": 0.7683,
      "step": 12270
    },
    {
      "epoch": 3.5085714285714285,
      "grad_norm": 11.482855796813965,
      "learning_rate": 1.532190476190476e-05,
      "loss": 0.4589,
      "step": 12280
    },
    {
      "epoch": 3.5114285714285716,
      "grad_norm": 0.8588703274726868,
      "learning_rate": 1.531809523809524e-05,
      "loss": 0.8467,
      "step": 12290
    },
    {
      "epoch": 3.5142857142857142,
      "grad_norm": 0.7510924339294434,
      "learning_rate": 1.5314285714285716e-05,
      "loss": 0.4229,
      "step": 12300
    },
    {
      "epoch": 3.517142857142857,
      "grad_norm": 0.5700657963752747,
      "learning_rate": 1.531047619047619e-05,
      "loss": 0.4676,
      "step": 12310
    },
    {
      "epoch": 3.52,
      "grad_norm": 0.773019015789032,
      "learning_rate": 1.5306666666666667e-05,
      "loss": 0.8578,
      "step": 12320
    },
    {
      "epoch": 3.522857142857143,
      "grad_norm": 10.643574714660645,
      "learning_rate": 1.5302857142857143e-05,
      "loss": 0.7209,
      "step": 12330
    },
    {
      "epoch": 3.525714285714286,
      "grad_norm": 11.067782402038574,
      "learning_rate": 1.529904761904762e-05,
      "loss": 0.4223,
      "step": 12340
    },
    {
      "epoch": 3.5285714285714285,
      "grad_norm": 10.933794021606445,
      "learning_rate": 1.5295238095238097e-05,
      "loss": 0.5264,
      "step": 12350
    },
    {
      "epoch": 3.5314285714285716,
      "grad_norm": 22.23163414001465,
      "learning_rate": 1.5291428571428573e-05,
      "loss": 0.9514,
      "step": 12360
    },
    {
      "epoch": 3.5342857142857143,
      "grad_norm": 10.472338676452637,
      "learning_rate": 1.5287619047619048e-05,
      "loss": 0.911,
      "step": 12370
    },
    {
      "epoch": 3.5371428571428574,
      "grad_norm": 1.1986130475997925,
      "learning_rate": 1.5283809523809524e-05,
      "loss": 0.3931,
      "step": 12380
    },
    {
      "epoch": 3.54,
      "grad_norm": 9.97364616394043,
      "learning_rate": 1.5280000000000003e-05,
      "loss": 0.7409,
      "step": 12390
    },
    {
      "epoch": 3.5428571428571427,
      "grad_norm": 1.2525917291641235,
      "learning_rate": 1.527619047619048e-05,
      "loss": 0.2857,
      "step": 12400
    },
    {
      "epoch": 3.545714285714286,
      "grad_norm": 0.7757786512374878,
      "learning_rate": 1.5272380952380954e-05,
      "loss": 0.3082,
      "step": 12410
    },
    {
      "epoch": 3.5485714285714285,
      "grad_norm": 0.8154909610748291,
      "learning_rate": 1.526857142857143e-05,
      "loss": 0.4318,
      "step": 12420
    },
    {
      "epoch": 3.5514285714285716,
      "grad_norm": 0.6293126940727234,
      "learning_rate": 1.5264761904761905e-05,
      "loss": 0.5449,
      "step": 12430
    },
    {
      "epoch": 3.5542857142857143,
      "grad_norm": 0.5752149224281311,
      "learning_rate": 1.5260952380952384e-05,
      "loss": 0.4401,
      "step": 12440
    },
    {
      "epoch": 3.557142857142857,
      "grad_norm": 11.441524505615234,
      "learning_rate": 1.525714285714286e-05,
      "loss": 0.6598,
      "step": 12450
    },
    {
      "epoch": 3.56,
      "grad_norm": 0.7967050671577454,
      "learning_rate": 1.5253333333333335e-05,
      "loss": 0.4283,
      "step": 12460
    },
    {
      "epoch": 3.5628571428571427,
      "grad_norm": 0.6791533827781677,
      "learning_rate": 1.5249523809523813e-05,
      "loss": 0.4325,
      "step": 12470
    },
    {
      "epoch": 3.565714285714286,
      "grad_norm": 0.5224733948707581,
      "learning_rate": 1.5245714285714286e-05,
      "loss": 0.2277,
      "step": 12480
    },
    {
      "epoch": 3.5685714285714285,
      "grad_norm": 0.3601476848125458,
      "learning_rate": 1.5241904761904762e-05,
      "loss": 0.3546,
      "step": 12490
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 23.435672760009766,
      "learning_rate": 1.523809523809524e-05,
      "loss": 0.6343,
      "step": 12500
    },
    {
      "epoch": 3.5742857142857143,
      "grad_norm": 0.40696078538894653,
      "learning_rate": 1.5234285714285715e-05,
      "loss": 1.4952,
      "step": 12510
    },
    {
      "epoch": 3.5771428571428574,
      "grad_norm": 0.48337623476982117,
      "learning_rate": 1.523047619047619e-05,
      "loss": 0.2369,
      "step": 12520
    },
    {
      "epoch": 3.58,
      "grad_norm": 0.5033568143844604,
      "learning_rate": 1.5226666666666668e-05,
      "loss": 0.2378,
      "step": 12530
    },
    {
      "epoch": 3.5828571428571427,
      "grad_norm": 0.4895421862602234,
      "learning_rate": 1.5222857142857143e-05,
      "loss": 0.5758,
      "step": 12540
    },
    {
      "epoch": 3.585714285714286,
      "grad_norm": 0.5130735635757446,
      "learning_rate": 1.521904761904762e-05,
      "loss": 0.238,
      "step": 12550
    },
    {
      "epoch": 3.5885714285714285,
      "grad_norm": 0.4627378284931183,
      "learning_rate": 1.5215238095238096e-05,
      "loss": 0.589,
      "step": 12560
    },
    {
      "epoch": 3.5914285714285716,
      "grad_norm": 22.352312088012695,
      "learning_rate": 1.5211428571428572e-05,
      "loss": 0.6035,
      "step": 12570
    },
    {
      "epoch": 3.5942857142857143,
      "grad_norm": 11.017125129699707,
      "learning_rate": 1.5207619047619049e-05,
      "loss": 0.4943,
      "step": 12580
    },
    {
      "epoch": 3.597142857142857,
      "grad_norm": 0.4832579791545868,
      "learning_rate": 1.5203809523809525e-05,
      "loss": 0.7078,
      "step": 12590
    },
    {
      "epoch": 3.6,
      "grad_norm": 0.5533754825592041,
      "learning_rate": 1.5200000000000002e-05,
      "loss": 0.347,
      "step": 12600
    },
    {
      "epoch": 3.6028571428571428,
      "grad_norm": 10.94608211517334,
      "learning_rate": 1.5196190476190477e-05,
      "loss": 0.5723,
      "step": 12610
    },
    {
      "epoch": 3.605714285714286,
      "grad_norm": 0.6472938656806946,
      "learning_rate": 1.5192380952380955e-05,
      "loss": 0.9848,
      "step": 12620
    },
    {
      "epoch": 3.6085714285714285,
      "grad_norm": 0.8908577561378479,
      "learning_rate": 1.518857142857143e-05,
      "loss": 0.4247,
      "step": 12630
    },
    {
      "epoch": 3.611428571428571,
      "grad_norm": 0.6916758418083191,
      "learning_rate": 1.5184761904761906e-05,
      "loss": 0.1187,
      "step": 12640
    },
    {
      "epoch": 3.6142857142857143,
      "grad_norm": 10.838282585144043,
      "learning_rate": 1.5180952380952383e-05,
      "loss": 0.6638,
      "step": 12650
    },
    {
      "epoch": 3.617142857142857,
      "grad_norm": 0.5037218928337097,
      "learning_rate": 1.5177142857142859e-05,
      "loss": 0.2339,
      "step": 12660
    },
    {
      "epoch": 3.62,
      "grad_norm": 0.5252050757408142,
      "learning_rate": 1.5173333333333336e-05,
      "loss": 0.6812,
      "step": 12670
    },
    {
      "epoch": 3.6228571428571428,
      "grad_norm": 0.6082496047019958,
      "learning_rate": 1.5169523809523812e-05,
      "loss": 0.6667,
      "step": 12680
    },
    {
      "epoch": 3.6257142857142854,
      "grad_norm": 10.979032516479492,
      "learning_rate": 1.5165714285714289e-05,
      "loss": 0.6458,
      "step": 12690
    },
    {
      "epoch": 3.6285714285714286,
      "grad_norm": 10.874540328979492,
      "learning_rate": 1.5161904761904763e-05,
      "loss": 0.7605,
      "step": 12700
    },
    {
      "epoch": 3.6314285714285717,
      "grad_norm": 0.7921342849731445,
      "learning_rate": 1.5158095238095238e-05,
      "loss": 0.5378,
      "step": 12710
    },
    {
      "epoch": 3.6342857142857143,
      "grad_norm": 0.7394687533378601,
      "learning_rate": 1.5154285714285714e-05,
      "loss": 0.425,
      "step": 12720
    },
    {
      "epoch": 3.637142857142857,
      "grad_norm": 0.6247106790542603,
      "learning_rate": 1.5150476190476191e-05,
      "loss": 0.3267,
      "step": 12730
    },
    {
      "epoch": 3.64,
      "grad_norm": 0.5433294773101807,
      "learning_rate": 1.5146666666666667e-05,
      "loss": 0.226,
      "step": 12740
    },
    {
      "epoch": 3.642857142857143,
      "grad_norm": 10.983683586120605,
      "learning_rate": 1.5142857142857144e-05,
      "loss": 0.7905,
      "step": 12750
    },
    {
      "epoch": 3.645714285714286,
      "grad_norm": 0.6135028600692749,
      "learning_rate": 1.513904761904762e-05,
      "loss": 0.5547,
      "step": 12760
    },
    {
      "epoch": 3.6485714285714286,
      "grad_norm": 0.6460004448890686,
      "learning_rate": 1.5135238095238097e-05,
      "loss": 0.5464,
      "step": 12770
    },
    {
      "epoch": 3.6514285714285712,
      "grad_norm": 11.041337013244629,
      "learning_rate": 1.5131428571428572e-05,
      "loss": 0.7613,
      "step": 12780
    },
    {
      "epoch": 3.6542857142857144,
      "grad_norm": 10.96982479095459,
      "learning_rate": 1.5127619047619048e-05,
      "loss": 0.5422,
      "step": 12790
    },
    {
      "epoch": 3.657142857142857,
      "grad_norm": 0.6833420395851135,
      "learning_rate": 1.5123809523809525e-05,
      "loss": 0.5407,
      "step": 12800
    },
    {
      "epoch": 3.66,
      "grad_norm": 0.6006399393081665,
      "learning_rate": 1.5120000000000001e-05,
      "loss": 0.5447,
      "step": 12810
    },
    {
      "epoch": 3.662857142857143,
      "grad_norm": 11.411075592041016,
      "learning_rate": 1.5116190476190478e-05,
      "loss": 0.8666,
      "step": 12820
    },
    {
      "epoch": 3.6657142857142855,
      "grad_norm": 33.65299606323242,
      "learning_rate": 1.5112380952380954e-05,
      "loss": 0.8298,
      "step": 12830
    },
    {
      "epoch": 3.6685714285714286,
      "grad_norm": 1.2132681608200073,
      "learning_rate": 1.5108571428571431e-05,
      "loss": 0.8002,
      "step": 12840
    },
    {
      "epoch": 3.6714285714285713,
      "grad_norm": 0.9669466018676758,
      "learning_rate": 1.5104761904761907e-05,
      "loss": 0.3944,
      "step": 12850
    },
    {
      "epoch": 3.6742857142857144,
      "grad_norm": 1.0892506837844849,
      "learning_rate": 1.5100952380952382e-05,
      "loss": 0.98,
      "step": 12860
    },
    {
      "epoch": 3.677142857142857,
      "grad_norm": 10.960295677185059,
      "learning_rate": 1.509714285714286e-05,
      "loss": 0.4028,
      "step": 12870
    },
    {
      "epoch": 3.68,
      "grad_norm": 0.861976683139801,
      "learning_rate": 1.5093333333333335e-05,
      "loss": 0.4073,
      "step": 12880
    },
    {
      "epoch": 3.682857142857143,
      "grad_norm": 10.996111869812012,
      "learning_rate": 1.5089523809523812e-05,
      "loss": 0.3302,
      "step": 12890
    },
    {
      "epoch": 3.685714285714286,
      "grad_norm": 0.6298448443412781,
      "learning_rate": 1.5085714285714288e-05,
      "loss": 0.766,
      "step": 12900
    },
    {
      "epoch": 3.6885714285714286,
      "grad_norm": 10.691548347473145,
      "learning_rate": 1.5081904761904762e-05,
      "loss": 0.6438,
      "step": 12910
    },
    {
      "epoch": 3.6914285714285713,
      "grad_norm": 0.6409305930137634,
      "learning_rate": 1.5078095238095239e-05,
      "loss": 0.225,
      "step": 12920
    },
    {
      "epoch": 3.6942857142857144,
      "grad_norm": 22.087215423583984,
      "learning_rate": 1.5074285714285715e-05,
      "loss": 0.7681,
      "step": 12930
    },
    {
      "epoch": 3.697142857142857,
      "grad_norm": 22.000606536865234,
      "learning_rate": 1.507047619047619e-05,
      "loss": 0.4389,
      "step": 12940
    },
    {
      "epoch": 3.7,
      "grad_norm": 10.939580917358398,
      "learning_rate": 1.5066666666666668e-05,
      "loss": 0.7703,
      "step": 12950
    },
    {
      "epoch": 3.702857142857143,
      "grad_norm": 0.6754913926124573,
      "learning_rate": 1.5062857142857143e-05,
      "loss": 0.4435,
      "step": 12960
    },
    {
      "epoch": 3.7057142857142855,
      "grad_norm": 0.6573643684387207,
      "learning_rate": 1.505904761904762e-05,
      "loss": 0.3289,
      "step": 12970
    },
    {
      "epoch": 3.7085714285714286,
      "grad_norm": 10.752285957336426,
      "learning_rate": 1.5055238095238096e-05,
      "loss": 0.4515,
      "step": 12980
    },
    {
      "epoch": 3.7114285714285713,
      "grad_norm": 10.718219757080078,
      "learning_rate": 1.5051428571428572e-05,
      "loss": 0.5626,
      "step": 12990
    },
    {
      "epoch": 3.7142857142857144,
      "grad_norm": 10.674725532531738,
      "learning_rate": 1.5047619047619049e-05,
      "loss": 0.4443,
      "step": 13000
    },
    {
      "epoch": 3.717142857142857,
      "grad_norm": 11.463497161865234,
      "learning_rate": 1.5043809523809524e-05,
      "loss": 0.5425,
      "step": 13010
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 0.7615940570831299,
      "learning_rate": 1.5040000000000002e-05,
      "loss": 0.4312,
      "step": 13020
    },
    {
      "epoch": 3.722857142857143,
      "grad_norm": 0.7493079304695129,
      "learning_rate": 1.5036190476190477e-05,
      "loss": 0.7284,
      "step": 13030
    },
    {
      "epoch": 3.725714285714286,
      "grad_norm": 10.563291549682617,
      "learning_rate": 1.5032380952380955e-05,
      "loss": 0.6105,
      "step": 13040
    },
    {
      "epoch": 3.7285714285714286,
      "grad_norm": 1.0171291828155518,
      "learning_rate": 1.502857142857143e-05,
      "loss": 0.4932,
      "step": 13050
    },
    {
      "epoch": 3.7314285714285713,
      "grad_norm": 0.8217533826828003,
      "learning_rate": 1.5024761904761906e-05,
      "loss": 0.4196,
      "step": 13060
    },
    {
      "epoch": 3.7342857142857144,
      "grad_norm": 11.185681343078613,
      "learning_rate": 1.5020952380952383e-05,
      "loss": 0.5234,
      "step": 13070
    },
    {
      "epoch": 3.737142857142857,
      "grad_norm": 0.6423882842063904,
      "learning_rate": 1.5017142857142859e-05,
      "loss": 0.3285,
      "step": 13080
    },
    {
      "epoch": 3.74,
      "grad_norm": 10.975556373596191,
      "learning_rate": 1.5013333333333336e-05,
      "loss": 0.4523,
      "step": 13090
    },
    {
      "epoch": 3.742857142857143,
      "grad_norm": 10.916651725769043,
      "learning_rate": 1.5009523809523811e-05,
      "loss": 0.665,
      "step": 13100
    },
    {
      "epoch": 3.7457142857142856,
      "grad_norm": 10.840065002441406,
      "learning_rate": 1.5005714285714289e-05,
      "loss": 0.3449,
      "step": 13110
    },
    {
      "epoch": 3.7485714285714287,
      "grad_norm": 11.001587867736816,
      "learning_rate": 1.5001904761904764e-05,
      "loss": 0.7699,
      "step": 13120
    },
    {
      "epoch": 3.7514285714285713,
      "grad_norm": 11.475311279296875,
      "learning_rate": 1.4998095238095238e-05,
      "loss": 0.2236,
      "step": 13130
    },
    {
      "epoch": 3.7542857142857144,
      "grad_norm": 11.272961616516113,
      "learning_rate": 1.4994285714285714e-05,
      "loss": 0.3505,
      "step": 13140
    },
    {
      "epoch": 3.757142857142857,
      "grad_norm": 22.492189407348633,
      "learning_rate": 1.4990476190476191e-05,
      "loss": 0.9129,
      "step": 13150
    },
    {
      "epoch": 3.76,
      "grad_norm": 0.6380626559257507,
      "learning_rate": 1.4986666666666667e-05,
      "loss": 0.768,
      "step": 13160
    },
    {
      "epoch": 3.762857142857143,
      "grad_norm": 13.621596336364746,
      "learning_rate": 1.4982857142857144e-05,
      "loss": 0.3325,
      "step": 13170
    },
    {
      "epoch": 3.7657142857142856,
      "grad_norm": 0.4802930951118469,
      "learning_rate": 1.497904761904762e-05,
      "loss": 0.2411,
      "step": 13180
    },
    {
      "epoch": 3.7685714285714287,
      "grad_norm": 11.048781394958496,
      "learning_rate": 1.4975238095238097e-05,
      "loss": 0.903,
      "step": 13190
    },
    {
      "epoch": 3.7714285714285714,
      "grad_norm": 11.536264419555664,
      "learning_rate": 1.4971428571428572e-05,
      "loss": 0.3328,
      "step": 13200
    },
    {
      "epoch": 3.774285714285714,
      "grad_norm": 0.6867618560791016,
      "learning_rate": 1.4967619047619048e-05,
      "loss": 0.428,
      "step": 13210
    },
    {
      "epoch": 3.777142857142857,
      "grad_norm": 0.7302031517028809,
      "learning_rate": 1.4963809523809525e-05,
      "loss": 0.4378,
      "step": 13220
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 0.6241176724433899,
      "learning_rate": 1.496e-05,
      "loss": 0.4492,
      "step": 13230
    },
    {
      "epoch": 3.782857142857143,
      "grad_norm": 22.574907302856445,
      "learning_rate": 1.4956190476190478e-05,
      "loss": 0.6778,
      "step": 13240
    },
    {
      "epoch": 3.7857142857142856,
      "grad_norm": 10.854022979736328,
      "learning_rate": 1.4952380952380954e-05,
      "loss": 0.6622,
      "step": 13250
    },
    {
      "epoch": 3.7885714285714287,
      "grad_norm": 0.5652456283569336,
      "learning_rate": 1.4948571428571431e-05,
      "loss": 0.225,
      "step": 13260
    },
    {
      "epoch": 3.7914285714285714,
      "grad_norm": 22.885007858276367,
      "learning_rate": 1.4944761904761906e-05,
      "loss": 0.8705,
      "step": 13270
    },
    {
      "epoch": 3.7942857142857145,
      "grad_norm": 10.659632682800293,
      "learning_rate": 1.4940952380952382e-05,
      "loss": 0.7467,
      "step": 13280
    },
    {
      "epoch": 3.797142857142857,
      "grad_norm": 0.8493232131004333,
      "learning_rate": 1.493714285714286e-05,
      "loss": 0.8371,
      "step": 13290
    },
    {
      "epoch": 3.8,
      "grad_norm": 1.0795350074768066,
      "learning_rate": 1.4933333333333335e-05,
      "loss": 0.6047,
      "step": 13300
    },
    {
      "epoch": 3.802857142857143,
      "grad_norm": 1.1387592554092407,
      "learning_rate": 1.4929523809523812e-05,
      "loss": 0.5759,
      "step": 13310
    },
    {
      "epoch": 3.8057142857142856,
      "grad_norm": 10.392756462097168,
      "learning_rate": 1.4925714285714288e-05,
      "loss": 0.577,
      "step": 13320
    },
    {
      "epoch": 3.8085714285714287,
      "grad_norm": 10.621697425842285,
      "learning_rate": 1.4921904761904763e-05,
      "loss": 0.4024,
      "step": 13330
    },
    {
      "epoch": 3.8114285714285714,
      "grad_norm": 22.371217727661133,
      "learning_rate": 1.491809523809524e-05,
      "loss": 0.5291,
      "step": 13340
    },
    {
      "epoch": 3.814285714285714,
      "grad_norm": 0.5930989980697632,
      "learning_rate": 1.4914285714285715e-05,
      "loss": 0.2308,
      "step": 13350
    },
    {
      "epoch": 3.817142857142857,
      "grad_norm": 11.196410179138184,
      "learning_rate": 1.491047619047619e-05,
      "loss": 0.9203,
      "step": 13360
    },
    {
      "epoch": 3.82,
      "grad_norm": 10.987320899963379,
      "learning_rate": 1.4906666666666667e-05,
      "loss": 0.7894,
      "step": 13370
    },
    {
      "epoch": 3.822857142857143,
      "grad_norm": 10.730916976928711,
      "learning_rate": 1.4902857142857143e-05,
      "loss": 0.6367,
      "step": 13380
    },
    {
      "epoch": 3.8257142857142856,
      "grad_norm": 10.538203239440918,
      "learning_rate": 1.489904761904762e-05,
      "loss": 0.7171,
      "step": 13390
    },
    {
      "epoch": 3.8285714285714287,
      "grad_norm": 1.1262179613113403,
      "learning_rate": 1.4895238095238096e-05,
      "loss": 0.6895,
      "step": 13400
    },
    {
      "epoch": 3.8314285714285714,
      "grad_norm": 10.373085975646973,
      "learning_rate": 1.4891428571428571e-05,
      "loss": 0.7821,
      "step": 13410
    },
    {
      "epoch": 3.8342857142857145,
      "grad_norm": 10.50843334197998,
      "learning_rate": 1.4887619047619049e-05,
      "loss": 0.7956,
      "step": 13420
    },
    {
      "epoch": 3.837142857142857,
      "grad_norm": 10.506903648376465,
      "learning_rate": 1.4883809523809524e-05,
      "loss": 0.3102,
      "step": 13430
    },
    {
      "epoch": 3.84,
      "grad_norm": 10.656017303466797,
      "learning_rate": 1.4880000000000002e-05,
      "loss": 0.497,
      "step": 13440
    },
    {
      "epoch": 3.842857142857143,
      "grad_norm": 10.623528480529785,
      "learning_rate": 1.4876190476190477e-05,
      "loss": 0.6224,
      "step": 13450
    },
    {
      "epoch": 3.8457142857142856,
      "grad_norm": 10.426613807678223,
      "learning_rate": 1.4872380952380954e-05,
      "loss": 0.5071,
      "step": 13460
    },
    {
      "epoch": 3.8485714285714288,
      "grad_norm": 1.0816268920898438,
      "learning_rate": 1.486857142857143e-05,
      "loss": 0.6985,
      "step": 13470
    },
    {
      "epoch": 3.8514285714285714,
      "grad_norm": 1.324341893196106,
      "learning_rate": 1.4864761904761906e-05,
      "loss": 0.4782,
      "step": 13480
    },
    {
      "epoch": 3.854285714285714,
      "grad_norm": 1.1897296905517578,
      "learning_rate": 1.4860952380952383e-05,
      "loss": 0.4732,
      "step": 13490
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 10.565674781799316,
      "learning_rate": 1.4857142857142858e-05,
      "loss": 0.3944,
      "step": 13500
    },
    {
      "epoch": 3.86,
      "grad_norm": 10.521820068359375,
      "learning_rate": 1.4853333333333336e-05,
      "loss": 0.6025,
      "step": 13510
    },
    {
      "epoch": 3.862857142857143,
      "grad_norm": 1.1396408081054688,
      "learning_rate": 1.4849523809523811e-05,
      "loss": 0.6821,
      "step": 13520
    },
    {
      "epoch": 3.8657142857142857,
      "grad_norm": 1.3182873725891113,
      "learning_rate": 1.4845714285714289e-05,
      "loss": 0.751,
      "step": 13530
    },
    {
      "epoch": 3.8685714285714283,
      "grad_norm": 1.4491761922836304,
      "learning_rate": 1.4841904761904764e-05,
      "loss": 0.5537,
      "step": 13540
    },
    {
      "epoch": 3.8714285714285714,
      "grad_norm": 1.3423811197280884,
      "learning_rate": 1.483809523809524e-05,
      "loss": 0.3738,
      "step": 13550
    },
    {
      "epoch": 3.8742857142857146,
      "grad_norm": 10.451741218566895,
      "learning_rate": 1.4834285714285714e-05,
      "loss": 0.2106,
      "step": 13560
    },
    {
      "epoch": 3.8771428571428572,
      "grad_norm": 0.7607683539390564,
      "learning_rate": 1.4830476190476191e-05,
      "loss": 0.6458,
      "step": 13570
    },
    {
      "epoch": 3.88,
      "grad_norm": 10.72236442565918,
      "learning_rate": 1.4826666666666666e-05,
      "loss": 0.7491,
      "step": 13580
    },
    {
      "epoch": 3.882857142857143,
      "grad_norm": 10.660852432250977,
      "learning_rate": 1.4822857142857144e-05,
      "loss": 0.4266,
      "step": 13590
    },
    {
      "epoch": 3.8857142857142857,
      "grad_norm": 0.6032965183258057,
      "learning_rate": 1.481904761904762e-05,
      "loss": 0.1188,
      "step": 13600
    },
    {
      "epoch": 3.888571428571429,
      "grad_norm": 0.4465809762477875,
      "learning_rate": 1.4815238095238097e-05,
      "loss": 0.2407,
      "step": 13610
    },
    {
      "epoch": 3.8914285714285715,
      "grad_norm": 11.532812118530273,
      "learning_rate": 1.4811428571428572e-05,
      "loss": 0.3639,
      "step": 13620
    },
    {
      "epoch": 3.894285714285714,
      "grad_norm": 11.060961723327637,
      "learning_rate": 1.4807619047619048e-05,
      "loss": 0.3672,
      "step": 13630
    },
    {
      "epoch": 3.8971428571428572,
      "grad_norm": 11.157373428344727,
      "learning_rate": 1.4803809523809525e-05,
      "loss": 0.8388,
      "step": 13640
    },
    {
      "epoch": 3.9,
      "grad_norm": 10.968376159667969,
      "learning_rate": 1.48e-05,
      "loss": 0.8061,
      "step": 13650
    },
    {
      "epoch": 3.902857142857143,
      "grad_norm": 0.6778205633163452,
      "learning_rate": 1.4796190476190478e-05,
      "loss": 0.5604,
      "step": 13660
    },
    {
      "epoch": 3.9057142857142857,
      "grad_norm": 10.731724739074707,
      "learning_rate": 1.4792380952380953e-05,
      "loss": 0.832,
      "step": 13670
    },
    {
      "epoch": 3.9085714285714284,
      "grad_norm": 1.1117722988128662,
      "learning_rate": 1.478857142857143e-05,
      "loss": 0.5982,
      "step": 13680
    },
    {
      "epoch": 3.9114285714285715,
      "grad_norm": 1.0632479190826416,
      "learning_rate": 1.4784761904761906e-05,
      "loss": 0.6822,
      "step": 13690
    },
    {
      "epoch": 3.914285714285714,
      "grad_norm": 10.3811674118042,
      "learning_rate": 1.4780952380952382e-05,
      "loss": 0.5806,
      "step": 13700
    },
    {
      "epoch": 3.9171428571428573,
      "grad_norm": 0.893397867679596,
      "learning_rate": 1.477714285714286e-05,
      "loss": 0.3055,
      "step": 13710
    },
    {
      "epoch": 3.92,
      "grad_norm": 22.934906005859375,
      "learning_rate": 1.4773333333333335e-05,
      "loss": 0.8386,
      "step": 13720
    },
    {
      "epoch": 3.9228571428571426,
      "grad_norm": 0.8412414193153381,
      "learning_rate": 1.4769523809523812e-05,
      "loss": 0.5322,
      "step": 13730
    },
    {
      "epoch": 3.9257142857142857,
      "grad_norm": 10.650877952575684,
      "learning_rate": 1.4765714285714288e-05,
      "loss": 0.7034,
      "step": 13740
    },
    {
      "epoch": 3.928571428571429,
      "grad_norm": 0.8127536177635193,
      "learning_rate": 1.4761904761904763e-05,
      "loss": 0.4093,
      "step": 13750
    },
    {
      "epoch": 3.9314285714285715,
      "grad_norm": 10.756998062133789,
      "learning_rate": 1.475809523809524e-05,
      "loss": 0.2226,
      "step": 13760
    },
    {
      "epoch": 3.934285714285714,
      "grad_norm": 22.162797927856445,
      "learning_rate": 1.4754285714285716e-05,
      "loss": 0.7585,
      "step": 13770
    },
    {
      "epoch": 3.9371428571428573,
      "grad_norm": 0.5725289583206177,
      "learning_rate": 1.475047619047619e-05,
      "loss": 0.4463,
      "step": 13780
    },
    {
      "epoch": 3.94,
      "grad_norm": 0.6177516579627991,
      "learning_rate": 1.4746666666666667e-05,
      "loss": 0.5606,
      "step": 13790
    },
    {
      "epoch": 3.942857142857143,
      "grad_norm": 10.647998809814453,
      "learning_rate": 1.4742857142857143e-05,
      "loss": 0.548,
      "step": 13800
    },
    {
      "epoch": 3.9457142857142857,
      "grad_norm": 10.834725379943848,
      "learning_rate": 1.473904761904762e-05,
      "loss": 0.4359,
      "step": 13810
    },
    {
      "epoch": 3.9485714285714284,
      "grad_norm": 11.024361610412598,
      "learning_rate": 1.4735238095238096e-05,
      "loss": 0.4365,
      "step": 13820
    },
    {
      "epoch": 3.9514285714285715,
      "grad_norm": 0.6081298589706421,
      "learning_rate": 1.4731428571428571e-05,
      "loss": 0.5458,
      "step": 13830
    },
    {
      "epoch": 3.954285714285714,
      "grad_norm": 22.29374885559082,
      "learning_rate": 1.4727619047619049e-05,
      "loss": 0.5436,
      "step": 13840
    },
    {
      "epoch": 3.9571428571428573,
      "grad_norm": 0.6523895263671875,
      "learning_rate": 1.4723809523809524e-05,
      "loss": 0.6611,
      "step": 13850
    },
    {
      "epoch": 3.96,
      "grad_norm": 0.5891804099082947,
      "learning_rate": 1.4720000000000001e-05,
      "loss": 0.5601,
      "step": 13860
    },
    {
      "epoch": 3.9628571428571426,
      "grad_norm": 0.5784995555877686,
      "learning_rate": 1.4716190476190477e-05,
      "loss": 0.2312,
      "step": 13870
    },
    {
      "epoch": 3.9657142857142857,
      "grad_norm": 0.5505403876304626,
      "learning_rate": 1.4712380952380954e-05,
      "loss": 0.6683,
      "step": 13880
    },
    {
      "epoch": 3.9685714285714284,
      "grad_norm": 0.5989769101142883,
      "learning_rate": 1.470857142857143e-05,
      "loss": 0.6696,
      "step": 13890
    },
    {
      "epoch": 3.9714285714285715,
      "grad_norm": 10.730825424194336,
      "learning_rate": 1.4704761904761905e-05,
      "loss": 0.5502,
      "step": 13900
    },
    {
      "epoch": 3.974285714285714,
      "grad_norm": 10.69829273223877,
      "learning_rate": 1.4700952380952383e-05,
      "loss": 0.9434,
      "step": 13910
    },
    {
      "epoch": 3.977142857142857,
      "grad_norm": 11.168092727661133,
      "learning_rate": 1.4697142857142858e-05,
      "loss": 0.6212,
      "step": 13920
    },
    {
      "epoch": 3.98,
      "grad_norm": 10.477856636047363,
      "learning_rate": 1.4693333333333336e-05,
      "loss": 0.5095,
      "step": 13930
    },
    {
      "epoch": 3.982857142857143,
      "grad_norm": 10.330174446105957,
      "learning_rate": 1.4689523809523811e-05,
      "loss": 0.6,
      "step": 13940
    },
    {
      "epoch": 3.9857142857142858,
      "grad_norm": 0.976360559463501,
      "learning_rate": 1.4685714285714288e-05,
      "loss": 0.3972,
      "step": 13950
    },
    {
      "epoch": 3.9885714285714284,
      "grad_norm": 0.9090543389320374,
      "learning_rate": 1.4681904761904764e-05,
      "loss": 0.2112,
      "step": 13960
    },
    {
      "epoch": 3.9914285714285715,
      "grad_norm": 10.574490547180176,
      "learning_rate": 1.467809523809524e-05,
      "loss": 0.5208,
      "step": 13970
    },
    {
      "epoch": 3.994285714285714,
      "grad_norm": 0.6545230150222778,
      "learning_rate": 1.4674285714285717e-05,
      "loss": 0.4299,
      "step": 13980
    },
    {
      "epoch": 3.9971428571428573,
      "grad_norm": 0.5573827624320984,
      "learning_rate": 1.4670476190476192e-05,
      "loss": 0.1224,
      "step": 13990
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.5304449200630188,
      "learning_rate": 1.4666666666666666e-05,
      "loss": 0.8926,
      "step": 14000
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.8709677419354839,
      "eval_f1": 0.0,
      "eval_loss": 0.5812245607376099,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 45.5449,
      "eval_samples_per_second": 65.869,
      "eval_steps_per_second": 2.064,
      "step": 14000
    },
    {
      "epoch": 4.002857142857143,
      "grad_norm": 0.5195053815841675,
      "learning_rate": 1.4662857142857144e-05,
      "loss": 0.2334,
      "step": 14010
    },
    {
      "epoch": 4.005714285714285,
      "grad_norm": 0.466195285320282,
      "learning_rate": 1.4659047619047619e-05,
      "loss": 0.2371,
      "step": 14020
    },
    {
      "epoch": 4.008571428571429,
      "grad_norm": 0.4038654863834381,
      "learning_rate": 1.4655238095238096e-05,
      "loss": 0.3606,
      "step": 14030
    },
    {
      "epoch": 4.011428571428572,
      "grad_norm": 11.07935905456543,
      "learning_rate": 1.4651428571428572e-05,
      "loss": 0.7177,
      "step": 14040
    },
    {
      "epoch": 4.014285714285714,
      "grad_norm": 0.4927383065223694,
      "learning_rate": 1.4647619047619048e-05,
      "loss": 0.7065,
      "step": 14050
    },
    {
      "epoch": 4.017142857142857,
      "grad_norm": 0.46053561568260193,
      "learning_rate": 1.4643809523809525e-05,
      "loss": 0.1233,
      "step": 14060
    },
    {
      "epoch": 4.02,
      "grad_norm": 10.876726150512695,
      "learning_rate": 1.464e-05,
      "loss": 0.4708,
      "step": 14070
    },
    {
      "epoch": 4.022857142857143,
      "grad_norm": 10.884613037109375,
      "learning_rate": 1.4636190476190478e-05,
      "loss": 0.5824,
      "step": 14080
    },
    {
      "epoch": 4.025714285714286,
      "grad_norm": 0.5139951705932617,
      "learning_rate": 1.4632380952380953e-05,
      "loss": 0.9117,
      "step": 14090
    },
    {
      "epoch": 4.0285714285714285,
      "grad_norm": 0.527981162071228,
      "learning_rate": 1.462857142857143e-05,
      "loss": 0.3471,
      "step": 14100
    },
    {
      "epoch": 4.031428571428571,
      "grad_norm": 0.5674115419387817,
      "learning_rate": 1.4624761904761906e-05,
      "loss": 0.5658,
      "step": 14110
    },
    {
      "epoch": 4.034285714285715,
      "grad_norm": 0.6560362577438354,
      "learning_rate": 1.4620952380952382e-05,
      "loss": 0.6589,
      "step": 14120
    },
    {
      "epoch": 4.037142857142857,
      "grad_norm": 10.635937690734863,
      "learning_rate": 1.4617142857142859e-05,
      "loss": 0.4354,
      "step": 14130
    },
    {
      "epoch": 4.04,
      "grad_norm": 0.7454420328140259,
      "learning_rate": 1.4613333333333335e-05,
      "loss": 0.8465,
      "step": 14140
    },
    {
      "epoch": 4.042857142857143,
      "grad_norm": 10.509040832519531,
      "learning_rate": 1.4609523809523812e-05,
      "loss": 0.6201,
      "step": 14150
    },
    {
      "epoch": 4.045714285714285,
      "grad_norm": 0.8064309358596802,
      "learning_rate": 1.4605714285714287e-05,
      "loss": 0.2167,
      "step": 14160
    },
    {
      "epoch": 4.048571428571429,
      "grad_norm": 0.6803878545761108,
      "learning_rate": 1.4601904761904763e-05,
      "loss": 0.4317,
      "step": 14170
    },
    {
      "epoch": 4.051428571428572,
      "grad_norm": 0.6715745329856873,
      "learning_rate": 1.459809523809524e-05,
      "loss": 0.4326,
      "step": 14180
    },
    {
      "epoch": 4.054285714285714,
      "grad_norm": 10.796391487121582,
      "learning_rate": 1.4594285714285716e-05,
      "loss": 0.4417,
      "step": 14190
    },
    {
      "epoch": 4.057142857142857,
      "grad_norm": 10.76896858215332,
      "learning_rate": 1.4590476190476193e-05,
      "loss": 1.0882,
      "step": 14200
    },
    {
      "epoch": 4.06,
      "grad_norm": 11.175335884094238,
      "learning_rate": 1.4586666666666667e-05,
      "loss": 0.4386,
      "step": 14210
    },
    {
      "epoch": 4.062857142857143,
      "grad_norm": 0.7460872530937195,
      "learning_rate": 1.4582857142857143e-05,
      "loss": 0.7484,
      "step": 14220
    },
    {
      "epoch": 4.065714285714286,
      "grad_norm": 0.7181310653686523,
      "learning_rate": 1.457904761904762e-05,
      "loss": 0.4249,
      "step": 14230
    },
    {
      "epoch": 4.0685714285714285,
      "grad_norm": 0.7226932644844055,
      "learning_rate": 1.4575238095238095e-05,
      "loss": 0.4275,
      "step": 14240
    },
    {
      "epoch": 4.071428571428571,
      "grad_norm": 0.6520949006080627,
      "learning_rate": 1.4571428571428573e-05,
      "loss": 0.5455,
      "step": 14250
    },
    {
      "epoch": 4.074285714285715,
      "grad_norm": 10.608339309692383,
      "learning_rate": 1.4567619047619048e-05,
      "loss": 0.9608,
      "step": 14260
    },
    {
      "epoch": 4.077142857142857,
      "grad_norm": 0.7237224578857422,
      "learning_rate": 1.4563809523809524e-05,
      "loss": 0.3222,
      "step": 14270
    },
    {
      "epoch": 4.08,
      "grad_norm": 0.6253209710121155,
      "learning_rate": 1.4560000000000001e-05,
      "loss": 0.3324,
      "step": 14280
    },
    {
      "epoch": 4.082857142857143,
      "grad_norm": 10.78718376159668,
      "learning_rate": 1.4556190476190477e-05,
      "loss": 0.4448,
      "step": 14290
    },
    {
      "epoch": 4.085714285714285,
      "grad_norm": 0.6175733804702759,
      "learning_rate": 1.4552380952380954e-05,
      "loss": 0.8764,
      "step": 14300
    },
    {
      "epoch": 4.088571428571429,
      "grad_norm": 10.634675025939941,
      "learning_rate": 1.454857142857143e-05,
      "loss": 0.7478,
      "step": 14310
    },
    {
      "epoch": 4.091428571428572,
      "grad_norm": 10.70765209197998,
      "learning_rate": 1.4544761904761905e-05,
      "loss": 1.0505,
      "step": 14320
    },
    {
      "epoch": 4.094285714285714,
      "grad_norm": 10.559922218322754,
      "learning_rate": 1.4540952380952383e-05,
      "loss": 0.5209,
      "step": 14330
    },
    {
      "epoch": 4.097142857142857,
      "grad_norm": 0.7636244297027588,
      "learning_rate": 1.4537142857142858e-05,
      "loss": 0.2167,
      "step": 14340
    },
    {
      "epoch": 4.1,
      "grad_norm": 0.7801924347877502,
      "learning_rate": 1.4533333333333335e-05,
      "loss": 0.5257,
      "step": 14350
    },
    {
      "epoch": 4.102857142857143,
      "grad_norm": 10.689993858337402,
      "learning_rate": 1.4529523809523811e-05,
      "loss": 0.4265,
      "step": 14360
    },
    {
      "epoch": 4.105714285714286,
      "grad_norm": 0.6705824732780457,
      "learning_rate": 1.4525714285714288e-05,
      "loss": 0.322,
      "step": 14370
    },
    {
      "epoch": 4.1085714285714285,
      "grad_norm": 33.15932083129883,
      "learning_rate": 1.4521904761904764e-05,
      "loss": 0.9661,
      "step": 14380
    },
    {
      "epoch": 4.111428571428571,
      "grad_norm": 0.5477566123008728,
      "learning_rate": 1.451809523809524e-05,
      "loss": 0.5574,
      "step": 14390
    },
    {
      "epoch": 4.114285714285714,
      "grad_norm": 0.6224216818809509,
      "learning_rate": 1.4514285714285717e-05,
      "loss": 0.6636,
      "step": 14400
    },
    {
      "epoch": 4.117142857142857,
      "grad_norm": 10.71130657196045,
      "learning_rate": 1.4510476190476192e-05,
      "loss": 0.643,
      "step": 14410
    },
    {
      "epoch": 4.12,
      "grad_norm": 0.657569408416748,
      "learning_rate": 1.450666666666667e-05,
      "loss": 0.8732,
      "step": 14420
    },
    {
      "epoch": 4.122857142857143,
      "grad_norm": 0.6627997756004333,
      "learning_rate": 1.4502857142857143e-05,
      "loss": 0.5442,
      "step": 14430
    },
    {
      "epoch": 4.1257142857142854,
      "grad_norm": 0.7835062146186829,
      "learning_rate": 1.4499047619047619e-05,
      "loss": 0.7416,
      "step": 14440
    },
    {
      "epoch": 4.128571428571428,
      "grad_norm": 21.950464248657227,
      "learning_rate": 1.4495238095238096e-05,
      "loss": 0.627,
      "step": 14450
    },
    {
      "epoch": 4.131428571428572,
      "grad_norm": 10.613322257995605,
      "learning_rate": 1.4491428571428572e-05,
      "loss": 0.6331,
      "step": 14460
    },
    {
      "epoch": 4.134285714285714,
      "grad_norm": 10.68431282043457,
      "learning_rate": 1.4487619047619047e-05,
      "loss": 0.54,
      "step": 14470
    },
    {
      "epoch": 4.137142857142857,
      "grad_norm": 0.6636756062507629,
      "learning_rate": 1.4483809523809525e-05,
      "loss": 0.2265,
      "step": 14480
    },
    {
      "epoch": 4.14,
      "grad_norm": 10.811493873596191,
      "learning_rate": 1.448e-05,
      "loss": 0.2303,
      "step": 14490
    },
    {
      "epoch": 4.142857142857143,
      "grad_norm": 10.796748161315918,
      "learning_rate": 1.4476190476190478e-05,
      "loss": 0.5579,
      "step": 14500
    },
    {
      "epoch": 4.145714285714286,
      "grad_norm": 10.711244583129883,
      "learning_rate": 1.4472380952380953e-05,
      "loss": 0.9843,
      "step": 14510
    },
    {
      "epoch": 4.148571428571429,
      "grad_norm": 0.7325460314750671,
      "learning_rate": 1.446857142857143e-05,
      "loss": 0.3268,
      "step": 14520
    },
    {
      "epoch": 4.151428571428571,
      "grad_norm": 0.7181301116943359,
      "learning_rate": 1.4464761904761906e-05,
      "loss": 0.5304,
      "step": 14530
    },
    {
      "epoch": 4.154285714285714,
      "grad_norm": 10.608759880065918,
      "learning_rate": 1.4460952380952382e-05,
      "loss": 1.0449,
      "step": 14540
    },
    {
      "epoch": 4.1571428571428575,
      "grad_norm": 10.426737785339355,
      "learning_rate": 1.4457142857142859e-05,
      "loss": 0.6159,
      "step": 14550
    },
    {
      "epoch": 4.16,
      "grad_norm": 1.0584094524383545,
      "learning_rate": 1.4453333333333334e-05,
      "loss": 0.5933,
      "step": 14560
    },
    {
      "epoch": 4.162857142857143,
      "grad_norm": 10.326262474060059,
      "learning_rate": 1.4449523809523812e-05,
      "loss": 0.587,
      "step": 14570
    },
    {
      "epoch": 4.1657142857142855,
      "grad_norm": 10.358383178710938,
      "learning_rate": 1.4445714285714287e-05,
      "loss": 0.4844,
      "step": 14580
    },
    {
      "epoch": 4.168571428571428,
      "grad_norm": 0.922313928604126,
      "learning_rate": 1.4441904761904763e-05,
      "loss": 0.4968,
      "step": 14590
    },
    {
      "epoch": 4.171428571428572,
      "grad_norm": 10.901025772094727,
      "learning_rate": 1.443809523809524e-05,
      "loss": 0.8817,
      "step": 14600
    },
    {
      "epoch": 4.174285714285714,
      "grad_norm": 1.0878729820251465,
      "learning_rate": 1.4434285714285716e-05,
      "loss": 0.4888,
      "step": 14610
    },
    {
      "epoch": 4.177142857142857,
      "grad_norm": 10.481548309326172,
      "learning_rate": 1.4430476190476193e-05,
      "loss": 0.6788,
      "step": 14620
    },
    {
      "epoch": 4.18,
      "grad_norm": 21.807668685913086,
      "learning_rate": 1.4426666666666669e-05,
      "loss": 0.3168,
      "step": 14630
    },
    {
      "epoch": 4.182857142857143,
      "grad_norm": 0.7920969724655151,
      "learning_rate": 1.4422857142857146e-05,
      "loss": 0.2178,
      "step": 14640
    },
    {
      "epoch": 4.185714285714286,
      "grad_norm": 0.7721964716911316,
      "learning_rate": 1.441904761904762e-05,
      "loss": 0.6234,
      "step": 14650
    },
    {
      "epoch": 4.188571428571429,
      "grad_norm": 10.596463203430176,
      "learning_rate": 1.4415238095238095e-05,
      "loss": 0.8328,
      "step": 14660
    },
    {
      "epoch": 4.191428571428571,
      "grad_norm": 10.592618942260742,
      "learning_rate": 1.4411428571428573e-05,
      "loss": 0.9219,
      "step": 14670
    },
    {
      "epoch": 4.194285714285714,
      "grad_norm": 10.525368690490723,
      "learning_rate": 1.4407619047619048e-05,
      "loss": 0.4129,
      "step": 14680
    },
    {
      "epoch": 4.1971428571428575,
      "grad_norm": 0.8342152833938599,
      "learning_rate": 1.4403809523809524e-05,
      "loss": 0.5136,
      "step": 14690
    },
    {
      "epoch": 4.2,
      "grad_norm": 21.694379806518555,
      "learning_rate": 1.4400000000000001e-05,
      "loss": 0.9144,
      "step": 14700
    },
    {
      "epoch": 4.202857142857143,
      "grad_norm": 10.69719409942627,
      "learning_rate": 1.4396190476190477e-05,
      "loss": 0.5948,
      "step": 14710
    },
    {
      "epoch": 4.2057142857142855,
      "grad_norm": 10.404973983764648,
      "learning_rate": 1.4392380952380954e-05,
      "loss": 0.7736,
      "step": 14720
    },
    {
      "epoch": 4.208571428571428,
      "grad_norm": 0.8928778767585754,
      "learning_rate": 1.438857142857143e-05,
      "loss": 0.4033,
      "step": 14730
    },
    {
      "epoch": 4.211428571428572,
      "grad_norm": 10.510732650756836,
      "learning_rate": 1.4384761904761905e-05,
      "loss": 0.8807,
      "step": 14740
    },
    {
      "epoch": 4.214285714285714,
      "grad_norm": 10.284942626953125,
      "learning_rate": 1.4380952380952382e-05,
      "loss": 0.5844,
      "step": 14750
    },
    {
      "epoch": 4.217142857142857,
      "grad_norm": 21.570863723754883,
      "learning_rate": 1.4377142857142858e-05,
      "loss": 0.6797,
      "step": 14760
    },
    {
      "epoch": 4.22,
      "grad_norm": 10.591543197631836,
      "learning_rate": 1.4373333333333335e-05,
      "loss": 0.7587,
      "step": 14770
    },
    {
      "epoch": 4.222857142857142,
      "grad_norm": 1.1617686748504639,
      "learning_rate": 1.436952380952381e-05,
      "loss": 0.2945,
      "step": 14780
    },
    {
      "epoch": 4.225714285714286,
      "grad_norm": 0.9738771319389343,
      "learning_rate": 1.4365714285714288e-05,
      "loss": 0.4906,
      "step": 14790
    },
    {
      "epoch": 4.228571428571429,
      "grad_norm": 0.7538649439811707,
      "learning_rate": 1.4361904761904764e-05,
      "loss": 0.41,
      "step": 14800
    },
    {
      "epoch": 4.231428571428571,
      "grad_norm": 0.5759157538414001,
      "learning_rate": 1.435809523809524e-05,
      "loss": 0.4379,
      "step": 14810
    },
    {
      "epoch": 4.234285714285714,
      "grad_norm": 0.5923952460289001,
      "learning_rate": 1.4354285714285716e-05,
      "loss": 0.447,
      "step": 14820
    },
    {
      "epoch": 4.2371428571428575,
      "grad_norm": 0.540846049785614,
      "learning_rate": 1.4350476190476192e-05,
      "loss": 0.2312,
      "step": 14830
    },
    {
      "epoch": 4.24,
      "grad_norm": 22.375701904296875,
      "learning_rate": 1.434666666666667e-05,
      "loss": 0.4592,
      "step": 14840
    },
    {
      "epoch": 4.242857142857143,
      "grad_norm": 10.902710914611816,
      "learning_rate": 1.4342857142857145e-05,
      "loss": 0.5848,
      "step": 14850
    },
    {
      "epoch": 4.2457142857142856,
      "grad_norm": 11.05742359161377,
      "learning_rate": 1.433904761904762e-05,
      "loss": 0.922,
      "step": 14860
    },
    {
      "epoch": 4.248571428571428,
      "grad_norm": 0.6468137502670288,
      "learning_rate": 1.4335238095238096e-05,
      "loss": 0.4537,
      "step": 14870
    },
    {
      "epoch": 4.251428571428572,
      "grad_norm": 10.579933166503906,
      "learning_rate": 1.4331428571428572e-05,
      "loss": 0.8503,
      "step": 14880
    },
    {
      "epoch": 4.2542857142857144,
      "grad_norm": 0.7332554459571838,
      "learning_rate": 1.4327619047619047e-05,
      "loss": 0.4275,
      "step": 14890
    },
    {
      "epoch": 4.257142857142857,
      "grad_norm": 0.6292675733566284,
      "learning_rate": 1.4323809523809525e-05,
      "loss": 0.7603,
      "step": 14900
    },
    {
      "epoch": 4.26,
      "grad_norm": 10.931235313415527,
      "learning_rate": 1.432e-05,
      "loss": 0.6479,
      "step": 14910
    },
    {
      "epoch": 4.2628571428571425,
      "grad_norm": 0.6415388584136963,
      "learning_rate": 1.4316190476190477e-05,
      "loss": 0.6599,
      "step": 14920
    },
    {
      "epoch": 4.265714285714286,
      "grad_norm": 0.7380545139312744,
      "learning_rate": 1.4312380952380953e-05,
      "loss": 0.3215,
      "step": 14930
    },
    {
      "epoch": 4.268571428571429,
      "grad_norm": 11.000833511352539,
      "learning_rate": 1.430857142857143e-05,
      "loss": 0.5409,
      "step": 14940
    },
    {
      "epoch": 4.271428571428571,
      "grad_norm": 0.5604037046432495,
      "learning_rate": 1.4304761904761906e-05,
      "loss": 0.3492,
      "step": 14950
    },
    {
      "epoch": 4.274285714285714,
      "grad_norm": 0.5106163620948792,
      "learning_rate": 1.4300952380952381e-05,
      "loss": 0.5777,
      "step": 14960
    },
    {
      "epoch": 4.277142857142858,
      "grad_norm": 0.5155255198478699,
      "learning_rate": 1.4297142857142859e-05,
      "loss": 0.6915,
      "step": 14970
    },
    {
      "epoch": 4.28,
      "grad_norm": 11.122947692871094,
      "learning_rate": 1.4293333333333334e-05,
      "loss": 0.7723,
      "step": 14980
    },
    {
      "epoch": 4.282857142857143,
      "grad_norm": 10.828603744506836,
      "learning_rate": 1.4289523809523812e-05,
      "loss": 0.7114,
      "step": 14990
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 0.6373900771141052,
      "learning_rate": 1.4285714285714287e-05,
      "loss": 0.2114,
      "step": 15000
    },
    {
      "epoch": 4.288571428571428,
      "grad_norm": 0.5032891631126404,
      "learning_rate": 1.4281904761904763e-05,
      "loss": 0.5708,
      "step": 15010
    },
    {
      "epoch": 4.291428571428572,
      "grad_norm": 11.136882781982422,
      "learning_rate": 1.427809523809524e-05,
      "loss": 0.3526,
      "step": 15020
    },
    {
      "epoch": 4.2942857142857145,
      "grad_norm": 0.47405385971069336,
      "learning_rate": 1.4274285714285716e-05,
      "loss": 0.4713,
      "step": 15030
    },
    {
      "epoch": 4.297142857142857,
      "grad_norm": 0.4338405430316925,
      "learning_rate": 1.4270476190476193e-05,
      "loss": 0.24,
      "step": 15040
    },
    {
      "epoch": 4.3,
      "grad_norm": 11.093656539916992,
      "learning_rate": 1.4266666666666668e-05,
      "loss": 0.6091,
      "step": 15050
    },
    {
      "epoch": 4.3028571428571425,
      "grad_norm": 11.155777931213379,
      "learning_rate": 1.4262857142857146e-05,
      "loss": 1.0603,
      "step": 15060
    },
    {
      "epoch": 4.305714285714286,
      "grad_norm": 0.5091701149940491,
      "learning_rate": 1.4259047619047621e-05,
      "loss": 0.6921,
      "step": 15070
    },
    {
      "epoch": 4.308571428571429,
      "grad_norm": 10.907614707946777,
      "learning_rate": 1.4255238095238095e-05,
      "loss": 0.5725,
      "step": 15080
    },
    {
      "epoch": 4.311428571428571,
      "grad_norm": 0.5172649025917053,
      "learning_rate": 1.4251428571428572e-05,
      "loss": 0.4556,
      "step": 15090
    },
    {
      "epoch": 4.314285714285714,
      "grad_norm": 10.800399780273438,
      "learning_rate": 1.4247619047619048e-05,
      "loss": 0.785,
      "step": 15100
    },
    {
      "epoch": 4.317142857142857,
      "grad_norm": 0.6730027794837952,
      "learning_rate": 1.4243809523809524e-05,
      "loss": 0.5448,
      "step": 15110
    },
    {
      "epoch": 4.32,
      "grad_norm": 0.7300863265991211,
      "learning_rate": 1.4240000000000001e-05,
      "loss": 0.3273,
      "step": 15120
    },
    {
      "epoch": 4.322857142857143,
      "grad_norm": 0.7082145810127258,
      "learning_rate": 1.4236190476190476e-05,
      "loss": 0.4285,
      "step": 15130
    },
    {
      "epoch": 4.325714285714286,
      "grad_norm": 11.163565635681152,
      "learning_rate": 1.4232380952380954e-05,
      "loss": 0.652,
      "step": 15140
    },
    {
      "epoch": 4.328571428571428,
      "grad_norm": 10.710925102233887,
      "learning_rate": 1.422857142857143e-05,
      "loss": 0.425,
      "step": 15150
    },
    {
      "epoch": 4.331428571428571,
      "grad_norm": 10.678229331970215,
      "learning_rate": 1.4224761904761905e-05,
      "loss": 0.5289,
      "step": 15160
    },
    {
      "epoch": 4.3342857142857145,
      "grad_norm": 0.790492057800293,
      "learning_rate": 1.4220952380952382e-05,
      "loss": 0.7316,
      "step": 15170
    },
    {
      "epoch": 4.337142857142857,
      "grad_norm": 0.793002188205719,
      "learning_rate": 1.4217142857142858e-05,
      "loss": 0.7205,
      "step": 15180
    },
    {
      "epoch": 4.34,
      "grad_norm": 0.9444660544395447,
      "learning_rate": 1.4213333333333335e-05,
      "loss": 0.5133,
      "step": 15190
    },
    {
      "epoch": 4.3428571428571425,
      "grad_norm": 0.8339519500732422,
      "learning_rate": 1.420952380952381e-05,
      "loss": 0.4181,
      "step": 15200
    },
    {
      "epoch": 4.345714285714286,
      "grad_norm": 0.9506412148475647,
      "learning_rate": 1.4205714285714288e-05,
      "loss": 0.7215,
      "step": 15210
    },
    {
      "epoch": 4.348571428571429,
      "grad_norm": 10.569856643676758,
      "learning_rate": 1.4201904761904763e-05,
      "loss": 0.3208,
      "step": 15220
    },
    {
      "epoch": 4.351428571428571,
      "grad_norm": 10.80842399597168,
      "learning_rate": 1.4198095238095239e-05,
      "loss": 0.6396,
      "step": 15230
    },
    {
      "epoch": 4.354285714285714,
      "grad_norm": 10.962529182434082,
      "learning_rate": 1.4194285714285716e-05,
      "loss": 0.4363,
      "step": 15240
    },
    {
      "epoch": 4.357142857142857,
      "grad_norm": 10.842998504638672,
      "learning_rate": 1.4190476190476192e-05,
      "loss": 0.7626,
      "step": 15250
    },
    {
      "epoch": 4.36,
      "grad_norm": 22.049846649169922,
      "learning_rate": 1.418666666666667e-05,
      "loss": 0.5409,
      "step": 15260
    },
    {
      "epoch": 4.362857142857143,
      "grad_norm": 10.757477760314941,
      "learning_rate": 1.4182857142857145e-05,
      "loss": 0.4399,
      "step": 15270
    },
    {
      "epoch": 4.365714285714286,
      "grad_norm": 0.6224965453147888,
      "learning_rate": 1.417904761904762e-05,
      "loss": 0.2262,
      "step": 15280
    },
    {
      "epoch": 4.368571428571428,
      "grad_norm": 10.695953369140625,
      "learning_rate": 1.4175238095238098e-05,
      "loss": 0.9955,
      "step": 15290
    },
    {
      "epoch": 4.371428571428572,
      "grad_norm": 0.7392048835754395,
      "learning_rate": 1.4171428571428572e-05,
      "loss": 0.7502,
      "step": 15300
    },
    {
      "epoch": 4.3742857142857146,
      "grad_norm": 10.82021713256836,
      "learning_rate": 1.4167619047619047e-05,
      "loss": 0.7269,
      "step": 15310
    },
    {
      "epoch": 4.377142857142857,
      "grad_norm": 10.464545249938965,
      "learning_rate": 1.4163809523809524e-05,
      "loss": 0.5145,
      "step": 15320
    },
    {
      "epoch": 4.38,
      "grad_norm": 10.453004837036133,
      "learning_rate": 1.416e-05,
      "loss": 0.4161,
      "step": 15330
    },
    {
      "epoch": 4.382857142857143,
      "grad_norm": 0.8370674252510071,
      "learning_rate": 1.4156190476190477e-05,
      "loss": 0.8076,
      "step": 15340
    },
    {
      "epoch": 4.385714285714286,
      "grad_norm": 10.545466423034668,
      "learning_rate": 1.4152380952380953e-05,
      "loss": 0.5177,
      "step": 15350
    },
    {
      "epoch": 4.388571428571429,
      "grad_norm": 0.8244901895523071,
      "learning_rate": 1.414857142857143e-05,
      "loss": 0.3174,
      "step": 15360
    },
    {
      "epoch": 4.3914285714285715,
      "grad_norm": 0.7449519038200378,
      "learning_rate": 1.4144761904761906e-05,
      "loss": 0.4185,
      "step": 15370
    },
    {
      "epoch": 4.394285714285714,
      "grad_norm": 0.5784619450569153,
      "learning_rate": 1.4140952380952381e-05,
      "loss": 0.2291,
      "step": 15380
    },
    {
      "epoch": 4.397142857142857,
      "grad_norm": 11.06417465209961,
      "learning_rate": 1.4137142857142859e-05,
      "loss": 0.8866,
      "step": 15390
    },
    {
      "epoch": 4.4,
      "grad_norm": 0.5351006984710693,
      "learning_rate": 1.4133333333333334e-05,
      "loss": 0.2299,
      "step": 15400
    },
    {
      "epoch": 4.402857142857143,
      "grad_norm": 0.514633059501648,
      "learning_rate": 1.4129523809523811e-05,
      "loss": 0.4588,
      "step": 15410
    },
    {
      "epoch": 4.405714285714286,
      "grad_norm": 21.992673873901367,
      "learning_rate": 1.4125714285714287e-05,
      "loss": 0.9073,
      "step": 15420
    },
    {
      "epoch": 4.408571428571428,
      "grad_norm": 0.6190235018730164,
      "learning_rate": 1.4121904761904763e-05,
      "loss": 0.6794,
      "step": 15430
    },
    {
      "epoch": 4.411428571428571,
      "grad_norm": 0.5641958117485046,
      "learning_rate": 1.411809523809524e-05,
      "loss": 0.3376,
      "step": 15440
    },
    {
      "epoch": 4.414285714285715,
      "grad_norm": 10.883035659790039,
      "learning_rate": 1.4114285714285715e-05,
      "loss": 0.4557,
      "step": 15450
    },
    {
      "epoch": 4.417142857142857,
      "grad_norm": 22.000917434692383,
      "learning_rate": 1.4110476190476193e-05,
      "loss": 0.5684,
      "step": 15460
    },
    {
      "epoch": 4.42,
      "grad_norm": 0.514022707939148,
      "learning_rate": 1.4106666666666668e-05,
      "loss": 0.2348,
      "step": 15470
    },
    {
      "epoch": 4.422857142857143,
      "grad_norm": 0.463127464056015,
      "learning_rate": 1.4102857142857146e-05,
      "loss": 0.5765,
      "step": 15480
    },
    {
      "epoch": 4.425714285714285,
      "grad_norm": 0.5196963548660278,
      "learning_rate": 1.4099047619047621e-05,
      "loss": 0.8055,
      "step": 15490
    },
    {
      "epoch": 4.428571428571429,
      "grad_norm": 0.5224733352661133,
      "learning_rate": 1.4095238095238097e-05,
      "loss": 0.3442,
      "step": 15500
    },
    {
      "epoch": 4.4314285714285715,
      "grad_norm": 10.785834312438965,
      "learning_rate": 1.4091428571428574e-05,
      "loss": 0.5713,
      "step": 15510
    },
    {
      "epoch": 4.434285714285714,
      "grad_norm": 10.908365249633789,
      "learning_rate": 1.4087619047619048e-05,
      "loss": 0.9136,
      "step": 15520
    },
    {
      "epoch": 4.437142857142857,
      "grad_norm": 33.33991241455078,
      "learning_rate": 1.4083809523809523e-05,
      "loss": 0.6759,
      "step": 15530
    },
    {
      "epoch": 4.44,
      "grad_norm": 0.6196833252906799,
      "learning_rate": 1.408e-05,
      "loss": 0.6649,
      "step": 15540
    },
    {
      "epoch": 4.442857142857143,
      "grad_norm": 10.648063659667969,
      "learning_rate": 1.4076190476190476e-05,
      "loss": 0.5379,
      "step": 15550
    },
    {
      "epoch": 4.445714285714286,
      "grad_norm": 0.7496627569198608,
      "learning_rate": 1.4072380952380954e-05,
      "loss": 0.7308,
      "step": 15560
    },
    {
      "epoch": 4.448571428571428,
      "grad_norm": 0.7893981337547302,
      "learning_rate": 1.406857142857143e-05,
      "loss": 0.526,
      "step": 15570
    },
    {
      "epoch": 4.451428571428571,
      "grad_norm": 0.7863325476646423,
      "learning_rate": 1.4064761904761905e-05,
      "loss": 0.719,
      "step": 15580
    },
    {
      "epoch": 4.454285714285715,
      "grad_norm": 10.855432510375977,
      "learning_rate": 1.4060952380952382e-05,
      "loss": 0.8184,
      "step": 15590
    },
    {
      "epoch": 4.457142857142857,
      "grad_norm": 10.577646255493164,
      "learning_rate": 1.4057142857142858e-05,
      "loss": 0.3117,
      "step": 15600
    },
    {
      "epoch": 4.46,
      "grad_norm": 0.7681456208229065,
      "learning_rate": 1.4053333333333335e-05,
      "loss": 0.416,
      "step": 15610
    },
    {
      "epoch": 4.462857142857143,
      "grad_norm": 0.5967156291007996,
      "learning_rate": 1.404952380952381e-05,
      "loss": 0.2196,
      "step": 15620
    },
    {
      "epoch": 4.465714285714285,
      "grad_norm": 0.5424668788909912,
      "learning_rate": 1.4045714285714288e-05,
      "loss": 0.8988,
      "step": 15630
    },
    {
      "epoch": 4.468571428571429,
      "grad_norm": 0.5328525304794312,
      "learning_rate": 1.4041904761904763e-05,
      "loss": 0.6673,
      "step": 15640
    },
    {
      "epoch": 4.4714285714285715,
      "grad_norm": 11.10252857208252,
      "learning_rate": 1.4038095238095239e-05,
      "loss": 0.4612,
      "step": 15650
    },
    {
      "epoch": 4.474285714285714,
      "grad_norm": 11.287858009338379,
      "learning_rate": 1.4034285714285716e-05,
      "loss": 0.4595,
      "step": 15660
    },
    {
      "epoch": 4.477142857142857,
      "grad_norm": 21.98008918762207,
      "learning_rate": 1.4030476190476192e-05,
      "loss": 0.7879,
      "step": 15670
    },
    {
      "epoch": 4.48,
      "grad_norm": 0.5887477397918701,
      "learning_rate": 1.4026666666666669e-05,
      "loss": 0.6685,
      "step": 15680
    },
    {
      "epoch": 4.482857142857143,
      "grad_norm": 0.6628847718238831,
      "learning_rate": 1.4022857142857145e-05,
      "loss": 0.6597,
      "step": 15690
    },
    {
      "epoch": 4.485714285714286,
      "grad_norm": 10.931585311889648,
      "learning_rate": 1.401904761904762e-05,
      "loss": 0.6369,
      "step": 15700
    },
    {
      "epoch": 4.488571428571428,
      "grad_norm": 0.8776813745498657,
      "learning_rate": 1.4015238095238097e-05,
      "loss": 0.7216,
      "step": 15710
    },
    {
      "epoch": 4.491428571428571,
      "grad_norm": 10.479755401611328,
      "learning_rate": 1.4011428571428573e-05,
      "loss": 0.5978,
      "step": 15720
    },
    {
      "epoch": 4.494285714285715,
      "grad_norm": 21.87398910522461,
      "learning_rate": 1.4007619047619047e-05,
      "loss": 0.504,
      "step": 15730
    },
    {
      "epoch": 4.497142857142857,
      "grad_norm": 10.535696029663086,
      "learning_rate": 1.4003809523809524e-05,
      "loss": 0.5177,
      "step": 15740
    },
    {
      "epoch": 4.5,
      "grad_norm": 10.606952667236328,
      "learning_rate": 1.4e-05,
      "loss": 0.6264,
      "step": 15750
    },
    {
      "epoch": 4.502857142857143,
      "grad_norm": 0.7343526482582092,
      "learning_rate": 1.3996190476190477e-05,
      "loss": 0.526,
      "step": 15760
    },
    {
      "epoch": 4.505714285714285,
      "grad_norm": 0.7372512221336365,
      "learning_rate": 1.3992380952380953e-05,
      "loss": 0.631,
      "step": 15770
    },
    {
      "epoch": 4.508571428571429,
      "grad_norm": 21.85366439819336,
      "learning_rate": 1.398857142857143e-05,
      "loss": 0.6345,
      "step": 15780
    },
    {
      "epoch": 4.511428571428572,
      "grad_norm": 0.705620288848877,
      "learning_rate": 1.3984761904761906e-05,
      "loss": 0.4303,
      "step": 15790
    },
    {
      "epoch": 4.514285714285714,
      "grad_norm": 10.772790908813477,
      "learning_rate": 1.3980952380952381e-05,
      "loss": 0.2271,
      "step": 15800
    },
    {
      "epoch": 4.517142857142857,
      "grad_norm": 0.5448744297027588,
      "learning_rate": 1.3977142857142858e-05,
      "loss": 0.5674,
      "step": 15810
    },
    {
      "epoch": 4.52,
      "grad_norm": 0.5073018074035645,
      "learning_rate": 1.3973333333333334e-05,
      "loss": 0.1237,
      "step": 15820
    },
    {
      "epoch": 4.522857142857143,
      "grad_norm": 0.4682290554046631,
      "learning_rate": 1.3969523809523811e-05,
      "loss": 0.3461,
      "step": 15830
    },
    {
      "epoch": 4.525714285714286,
      "grad_norm": 0.46620652079582214,
      "learning_rate": 1.3965714285714287e-05,
      "loss": 0.4619,
      "step": 15840
    },
    {
      "epoch": 4.5285714285714285,
      "grad_norm": 0.45861372351646423,
      "learning_rate": 1.3961904761904762e-05,
      "loss": 0.7004,
      "step": 15850
    },
    {
      "epoch": 4.531428571428571,
      "grad_norm": 0.48917901515960693,
      "learning_rate": 1.395809523809524e-05,
      "loss": 0.696,
      "step": 15860
    },
    {
      "epoch": 4.534285714285714,
      "grad_norm": 0.5175766944885254,
      "learning_rate": 1.3954285714285715e-05,
      "loss": 1.0156,
      "step": 15870
    },
    {
      "epoch": 4.537142857142857,
      "grad_norm": 0.6110150814056396,
      "learning_rate": 1.3950476190476193e-05,
      "loss": 0.4504,
      "step": 15880
    },
    {
      "epoch": 4.54,
      "grad_norm": 0.5526010990142822,
      "learning_rate": 1.3946666666666668e-05,
      "loss": 0.232,
      "step": 15890
    },
    {
      "epoch": 4.542857142857143,
      "grad_norm": 10.850913047790527,
      "learning_rate": 1.3942857142857145e-05,
      "loss": 0.7915,
      "step": 15900
    },
    {
      "epoch": 4.545714285714285,
      "grad_norm": 10.858332633972168,
      "learning_rate": 1.3939047619047621e-05,
      "loss": 0.2349,
      "step": 15910
    },
    {
      "epoch": 4.548571428571429,
      "grad_norm": 0.477518767118454,
      "learning_rate": 1.3935238095238097e-05,
      "loss": 0.3482,
      "step": 15920
    },
    {
      "epoch": 4.551428571428572,
      "grad_norm": 11.39249324798584,
      "learning_rate": 1.3931428571428574e-05,
      "loss": 0.8094,
      "step": 15930
    },
    {
      "epoch": 4.554285714285714,
      "grad_norm": 0.48820942640304565,
      "learning_rate": 1.392761904761905e-05,
      "loss": 0.3464,
      "step": 15940
    },
    {
      "epoch": 4.557142857142857,
      "grad_norm": 0.45164909958839417,
      "learning_rate": 1.3923809523809523e-05,
      "loss": 0.4697,
      "step": 15950
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 10.96773624420166,
      "learning_rate": 1.392e-05,
      "loss": 0.9127,
      "step": 15960
    },
    {
      "epoch": 4.562857142857143,
      "grad_norm": 0.678174614906311,
      "learning_rate": 1.3916190476190476e-05,
      "loss": 0.7646,
      "step": 15970
    },
    {
      "epoch": 4.565714285714286,
      "grad_norm": 10.741466522216797,
      "learning_rate": 1.3912380952380953e-05,
      "loss": 0.2234,
      "step": 15980
    },
    {
      "epoch": 4.5685714285714285,
      "grad_norm": 0.7391200661659241,
      "learning_rate": 1.3908571428571429e-05,
      "loss": 0.7449,
      "step": 15990
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 10.631852149963379,
      "learning_rate": 1.3904761904761905e-05,
      "loss": 0.3239,
      "step": 16000
    },
    {
      "epoch": 4.574285714285715,
      "grad_norm": 10.57935905456543,
      "learning_rate": 1.3900952380952382e-05,
      "loss": 0.6336,
      "step": 16010
    },
    {
      "epoch": 4.577142857142857,
      "grad_norm": 10.742134094238281,
      "learning_rate": 1.3897142857142857e-05,
      "loss": 0.5351,
      "step": 16020
    },
    {
      "epoch": 4.58,
      "grad_norm": 10.714008331298828,
      "learning_rate": 1.3893333333333335e-05,
      "loss": 0.4235,
      "step": 16030
    },
    {
      "epoch": 4.582857142857143,
      "grad_norm": 10.695639610290527,
      "learning_rate": 1.388952380952381e-05,
      "loss": 0.7212,
      "step": 16040
    },
    {
      "epoch": 4.585714285714285,
      "grad_norm": 0.8554865121841431,
      "learning_rate": 1.3885714285714288e-05,
      "loss": 0.218,
      "step": 16050
    },
    {
      "epoch": 4.588571428571429,
      "grad_norm": 0.7393004894256592,
      "learning_rate": 1.3881904761904763e-05,
      "loss": 0.8204,
      "step": 16060
    },
    {
      "epoch": 4.591428571428572,
      "grad_norm": 0.8920363783836365,
      "learning_rate": 1.3878095238095239e-05,
      "loss": 1.0223,
      "step": 16070
    },
    {
      "epoch": 4.594285714285714,
      "grad_norm": 22.301666259765625,
      "learning_rate": 1.3874285714285716e-05,
      "loss": 0.509,
      "step": 16080
    },
    {
      "epoch": 4.597142857142857,
      "grad_norm": 10.607051849365234,
      "learning_rate": 1.3870476190476192e-05,
      "loss": 0.3157,
      "step": 16090
    },
    {
      "epoch": 4.6,
      "grad_norm": 0.8272075057029724,
      "learning_rate": 1.3866666666666669e-05,
      "loss": 0.4106,
      "step": 16100
    },
    {
      "epoch": 4.602857142857143,
      "grad_norm": 10.547725677490234,
      "learning_rate": 1.3862857142857144e-05,
      "loss": 0.9272,
      "step": 16110
    },
    {
      "epoch": 4.605714285714286,
      "grad_norm": 10.447002410888672,
      "learning_rate": 1.385904761904762e-05,
      "loss": 0.5128,
      "step": 16120
    },
    {
      "epoch": 4.6085714285714285,
      "grad_norm": 10.344062805175781,
      "learning_rate": 1.3855238095238097e-05,
      "loss": 0.6979,
      "step": 16130
    },
    {
      "epoch": 4.611428571428571,
      "grad_norm": 0.8030240535736084,
      "learning_rate": 1.3851428571428573e-05,
      "loss": 0.6216,
      "step": 16140
    },
    {
      "epoch": 4.614285714285714,
      "grad_norm": 10.567564010620117,
      "learning_rate": 1.384761904761905e-05,
      "loss": 0.7262,
      "step": 16150
    },
    {
      "epoch": 4.617142857142857,
      "grad_norm": 10.436363220214844,
      "learning_rate": 1.3843809523809526e-05,
      "loss": 0.9075,
      "step": 16160
    },
    {
      "epoch": 4.62,
      "grad_norm": 0.9957191944122314,
      "learning_rate": 1.384e-05,
      "loss": 0.3053,
      "step": 16170
    },
    {
      "epoch": 4.622857142857143,
      "grad_norm": 0.8138203024864197,
      "learning_rate": 1.3836190476190477e-05,
      "loss": 0.118,
      "step": 16180
    },
    {
      "epoch": 4.6257142857142854,
      "grad_norm": 21.920259475708008,
      "learning_rate": 1.3832380952380952e-05,
      "loss": 0.6365,
      "step": 16190
    },
    {
      "epoch": 4.628571428571428,
      "grad_norm": 21.928421020507812,
      "learning_rate": 1.382857142857143e-05,
      "loss": 1.0435,
      "step": 16200
    },
    {
      "epoch": 4.631428571428572,
      "grad_norm": 21.771289825439453,
      "learning_rate": 1.3824761904761905e-05,
      "loss": 0.4149,
      "step": 16210
    },
    {
      "epoch": 4.634285714285714,
      "grad_norm": 0.8372983336448669,
      "learning_rate": 1.3820952380952381e-05,
      "loss": 0.4163,
      "step": 16220
    },
    {
      "epoch": 4.637142857142857,
      "grad_norm": 10.741718292236328,
      "learning_rate": 1.3817142857142858e-05,
      "loss": 0.5441,
      "step": 16230
    },
    {
      "epoch": 4.64,
      "grad_norm": 33.12211990356445,
      "learning_rate": 1.3813333333333334e-05,
      "loss": 1.056,
      "step": 16240
    },
    {
      "epoch": 4.642857142857143,
      "grad_norm": 0.8249914646148682,
      "learning_rate": 1.3809523809523811e-05,
      "loss": 0.7325,
      "step": 16250
    },
    {
      "epoch": 4.645714285714286,
      "grad_norm": 22.457712173461914,
      "learning_rate": 1.3805714285714287e-05,
      "loss": 0.6117,
      "step": 16260
    },
    {
      "epoch": 4.648571428571429,
      "grad_norm": 0.9852964282035828,
      "learning_rate": 1.3801904761904762e-05,
      "loss": 0.5081,
      "step": 16270
    },
    {
      "epoch": 4.651428571428571,
      "grad_norm": 0.8414393067359924,
      "learning_rate": 1.379809523809524e-05,
      "loss": 0.3123,
      "step": 16280
    },
    {
      "epoch": 4.654285714285714,
      "grad_norm": 10.950889587402344,
      "learning_rate": 1.3794285714285715e-05,
      "loss": 0.6269,
      "step": 16290
    },
    {
      "epoch": 4.6571428571428575,
      "grad_norm": 0.8442161679267883,
      "learning_rate": 1.3790476190476192e-05,
      "loss": 0.5276,
      "step": 16300
    },
    {
      "epoch": 4.66,
      "grad_norm": 21.88299560546875,
      "learning_rate": 1.3786666666666668e-05,
      "loss": 0.7468,
      "step": 16310
    },
    {
      "epoch": 4.662857142857143,
      "grad_norm": 0.7090669870376587,
      "learning_rate": 1.3782857142857145e-05,
      "loss": 0.3258,
      "step": 16320
    },
    {
      "epoch": 4.6657142857142855,
      "grad_norm": 11.382284164428711,
      "learning_rate": 1.377904761904762e-05,
      "loss": 0.6533,
      "step": 16330
    },
    {
      "epoch": 4.668571428571429,
      "grad_norm": 11.14797306060791,
      "learning_rate": 1.3775238095238096e-05,
      "loss": 0.5451,
      "step": 16340
    },
    {
      "epoch": 4.671428571428572,
      "grad_norm": 21.849563598632812,
      "learning_rate": 1.3771428571428574e-05,
      "loss": 0.7324,
      "step": 16350
    },
    {
      "epoch": 4.674285714285714,
      "grad_norm": 0.8369684219360352,
      "learning_rate": 1.376761904761905e-05,
      "loss": 0.2212,
      "step": 16360
    },
    {
      "epoch": 4.677142857142857,
      "grad_norm": 0.7522993683815002,
      "learning_rate": 1.3763809523809527e-05,
      "loss": 0.6251,
      "step": 16370
    },
    {
      "epoch": 4.68,
      "grad_norm": 0.7503753900527954,
      "learning_rate": 1.376e-05,
      "loss": 0.5219,
      "step": 16380
    },
    {
      "epoch": 4.682857142857143,
      "grad_norm": 0.7421490550041199,
      "learning_rate": 1.3756190476190476e-05,
      "loss": 0.6351,
      "step": 16390
    },
    {
      "epoch": 4.685714285714286,
      "grad_norm": 10.562416076660156,
      "learning_rate": 1.3752380952380953e-05,
      "loss": 0.6327,
      "step": 16400
    },
    {
      "epoch": 4.688571428571429,
      "grad_norm": 10.920659065246582,
      "learning_rate": 1.3748571428571429e-05,
      "loss": 0.6285,
      "step": 16410
    },
    {
      "epoch": 4.691428571428571,
      "grad_norm": 0.7998160719871521,
      "learning_rate": 1.3744761904761904e-05,
      "loss": 0.5257,
      "step": 16420
    },
    {
      "epoch": 4.694285714285714,
      "grad_norm": 10.606101036071777,
      "learning_rate": 1.3740952380952382e-05,
      "loss": 0.736,
      "step": 16430
    },
    {
      "epoch": 4.6971428571428575,
      "grad_norm": 23.272207260131836,
      "learning_rate": 1.3737142857142857e-05,
      "loss": 0.9191,
      "step": 16440
    },
    {
      "epoch": 4.7,
      "grad_norm": 1.0621216297149658,
      "learning_rate": 1.3733333333333335e-05,
      "loss": 0.5926,
      "step": 16450
    },
    {
      "epoch": 4.702857142857143,
      "grad_norm": 21.481765747070312,
      "learning_rate": 1.372952380952381e-05,
      "loss": 0.4869,
      "step": 16460
    },
    {
      "epoch": 4.7057142857142855,
      "grad_norm": 0.96738600730896,
      "learning_rate": 1.3725714285714287e-05,
      "loss": 0.3033,
      "step": 16470
    },
    {
      "epoch": 4.708571428571428,
      "grad_norm": 11.043213844299316,
      "learning_rate": 1.3721904761904763e-05,
      "loss": 0.61,
      "step": 16480
    },
    {
      "epoch": 4.711428571428572,
      "grad_norm": 10.636098861694336,
      "learning_rate": 1.3718095238095239e-05,
      "loss": 0.4126,
      "step": 16490
    },
    {
      "epoch": 4.714285714285714,
      "grad_norm": 0.7154917120933533,
      "learning_rate": 1.3714285714285716e-05,
      "loss": 0.3219,
      "step": 16500
    },
    {
      "epoch": 4.717142857142857,
      "grad_norm": 0.5523557662963867,
      "learning_rate": 1.3710476190476191e-05,
      "loss": 0.2301,
      "step": 16510
    },
    {
      "epoch": 4.72,
      "grad_norm": 10.825295448303223,
      "learning_rate": 1.3706666666666669e-05,
      "loss": 0.9021,
      "step": 16520
    },
    {
      "epoch": 4.722857142857142,
      "grad_norm": 0.49718818068504333,
      "learning_rate": 1.3702857142857144e-05,
      "loss": 0.4636,
      "step": 16530
    },
    {
      "epoch": 4.725714285714286,
      "grad_norm": 0.5343272686004639,
      "learning_rate": 1.369904761904762e-05,
      "loss": 0.6915,
      "step": 16540
    },
    {
      "epoch": 4.728571428571429,
      "grad_norm": 0.5800603032112122,
      "learning_rate": 1.3695238095238097e-05,
      "loss": 0.6741,
      "step": 16550
    },
    {
      "epoch": 4.731428571428571,
      "grad_norm": 10.812305450439453,
      "learning_rate": 1.3691428571428573e-05,
      "loss": 0.4474,
      "step": 16560
    },
    {
      "epoch": 4.734285714285714,
      "grad_norm": 0.578010618686676,
      "learning_rate": 1.368761904761905e-05,
      "loss": 0.4493,
      "step": 16570
    },
    {
      "epoch": 4.737142857142857,
      "grad_norm": 10.932135581970215,
      "learning_rate": 1.3683809523809526e-05,
      "loss": 0.6661,
      "step": 16580
    },
    {
      "epoch": 4.74,
      "grad_norm": 10.913769721984863,
      "learning_rate": 1.3680000000000003e-05,
      "loss": 0.7721,
      "step": 16590
    },
    {
      "epoch": 4.742857142857143,
      "grad_norm": 0.6190670728683472,
      "learning_rate": 1.3676190476190477e-05,
      "loss": 0.4444,
      "step": 16600
    },
    {
      "epoch": 4.7457142857142856,
      "grad_norm": 0.5950298309326172,
      "learning_rate": 1.3672380952380952e-05,
      "loss": 0.4467,
      "step": 16610
    },
    {
      "epoch": 4.748571428571428,
      "grad_norm": 22.09630012512207,
      "learning_rate": 1.366857142857143e-05,
      "loss": 0.6664,
      "step": 16620
    },
    {
      "epoch": 4.751428571428572,
      "grad_norm": 0.5608217120170593,
      "learning_rate": 1.3664761904761905e-05,
      "loss": 0.6699,
      "step": 16630
    },
    {
      "epoch": 4.7542857142857144,
      "grad_norm": 0.5413658022880554,
      "learning_rate": 1.366095238095238e-05,
      "loss": 0.4533,
      "step": 16640
    },
    {
      "epoch": 4.757142857142857,
      "grad_norm": 11.070112228393555,
      "learning_rate": 1.3657142857142858e-05,
      "loss": 0.4531,
      "step": 16650
    },
    {
      "epoch": 4.76,
      "grad_norm": 10.854100227355957,
      "learning_rate": 1.3653333333333334e-05,
      "loss": 0.7841,
      "step": 16660
    },
    {
      "epoch": 4.762857142857143,
      "grad_norm": 0.6490783095359802,
      "learning_rate": 1.3649523809523811e-05,
      "loss": 0.6711,
      "step": 16670
    },
    {
      "epoch": 4.765714285714286,
      "grad_norm": 11.26844310760498,
      "learning_rate": 1.3645714285714286e-05,
      "loss": 0.5475,
      "step": 16680
    },
    {
      "epoch": 4.768571428571429,
      "grad_norm": 11.013697624206543,
      "learning_rate": 1.3641904761904762e-05,
      "loss": 0.4391,
      "step": 16690
    },
    {
      "epoch": 4.771428571428571,
      "grad_norm": 0.6399512887001038,
      "learning_rate": 1.363809523809524e-05,
      "loss": 0.332,
      "step": 16700
    },
    {
      "epoch": 4.774285714285714,
      "grad_norm": 10.712149620056152,
      "learning_rate": 1.3634285714285715e-05,
      "loss": 0.9633,
      "step": 16710
    },
    {
      "epoch": 4.777142857142858,
      "grad_norm": 10.664568901062012,
      "learning_rate": 1.3630476190476192e-05,
      "loss": 0.4349,
      "step": 16720
    },
    {
      "epoch": 4.78,
      "grad_norm": 11.507573127746582,
      "learning_rate": 1.3626666666666668e-05,
      "loss": 0.6419,
      "step": 16730
    },
    {
      "epoch": 4.782857142857143,
      "grad_norm": 10.683966636657715,
      "learning_rate": 1.3622857142857145e-05,
      "loss": 0.4367,
      "step": 16740
    },
    {
      "epoch": 4.785714285714286,
      "grad_norm": 10.929256439208984,
      "learning_rate": 1.361904761904762e-05,
      "loss": 0.4408,
      "step": 16750
    },
    {
      "epoch": 4.788571428571428,
      "grad_norm": 10.903138160705566,
      "learning_rate": 1.3615238095238096e-05,
      "loss": 0.5527,
      "step": 16760
    },
    {
      "epoch": 4.791428571428572,
      "grad_norm": 0.5991921424865723,
      "learning_rate": 1.3611428571428573e-05,
      "loss": 0.3345,
      "step": 16770
    },
    {
      "epoch": 4.7942857142857145,
      "grad_norm": 0.606407105922699,
      "learning_rate": 1.3607619047619049e-05,
      "loss": 0.6564,
      "step": 16780
    },
    {
      "epoch": 4.797142857142857,
      "grad_norm": 0.6014454960823059,
      "learning_rate": 1.3603809523809526e-05,
      "loss": 0.6623,
      "step": 16790
    },
    {
      "epoch": 4.8,
      "grad_norm": 0.5785442590713501,
      "learning_rate": 1.3600000000000002e-05,
      "loss": 0.3372,
      "step": 16800
    },
    {
      "epoch": 4.8028571428571425,
      "grad_norm": 0.6313566565513611,
      "learning_rate": 1.359619047619048e-05,
      "loss": 0.6635,
      "step": 16810
    },
    {
      "epoch": 4.805714285714286,
      "grad_norm": 0.5918397307395935,
      "learning_rate": 1.3592380952380953e-05,
      "loss": 0.1193,
      "step": 16820
    },
    {
      "epoch": 4.808571428571429,
      "grad_norm": 22.179588317871094,
      "learning_rate": 1.3588571428571429e-05,
      "loss": 0.672,
      "step": 16830
    },
    {
      "epoch": 4.811428571428571,
      "grad_norm": 0.5959492921829224,
      "learning_rate": 1.3584761904761904e-05,
      "loss": 0.6714,
      "step": 16840
    },
    {
      "epoch": 4.814285714285714,
      "grad_norm": 0.6219691038131714,
      "learning_rate": 1.3580952380952382e-05,
      "loss": 0.4373,
      "step": 16850
    },
    {
      "epoch": 4.817142857142857,
      "grad_norm": 10.933463096618652,
      "learning_rate": 1.3577142857142857e-05,
      "loss": 0.8647,
      "step": 16860
    },
    {
      "epoch": 4.82,
      "grad_norm": 0.6410742402076721,
      "learning_rate": 1.3573333333333334e-05,
      "loss": 0.5428,
      "step": 16870
    },
    {
      "epoch": 4.822857142857143,
      "grad_norm": 10.666342735290527,
      "learning_rate": 1.356952380952381e-05,
      "loss": 0.9656,
      "step": 16880
    },
    {
      "epoch": 4.825714285714286,
      "grad_norm": 21.76133155822754,
      "learning_rate": 1.3565714285714287e-05,
      "loss": 0.7296,
      "step": 16890
    },
    {
      "epoch": 4.828571428571428,
      "grad_norm": 10.391130447387695,
      "learning_rate": 1.3561904761904763e-05,
      "loss": 0.8085,
      "step": 16900
    },
    {
      "epoch": 4.831428571428571,
      "grad_norm": 0.9927061796188354,
      "learning_rate": 1.3558095238095238e-05,
      "loss": 0.3059,
      "step": 16910
    },
    {
      "epoch": 4.8342857142857145,
      "grad_norm": 0.8745622634887695,
      "learning_rate": 1.3554285714285716e-05,
      "loss": 0.4972,
      "step": 16920
    },
    {
      "epoch": 4.837142857142857,
      "grad_norm": 10.485267639160156,
      "learning_rate": 1.3550476190476191e-05,
      "loss": 0.9132,
      "step": 16930
    },
    {
      "epoch": 4.84,
      "grad_norm": 0.8416743874549866,
      "learning_rate": 1.3546666666666669e-05,
      "loss": 0.3127,
      "step": 16940
    },
    {
      "epoch": 4.8428571428571425,
      "grad_norm": 0.7373243570327759,
      "learning_rate": 1.3542857142857144e-05,
      "loss": 0.4232,
      "step": 16950
    },
    {
      "epoch": 4.845714285714286,
      "grad_norm": 0.5328205823898315,
      "learning_rate": 1.3539047619047621e-05,
      "loss": 0.1227,
      "step": 16960
    },
    {
      "epoch": 4.848571428571429,
      "grad_norm": 0.48424309492111206,
      "learning_rate": 1.3535238095238097e-05,
      "loss": 0.5771,
      "step": 16970
    },
    {
      "epoch": 4.851428571428571,
      "grad_norm": 0.4853157699108124,
      "learning_rate": 1.3531428571428573e-05,
      "loss": 0.573,
      "step": 16980
    },
    {
      "epoch": 4.854285714285714,
      "grad_norm": 0.4556492269039154,
      "learning_rate": 1.352761904761905e-05,
      "loss": 0.2404,
      "step": 16990
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 0.44200772047042847,
      "learning_rate": 1.3523809523809525e-05,
      "loss": 0.5848,
      "step": 17000
    },
    {
      "epoch": 4.86,
      "grad_norm": 0.496313214302063,
      "learning_rate": 1.3520000000000003e-05,
      "loss": 1.157,
      "step": 17010
    },
    {
      "epoch": 4.862857142857143,
      "grad_norm": 0.5164303183555603,
      "learning_rate": 1.3516190476190478e-05,
      "loss": 0.2337,
      "step": 17020
    },
    {
      "epoch": 4.865714285714286,
      "grad_norm": 0.5249594449996948,
      "learning_rate": 1.3512380952380952e-05,
      "loss": 0.797,
      "step": 17030
    },
    {
      "epoch": 4.868571428571428,
      "grad_norm": 0.5576965808868408,
      "learning_rate": 1.350857142857143e-05,
      "loss": 0.7865,
      "step": 17040
    },
    {
      "epoch": 4.871428571428572,
      "grad_norm": 21.95784568786621,
      "learning_rate": 1.3504761904761905e-05,
      "loss": 0.7721,
      "step": 17050
    },
    {
      "epoch": 4.8742857142857146,
      "grad_norm": 10.709095001220703,
      "learning_rate": 1.350095238095238e-05,
      "loss": 0.5486,
      "step": 17060
    },
    {
      "epoch": 4.877142857142857,
      "grad_norm": 0.6628607511520386,
      "learning_rate": 1.3497142857142858e-05,
      "loss": 0.4355,
      "step": 17070
    },
    {
      "epoch": 4.88,
      "grad_norm": 0.6158725023269653,
      "learning_rate": 1.3493333333333333e-05,
      "loss": 0.3295,
      "step": 17080
    },
    {
      "epoch": 4.882857142857143,
      "grad_norm": 0.576658308506012,
      "learning_rate": 1.348952380952381e-05,
      "loss": 0.6673,
      "step": 17090
    },
    {
      "epoch": 4.885714285714286,
      "grad_norm": 0.6569913625717163,
      "learning_rate": 1.3485714285714286e-05,
      "loss": 1.1018,
      "step": 17100
    },
    {
      "epoch": 4.888571428571429,
      "grad_norm": 0.7373246550559998,
      "learning_rate": 1.3481904761904762e-05,
      "loss": 0.6393,
      "step": 17110
    },
    {
      "epoch": 4.8914285714285715,
      "grad_norm": 10.73587417602539,
      "learning_rate": 1.347809523809524e-05,
      "loss": 0.6338,
      "step": 17120
    },
    {
      "epoch": 4.894285714285714,
      "grad_norm": 10.646966934204102,
      "learning_rate": 1.3474285714285715e-05,
      "loss": 0.6303,
      "step": 17130
    },
    {
      "epoch": 4.897142857142857,
      "grad_norm": 0.7072435617446899,
      "learning_rate": 1.3470476190476192e-05,
      "loss": 0.1187,
      "step": 17140
    },
    {
      "epoch": 4.9,
      "grad_norm": 0.6209650635719299,
      "learning_rate": 1.3466666666666668e-05,
      "loss": 0.4367,
      "step": 17150
    },
    {
      "epoch": 4.902857142857143,
      "grad_norm": 22.053421020507812,
      "learning_rate": 1.3462857142857145e-05,
      "loss": 0.8818,
      "step": 17160
    },
    {
      "epoch": 4.905714285714286,
      "grad_norm": 0.6713886857032776,
      "learning_rate": 1.345904761904762e-05,
      "loss": 0.653,
      "step": 17170
    },
    {
      "epoch": 4.908571428571428,
      "grad_norm": 0.5646215081214905,
      "learning_rate": 1.3455238095238096e-05,
      "loss": 0.0139,
      "step": 17180
    },
    {
      "epoch": 4.911428571428571,
      "grad_norm": 10.807859420776367,
      "learning_rate": 1.3451428571428573e-05,
      "loss": 1.0032,
      "step": 17190
    },
    {
      "epoch": 4.914285714285715,
      "grad_norm": 0.5751630663871765,
      "learning_rate": 1.3447619047619049e-05,
      "loss": 0.6725,
      "step": 17200
    },
    {
      "epoch": 4.917142857142857,
      "grad_norm": 10.794357299804688,
      "learning_rate": 1.3443809523809526e-05,
      "loss": 0.2283,
      "step": 17210
    },
    {
      "epoch": 4.92,
      "grad_norm": 22.358978271484375,
      "learning_rate": 1.3440000000000002e-05,
      "loss": 0.6647,
      "step": 17220
    },
    {
      "epoch": 4.922857142857143,
      "grad_norm": 0.5991240739822388,
      "learning_rate": 1.3436190476190479e-05,
      "loss": 0.4463,
      "step": 17230
    },
    {
      "epoch": 4.925714285714285,
      "grad_norm": 0.6325109004974365,
      "learning_rate": 1.3432380952380955e-05,
      "loss": 0.8708,
      "step": 17240
    },
    {
      "epoch": 4.928571428571429,
      "grad_norm": 10.674125671386719,
      "learning_rate": 1.3428571428571429e-05,
      "loss": 0.437,
      "step": 17250
    },
    {
      "epoch": 4.9314285714285715,
      "grad_norm": 0.7004731297492981,
      "learning_rate": 1.3424761904761904e-05,
      "loss": 0.4316,
      "step": 17260
    },
    {
      "epoch": 4.934285714285714,
      "grad_norm": 0.7440173625946045,
      "learning_rate": 1.3420952380952381e-05,
      "loss": 0.4328,
      "step": 17270
    },
    {
      "epoch": 4.937142857142857,
      "grad_norm": 0.8392875790596008,
      "learning_rate": 1.3417142857142857e-05,
      "loss": 0.8407,
      "step": 17280
    },
    {
      "epoch": 4.9399999999999995,
      "grad_norm": 0.8664502501487732,
      "learning_rate": 1.3413333333333334e-05,
      "loss": 0.616,
      "step": 17290
    },
    {
      "epoch": 4.942857142857143,
      "grad_norm": 0.8535065054893494,
      "learning_rate": 1.340952380952381e-05,
      "loss": 0.8256,
      "step": 17300
    },
    {
      "epoch": 4.945714285714286,
      "grad_norm": 10.791729927062988,
      "learning_rate": 1.3405714285714287e-05,
      "loss": 0.3277,
      "step": 17310
    },
    {
      "epoch": 4.948571428571428,
      "grad_norm": 10.983105659484863,
      "learning_rate": 1.3401904761904763e-05,
      "loss": 0.3356,
      "step": 17320
    },
    {
      "epoch": 4.951428571428571,
      "grad_norm": 10.812605857849121,
      "learning_rate": 1.3398095238095238e-05,
      "loss": 0.7844,
      "step": 17330
    },
    {
      "epoch": 4.954285714285715,
      "grad_norm": 10.737074851989746,
      "learning_rate": 1.3394285714285716e-05,
      "loss": 0.3392,
      "step": 17340
    },
    {
      "epoch": 4.957142857142857,
      "grad_norm": 0.5524463057518005,
      "learning_rate": 1.3390476190476191e-05,
      "loss": 0.4438,
      "step": 17350
    },
    {
      "epoch": 4.96,
      "grad_norm": 10.852513313293457,
      "learning_rate": 1.3386666666666668e-05,
      "loss": 0.6773,
      "step": 17360
    },
    {
      "epoch": 4.962857142857143,
      "grad_norm": 33.23404312133789,
      "learning_rate": 1.3382857142857144e-05,
      "loss": 0.6751,
      "step": 17370
    },
    {
      "epoch": 4.965714285714286,
      "grad_norm": 0.5585606694221497,
      "learning_rate": 1.3379047619047621e-05,
      "loss": 0.3403,
      "step": 17380
    },
    {
      "epoch": 4.968571428571429,
      "grad_norm": 0.5170692205429077,
      "learning_rate": 1.3375238095238097e-05,
      "loss": 0.3452,
      "step": 17390
    },
    {
      "epoch": 4.9714285714285715,
      "grad_norm": 0.5007812976837158,
      "learning_rate": 1.3371428571428572e-05,
      "loss": 0.8062,
      "step": 17400
    },
    {
      "epoch": 4.974285714285714,
      "grad_norm": 0.524071991443634,
      "learning_rate": 1.336761904761905e-05,
      "loss": 0.7975,
      "step": 17410
    },
    {
      "epoch": 4.977142857142857,
      "grad_norm": 10.708971977233887,
      "learning_rate": 1.3363809523809525e-05,
      "loss": 0.8894,
      "step": 17420
    },
    {
      "epoch": 4.98,
      "grad_norm": 0.6518566012382507,
      "learning_rate": 1.3360000000000003e-05,
      "loss": 0.2277,
      "step": 17430
    },
    {
      "epoch": 4.982857142857143,
      "grad_norm": 22.126358032226562,
      "learning_rate": 1.3356190476190478e-05,
      "loss": 0.9676,
      "step": 17440
    },
    {
      "epoch": 4.985714285714286,
      "grad_norm": 0.8216153383255005,
      "learning_rate": 1.3352380952380954e-05,
      "loss": 0.5331,
      "step": 17450
    },
    {
      "epoch": 4.988571428571428,
      "grad_norm": 10.49850082397461,
      "learning_rate": 1.3348571428571431e-05,
      "loss": 0.6195,
      "step": 17460
    },
    {
      "epoch": 4.991428571428571,
      "grad_norm": 0.7899045348167419,
      "learning_rate": 1.3344761904761905e-05,
      "loss": 0.2175,
      "step": 17470
    },
    {
      "epoch": 4.994285714285715,
      "grad_norm": 0.664334774017334,
      "learning_rate": 1.334095238095238e-05,
      "loss": 0.33,
      "step": 17480
    },
    {
      "epoch": 4.997142857142857,
      "grad_norm": 10.685090065002441,
      "learning_rate": 1.3337142857142858e-05,
      "loss": 0.4397,
      "step": 17490
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.6656795740127563,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.4419,
      "step": 17500
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.8709677419354839,
      "eval_f1": 0.0,
      "eval_loss": 0.5569617748260498,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 45.2888,
      "eval_samples_per_second": 66.242,
      "eval_steps_per_second": 2.076,
      "step": 17500
    },
    {
      "epoch": 5.002857142857143,
      "grad_norm": 0.5786840319633484,
      "learning_rate": 1.332952380952381e-05,
      "loss": 0.3327,
      "step": 17510
    },
    {
      "epoch": 5.005714285714285,
      "grad_norm": 0.5338892340660095,
      "learning_rate": 1.3325714285714286e-05,
      "loss": 0.2292,
      "step": 17520
    },
    {
      "epoch": 5.008571428571429,
      "grad_norm": 0.5622966885566711,
      "learning_rate": 1.3321904761904762e-05,
      "loss": 0.7873,
      "step": 17530
    },
    {
      "epoch": 5.011428571428572,
      "grad_norm": 10.738685607910156,
      "learning_rate": 1.3318095238095239e-05,
      "loss": 0.8846,
      "step": 17540
    },
    {
      "epoch": 5.014285714285714,
      "grad_norm": 0.5668658018112183,
      "learning_rate": 1.3314285714285715e-05,
      "loss": 0.4493,
      "step": 17550
    },
    {
      "epoch": 5.017142857142857,
      "grad_norm": 0.5739182233810425,
      "learning_rate": 1.3310476190476192e-05,
      "loss": 0.4482,
      "step": 17560
    },
    {
      "epoch": 5.02,
      "grad_norm": 0.5639461874961853,
      "learning_rate": 1.3306666666666667e-05,
      "loss": 0.5703,
      "step": 17570
    },
    {
      "epoch": 5.022857142857143,
      "grad_norm": 0.5709309577941895,
      "learning_rate": 1.3302857142857145e-05,
      "loss": 0.7747,
      "step": 17580
    },
    {
      "epoch": 5.025714285714286,
      "grad_norm": 10.68587875366211,
      "learning_rate": 1.329904761904762e-05,
      "loss": 0.766,
      "step": 17590
    },
    {
      "epoch": 5.0285714285714285,
      "grad_norm": 10.62602424621582,
      "learning_rate": 1.3295238095238096e-05,
      "loss": 0.4293,
      "step": 17600
    },
    {
      "epoch": 5.031428571428571,
      "grad_norm": 10.582510948181152,
      "learning_rate": 1.3291428571428573e-05,
      "loss": 0.6356,
      "step": 17610
    },
    {
      "epoch": 5.034285714285715,
      "grad_norm": 0.8985510468482971,
      "learning_rate": 1.3287619047619049e-05,
      "loss": 0.4168,
      "step": 17620
    },
    {
      "epoch": 5.037142857142857,
      "grad_norm": 0.9021664261817932,
      "learning_rate": 1.3283809523809526e-05,
      "loss": 0.5119,
      "step": 17630
    },
    {
      "epoch": 5.04,
      "grad_norm": 0.785081684589386,
      "learning_rate": 1.3280000000000002e-05,
      "loss": 0.4166,
      "step": 17640
    },
    {
      "epoch": 5.042857142857143,
      "grad_norm": 22.477142333984375,
      "learning_rate": 1.3276190476190479e-05,
      "loss": 0.8337,
      "step": 17650
    },
    {
      "epoch": 5.045714285714285,
      "grad_norm": 21.986751556396484,
      "learning_rate": 1.3272380952380954e-05,
      "loss": 0.3322,
      "step": 17660
    },
    {
      "epoch": 5.048571428571429,
      "grad_norm": 10.763389587402344,
      "learning_rate": 1.326857142857143e-05,
      "loss": 0.5544,
      "step": 17670
    },
    {
      "epoch": 5.051428571428572,
      "grad_norm": 0.5902866125106812,
      "learning_rate": 1.3264761904761907e-05,
      "loss": 0.4461,
      "step": 17680
    },
    {
      "epoch": 5.054285714285714,
      "grad_norm": 0.6119920611381531,
      "learning_rate": 1.3260952380952381e-05,
      "loss": 0.6656,
      "step": 17690
    },
    {
      "epoch": 5.057142857142857,
      "grad_norm": 0.6711043119430542,
      "learning_rate": 1.3257142857142857e-05,
      "loss": 0.868,
      "step": 17700
    },
    {
      "epoch": 5.06,
      "grad_norm": 0.7292543649673462,
      "learning_rate": 1.3253333333333334e-05,
      "loss": 0.2221,
      "step": 17710
    },
    {
      "epoch": 5.062857142857143,
      "grad_norm": 10.664536476135254,
      "learning_rate": 1.324952380952381e-05,
      "loss": 0.4374,
      "step": 17720
    },
    {
      "epoch": 5.065714285714286,
      "grad_norm": 0.7089083194732666,
      "learning_rate": 1.3245714285714287e-05,
      "loss": 0.4369,
      "step": 17730
    },
    {
      "epoch": 5.0685714285714285,
      "grad_norm": 10.679576873779297,
      "learning_rate": 1.3241904761904762e-05,
      "loss": 1.0565,
      "step": 17740
    },
    {
      "epoch": 5.071428571428571,
      "grad_norm": 10.603219032287598,
      "learning_rate": 1.3238095238095238e-05,
      "loss": 0.4245,
      "step": 17750
    },
    {
      "epoch": 5.074285714285715,
      "grad_norm": 10.676762580871582,
      "learning_rate": 1.3234285714285715e-05,
      "loss": 0.3229,
      "step": 17760
    },
    {
      "epoch": 5.077142857142857,
      "grad_norm": 0.6620557308197021,
      "learning_rate": 1.3230476190476191e-05,
      "loss": 0.4275,
      "step": 17770
    },
    {
      "epoch": 5.08,
      "grad_norm": 10.490483283996582,
      "learning_rate": 1.3226666666666668e-05,
      "loss": 1.0404,
      "step": 17780
    },
    {
      "epoch": 5.082857142857143,
      "grad_norm": 0.9784283638000488,
      "learning_rate": 1.3222857142857144e-05,
      "loss": 0.5971,
      "step": 17790
    },
    {
      "epoch": 5.085714285714285,
      "grad_norm": 0.8342053890228271,
      "learning_rate": 1.3219047619047621e-05,
      "loss": 0.3134,
      "step": 17800
    },
    {
      "epoch": 5.088571428571429,
      "grad_norm": 0.7788705229759216,
      "learning_rate": 1.3215238095238097e-05,
      "loss": 0.7163,
      "step": 17810
    },
    {
      "epoch": 5.091428571428572,
      "grad_norm": 0.7415378093719482,
      "learning_rate": 1.3211428571428572e-05,
      "loss": 0.6396,
      "step": 17820
    },
    {
      "epoch": 5.094285714285714,
      "grad_norm": 10.50632095336914,
      "learning_rate": 1.320761904761905e-05,
      "loss": 0.8316,
      "step": 17830
    },
    {
      "epoch": 5.097142857142857,
      "grad_norm": 10.539548873901367,
      "learning_rate": 1.3203809523809525e-05,
      "loss": 0.3211,
      "step": 17840
    },
    {
      "epoch": 5.1,
      "grad_norm": 0.7076801061630249,
      "learning_rate": 1.3200000000000002e-05,
      "loss": 0.3232,
      "step": 17850
    },
    {
      "epoch": 5.102857142857143,
      "grad_norm": 10.711183547973633,
      "learning_rate": 1.3196190476190478e-05,
      "loss": 0.8454,
      "step": 17860
    },
    {
      "epoch": 5.105714285714286,
      "grad_norm": 0.7687745094299316,
      "learning_rate": 1.3192380952380954e-05,
      "loss": 0.8318,
      "step": 17870
    },
    {
      "epoch": 5.1085714285714285,
      "grad_norm": 10.613380432128906,
      "learning_rate": 1.318857142857143e-05,
      "loss": 0.7238,
      "step": 17880
    },
    {
      "epoch": 5.111428571428571,
      "grad_norm": 21.8183536529541,
      "learning_rate": 1.3184761904761906e-05,
      "loss": 0.5158,
      "step": 17890
    },
    {
      "epoch": 5.114285714285714,
      "grad_norm": 10.514570236206055,
      "learning_rate": 1.318095238095238e-05,
      "loss": 0.7132,
      "step": 17900
    },
    {
      "epoch": 5.117142857142857,
      "grad_norm": 0.9290032386779785,
      "learning_rate": 1.3177142857142858e-05,
      "loss": 0.3192,
      "step": 17910
    },
    {
      "epoch": 5.12,
      "grad_norm": 0.6315667033195496,
      "learning_rate": 1.3173333333333333e-05,
      "loss": 0.2202,
      "step": 17920
    },
    {
      "epoch": 5.122857142857143,
      "grad_norm": 0.5141118764877319,
      "learning_rate": 1.316952380952381e-05,
      "loss": 0.6827,
      "step": 17930
    },
    {
      "epoch": 5.1257142857142854,
      "grad_norm": 10.718047142028809,
      "learning_rate": 1.3165714285714286e-05,
      "loss": 0.9968,
      "step": 17940
    },
    {
      "epoch": 5.128571428571428,
      "grad_norm": 0.6829315423965454,
      "learning_rate": 1.3161904761904762e-05,
      "loss": 0.5444,
      "step": 17950
    },
    {
      "epoch": 5.131428571428572,
      "grad_norm": 0.6355413198471069,
      "learning_rate": 1.3158095238095239e-05,
      "loss": 0.3304,
      "step": 17960
    },
    {
      "epoch": 5.134285714285714,
      "grad_norm": 11.142657279968262,
      "learning_rate": 1.3154285714285714e-05,
      "loss": 0.4452,
      "step": 17970
    },
    {
      "epoch": 5.137142857142857,
      "grad_norm": 0.5329684019088745,
      "learning_rate": 1.3150476190476192e-05,
      "loss": 0.6769,
      "step": 17980
    },
    {
      "epoch": 5.14,
      "grad_norm": 10.984888076782227,
      "learning_rate": 1.3146666666666667e-05,
      "loss": 0.6824,
      "step": 17990
    },
    {
      "epoch": 5.142857142857143,
      "grad_norm": 11.022599220275879,
      "learning_rate": 1.3142857142857145e-05,
      "loss": 1.2316,
      "step": 18000
    },
    {
      "epoch": 5.145714285714286,
      "grad_norm": 10.755410194396973,
      "learning_rate": 1.313904761904762e-05,
      "loss": 0.8383,
      "step": 18010
    },
    {
      "epoch": 5.148571428571429,
      "grad_norm": 0.9199429154396057,
      "learning_rate": 1.3135238095238096e-05,
      "loss": 0.4103,
      "step": 18020
    },
    {
      "epoch": 5.151428571428571,
      "grad_norm": 0.8797579407691956,
      "learning_rate": 1.3131428571428573e-05,
      "loss": 0.409,
      "step": 18030
    },
    {
      "epoch": 5.154285714285714,
      "grad_norm": 22.096223831176758,
      "learning_rate": 1.3127619047619049e-05,
      "loss": 0.5118,
      "step": 18040
    },
    {
      "epoch": 5.1571428571428575,
      "grad_norm": 11.413431167602539,
      "learning_rate": 1.3123809523809526e-05,
      "loss": 0.4168,
      "step": 18050
    },
    {
      "epoch": 5.16,
      "grad_norm": 11.621479034423828,
      "learning_rate": 1.3120000000000001e-05,
      "loss": 0.3212,
      "step": 18060
    },
    {
      "epoch": 5.162857142857143,
      "grad_norm": 0.7209063172340393,
      "learning_rate": 1.3116190476190479e-05,
      "loss": 0.5285,
      "step": 18070
    },
    {
      "epoch": 5.1657142857142855,
      "grad_norm": 11.126799583435059,
      "learning_rate": 1.3112380952380954e-05,
      "loss": 0.5509,
      "step": 18080
    },
    {
      "epoch": 5.168571428571428,
      "grad_norm": 0.5886553525924683,
      "learning_rate": 1.310857142857143e-05,
      "loss": 0.4453,
      "step": 18090
    },
    {
      "epoch": 5.171428571428572,
      "grad_norm": 10.973706245422363,
      "learning_rate": 1.3104761904761907e-05,
      "loss": 0.6558,
      "step": 18100
    },
    {
      "epoch": 5.174285714285714,
      "grad_norm": 0.9923017024993896,
      "learning_rate": 1.3100952380952383e-05,
      "loss": 0.9354,
      "step": 18110
    },
    {
      "epoch": 5.177142857142857,
      "grad_norm": 0.9107248187065125,
      "learning_rate": 1.3097142857142857e-05,
      "loss": 0.1157,
      "step": 18120
    },
    {
      "epoch": 5.18,
      "grad_norm": 11.490678787231445,
      "learning_rate": 1.3093333333333334e-05,
      "loss": 0.7219,
      "step": 18130
    },
    {
      "epoch": 5.182857142857143,
      "grad_norm": 0.6920642256736755,
      "learning_rate": 1.308952380952381e-05,
      "loss": 0.331,
      "step": 18140
    },
    {
      "epoch": 5.185714285714286,
      "grad_norm": 11.63205337524414,
      "learning_rate": 1.3085714285714287e-05,
      "loss": 0.5447,
      "step": 18150
    },
    {
      "epoch": 5.188571428571429,
      "grad_norm": 0.6326860189437866,
      "learning_rate": 1.3081904761904762e-05,
      "loss": 0.1222,
      "step": 18160
    },
    {
      "epoch": 5.191428571428571,
      "grad_norm": 11.40462875366211,
      "learning_rate": 1.3078095238095238e-05,
      "loss": 0.5578,
      "step": 18170
    },
    {
      "epoch": 5.194285714285714,
      "grad_norm": 0.5293899178504944,
      "learning_rate": 1.3074285714285715e-05,
      "loss": 0.2335,
      "step": 18180
    },
    {
      "epoch": 5.1971428571428575,
      "grad_norm": 0.5172998309135437,
      "learning_rate": 1.307047619047619e-05,
      "loss": 0.4622,
      "step": 18190
    },
    {
      "epoch": 5.2,
      "grad_norm": 0.527317225933075,
      "learning_rate": 1.3066666666666668e-05,
      "loss": 0.2414,
      "step": 18200
    },
    {
      "epoch": 5.202857142857143,
      "grad_norm": 10.972513198852539,
      "learning_rate": 1.3062857142857144e-05,
      "loss": 0.4781,
      "step": 18210
    },
    {
      "epoch": 5.2057142857142855,
      "grad_norm": 0.43424803018569946,
      "learning_rate": 1.3059047619047621e-05,
      "loss": 0.5926,
      "step": 18220
    },
    {
      "epoch": 5.208571428571428,
      "grad_norm": 0.5031774044036865,
      "learning_rate": 1.3055238095238096e-05,
      "loss": 0.6943,
      "step": 18230
    },
    {
      "epoch": 5.211428571428572,
      "grad_norm": 0.5241846442222595,
      "learning_rate": 1.3051428571428572e-05,
      "loss": 0.5719,
      "step": 18240
    },
    {
      "epoch": 5.214285714285714,
      "grad_norm": 10.880953788757324,
      "learning_rate": 1.304761904761905e-05,
      "loss": 0.4547,
      "step": 18250
    },
    {
      "epoch": 5.217142857142857,
      "grad_norm": 21.897340774536133,
      "learning_rate": 1.3043809523809525e-05,
      "loss": 0.7841,
      "step": 18260
    },
    {
      "epoch": 5.22,
      "grad_norm": 10.717020988464355,
      "learning_rate": 1.3040000000000002e-05,
      "loss": 0.3372,
      "step": 18270
    },
    {
      "epoch": 5.222857142857142,
      "grad_norm": 0.6163255572319031,
      "learning_rate": 1.3036190476190478e-05,
      "loss": 0.4458,
      "step": 18280
    },
    {
      "epoch": 5.225714285714286,
      "grad_norm": 0.7021677494049072,
      "learning_rate": 1.3032380952380953e-05,
      "loss": 0.8624,
      "step": 18290
    },
    {
      "epoch": 5.228571428571429,
      "grad_norm": 10.584548950195312,
      "learning_rate": 1.302857142857143e-05,
      "loss": 0.6308,
      "step": 18300
    },
    {
      "epoch": 5.231428571428571,
      "grad_norm": 0.7909708023071289,
      "learning_rate": 1.3024761904761906e-05,
      "loss": 0.3193,
      "step": 18310
    },
    {
      "epoch": 5.234285714285714,
      "grad_norm": 0.6365761160850525,
      "learning_rate": 1.3020952380952384e-05,
      "loss": 0.327,
      "step": 18320
    },
    {
      "epoch": 5.2371428571428575,
      "grad_norm": 0.7175065875053406,
      "learning_rate": 1.3017142857142859e-05,
      "loss": 0.5374,
      "step": 18330
    },
    {
      "epoch": 5.24,
      "grad_norm": 21.850303649902344,
      "learning_rate": 1.3013333333333333e-05,
      "loss": 0.6472,
      "step": 18340
    },
    {
      "epoch": 5.242857142857143,
      "grad_norm": 0.7392436265945435,
      "learning_rate": 1.300952380952381e-05,
      "loss": 0.5373,
      "step": 18350
    },
    {
      "epoch": 5.2457142857142856,
      "grad_norm": 10.555657386779785,
      "learning_rate": 1.3005714285714286e-05,
      "loss": 0.4246,
      "step": 18360
    },
    {
      "epoch": 5.248571428571428,
      "grad_norm": 0.8021770119667053,
      "learning_rate": 1.3001904761904761e-05,
      "loss": 0.8323,
      "step": 18370
    },
    {
      "epoch": 5.251428571428572,
      "grad_norm": 10.522753715515137,
      "learning_rate": 1.2998095238095239e-05,
      "loss": 0.5109,
      "step": 18380
    },
    {
      "epoch": 5.2542857142857144,
      "grad_norm": 21.95332145690918,
      "learning_rate": 1.2994285714285714e-05,
      "loss": 0.5998,
      "step": 18390
    },
    {
      "epoch": 5.257142857142857,
      "grad_norm": 1.0683846473693848,
      "learning_rate": 1.2990476190476192e-05,
      "loss": 0.8727,
      "step": 18400
    },
    {
      "epoch": 5.26,
      "grad_norm": 1.024688482284546,
      "learning_rate": 1.2986666666666667e-05,
      "loss": 0.3969,
      "step": 18410
    },
    {
      "epoch": 5.2628571428571425,
      "grad_norm": 21.615955352783203,
      "learning_rate": 1.2982857142857144e-05,
      "loss": 0.893,
      "step": 18420
    },
    {
      "epoch": 5.265714285714286,
      "grad_norm": 0.9251007437705994,
      "learning_rate": 1.297904761904762e-05,
      "loss": 0.3133,
      "step": 18430
    },
    {
      "epoch": 5.268571428571429,
      "grad_norm": 0.9271954894065857,
      "learning_rate": 1.2975238095238096e-05,
      "loss": 0.6049,
      "step": 18440
    },
    {
      "epoch": 5.271428571428571,
      "grad_norm": 10.37197494506836,
      "learning_rate": 1.2971428571428573e-05,
      "loss": 0.7925,
      "step": 18450
    },
    {
      "epoch": 5.274285714285714,
      "grad_norm": 10.459189414978027,
      "learning_rate": 1.2967619047619048e-05,
      "loss": 0.5951,
      "step": 18460
    },
    {
      "epoch": 5.277142857142858,
      "grad_norm": 0.9533594846725464,
      "learning_rate": 1.2963809523809526e-05,
      "loss": 0.5995,
      "step": 18470
    },
    {
      "epoch": 5.28,
      "grad_norm": 0.9249082207679749,
      "learning_rate": 1.2960000000000001e-05,
      "loss": 0.6967,
      "step": 18480
    },
    {
      "epoch": 5.282857142857143,
      "grad_norm": 10.58953857421875,
      "learning_rate": 1.2956190476190479e-05,
      "loss": 0.5985,
      "step": 18490
    },
    {
      "epoch": 5.285714285714286,
      "grad_norm": 10.44925594329834,
      "learning_rate": 1.2952380952380954e-05,
      "loss": 0.6891,
      "step": 18500
    },
    {
      "epoch": 5.288571428571428,
      "grad_norm": 0.8723795413970947,
      "learning_rate": 1.294857142857143e-05,
      "loss": 0.214,
      "step": 18510
    },
    {
      "epoch": 5.291428571428572,
      "grad_norm": 0.6146700978279114,
      "learning_rate": 1.2944761904761907e-05,
      "loss": 0.1168,
      "step": 18520
    },
    {
      "epoch": 5.2942857142857145,
      "grad_norm": 10.75143814086914,
      "learning_rate": 1.2940952380952383e-05,
      "loss": 0.5577,
      "step": 18530
    },
    {
      "epoch": 5.297142857142857,
      "grad_norm": 10.8002290725708,
      "learning_rate": 1.293714285714286e-05,
      "loss": 0.5653,
      "step": 18540
    },
    {
      "epoch": 5.3,
      "grad_norm": 22.052507400512695,
      "learning_rate": 1.2933333333333334e-05,
      "loss": 0.7861,
      "step": 18550
    },
    {
      "epoch": 5.3028571428571425,
      "grad_norm": 10.79969596862793,
      "learning_rate": 1.292952380952381e-05,
      "loss": 0.4586,
      "step": 18560
    },
    {
      "epoch": 5.305714285714286,
      "grad_norm": 0.5008804202079773,
      "learning_rate": 1.2925714285714287e-05,
      "loss": 0.3487,
      "step": 18570
    },
    {
      "epoch": 5.308571428571429,
      "grad_norm": 10.940813064575195,
      "learning_rate": 1.2921904761904762e-05,
      "loss": 0.7989,
      "step": 18580
    },
    {
      "epoch": 5.311428571428571,
      "grad_norm": 10.852249145507812,
      "learning_rate": 1.2918095238095238e-05,
      "loss": 0.4529,
      "step": 18590
    },
    {
      "epoch": 5.314285714285714,
      "grad_norm": 0.6140546798706055,
      "learning_rate": 1.2914285714285715e-05,
      "loss": 0.6603,
      "step": 18600
    },
    {
      "epoch": 5.317142857142857,
      "grad_norm": 10.673856735229492,
      "learning_rate": 1.291047619047619e-05,
      "loss": 0.5523,
      "step": 18610
    },
    {
      "epoch": 5.32,
      "grad_norm": 10.727333068847656,
      "learning_rate": 1.2906666666666668e-05,
      "loss": 0.3316,
      "step": 18620
    },
    {
      "epoch": 5.322857142857143,
      "grad_norm": 11.184430122375488,
      "learning_rate": 1.2902857142857143e-05,
      "loss": 0.8676,
      "step": 18630
    },
    {
      "epoch": 5.325714285714286,
      "grad_norm": 0.7084019780158997,
      "learning_rate": 1.289904761904762e-05,
      "loss": 0.4385,
      "step": 18640
    },
    {
      "epoch": 5.328571428571428,
      "grad_norm": 0.6572547554969788,
      "learning_rate": 1.2895238095238096e-05,
      "loss": 0.2244,
      "step": 18650
    },
    {
      "epoch": 5.331428571428571,
      "grad_norm": 10.66816234588623,
      "learning_rate": 1.2891428571428572e-05,
      "loss": 0.4393,
      "step": 18660
    },
    {
      "epoch": 5.3342857142857145,
      "grad_norm": 0.6754223704338074,
      "learning_rate": 1.288761904761905e-05,
      "loss": 0.4356,
      "step": 18670
    },
    {
      "epoch": 5.337142857142857,
      "grad_norm": 0.5934814810752869,
      "learning_rate": 1.2883809523809525e-05,
      "loss": 0.2303,
      "step": 18680
    },
    {
      "epoch": 5.34,
      "grad_norm": 22.045007705688477,
      "learning_rate": 1.2880000000000002e-05,
      "loss": 0.6736,
      "step": 18690
    },
    {
      "epoch": 5.3428571428571425,
      "grad_norm": 11.278748512268066,
      "learning_rate": 1.2876190476190478e-05,
      "loss": 0.5659,
      "step": 18700
    },
    {
      "epoch": 5.345714285714286,
      "grad_norm": 0.5723013877868652,
      "learning_rate": 1.2872380952380953e-05,
      "loss": 0.8903,
      "step": 18710
    },
    {
      "epoch": 5.348571428571429,
      "grad_norm": 0.6155781149864197,
      "learning_rate": 1.286857142857143e-05,
      "loss": 0.2275,
      "step": 18720
    },
    {
      "epoch": 5.351428571428571,
      "grad_norm": 0.643990159034729,
      "learning_rate": 1.2864761904761906e-05,
      "loss": 0.8662,
      "step": 18730
    },
    {
      "epoch": 5.354285714285714,
      "grad_norm": 10.719563484191895,
      "learning_rate": 1.2860952380952383e-05,
      "loss": 0.7502,
      "step": 18740
    },
    {
      "epoch": 5.357142857142857,
      "grad_norm": 10.639209747314453,
      "learning_rate": 1.2857142857142859e-05,
      "loss": 0.3269,
      "step": 18750
    },
    {
      "epoch": 5.36,
      "grad_norm": 11.893819808959961,
      "learning_rate": 1.2853333333333336e-05,
      "loss": 0.5284,
      "step": 18760
    },
    {
      "epoch": 5.362857142857143,
      "grad_norm": 0.7502291798591614,
      "learning_rate": 1.284952380952381e-05,
      "loss": 0.4236,
      "step": 18770
    },
    {
      "epoch": 5.365714285714286,
      "grad_norm": 0.7680897116661072,
      "learning_rate": 1.2845714285714286e-05,
      "loss": 0.627,
      "step": 18780
    },
    {
      "epoch": 5.368571428571428,
      "grad_norm": 0.7311438918113708,
      "learning_rate": 1.2841904761904763e-05,
      "loss": 0.2211,
      "step": 18790
    },
    {
      "epoch": 5.371428571428572,
      "grad_norm": 10.76293659210205,
      "learning_rate": 1.2838095238095239e-05,
      "loss": 0.5579,
      "step": 18800
    },
    {
      "epoch": 5.3742857142857146,
      "grad_norm": 0.5358253121376038,
      "learning_rate": 1.2834285714285714e-05,
      "loss": 0.3396,
      "step": 18810
    },
    {
      "epoch": 5.377142857142857,
      "grad_norm": 0.5239452123641968,
      "learning_rate": 1.2830476190476191e-05,
      "loss": 0.6737,
      "step": 18820
    },
    {
      "epoch": 5.38,
      "grad_norm": 11.003396987915039,
      "learning_rate": 1.2826666666666667e-05,
      "loss": 0.6776,
      "step": 18830
    },
    {
      "epoch": 5.382857142857143,
      "grad_norm": 10.696049690246582,
      "learning_rate": 1.2822857142857144e-05,
      "loss": 0.9719,
      "step": 18840
    },
    {
      "epoch": 5.385714285714286,
      "grad_norm": 0.7296628355979919,
      "learning_rate": 1.281904761904762e-05,
      "loss": 0.3223,
      "step": 18850
    },
    {
      "epoch": 5.388571428571429,
      "grad_norm": 10.584753036499023,
      "learning_rate": 1.2815238095238095e-05,
      "loss": 0.6263,
      "step": 18860
    },
    {
      "epoch": 5.3914285714285715,
      "grad_norm": 0.7583431601524353,
      "learning_rate": 1.2811428571428573e-05,
      "loss": 0.2202,
      "step": 18870
    },
    {
      "epoch": 5.394285714285714,
      "grad_norm": 0.6932529807090759,
      "learning_rate": 1.2807619047619048e-05,
      "loss": 0.4305,
      "step": 18880
    },
    {
      "epoch": 5.397142857142857,
      "grad_norm": 0.625179648399353,
      "learning_rate": 1.2803809523809526e-05,
      "loss": 0.4367,
      "step": 18890
    },
    {
      "epoch": 5.4,
      "grad_norm": 10.865096092224121,
      "learning_rate": 1.2800000000000001e-05,
      "loss": 0.6721,
      "step": 18900
    },
    {
      "epoch": 5.402857142857143,
      "grad_norm": 0.611808180809021,
      "learning_rate": 1.2796190476190478e-05,
      "loss": 0.7682,
      "step": 18910
    },
    {
      "epoch": 5.405714285714286,
      "grad_norm": 0.7870530486106873,
      "learning_rate": 1.2792380952380954e-05,
      "loss": 0.8455,
      "step": 18920
    },
    {
      "epoch": 5.408571428571428,
      "grad_norm": 0.7814875841140747,
      "learning_rate": 1.278857142857143e-05,
      "loss": 0.4181,
      "step": 18930
    },
    {
      "epoch": 5.411428571428571,
      "grad_norm": 0.7108253836631775,
      "learning_rate": 1.2784761904761907e-05,
      "loss": 0.3248,
      "step": 18940
    },
    {
      "epoch": 5.414285714285715,
      "grad_norm": 0.6814689636230469,
      "learning_rate": 1.2780952380952382e-05,
      "loss": 0.7431,
      "step": 18950
    },
    {
      "epoch": 5.417142857142857,
      "grad_norm": 0.7126937508583069,
      "learning_rate": 1.277714285714286e-05,
      "loss": 0.5379,
      "step": 18960
    },
    {
      "epoch": 5.42,
      "grad_norm": 0.778293251991272,
      "learning_rate": 1.2773333333333335e-05,
      "loss": 0.5269,
      "step": 18970
    },
    {
      "epoch": 5.422857142857143,
      "grad_norm": 0.8084923028945923,
      "learning_rate": 1.2769523809523811e-05,
      "loss": 0.1161,
      "step": 18980
    },
    {
      "epoch": 5.425714285714285,
      "grad_norm": 21.94991111755371,
      "learning_rate": 1.2765714285714286e-05,
      "loss": 1.0767,
      "step": 18990
    },
    {
      "epoch": 5.428571428571429,
      "grad_norm": 0.6307471990585327,
      "learning_rate": 1.2761904761904762e-05,
      "loss": 0.4437,
      "step": 19000
    },
    {
      "epoch": 5.4314285714285715,
      "grad_norm": 10.64063835144043,
      "learning_rate": 1.2758095238095238e-05,
      "loss": 0.65,
      "step": 19010
    },
    {
      "epoch": 5.434285714285714,
      "grad_norm": 10.679973602294922,
      "learning_rate": 1.2754285714285715e-05,
      "loss": 0.5357,
      "step": 19020
    },
    {
      "epoch": 5.437142857142857,
      "grad_norm": 0.9276805520057678,
      "learning_rate": 1.275047619047619e-05,
      "loss": 0.8155,
      "step": 19030
    },
    {
      "epoch": 5.44,
      "grad_norm": 0.9751114845275879,
      "learning_rate": 1.2746666666666668e-05,
      "loss": 0.5011,
      "step": 19040
    },
    {
      "epoch": 5.442857142857143,
      "grad_norm": 0.9661726951599121,
      "learning_rate": 1.2742857142857143e-05,
      "loss": 0.4019,
      "step": 19050
    },
    {
      "epoch": 5.445714285714286,
      "grad_norm": 10.623785018920898,
      "learning_rate": 1.273904761904762e-05,
      "loss": 0.796,
      "step": 19060
    },
    {
      "epoch": 5.448571428571428,
      "grad_norm": 1.0067299604415894,
      "learning_rate": 1.2735238095238096e-05,
      "loss": 0.5959,
      "step": 19070
    },
    {
      "epoch": 5.451428571428571,
      "grad_norm": 10.63843059539795,
      "learning_rate": 1.2731428571428572e-05,
      "loss": 0.5883,
      "step": 19080
    },
    {
      "epoch": 5.454285714285715,
      "grad_norm": 1.109858751296997,
      "learning_rate": 1.2727619047619049e-05,
      "loss": 0.3987,
      "step": 19090
    },
    {
      "epoch": 5.457142857142857,
      "grad_norm": 1.059748649597168,
      "learning_rate": 1.2723809523809525e-05,
      "loss": 0.581,
      "step": 19100
    },
    {
      "epoch": 5.46,
      "grad_norm": 10.357089042663574,
      "learning_rate": 1.2720000000000002e-05,
      "loss": 0.7812,
      "step": 19110
    },
    {
      "epoch": 5.462857142857143,
      "grad_norm": 32.7852897644043,
      "learning_rate": 1.2716190476190477e-05,
      "loss": 0.8612,
      "step": 19120
    },
    {
      "epoch": 5.465714285714285,
      "grad_norm": 10.179414749145508,
      "learning_rate": 1.2712380952380953e-05,
      "loss": 0.8452,
      "step": 19130
    },
    {
      "epoch": 5.468571428571429,
      "grad_norm": 10.109710693359375,
      "learning_rate": 1.270857142857143e-05,
      "loss": 0.6478,
      "step": 19140
    },
    {
      "epoch": 5.4714285714285715,
      "grad_norm": 21.22779655456543,
      "learning_rate": 1.2704761904761906e-05,
      "loss": 0.4636,
      "step": 19150
    },
    {
      "epoch": 5.474285714285714,
      "grad_norm": 10.74665355682373,
      "learning_rate": 1.2700952380952383e-05,
      "loss": 0.5541,
      "step": 19160
    },
    {
      "epoch": 5.477142857142857,
      "grad_norm": 0.9786840081214905,
      "learning_rate": 1.2697142857142859e-05,
      "loss": 0.3064,
      "step": 19170
    },
    {
      "epoch": 5.48,
      "grad_norm": 0.7518486380577087,
      "learning_rate": 1.2693333333333336e-05,
      "loss": 0.2212,
      "step": 19180
    },
    {
      "epoch": 5.482857142857143,
      "grad_norm": 0.7100573778152466,
      "learning_rate": 1.2689523809523812e-05,
      "loss": 0.6348,
      "step": 19190
    },
    {
      "epoch": 5.485714285714286,
      "grad_norm": 10.52354621887207,
      "learning_rate": 1.2685714285714286e-05,
      "loss": 0.93,
      "step": 19200
    },
    {
      "epoch": 5.488571428571428,
      "grad_norm": 0.8599280118942261,
      "learning_rate": 1.2681904761904763e-05,
      "loss": 0.4158,
      "step": 19210
    },
    {
      "epoch": 5.491428571428571,
      "grad_norm": 0.8165996670722961,
      "learning_rate": 1.2678095238095238e-05,
      "loss": 0.9184,
      "step": 19220
    },
    {
      "epoch": 5.494285714285715,
      "grad_norm": 10.895097732543945,
      "learning_rate": 1.2674285714285714e-05,
      "loss": 0.4201,
      "step": 19230
    },
    {
      "epoch": 5.497142857142857,
      "grad_norm": 10.488607406616211,
      "learning_rate": 1.2670476190476191e-05,
      "loss": 0.9212,
      "step": 19240
    },
    {
      "epoch": 5.5,
      "grad_norm": 0.7868258357048035,
      "learning_rate": 1.2666666666666667e-05,
      "loss": 0.2173,
      "step": 19250
    },
    {
      "epoch": 5.502857142857143,
      "grad_norm": 10.634404182434082,
      "learning_rate": 1.2662857142857144e-05,
      "loss": 0.632,
      "step": 19260
    },
    {
      "epoch": 5.505714285714285,
      "grad_norm": 0.6868778467178345,
      "learning_rate": 1.265904761904762e-05,
      "loss": 0.5407,
      "step": 19270
    },
    {
      "epoch": 5.508571428571429,
      "grad_norm": 0.6902821063995361,
      "learning_rate": 1.2655238095238095e-05,
      "loss": 0.2229,
      "step": 19280
    },
    {
      "epoch": 5.511428571428572,
      "grad_norm": 22.25367546081543,
      "learning_rate": 1.2651428571428573e-05,
      "loss": 0.7637,
      "step": 19290
    },
    {
      "epoch": 5.514285714285714,
      "grad_norm": 0.669064462184906,
      "learning_rate": 1.2647619047619048e-05,
      "loss": 0.4378,
      "step": 19300
    },
    {
      "epoch": 5.517142857142857,
      "grad_norm": 0.6375569105148315,
      "learning_rate": 1.2643809523809525e-05,
      "loss": 0.3331,
      "step": 19310
    },
    {
      "epoch": 5.52,
      "grad_norm": 10.737848281860352,
      "learning_rate": 1.2640000000000001e-05,
      "loss": 0.2294,
      "step": 19320
    },
    {
      "epoch": 5.522857142857143,
      "grad_norm": 10.805898666381836,
      "learning_rate": 1.2636190476190478e-05,
      "loss": 0.4568,
      "step": 19330
    },
    {
      "epoch": 5.525714285714286,
      "grad_norm": 11.061863899230957,
      "learning_rate": 1.2632380952380954e-05,
      "loss": 0.6797,
      "step": 19340
    },
    {
      "epoch": 5.5285714285714285,
      "grad_norm": 10.686187744140625,
      "learning_rate": 1.262857142857143e-05,
      "loss": 0.8773,
      "step": 19350
    },
    {
      "epoch": 5.531428571428571,
      "grad_norm": 21.782060623168945,
      "learning_rate": 1.2624761904761907e-05,
      "loss": 1.0197,
      "step": 19360
    },
    {
      "epoch": 5.534285714285714,
      "grad_norm": 0.8992487192153931,
      "learning_rate": 1.2620952380952382e-05,
      "loss": 0.2148,
      "step": 19370
    },
    {
      "epoch": 5.537142857142857,
      "grad_norm": 0.8723887801170349,
      "learning_rate": 1.261714285714286e-05,
      "loss": 1.0002,
      "step": 19380
    },
    {
      "epoch": 5.54,
      "grad_norm": 0.7322208881378174,
      "learning_rate": 1.2613333333333335e-05,
      "loss": 0.4289,
      "step": 19390
    },
    {
      "epoch": 5.542857142857143,
      "grad_norm": 10.55689811706543,
      "learning_rate": 1.260952380952381e-05,
      "loss": 1.228,
      "step": 19400
    },
    {
      "epoch": 5.545714285714285,
      "grad_norm": 1.0981988906860352,
      "learning_rate": 1.2605714285714288e-05,
      "loss": 0.5984,
      "step": 19410
    },
    {
      "epoch": 5.548571428571429,
      "grad_norm": 10.290119171142578,
      "learning_rate": 1.2601904761904762e-05,
      "loss": 0.584,
      "step": 19420
    },
    {
      "epoch": 5.551428571428572,
      "grad_norm": 1.15097975730896,
      "learning_rate": 1.2598095238095237e-05,
      "loss": 0.9663,
      "step": 19430
    },
    {
      "epoch": 5.554285714285714,
      "grad_norm": 1.2552292346954346,
      "learning_rate": 1.2594285714285715e-05,
      "loss": 0.3893,
      "step": 19440
    },
    {
      "epoch": 5.557142857142857,
      "grad_norm": 1.1254328489303589,
      "learning_rate": 1.259047619047619e-05,
      "loss": 0.2987,
      "step": 19450
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 0.9999490976333618,
      "learning_rate": 1.2586666666666668e-05,
      "loss": 0.3944,
      "step": 19460
    },
    {
      "epoch": 5.562857142857143,
      "grad_norm": 0.9187194108963013,
      "learning_rate": 1.2582857142857143e-05,
      "loss": 0.604,
      "step": 19470
    },
    {
      "epoch": 5.565714285714286,
      "grad_norm": 0.8332972526550293,
      "learning_rate": 1.257904761904762e-05,
      "loss": 0.3148,
      "step": 19480
    },
    {
      "epoch": 5.5685714285714285,
      "grad_norm": 0.7077900767326355,
      "learning_rate": 1.2575238095238096e-05,
      "loss": 0.2148,
      "step": 19490
    },
    {
      "epoch": 5.571428571428571,
      "grad_norm": 0.5356255769729614,
      "learning_rate": 1.2571428571428572e-05,
      "loss": 0.332,
      "step": 19500
    },
    {
      "epoch": 5.574285714285715,
      "grad_norm": 0.5421392321586609,
      "learning_rate": 1.2567619047619049e-05,
      "loss": 0.7913,
      "step": 19510
    },
    {
      "epoch": 5.577142857142857,
      "grad_norm": 0.6254854202270508,
      "learning_rate": 1.2563809523809524e-05,
      "loss": 0.4452,
      "step": 19520
    },
    {
      "epoch": 5.58,
      "grad_norm": 0.5136846899986267,
      "learning_rate": 1.2560000000000002e-05,
      "loss": 0.5738,
      "step": 19530
    },
    {
      "epoch": 5.582857142857143,
      "grad_norm": 0.561748206615448,
      "learning_rate": 1.2556190476190477e-05,
      "loss": 0.4539,
      "step": 19540
    },
    {
      "epoch": 5.585714285714285,
      "grad_norm": 11.094318389892578,
      "learning_rate": 1.2552380952380953e-05,
      "loss": 1.2022,
      "step": 19550
    },
    {
      "epoch": 5.588571428571429,
      "grad_norm": 11.052270889282227,
      "learning_rate": 1.254857142857143e-05,
      "loss": 0.7279,
      "step": 19560
    },
    {
      "epoch": 5.591428571428572,
      "grad_norm": 0.8462009429931641,
      "learning_rate": 1.2544761904761906e-05,
      "loss": 0.4163,
      "step": 19570
    },
    {
      "epoch": 5.594285714285714,
      "grad_norm": 0.765353262424469,
      "learning_rate": 1.2540952380952383e-05,
      "loss": 0.4203,
      "step": 19580
    },
    {
      "epoch": 5.597142857142857,
      "grad_norm": 0.6437896490097046,
      "learning_rate": 1.2537142857142859e-05,
      "loss": 0.1208,
      "step": 19590
    },
    {
      "epoch": 5.6,
      "grad_norm": 22.20798110961914,
      "learning_rate": 1.2533333333333336e-05,
      "loss": 0.6798,
      "step": 19600
    },
    {
      "epoch": 5.602857142857143,
      "grad_norm": 10.86211109161377,
      "learning_rate": 1.2529523809523811e-05,
      "loss": 0.5602,
      "step": 19610
    },
    {
      "epoch": 5.605714285714286,
      "grad_norm": 0.5780025124549866,
      "learning_rate": 1.2525714285714287e-05,
      "loss": 0.5614,
      "step": 19620
    },
    {
      "epoch": 5.6085714285714285,
      "grad_norm": 0.5455995798110962,
      "learning_rate": 1.2521904761904764e-05,
      "loss": 0.4506,
      "step": 19630
    },
    {
      "epoch": 5.611428571428571,
      "grad_norm": 10.820096015930176,
      "learning_rate": 1.2518095238095238e-05,
      "loss": 0.6786,
      "step": 19640
    },
    {
      "epoch": 5.614285714285714,
      "grad_norm": 0.6537089943885803,
      "learning_rate": 1.2514285714285714e-05,
      "loss": 0.5571,
      "step": 19650
    },
    {
      "epoch": 5.617142857142857,
      "grad_norm": 0.7275430560112,
      "learning_rate": 1.2510476190476191e-05,
      "loss": 0.7474,
      "step": 19660
    },
    {
      "epoch": 5.62,
      "grad_norm": 0.6575480699539185,
      "learning_rate": 1.2506666666666667e-05,
      "loss": 0.4357,
      "step": 19670
    },
    {
      "epoch": 5.622857142857143,
      "grad_norm": 0.6502165198326111,
      "learning_rate": 1.2502857142857144e-05,
      "loss": 0.7657,
      "step": 19680
    },
    {
      "epoch": 5.6257142857142854,
      "grad_norm": 10.759099960327148,
      "learning_rate": 1.249904761904762e-05,
      "loss": 0.3377,
      "step": 19690
    },
    {
      "epoch": 5.628571428571428,
      "grad_norm": 0.43837228417396545,
      "learning_rate": 1.2495238095238095e-05,
      "loss": 0.1224,
      "step": 19700
    },
    {
      "epoch": 5.631428571428572,
      "grad_norm": 0.5044270157814026,
      "learning_rate": 1.2491428571428572e-05,
      "loss": 0.818,
      "step": 19710
    },
    {
      "epoch": 5.634285714285714,
      "grad_norm": 0.6575421094894409,
      "learning_rate": 1.2487619047619048e-05,
      "loss": 0.5525,
      "step": 19720
    },
    {
      "epoch": 5.637142857142857,
      "grad_norm": 10.64367961883545,
      "learning_rate": 1.2483809523809525e-05,
      "loss": 1.2702,
      "step": 19730
    },
    {
      "epoch": 5.64,
      "grad_norm": 10.671274185180664,
      "learning_rate": 1.248e-05,
      "loss": 0.1205,
      "step": 19740
    },
    {
      "epoch": 5.642857142857143,
      "grad_norm": 0.6879305839538574,
      "learning_rate": 1.2476190476190478e-05,
      "loss": 0.7459,
      "step": 19750
    },
    {
      "epoch": 5.645714285714286,
      "grad_norm": 10.756207466125488,
      "learning_rate": 1.2472380952380954e-05,
      "loss": 0.6388,
      "step": 19760
    },
    {
      "epoch": 5.648571428571429,
      "grad_norm": 0.7496182322502136,
      "learning_rate": 1.246857142857143e-05,
      "loss": 0.8491,
      "step": 19770
    },
    {
      "epoch": 5.651428571428571,
      "grad_norm": 11.097630500793457,
      "learning_rate": 1.2464761904761907e-05,
      "loss": 0.4252,
      "step": 19780
    },
    {
      "epoch": 5.654285714285714,
      "grad_norm": 0.6547803282737732,
      "learning_rate": 1.2460952380952382e-05,
      "loss": 0.2226,
      "step": 19790
    },
    {
      "epoch": 5.6571428571428575,
      "grad_norm": 0.6144382357597351,
      "learning_rate": 1.245714285714286e-05,
      "loss": 0.2245,
      "step": 19800
    },
    {
      "epoch": 5.66,
      "grad_norm": 22.255094528198242,
      "learning_rate": 1.2453333333333335e-05,
      "loss": 0.4499,
      "step": 19810
    },
    {
      "epoch": 5.662857142857143,
      "grad_norm": 0.520228385925293,
      "learning_rate": 1.244952380952381e-05,
      "loss": 0.2344,
      "step": 19820
    },
    {
      "epoch": 5.6657142857142855,
      "grad_norm": 0.4994488060474396,
      "learning_rate": 1.2445714285714288e-05,
      "loss": 0.6862,
      "step": 19830
    },
    {
      "epoch": 5.668571428571429,
      "grad_norm": 10.797749519348145,
      "learning_rate": 1.2441904761904763e-05,
      "loss": 0.9059,
      "step": 19840
    },
    {
      "epoch": 5.671428571428572,
      "grad_norm": 0.5680460929870605,
      "learning_rate": 1.243809523809524e-05,
      "loss": 0.3414,
      "step": 19850
    },
    {
      "epoch": 5.674285714285714,
      "grad_norm": 0.5447551012039185,
      "learning_rate": 1.2434285714285715e-05,
      "loss": 0.452,
      "step": 19860
    },
    {
      "epoch": 5.677142857142857,
      "grad_norm": 10.78134822845459,
      "learning_rate": 1.243047619047619e-05,
      "loss": 1.0974,
      "step": 19870
    },
    {
      "epoch": 5.68,
      "grad_norm": 0.735197901725769,
      "learning_rate": 1.2426666666666667e-05,
      "loss": 0.6409,
      "step": 19880
    },
    {
      "epoch": 5.682857142857143,
      "grad_norm": 0.8754127621650696,
      "learning_rate": 1.2422857142857143e-05,
      "loss": 0.7153,
      "step": 19890
    },
    {
      "epoch": 5.685714285714286,
      "grad_norm": 10.524164199829102,
      "learning_rate": 1.241904761904762e-05,
      "loss": 0.6972,
      "step": 19900
    },
    {
      "epoch": 5.688571428571429,
      "grad_norm": 21.5979061126709,
      "learning_rate": 1.2415238095238096e-05,
      "loss": 0.4957,
      "step": 19910
    },
    {
      "epoch": 5.691428571428571,
      "grad_norm": 0.8200629949569702,
      "learning_rate": 1.2411428571428571e-05,
      "loss": 0.1168,
      "step": 19920
    },
    {
      "epoch": 5.694285714285714,
      "grad_norm": 0.6359840035438538,
      "learning_rate": 1.2407619047619049e-05,
      "loss": 0.4398,
      "step": 19930
    },
    {
      "epoch": 5.6971428571428575,
      "grad_norm": 10.770066261291504,
      "learning_rate": 1.2403809523809524e-05,
      "loss": 0.6558,
      "step": 19940
    },
    {
      "epoch": 5.7,
      "grad_norm": 11.060047149658203,
      "learning_rate": 1.2400000000000002e-05,
      "loss": 0.3326,
      "step": 19950
    },
    {
      "epoch": 5.702857142857143,
      "grad_norm": 0.7061838507652283,
      "learning_rate": 1.2396190476190477e-05,
      "loss": 0.5547,
      "step": 19960
    },
    {
      "epoch": 5.7057142857142855,
      "grad_norm": 0.7245761156082153,
      "learning_rate": 1.2392380952380953e-05,
      "loss": 0.3246,
      "step": 19970
    },
    {
      "epoch": 5.708571428571428,
      "grad_norm": 0.7580600380897522,
      "learning_rate": 1.238857142857143e-05,
      "loss": 0.7349,
      "step": 19980
    },
    {
      "epoch": 5.711428571428572,
      "grad_norm": 0.8340325355529785,
      "learning_rate": 1.2384761904761906e-05,
      "loss": 0.5293,
      "step": 19990
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 10.861990928649902,
      "learning_rate": 1.2380952380952383e-05,
      "loss": 0.6169,
      "step": 20000
    },
    {
      "epoch": 5.717142857142857,
      "grad_norm": 1.0491163730621338,
      "learning_rate": 1.2377142857142858e-05,
      "loss": 0.7002,
      "step": 20010
    },
    {
      "epoch": 5.72,
      "grad_norm": 1.0254114866256714,
      "learning_rate": 1.2373333333333336e-05,
      "loss": 0.4006,
      "step": 20020
    },
    {
      "epoch": 5.722857142857142,
      "grad_norm": 0.9863917827606201,
      "learning_rate": 1.2369523809523811e-05,
      "loss": 0.4952,
      "step": 20030
    },
    {
      "epoch": 5.725714285714286,
      "grad_norm": 10.557242393493652,
      "learning_rate": 1.2365714285714287e-05,
      "loss": 0.4143,
      "step": 20040
    },
    {
      "epoch": 5.728571428571429,
      "grad_norm": 0.7938615083694458,
      "learning_rate": 1.2361904761904764e-05,
      "loss": 0.833,
      "step": 20050
    },
    {
      "epoch": 5.731428571428571,
      "grad_norm": 22.253307342529297,
      "learning_rate": 1.235809523809524e-05,
      "loss": 0.521,
      "step": 20060
    },
    {
      "epoch": 5.734285714285714,
      "grad_norm": 10.716880798339844,
      "learning_rate": 1.2354285714285714e-05,
      "loss": 0.5375,
      "step": 20070
    },
    {
      "epoch": 5.737142857142857,
      "grad_norm": 0.7360202074050903,
      "learning_rate": 1.2350476190476191e-05,
      "loss": 0.9471,
      "step": 20080
    },
    {
      "epoch": 5.74,
      "grad_norm": 0.7470659017562866,
      "learning_rate": 1.2346666666666666e-05,
      "loss": 0.4252,
      "step": 20090
    },
    {
      "epoch": 5.742857142857143,
      "grad_norm": 10.631601333618164,
      "learning_rate": 1.2342857142857144e-05,
      "loss": 0.7286,
      "step": 20100
    },
    {
      "epoch": 5.7457142857142856,
      "grad_norm": 0.9580235481262207,
      "learning_rate": 1.233904761904762e-05,
      "loss": 0.6097,
      "step": 20110
    },
    {
      "epoch": 5.748571428571428,
      "grad_norm": 33.24342346191406,
      "learning_rate": 1.2335238095238095e-05,
      "loss": 0.7045,
      "step": 20120
    },
    {
      "epoch": 5.751428571428572,
      "grad_norm": 0.7040747404098511,
      "learning_rate": 1.2331428571428572e-05,
      "loss": 0.4204,
      "step": 20130
    },
    {
      "epoch": 5.7542857142857144,
      "grad_norm": 10.565500259399414,
      "learning_rate": 1.2327619047619048e-05,
      "loss": 0.8403,
      "step": 20140
    },
    {
      "epoch": 5.757142857142857,
      "grad_norm": 10.628800392150879,
      "learning_rate": 1.2323809523809525e-05,
      "loss": 0.5209,
      "step": 20150
    },
    {
      "epoch": 5.76,
      "grad_norm": 0.9490132927894592,
      "learning_rate": 1.232e-05,
      "loss": 0.4079,
      "step": 20160
    },
    {
      "epoch": 5.762857142857143,
      "grad_norm": 21.561229705810547,
      "learning_rate": 1.2316190476190478e-05,
      "loss": 0.6895,
      "step": 20170
    },
    {
      "epoch": 5.765714285714286,
      "grad_norm": 10.591254234313965,
      "learning_rate": 1.2312380952380953e-05,
      "loss": 0.1195,
      "step": 20180
    },
    {
      "epoch": 5.768571428571429,
      "grad_norm": 10.516390800476074,
      "learning_rate": 1.2308571428571429e-05,
      "loss": 0.7202,
      "step": 20190
    },
    {
      "epoch": 5.771428571428571,
      "grad_norm": 0.8405025005340576,
      "learning_rate": 1.2304761904761906e-05,
      "loss": 0.3151,
      "step": 20200
    },
    {
      "epoch": 5.774285714285714,
      "grad_norm": 22.344934463500977,
      "learning_rate": 1.2300952380952382e-05,
      "loss": 0.5062,
      "step": 20210
    },
    {
      "epoch": 5.777142857142858,
      "grad_norm": 0.7022951245307922,
      "learning_rate": 1.229714285714286e-05,
      "loss": 0.3248,
      "step": 20220
    },
    {
      "epoch": 5.78,
      "grad_norm": 10.68222427368164,
      "learning_rate": 1.2293333333333335e-05,
      "loss": 0.5235,
      "step": 20230
    },
    {
      "epoch": 5.782857142857143,
      "grad_norm": 0.5644980072975159,
      "learning_rate": 1.228952380952381e-05,
      "loss": 0.3384,
      "step": 20240
    },
    {
      "epoch": 5.785714285714286,
      "grad_norm": 0.6324072480201721,
      "learning_rate": 1.2285714285714288e-05,
      "loss": 0.7787,
      "step": 20250
    },
    {
      "epoch": 5.788571428571428,
      "grad_norm": 10.964699745178223,
      "learning_rate": 1.2281904761904763e-05,
      "loss": 0.4378,
      "step": 20260
    },
    {
      "epoch": 5.791428571428572,
      "grad_norm": 0.6248789429664612,
      "learning_rate": 1.227809523809524e-05,
      "loss": 0.4421,
      "step": 20270
    },
    {
      "epoch": 5.7942857142857145,
      "grad_norm": 0.5551440119743347,
      "learning_rate": 1.2274285714285716e-05,
      "loss": 0.4482,
      "step": 20280
    },
    {
      "epoch": 5.797142857142857,
      "grad_norm": 0.5082250833511353,
      "learning_rate": 1.227047619047619e-05,
      "loss": 0.3487,
      "step": 20290
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.4548167884349823,
      "learning_rate": 1.2266666666666667e-05,
      "loss": 0.4697,
      "step": 20300
    },
    {
      "epoch": 5.8028571428571425,
      "grad_norm": 10.7891206741333,
      "learning_rate": 1.2262857142857143e-05,
      "loss": 0.803,
      "step": 20310
    },
    {
      "epoch": 5.805714285714286,
      "grad_norm": 0.49773210287094116,
      "learning_rate": 1.225904761904762e-05,
      "loss": 0.1243,
      "step": 20320
    },
    {
      "epoch": 5.808571428571429,
      "grad_norm": 0.48397502303123474,
      "learning_rate": 1.2255238095238096e-05,
      "loss": 0.4621,
      "step": 20330
    },
    {
      "epoch": 5.811428571428571,
      "grad_norm": 0.4623996615409851,
      "learning_rate": 1.2251428571428571e-05,
      "loss": 0.81,
      "step": 20340
    },
    {
      "epoch": 5.814285714285714,
      "grad_norm": 10.88432502746582,
      "learning_rate": 1.2247619047619049e-05,
      "loss": 0.2393,
      "step": 20350
    },
    {
      "epoch": 5.817142857142857,
      "grad_norm": 33.27132034301758,
      "learning_rate": 1.2243809523809524e-05,
      "loss": 0.5872,
      "step": 20360
    },
    {
      "epoch": 5.82,
      "grad_norm": 10.94927978515625,
      "learning_rate": 1.2240000000000001e-05,
      "loss": 0.9291,
      "step": 20370
    },
    {
      "epoch": 5.822857142857143,
      "grad_norm": 10.926427841186523,
      "learning_rate": 1.2236190476190477e-05,
      "loss": 0.4575,
      "step": 20380
    },
    {
      "epoch": 5.825714285714286,
      "grad_norm": 10.899588584899902,
      "learning_rate": 1.2232380952380953e-05,
      "loss": 0.6768,
      "step": 20390
    },
    {
      "epoch": 5.828571428571428,
      "grad_norm": 10.78982925415039,
      "learning_rate": 1.222857142857143e-05,
      "loss": 0.5578,
      "step": 20400
    },
    {
      "epoch": 5.831428571428571,
      "grad_norm": 0.6669900417327881,
      "learning_rate": 1.2224761904761905e-05,
      "loss": 0.6546,
      "step": 20410
    },
    {
      "epoch": 5.8342857142857145,
      "grad_norm": 0.6755504012107849,
      "learning_rate": 1.2220952380952383e-05,
      "loss": 0.7544,
      "step": 20420
    },
    {
      "epoch": 5.837142857142857,
      "grad_norm": 0.6849616169929504,
      "learning_rate": 1.2217142857142858e-05,
      "loss": 0.537,
      "step": 20430
    },
    {
      "epoch": 5.84,
      "grad_norm": 10.678478240966797,
      "learning_rate": 1.2213333333333336e-05,
      "loss": 0.6366,
      "step": 20440
    },
    {
      "epoch": 5.8428571428571425,
      "grad_norm": 0.7161741256713867,
      "learning_rate": 1.2209523809523811e-05,
      "loss": 0.3333,
      "step": 20450
    },
    {
      "epoch": 5.845714285714286,
      "grad_norm": 21.913537979125977,
      "learning_rate": 1.2205714285714287e-05,
      "loss": 0.9635,
      "step": 20460
    },
    {
      "epoch": 5.848571428571429,
      "grad_norm": 0.6832902431488037,
      "learning_rate": 1.2201904761904764e-05,
      "loss": 0.3287,
      "step": 20470
    },
    {
      "epoch": 5.851428571428571,
      "grad_norm": 0.6735618114471436,
      "learning_rate": 1.219809523809524e-05,
      "loss": 0.6405,
      "step": 20480
    },
    {
      "epoch": 5.854285714285714,
      "grad_norm": 10.87511920928955,
      "learning_rate": 1.2194285714285717e-05,
      "loss": 1.1489,
      "step": 20490
    },
    {
      "epoch": 5.857142857142857,
      "grad_norm": 22.28876304626465,
      "learning_rate": 1.2190476190476192e-05,
      "loss": 0.611,
      "step": 20500
    },
    {
      "epoch": 5.86,
      "grad_norm": 10.332859992980957,
      "learning_rate": 1.2186666666666666e-05,
      "loss": 0.787,
      "step": 20510
    },
    {
      "epoch": 5.862857142857143,
      "grad_norm": 0.8159821629524231,
      "learning_rate": 1.2182857142857144e-05,
      "loss": 0.2148,
      "step": 20520
    },
    {
      "epoch": 5.865714285714286,
      "grad_norm": 0.6792160868644714,
      "learning_rate": 1.217904761904762e-05,
      "loss": 0.6344,
      "step": 20530
    },
    {
      "epoch": 5.868571428571428,
      "grad_norm": 0.6585412621498108,
      "learning_rate": 1.2175238095238095e-05,
      "loss": 0.4352,
      "step": 20540
    },
    {
      "epoch": 5.871428571428572,
      "grad_norm": 0.584159255027771,
      "learning_rate": 1.2171428571428572e-05,
      "loss": 0.226,
      "step": 20550
    },
    {
      "epoch": 5.8742857142857146,
      "grad_norm": 0.5609556436538696,
      "learning_rate": 1.2167619047619048e-05,
      "loss": 0.7735,
      "step": 20560
    },
    {
      "epoch": 5.877142857142857,
      "grad_norm": 10.790724754333496,
      "learning_rate": 1.2163809523809525e-05,
      "loss": 0.5587,
      "step": 20570
    },
    {
      "epoch": 5.88,
      "grad_norm": 10.743292808532715,
      "learning_rate": 1.216e-05,
      "loss": 0.6642,
      "step": 20580
    },
    {
      "epoch": 5.882857142857143,
      "grad_norm": 0.6486002802848816,
      "learning_rate": 1.2156190476190478e-05,
      "loss": 0.6565,
      "step": 20590
    },
    {
      "epoch": 5.885714285714286,
      "grad_norm": 10.699012756347656,
      "learning_rate": 1.2152380952380953e-05,
      "loss": 0.121,
      "step": 20600
    },
    {
      "epoch": 5.888571428571429,
      "grad_norm": 10.95222282409668,
      "learning_rate": 1.2148571428571429e-05,
      "loss": 1.1962,
      "step": 20610
    },
    {
      "epoch": 5.8914285714285715,
      "grad_norm": 0.7351663112640381,
      "learning_rate": 1.2144761904761906e-05,
      "loss": 0.748,
      "step": 20620
    },
    {
      "epoch": 5.894285714285714,
      "grad_norm": 0.7964010834693909,
      "learning_rate": 1.2140952380952382e-05,
      "loss": 0.2186,
      "step": 20630
    },
    {
      "epoch": 5.897142857142857,
      "grad_norm": 10.664700508117676,
      "learning_rate": 1.2137142857142859e-05,
      "loss": 0.3261,
      "step": 20640
    },
    {
      "epoch": 5.9,
      "grad_norm": 10.689352989196777,
      "learning_rate": 1.2133333333333335e-05,
      "loss": 0.7477,
      "step": 20650
    },
    {
      "epoch": 5.902857142857143,
      "grad_norm": 0.7539211511611938,
      "learning_rate": 1.212952380952381e-05,
      "loss": 0.5337,
      "step": 20660
    },
    {
      "epoch": 5.905714285714286,
      "grad_norm": 0.7277665734291077,
      "learning_rate": 1.2125714285714287e-05,
      "loss": 0.2203,
      "step": 20670
    },
    {
      "epoch": 5.908571428571428,
      "grad_norm": 0.6732584238052368,
      "learning_rate": 1.2121904761904763e-05,
      "loss": 0.6367,
      "step": 20680
    },
    {
      "epoch": 5.911428571428571,
      "grad_norm": 10.660794258117676,
      "learning_rate": 1.211809523809524e-05,
      "loss": 0.7545,
      "step": 20690
    },
    {
      "epoch": 5.914285714285715,
      "grad_norm": 0.723963737487793,
      "learning_rate": 1.2114285714285716e-05,
      "loss": 0.3229,
      "step": 20700
    },
    {
      "epoch": 5.917142857142857,
      "grad_norm": 0.6879907846450806,
      "learning_rate": 1.2110476190476193e-05,
      "loss": 1.0721,
      "step": 20710
    },
    {
      "epoch": 5.92,
      "grad_norm": 22.05548095703125,
      "learning_rate": 1.2106666666666667e-05,
      "loss": 0.5447,
      "step": 20720
    },
    {
      "epoch": 5.922857142857143,
      "grad_norm": 21.986801147460938,
      "learning_rate": 1.2102857142857143e-05,
      "loss": 0.6497,
      "step": 20730
    },
    {
      "epoch": 5.925714285714285,
      "grad_norm": 10.784363746643066,
      "learning_rate": 1.209904761904762e-05,
      "loss": 0.6515,
      "step": 20740
    },
    {
      "epoch": 5.928571428571429,
      "grad_norm": 10.610282897949219,
      "learning_rate": 1.2095238095238096e-05,
      "loss": 0.3274,
      "step": 20750
    },
    {
      "epoch": 5.9314285714285715,
      "grad_norm": 10.645112037658691,
      "learning_rate": 1.2091428571428571e-05,
      "loss": 0.329,
      "step": 20760
    },
    {
      "epoch": 5.934285714285714,
      "grad_norm": 10.69481372833252,
      "learning_rate": 1.2087619047619048e-05,
      "loss": 0.5411,
      "step": 20770
    },
    {
      "epoch": 5.937142857142857,
      "grad_norm": 0.6789263486862183,
      "learning_rate": 1.2083809523809524e-05,
      "loss": 1.1825,
      "step": 20780
    },
    {
      "epoch": 5.9399999999999995,
      "grad_norm": 0.6805009841918945,
      "learning_rate": 1.2080000000000001e-05,
      "loss": 0.325,
      "step": 20790
    },
    {
      "epoch": 5.942857142857143,
      "grad_norm": 11.03846549987793,
      "learning_rate": 1.2076190476190477e-05,
      "loss": 0.845,
      "step": 20800
    },
    {
      "epoch": 5.945714285714286,
      "grad_norm": 10.519585609436035,
      "learning_rate": 1.2072380952380952e-05,
      "loss": 0.4204,
      "step": 20810
    },
    {
      "epoch": 5.948571428571428,
      "grad_norm": 10.535897254943848,
      "learning_rate": 1.206857142857143e-05,
      "loss": 0.3217,
      "step": 20820
    },
    {
      "epoch": 5.951428571428571,
      "grad_norm": 10.706409454345703,
      "learning_rate": 1.2064761904761905e-05,
      "loss": 0.8412,
      "step": 20830
    },
    {
      "epoch": 5.954285714285715,
      "grad_norm": 10.427438735961914,
      "learning_rate": 1.2060952380952383e-05,
      "loss": 0.8171,
      "step": 20840
    },
    {
      "epoch": 5.957142857142857,
      "grad_norm": 10.634252548217773,
      "learning_rate": 1.2057142857142858e-05,
      "loss": 0.5045,
      "step": 20850
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.9063011407852173,
      "learning_rate": 1.2053333333333335e-05,
      "loss": 0.3126,
      "step": 20860
    },
    {
      "epoch": 5.962857142857143,
      "grad_norm": 0.9096537232398987,
      "learning_rate": 1.2049523809523811e-05,
      "loss": 0.6091,
      "step": 20870
    },
    {
      "epoch": 5.965714285714286,
      "grad_norm": 0.8590867519378662,
      "learning_rate": 1.2045714285714287e-05,
      "loss": 0.6043,
      "step": 20880
    },
    {
      "epoch": 5.968571428571429,
      "grad_norm": 10.586441040039062,
      "learning_rate": 1.2041904761904764e-05,
      "loss": 0.5148,
      "step": 20890
    },
    {
      "epoch": 5.9714285714285715,
      "grad_norm": 0.8344385027885437,
      "learning_rate": 1.203809523809524e-05,
      "loss": 0.6131,
      "step": 20900
    },
    {
      "epoch": 5.974285714285714,
      "grad_norm": 0.7492969036102295,
      "learning_rate": 1.2034285714285717e-05,
      "loss": 0.3185,
      "step": 20910
    },
    {
      "epoch": 5.977142857142857,
      "grad_norm": 0.6460313200950623,
      "learning_rate": 1.2030476190476192e-05,
      "loss": 0.3263,
      "step": 20920
    },
    {
      "epoch": 5.98,
      "grad_norm": 0.6482441425323486,
      "learning_rate": 1.202666666666667e-05,
      "loss": 0.9734,
      "step": 20930
    },
    {
      "epoch": 5.982857142857143,
      "grad_norm": 0.6331067085266113,
      "learning_rate": 1.2022857142857143e-05,
      "loss": 0.1205,
      "step": 20940
    },
    {
      "epoch": 5.985714285714286,
      "grad_norm": 0.6274906396865845,
      "learning_rate": 1.2019047619047619e-05,
      "loss": 0.6561,
      "step": 20950
    },
    {
      "epoch": 5.988571428571428,
      "grad_norm": 10.655533790588379,
      "learning_rate": 1.2015238095238095e-05,
      "loss": 0.864,
      "step": 20960
    },
    {
      "epoch": 5.991428571428571,
      "grad_norm": 0.6721422672271729,
      "learning_rate": 1.2011428571428572e-05,
      "loss": 0.3305,
      "step": 20970
    },
    {
      "epoch": 5.994285714285715,
      "grad_norm": 10.643446922302246,
      "learning_rate": 1.2007619047619047e-05,
      "loss": 0.9594,
      "step": 20980
    },
    {
      "epoch": 5.997142857142857,
      "grad_norm": 21.84796714782715,
      "learning_rate": 1.2003809523809525e-05,
      "loss": 0.7412,
      "step": 20990
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.7479658126831055,
      "learning_rate": 1.2e-05,
      "loss": 0.5303,
      "step": 21000
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.8709677419354839,
      "eval_f1": 0.0,
      "eval_loss": 0.5420898795127869,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 44.5214,
      "eval_samples_per_second": 67.383,
      "eval_steps_per_second": 2.111,
      "step": 21000
    },
    {
      "epoch": 6.002857142857143,
      "grad_norm": 10.530803680419922,
      "learning_rate": 1.1996190476190478e-05,
      "loss": 0.7262,
      "step": 21010
    },
    {
      "epoch": 6.005714285714285,
      "grad_norm": 0.7401584982872009,
      "learning_rate": 1.1992380952380953e-05,
      "loss": 0.3242,
      "step": 21020
    },
    {
      "epoch": 6.008571428571429,
      "grad_norm": 10.677008628845215,
      "learning_rate": 1.1988571428571429e-05,
      "loss": 0.4301,
      "step": 21030
    },
    {
      "epoch": 6.011428571428572,
      "grad_norm": 0.6987794041633606,
      "learning_rate": 1.1984761904761906e-05,
      "loss": 0.6396,
      "step": 21040
    },
    {
      "epoch": 6.014285714285714,
      "grad_norm": 10.594132423400879,
      "learning_rate": 1.1980952380952382e-05,
      "loss": 0.4318,
      "step": 21050
    },
    {
      "epoch": 6.017142857142857,
      "grad_norm": 21.8265323638916,
      "learning_rate": 1.1977142857142859e-05,
      "loss": 0.744,
      "step": 21060
    },
    {
      "epoch": 6.02,
      "grad_norm": 10.60472583770752,
      "learning_rate": 1.1973333333333334e-05,
      "loss": 0.7351,
      "step": 21070
    },
    {
      "epoch": 6.022857142857143,
      "grad_norm": 0.735555112361908,
      "learning_rate": 1.196952380952381e-05,
      "loss": 0.3237,
      "step": 21080
    },
    {
      "epoch": 6.025714285714286,
      "grad_norm": 0.7272652983665466,
      "learning_rate": 1.1965714285714287e-05,
      "loss": 0.6291,
      "step": 21090
    },
    {
      "epoch": 6.0285714285714285,
      "grad_norm": 0.796358585357666,
      "learning_rate": 1.1961904761904763e-05,
      "loss": 0.9297,
      "step": 21100
    },
    {
      "epoch": 6.031428571428571,
      "grad_norm": 0.8900958299636841,
      "learning_rate": 1.195809523809524e-05,
      "loss": 0.8119,
      "step": 21110
    },
    {
      "epoch": 6.034285714285715,
      "grad_norm": 0.855726420879364,
      "learning_rate": 1.1954285714285716e-05,
      "loss": 0.3153,
      "step": 21120
    },
    {
      "epoch": 6.037142857142857,
      "grad_norm": 10.596046447753906,
      "learning_rate": 1.1950476190476193e-05,
      "loss": 0.4191,
      "step": 21130
    },
    {
      "epoch": 6.04,
      "grad_norm": 0.7740821838378906,
      "learning_rate": 1.1946666666666669e-05,
      "loss": 0.5228,
      "step": 21140
    },
    {
      "epoch": 6.042857142857143,
      "grad_norm": 10.656291007995605,
      "learning_rate": 1.1942857142857144e-05,
      "loss": 0.5242,
      "step": 21150
    },
    {
      "epoch": 6.045714285714285,
      "grad_norm": 0.8217970132827759,
      "learning_rate": 1.193904761904762e-05,
      "loss": 0.6245,
      "step": 21160
    },
    {
      "epoch": 6.048571428571429,
      "grad_norm": 0.7552169561386108,
      "learning_rate": 1.1935238095238095e-05,
      "loss": 0.2197,
      "step": 21170
    },
    {
      "epoch": 6.051428571428572,
      "grad_norm": 10.664993286132812,
      "learning_rate": 1.1931428571428571e-05,
      "loss": 0.9362,
      "step": 21180
    },
    {
      "epoch": 6.054285714285714,
      "grad_norm": 10.525729179382324,
      "learning_rate": 1.1927619047619048e-05,
      "loss": 0.6217,
      "step": 21190
    },
    {
      "epoch": 6.057142857142857,
      "grad_norm": 0.8485885858535767,
      "learning_rate": 1.1923809523809524e-05,
      "loss": 0.4196,
      "step": 21200
    },
    {
      "epoch": 6.06,
      "grad_norm": 0.8667908310890198,
      "learning_rate": 1.1920000000000001e-05,
      "loss": 0.914,
      "step": 21210
    },
    {
      "epoch": 6.062857142857143,
      "grad_norm": 10.578250885009766,
      "learning_rate": 1.1916190476190477e-05,
      "loss": 0.4125,
      "step": 21220
    },
    {
      "epoch": 6.065714285714286,
      "grad_norm": 10.594525337219238,
      "learning_rate": 1.1912380952380952e-05,
      "loss": 0.8003,
      "step": 21230
    },
    {
      "epoch": 6.0685714285714285,
      "grad_norm": 0.9166480302810669,
      "learning_rate": 1.190857142857143e-05,
      "loss": 0.2143,
      "step": 21240
    },
    {
      "epoch": 6.071428571428571,
      "grad_norm": 10.532255172729492,
      "learning_rate": 1.1904761904761905e-05,
      "loss": 0.5075,
      "step": 21250
    },
    {
      "epoch": 6.074285714285715,
      "grad_norm": 0.849959671497345,
      "learning_rate": 1.1900952380952382e-05,
      "loss": 0.7137,
      "step": 21260
    },
    {
      "epoch": 6.077142857142857,
      "grad_norm": 0.7452680468559265,
      "learning_rate": 1.1897142857142858e-05,
      "loss": 0.1192,
      "step": 21270
    },
    {
      "epoch": 6.08,
      "grad_norm": 0.7027400732040405,
      "learning_rate": 1.1893333333333335e-05,
      "loss": 0.5356,
      "step": 21280
    },
    {
      "epoch": 6.082857142857143,
      "grad_norm": 21.799766540527344,
      "learning_rate": 1.188952380952381e-05,
      "loss": 0.6383,
      "step": 21290
    },
    {
      "epoch": 6.085714285714285,
      "grad_norm": 10.577092170715332,
      "learning_rate": 1.1885714285714286e-05,
      "loss": 0.4285,
      "step": 21300
    },
    {
      "epoch": 6.088571428571429,
      "grad_norm": 0.7227553129196167,
      "learning_rate": 1.1881904761904764e-05,
      "loss": 0.3246,
      "step": 21310
    },
    {
      "epoch": 6.091428571428572,
      "grad_norm": 10.583536148071289,
      "learning_rate": 1.187809523809524e-05,
      "loss": 0.8433,
      "step": 21320
    },
    {
      "epoch": 6.094285714285714,
      "grad_norm": 0.7404639720916748,
      "learning_rate": 1.1874285714285717e-05,
      "loss": 0.2244,
      "step": 21330
    },
    {
      "epoch": 6.097142857142857,
      "grad_norm": 10.772811889648438,
      "learning_rate": 1.1870476190476192e-05,
      "loss": 0.5336,
      "step": 21340
    },
    {
      "epoch": 6.1,
      "grad_norm": 0.6885281801223755,
      "learning_rate": 1.186666666666667e-05,
      "loss": 0.3285,
      "step": 21350
    },
    {
      "epoch": 6.102857142857143,
      "grad_norm": 21.79380226135254,
      "learning_rate": 1.1862857142857145e-05,
      "loss": 0.7408,
      "step": 21360
    },
    {
      "epoch": 6.105714285714286,
      "grad_norm": 10.594886779785156,
      "learning_rate": 1.1859047619047619e-05,
      "loss": 0.4295,
      "step": 21370
    },
    {
      "epoch": 6.1085714285714285,
      "grad_norm": 0.7255483269691467,
      "learning_rate": 1.1855238095238094e-05,
      "loss": 0.5311,
      "step": 21380
    },
    {
      "epoch": 6.111428571428571,
      "grad_norm": 10.593570709228516,
      "learning_rate": 1.1851428571428572e-05,
      "loss": 0.4298,
      "step": 21390
    },
    {
      "epoch": 6.114285714285714,
      "grad_norm": 10.58470630645752,
      "learning_rate": 1.1847619047619047e-05,
      "loss": 0.5292,
      "step": 21400
    },
    {
      "epoch": 6.117142857142857,
      "grad_norm": 10.562515258789062,
      "learning_rate": 1.1843809523809525e-05,
      "loss": 0.8373,
      "step": 21410
    },
    {
      "epoch": 6.12,
      "grad_norm": 22.0568904876709,
      "learning_rate": 1.184e-05,
      "loss": 0.5252,
      "step": 21420
    },
    {
      "epoch": 6.122857142857143,
      "grad_norm": 10.620113372802734,
      "learning_rate": 1.1836190476190477e-05,
      "loss": 0.6313,
      "step": 21430
    },
    {
      "epoch": 6.1257142857142854,
      "grad_norm": 0.8298309445381165,
      "learning_rate": 1.1832380952380953e-05,
      "loss": 0.6258,
      "step": 21440
    },
    {
      "epoch": 6.128571428571428,
      "grad_norm": 32.939231872558594,
      "learning_rate": 1.1828571428571429e-05,
      "loss": 0.6163,
      "step": 21450
    },
    {
      "epoch": 6.131428571428572,
      "grad_norm": 10.881884574890137,
      "learning_rate": 1.1824761904761906e-05,
      "loss": 0.6084,
      "step": 21460
    },
    {
      "epoch": 6.134285714285714,
      "grad_norm": 10.3141508102417,
      "learning_rate": 1.1820952380952381e-05,
      "loss": 0.8678,
      "step": 21470
    },
    {
      "epoch": 6.137142857142857,
      "grad_norm": 11.087782859802246,
      "learning_rate": 1.1817142857142859e-05,
      "loss": 0.3946,
      "step": 21480
    },
    {
      "epoch": 6.14,
      "grad_norm": 0.8101184368133545,
      "learning_rate": 1.1813333333333334e-05,
      "loss": 0.3104,
      "step": 21490
    },
    {
      "epoch": 6.142857142857143,
      "grad_norm": 0.7737545967102051,
      "learning_rate": 1.180952380952381e-05,
      "loss": 0.4143,
      "step": 21500
    },
    {
      "epoch": 6.145714285714286,
      "grad_norm": 0.9114000797271729,
      "learning_rate": 1.1805714285714287e-05,
      "loss": 0.8215,
      "step": 21510
    },
    {
      "epoch": 6.148571428571429,
      "grad_norm": 0.7399349212646484,
      "learning_rate": 1.1801904761904763e-05,
      "loss": 0.2191,
      "step": 21520
    },
    {
      "epoch": 6.151428571428571,
      "grad_norm": 0.6171118021011353,
      "learning_rate": 1.179809523809524e-05,
      "loss": 0.5375,
      "step": 21530
    },
    {
      "epoch": 6.154285714285714,
      "grad_norm": 10.799872398376465,
      "learning_rate": 1.1794285714285716e-05,
      "loss": 0.4435,
      "step": 21540
    },
    {
      "epoch": 6.1571428571428575,
      "grad_norm": 0.6029701232910156,
      "learning_rate": 1.1790476190476193e-05,
      "loss": 0.6595,
      "step": 21550
    },
    {
      "epoch": 6.16,
      "grad_norm": 0.5084644556045532,
      "learning_rate": 1.1786666666666668e-05,
      "loss": 0.3444,
      "step": 21560
    },
    {
      "epoch": 6.162857142857143,
      "grad_norm": 0.6332094073295593,
      "learning_rate": 1.1782857142857144e-05,
      "loss": 0.6633,
      "step": 21570
    },
    {
      "epoch": 6.1657142857142855,
      "grad_norm": 10.842031478881836,
      "learning_rate": 1.1779047619047621e-05,
      "loss": 0.4442,
      "step": 21580
    },
    {
      "epoch": 6.168571428571428,
      "grad_norm": 10.707986831665039,
      "learning_rate": 1.1775238095238095e-05,
      "loss": 0.3378,
      "step": 21590
    },
    {
      "epoch": 6.171428571428572,
      "grad_norm": 10.733765602111816,
      "learning_rate": 1.177142857142857e-05,
      "loss": 0.5499,
      "step": 21600
    },
    {
      "epoch": 6.174285714285714,
      "grad_norm": 0.6457335352897644,
      "learning_rate": 1.1767619047619048e-05,
      "loss": 0.5461,
      "step": 21610
    },
    {
      "epoch": 6.177142857142857,
      "grad_norm": 11.261345863342285,
      "learning_rate": 1.1763809523809524e-05,
      "loss": 0.6564,
      "step": 21620
    },
    {
      "epoch": 6.18,
      "grad_norm": 0.7610082030296326,
      "learning_rate": 1.1760000000000001e-05,
      "loss": 0.4209,
      "step": 21630
    },
    {
      "epoch": 6.182857142857143,
      "grad_norm": 10.582181930541992,
      "learning_rate": 1.1756190476190476e-05,
      "loss": 0.4148,
      "step": 21640
    },
    {
      "epoch": 6.185714285714286,
      "grad_norm": 0.8012804388999939,
      "learning_rate": 1.1752380952380952e-05,
      "loss": 0.8221,
      "step": 21650
    },
    {
      "epoch": 6.188571428571429,
      "grad_norm": 0.7910634279251099,
      "learning_rate": 1.174857142857143e-05,
      "loss": 0.6149,
      "step": 21660
    },
    {
      "epoch": 6.191428571428571,
      "grad_norm": 0.6771475076675415,
      "learning_rate": 1.1744761904761905e-05,
      "loss": 0.2169,
      "step": 21670
    },
    {
      "epoch": 6.194285714285714,
      "grad_norm": 22.1741943359375,
      "learning_rate": 1.1740952380952382e-05,
      "loss": 0.6447,
      "step": 21680
    },
    {
      "epoch": 6.1971428571428575,
      "grad_norm": 0.6636450886726379,
      "learning_rate": 1.1737142857142858e-05,
      "loss": 0.3272,
      "step": 21690
    },
    {
      "epoch": 6.2,
      "grad_norm": 0.6552991271018982,
      "learning_rate": 1.1733333333333335e-05,
      "loss": 0.4328,
      "step": 21700
    },
    {
      "epoch": 6.202857142857143,
      "grad_norm": 0.5071907043457031,
      "learning_rate": 1.172952380952381e-05,
      "loss": 0.2325,
      "step": 21710
    },
    {
      "epoch": 6.2057142857142855,
      "grad_norm": 10.974431037902832,
      "learning_rate": 1.1725714285714286e-05,
      "loss": 0.5881,
      "step": 21720
    },
    {
      "epoch": 6.208571428571428,
      "grad_norm": 0.4422191381454468,
      "learning_rate": 1.1721904761904763e-05,
      "loss": 0.469,
      "step": 21730
    },
    {
      "epoch": 6.211428571428572,
      "grad_norm": 0.5224387049674988,
      "learning_rate": 1.1718095238095239e-05,
      "loss": 0.6902,
      "step": 21740
    },
    {
      "epoch": 6.214285714285714,
      "grad_norm": 11.301141738891602,
      "learning_rate": 1.1714285714285716e-05,
      "loss": 0.8549,
      "step": 21750
    },
    {
      "epoch": 6.217142857142857,
      "grad_norm": 10.520242691040039,
      "learning_rate": 1.1710476190476192e-05,
      "loss": 0.4117,
      "step": 21760
    },
    {
      "epoch": 6.22,
      "grad_norm": 0.741330087184906,
      "learning_rate": 1.170666666666667e-05,
      "loss": 0.4,
      "step": 21770
    },
    {
      "epoch": 6.222857142857142,
      "grad_norm": 0.5967127084732056,
      "learning_rate": 1.1702857142857145e-05,
      "loss": 0.869,
      "step": 21780
    },
    {
      "epoch": 6.225714285714286,
      "grad_norm": 0.5414391756057739,
      "learning_rate": 1.169904761904762e-05,
      "loss": 0.3327,
      "step": 21790
    },
    {
      "epoch": 6.228571428571429,
      "grad_norm": 10.79049301147461,
      "learning_rate": 1.1695238095238098e-05,
      "loss": 0.7919,
      "step": 21800
    },
    {
      "epoch": 6.231428571428571,
      "grad_norm": 0.5601102113723755,
      "learning_rate": 1.1691428571428572e-05,
      "loss": 0.9033,
      "step": 21810
    },
    {
      "epoch": 6.234285714285714,
      "grad_norm": 0.6383722424507141,
      "learning_rate": 1.1687619047619047e-05,
      "loss": 0.6631,
      "step": 21820
    },
    {
      "epoch": 6.2371428571428575,
      "grad_norm": 10.923681259155273,
      "learning_rate": 1.1683809523809524e-05,
      "loss": 0.5262,
      "step": 21830
    },
    {
      "epoch": 6.24,
      "grad_norm": 0.9202894568443298,
      "learning_rate": 1.168e-05,
      "loss": 0.53,
      "step": 21840
    },
    {
      "epoch": 6.242857142857143,
      "grad_norm": 11.567901611328125,
      "learning_rate": 1.1676190476190477e-05,
      "loss": 0.3361,
      "step": 21850
    },
    {
      "epoch": 6.2457142857142856,
      "grad_norm": 0.5718262791633606,
      "learning_rate": 1.1672380952380953e-05,
      "loss": 0.4443,
      "step": 21860
    },
    {
      "epoch": 6.248571428571428,
      "grad_norm": 0.6068637371063232,
      "learning_rate": 1.1668571428571428e-05,
      "loss": 0.5411,
      "step": 21870
    },
    {
      "epoch": 6.251428571428572,
      "grad_norm": 0.5764465928077698,
      "learning_rate": 1.1664761904761906e-05,
      "loss": 0.2997,
      "step": 21880
    },
    {
      "epoch": 6.2542857142857144,
      "grad_norm": 10.573570251464844,
      "learning_rate": 1.1660952380952381e-05,
      "loss": 0.6254,
      "step": 21890
    },
    {
      "epoch": 6.257142857142857,
      "grad_norm": 22.246437072753906,
      "learning_rate": 1.1657142857142859e-05,
      "loss": 0.6521,
      "step": 21900
    },
    {
      "epoch": 6.26,
      "grad_norm": 0.9468344449996948,
      "learning_rate": 1.1653333333333334e-05,
      "loss": 0.7128,
      "step": 21910
    },
    {
      "epoch": 6.2628571428571425,
      "grad_norm": 0.6310131549835205,
      "learning_rate": 1.1649523809523811e-05,
      "loss": 0.496,
      "step": 21920
    },
    {
      "epoch": 6.265714285714286,
      "grad_norm": 11.622977256774902,
      "learning_rate": 1.1645714285714287e-05,
      "loss": 0.7257,
      "step": 21930
    },
    {
      "epoch": 6.268571428571429,
      "grad_norm": 0.5821564793586731,
      "learning_rate": 1.1641904761904763e-05,
      "loss": 0.0974,
      "step": 21940
    },
    {
      "epoch": 6.271428571428571,
      "grad_norm": 33.05147171020508,
      "learning_rate": 1.163809523809524e-05,
      "loss": 0.8163,
      "step": 21950
    },
    {
      "epoch": 6.274285714285714,
      "grad_norm": 0.49989432096481323,
      "learning_rate": 1.1634285714285715e-05,
      "loss": 0.4307,
      "step": 21960
    },
    {
      "epoch": 6.277142857142858,
      "grad_norm": 0.8316810727119446,
      "learning_rate": 1.1630476190476193e-05,
      "loss": 0.4366,
      "step": 21970
    },
    {
      "epoch": 6.28,
      "grad_norm": 1.0279208421707153,
      "learning_rate": 1.1626666666666668e-05,
      "loss": 0.3026,
      "step": 21980
    },
    {
      "epoch": 6.282857142857143,
      "grad_norm": 0.47276026010513306,
      "learning_rate": 1.1622857142857144e-05,
      "loss": 0.4769,
      "step": 21990
    },
    {
      "epoch": 6.285714285714286,
      "grad_norm": 31.982135772705078,
      "learning_rate": 1.1619047619047621e-05,
      "loss": 0.7095,
      "step": 22000
    },
    {
      "epoch": 6.288571428571428,
      "grad_norm": 14.899603843688965,
      "learning_rate": 1.1615238095238097e-05,
      "loss": 0.5618,
      "step": 22010
    },
    {
      "epoch": 6.291428571428572,
      "grad_norm": 16.914329528808594,
      "learning_rate": 1.161142857142857e-05,
      "loss": 0.7626,
      "step": 22020
    },
    {
      "epoch": 6.2942857142857145,
      "grad_norm": 0.5969921350479126,
      "learning_rate": 1.1607619047619048e-05,
      "loss": 0.5989,
      "step": 22030
    },
    {
      "epoch": 6.297142857142857,
      "grad_norm": 6.981544494628906,
      "learning_rate": 1.1603809523809523e-05,
      "loss": 0.5443,
      "step": 22040
    },
    {
      "epoch": 6.3,
      "grad_norm": 11.128129959106445,
      "learning_rate": 1.16e-05,
      "loss": 0.3535,
      "step": 22050
    },
    {
      "epoch": 6.3028571428571425,
      "grad_norm": 4.2360429763793945,
      "learning_rate": 1.1596190476190476e-05,
      "loss": 0.3788,
      "step": 22060
    },
    {
      "epoch": 6.305714285714286,
      "grad_norm": 11.385383605957031,
      "learning_rate": 1.1592380952380952e-05,
      "loss": 0.7805,
      "step": 22070
    },
    {
      "epoch": 6.308571428571429,
      "grad_norm": 0.6997233629226685,
      "learning_rate": 1.158857142857143e-05,
      "loss": 0.49,
      "step": 22080
    },
    {
      "epoch": 6.311428571428571,
      "grad_norm": 16.262367248535156,
      "learning_rate": 1.1584761904761905e-05,
      "loss": 0.6179,
      "step": 22090
    },
    {
      "epoch": 6.314285714285714,
      "grad_norm": 1.475038766860962,
      "learning_rate": 1.1580952380952382e-05,
      "loss": 0.5487,
      "step": 22100
    },
    {
      "epoch": 6.317142857142857,
      "grad_norm": 1.8596932888031006,
      "learning_rate": 1.1577142857142858e-05,
      "loss": 0.5339,
      "step": 22110
    },
    {
      "epoch": 6.32,
      "grad_norm": 1.438350796699524,
      "learning_rate": 1.1573333333333335e-05,
      "loss": 0.4702,
      "step": 22120
    },
    {
      "epoch": 6.322857142857143,
      "grad_norm": 12.324248313903809,
      "learning_rate": 1.156952380952381e-05,
      "loss": 0.7755,
      "step": 22130
    },
    {
      "epoch": 6.325714285714286,
      "grad_norm": 0.5787785649299622,
      "learning_rate": 1.1565714285714286e-05,
      "loss": 0.3762,
      "step": 22140
    },
    {
      "epoch": 6.328571428571428,
      "grad_norm": 0.35282817482948303,
      "learning_rate": 1.1561904761904763e-05,
      "loss": 0.1203,
      "step": 22150
    },
    {
      "epoch": 6.331428571428571,
      "grad_norm": 1.18435800075531,
      "learning_rate": 1.1558095238095239e-05,
      "loss": 0.3025,
      "step": 22160
    },
    {
      "epoch": 6.3342857142857145,
      "grad_norm": 0.30665120482444763,
      "learning_rate": 1.1554285714285716e-05,
      "loss": 0.6487,
      "step": 22170
    },
    {
      "epoch": 6.337142857142857,
      "grad_norm": 0.4193763732910156,
      "learning_rate": 1.1550476190476192e-05,
      "loss": 0.5118,
      "step": 22180
    },
    {
      "epoch": 6.34,
      "grad_norm": 0.4908418357372284,
      "learning_rate": 1.1546666666666669e-05,
      "loss": 0.4747,
      "step": 22190
    },
    {
      "epoch": 6.3428571428571425,
      "grad_norm": 11.669163703918457,
      "learning_rate": 1.1542857142857145e-05,
      "loss": 0.8706,
      "step": 22200
    },
    {
      "epoch": 6.345714285714286,
      "grad_norm": 11.129098892211914,
      "learning_rate": 1.153904761904762e-05,
      "loss": 0.6422,
      "step": 22210
    },
    {
      "epoch": 6.348571428571429,
      "grad_norm": 11.219322204589844,
      "learning_rate": 1.1535238095238097e-05,
      "loss": 0.8219,
      "step": 22220
    },
    {
      "epoch": 6.351428571428571,
      "grad_norm": 0.7808196544647217,
      "learning_rate": 1.1531428571428573e-05,
      "loss": 0.2208,
      "step": 22230
    },
    {
      "epoch": 6.354285714285714,
      "grad_norm": 22.773670196533203,
      "learning_rate": 1.1527619047619047e-05,
      "loss": 0.7251,
      "step": 22240
    },
    {
      "epoch": 6.357142857142857,
      "grad_norm": 0.5206104516983032,
      "learning_rate": 1.1523809523809524e-05,
      "loss": 0.1188,
      "step": 22250
    },
    {
      "epoch": 6.36,
      "grad_norm": 0.5449341535568237,
      "learning_rate": 1.152e-05,
      "loss": 0.1873,
      "step": 22260
    },
    {
      "epoch": 6.362857142857143,
      "grad_norm": 0.7080575227737427,
      "learning_rate": 1.1516190476190477e-05,
      "loss": 0.5098,
      "step": 22270
    },
    {
      "epoch": 6.365714285714286,
      "grad_norm": 0.39961305260658264,
      "learning_rate": 1.1512380952380953e-05,
      "loss": 0.725,
      "step": 22280
    },
    {
      "epoch": 6.368571428571428,
      "grad_norm": 0.36709392070770264,
      "learning_rate": 1.1508571428571428e-05,
      "loss": 0.4042,
      "step": 22290
    },
    {
      "epoch": 6.371428571428572,
      "grad_norm": 1.0379090309143066,
      "learning_rate": 1.1504761904761906e-05,
      "loss": 0.5078,
      "step": 22300
    },
    {
      "epoch": 6.3742857142857146,
      "grad_norm": 10.831491470336914,
      "learning_rate": 1.1500952380952381e-05,
      "loss": 0.9489,
      "step": 22310
    },
    {
      "epoch": 6.377142857142857,
      "grad_norm": 12.101876258850098,
      "learning_rate": 1.1497142857142858e-05,
      "loss": 0.4229,
      "step": 22320
    },
    {
      "epoch": 6.38,
      "grad_norm": 0.4565604031085968,
      "learning_rate": 1.1493333333333334e-05,
      "loss": 0.3429,
      "step": 22330
    },
    {
      "epoch": 6.382857142857143,
      "grad_norm": 0.4001513421535492,
      "learning_rate": 1.1489523809523811e-05,
      "loss": 0.226,
      "step": 22340
    },
    {
      "epoch": 6.385714285714286,
      "grad_norm": 1.8976166248321533,
      "learning_rate": 1.1485714285714287e-05,
      "loss": 0.5839,
      "step": 22350
    },
    {
      "epoch": 6.388571428571429,
      "grad_norm": 21.926403045654297,
      "learning_rate": 1.1481904761904762e-05,
      "loss": 0.8076,
      "step": 22360
    },
    {
      "epoch": 6.3914285714285715,
      "grad_norm": 4.023288249969482,
      "learning_rate": 1.147809523809524e-05,
      "loss": 0.678,
      "step": 22370
    },
    {
      "epoch": 6.394285714285714,
      "grad_norm": 0.8562849760055542,
      "learning_rate": 1.1474285714285715e-05,
      "loss": 0.6619,
      "step": 22380
    },
    {
      "epoch": 6.397142857142857,
      "grad_norm": 0.9385457634925842,
      "learning_rate": 1.1470476190476193e-05,
      "loss": 0.6154,
      "step": 22390
    },
    {
      "epoch": 6.4,
      "grad_norm": 21.88961410522461,
      "learning_rate": 1.1466666666666668e-05,
      "loss": 0.8078,
      "step": 22400
    },
    {
      "epoch": 6.402857142857143,
      "grad_norm": 1.5268324613571167,
      "learning_rate": 1.1462857142857144e-05,
      "loss": 0.5017,
      "step": 22410
    },
    {
      "epoch": 6.405714285714286,
      "grad_norm": 0.628703236579895,
      "learning_rate": 1.1459047619047621e-05,
      "loss": 0.7456,
      "step": 22420
    },
    {
      "epoch": 6.408571428571428,
      "grad_norm": 0.8135870695114136,
      "learning_rate": 1.1455238095238097e-05,
      "loss": 0.3604,
      "step": 22430
    },
    {
      "epoch": 6.411428571428571,
      "grad_norm": 0.5936248898506165,
      "learning_rate": 1.1451428571428574e-05,
      "loss": 0.0866,
      "step": 22440
    },
    {
      "epoch": 6.414285714285715,
      "grad_norm": 0.4746556580066681,
      "learning_rate": 1.144761904761905e-05,
      "loss": 0.4295,
      "step": 22450
    },
    {
      "epoch": 6.417142857142857,
      "grad_norm": 0.49958232045173645,
      "learning_rate": 1.1443809523809523e-05,
      "loss": 0.4638,
      "step": 22460
    },
    {
      "epoch": 6.42,
      "grad_norm": 0.5079879760742188,
      "learning_rate": 1.144e-05,
      "loss": 0.741,
      "step": 22470
    },
    {
      "epoch": 6.422857142857143,
      "grad_norm": 13.259317398071289,
      "learning_rate": 1.1436190476190476e-05,
      "loss": 0.5654,
      "step": 22480
    },
    {
      "epoch": 6.425714285714285,
      "grad_norm": 11.864059448242188,
      "learning_rate": 1.1432380952380952e-05,
      "loss": 0.5229,
      "step": 22490
    },
    {
      "epoch": 6.428571428571429,
      "grad_norm": 1.1822301149368286,
      "learning_rate": 1.1428571428571429e-05,
      "loss": 0.4377,
      "step": 22500
    },
    {
      "epoch": 6.4314285714285715,
      "grad_norm": 3.0005266666412354,
      "learning_rate": 1.1424761904761905e-05,
      "loss": 0.3938,
      "step": 22510
    },
    {
      "epoch": 6.434285714285714,
      "grad_norm": 1.2310118675231934,
      "learning_rate": 1.1420952380952382e-05,
      "loss": 0.4964,
      "step": 22520
    },
    {
      "epoch": 6.437142857142857,
      "grad_norm": 11.456002235412598,
      "learning_rate": 1.1417142857142857e-05,
      "loss": 0.5011,
      "step": 22530
    },
    {
      "epoch": 6.44,
      "grad_norm": 0.45002251863479614,
      "learning_rate": 1.1413333333333335e-05,
      "loss": 0.5059,
      "step": 22540
    },
    {
      "epoch": 6.442857142857143,
      "grad_norm": 0.6391275525093079,
      "learning_rate": 1.140952380952381e-05,
      "loss": 0.5165,
      "step": 22550
    },
    {
      "epoch": 6.445714285714286,
      "grad_norm": 0.6022062301635742,
      "learning_rate": 1.1405714285714286e-05,
      "loss": 0.734,
      "step": 22560
    },
    {
      "epoch": 6.448571428571428,
      "grad_norm": 0.681380033493042,
      "learning_rate": 1.1401904761904763e-05,
      "loss": 0.7041,
      "step": 22570
    },
    {
      "epoch": 6.451428571428571,
      "grad_norm": 11.54228401184082,
      "learning_rate": 1.1398095238095239e-05,
      "loss": 0.5569,
      "step": 22580
    },
    {
      "epoch": 6.454285714285715,
      "grad_norm": 11.738463401794434,
      "learning_rate": 1.1394285714285716e-05,
      "loss": 0.3076,
      "step": 22590
    },
    {
      "epoch": 6.457142857142857,
      "grad_norm": 0.6269361972808838,
      "learning_rate": 1.1390476190476192e-05,
      "loss": 0.2915,
      "step": 22600
    },
    {
      "epoch": 6.46,
      "grad_norm": 12.221307754516602,
      "learning_rate": 1.1386666666666669e-05,
      "loss": 0.7544,
      "step": 22610
    },
    {
      "epoch": 6.462857142857143,
      "grad_norm": 0.8116354942321777,
      "learning_rate": 1.1382857142857144e-05,
      "loss": 0.8063,
      "step": 22620
    },
    {
      "epoch": 6.465714285714285,
      "grad_norm": 1.5705926418304443,
      "learning_rate": 1.137904761904762e-05,
      "loss": 0.2273,
      "step": 22630
    },
    {
      "epoch": 6.468571428571429,
      "grad_norm": 11.139588356018066,
      "learning_rate": 1.1375238095238097e-05,
      "loss": 0.5967,
      "step": 22640
    },
    {
      "epoch": 6.4714285714285715,
      "grad_norm": 11.291604995727539,
      "learning_rate": 1.1371428571428573e-05,
      "loss": 0.4426,
      "step": 22650
    },
    {
      "epoch": 6.474285714285714,
      "grad_norm": 0.5551568269729614,
      "learning_rate": 1.136761904761905e-05,
      "loss": 0.6998,
      "step": 22660
    },
    {
      "epoch": 6.477142857142857,
      "grad_norm": 10.697811126708984,
      "learning_rate": 1.1363809523809526e-05,
      "loss": 0.7657,
      "step": 22670
    },
    {
      "epoch": 6.48,
      "grad_norm": 12.240252494812012,
      "learning_rate": 1.136e-05,
      "loss": 0.7108,
      "step": 22680
    },
    {
      "epoch": 6.482857142857143,
      "grad_norm": 1.9859126806259155,
      "learning_rate": 1.1356190476190477e-05,
      "loss": 0.4114,
      "step": 22690
    },
    {
      "epoch": 6.485714285714286,
      "grad_norm": 0.7817918062210083,
      "learning_rate": 1.1352380952380953e-05,
      "loss": 0.461,
      "step": 22700
    },
    {
      "epoch": 6.488571428571428,
      "grad_norm": 0.6770567893981934,
      "learning_rate": 1.1348571428571428e-05,
      "loss": 0.0249,
      "step": 22710
    },
    {
      "epoch": 6.491428571428571,
      "grad_norm": 0.5129097700119019,
      "learning_rate": 1.1344761904761905e-05,
      "loss": 0.8862,
      "step": 22720
    },
    {
      "epoch": 6.494285714285715,
      "grad_norm": 0.7597235441207886,
      "learning_rate": 1.1340952380952381e-05,
      "loss": 0.3615,
      "step": 22730
    },
    {
      "epoch": 6.497142857142857,
      "grad_norm": 0.5340723395347595,
      "learning_rate": 1.1337142857142858e-05,
      "loss": 0.5005,
      "step": 22740
    },
    {
      "epoch": 6.5,
      "grad_norm": 1.8347795009613037,
      "learning_rate": 1.1333333333333334e-05,
      "loss": 0.2734,
      "step": 22750
    },
    {
      "epoch": 6.502857142857143,
      "grad_norm": 10.697437286376953,
      "learning_rate": 1.1329523809523811e-05,
      "loss": 0.9139,
      "step": 22760
    },
    {
      "epoch": 6.505714285714285,
      "grad_norm": 0.5360003709793091,
      "learning_rate": 1.1325714285714287e-05,
      "loss": 0.2834,
      "step": 22770
    },
    {
      "epoch": 6.508571428571429,
      "grad_norm": 11.077066421508789,
      "learning_rate": 1.1321904761904762e-05,
      "loss": 0.4425,
      "step": 22780
    },
    {
      "epoch": 6.511428571428572,
      "grad_norm": 11.26937198638916,
      "learning_rate": 1.131809523809524e-05,
      "loss": 0.3768,
      "step": 22790
    },
    {
      "epoch": 6.514285714285714,
      "grad_norm": 0.4083792269229889,
      "learning_rate": 1.1314285714285715e-05,
      "loss": 0.5996,
      "step": 22800
    },
    {
      "epoch": 6.517142857142857,
      "grad_norm": 0.4618983268737793,
      "learning_rate": 1.1310476190476192e-05,
      "loss": 0.4036,
      "step": 22810
    },
    {
      "epoch": 6.52,
      "grad_norm": 12.122734069824219,
      "learning_rate": 1.1306666666666668e-05,
      "loss": 0.5301,
      "step": 22820
    },
    {
      "epoch": 6.522857142857143,
      "grad_norm": 0.39155590534210205,
      "learning_rate": 1.1302857142857144e-05,
      "loss": 0.9738,
      "step": 22830
    },
    {
      "epoch": 6.525714285714286,
      "grad_norm": 11.465349197387695,
      "learning_rate": 1.129904761904762e-05,
      "loss": 0.7234,
      "step": 22840
    },
    {
      "epoch": 6.5285714285714285,
      "grad_norm": 10.667194366455078,
      "learning_rate": 1.1295238095238096e-05,
      "loss": 0.4326,
      "step": 22850
    },
    {
      "epoch": 6.531428571428571,
      "grad_norm": 0.43103817105293274,
      "learning_rate": 1.1291428571428574e-05,
      "loss": 0.7182,
      "step": 22860
    },
    {
      "epoch": 6.534285714285714,
      "grad_norm": 0.42072978615760803,
      "learning_rate": 1.128761904761905e-05,
      "loss": 0.6193,
      "step": 22870
    },
    {
      "epoch": 6.537142857142857,
      "grad_norm": 0.7459931373596191,
      "learning_rate": 1.1283809523809527e-05,
      "loss": 0.5102,
      "step": 22880
    },
    {
      "epoch": 6.54,
      "grad_norm": 2.277191162109375,
      "learning_rate": 1.128e-05,
      "loss": 0.5266,
      "step": 22890
    },
    {
      "epoch": 6.542857142857143,
      "grad_norm": 2.2416582107543945,
      "learning_rate": 1.1276190476190476e-05,
      "loss": 0.6517,
      "step": 22900
    },
    {
      "epoch": 6.545714285714285,
      "grad_norm": 11.3579740524292,
      "learning_rate": 1.1272380952380952e-05,
      "loss": 0.5735,
      "step": 22910
    },
    {
      "epoch": 6.548571428571429,
      "grad_norm": 0.5414755940437317,
      "learning_rate": 1.1268571428571429e-05,
      "loss": 0.2024,
      "step": 22920
    },
    {
      "epoch": 6.551428571428572,
      "grad_norm": 0.5734747052192688,
      "learning_rate": 1.1264761904761904e-05,
      "loss": 0.3467,
      "step": 22930
    },
    {
      "epoch": 6.554285714285714,
      "grad_norm": 0.5415908098220825,
      "learning_rate": 1.1260952380952382e-05,
      "loss": 0.3469,
      "step": 22940
    },
    {
      "epoch": 6.557142857142857,
      "grad_norm": 0.5101885795593262,
      "learning_rate": 1.1257142857142857e-05,
      "loss": 0.4067,
      "step": 22950
    },
    {
      "epoch": 6.5600000000000005,
      "grad_norm": 0.369685560464859,
      "learning_rate": 1.1253333333333335e-05,
      "loss": 0.4372,
      "step": 22960
    },
    {
      "epoch": 6.562857142857143,
      "grad_norm": 0.4967692494392395,
      "learning_rate": 1.124952380952381e-05,
      "loss": 0.3074,
      "step": 22970
    },
    {
      "epoch": 6.565714285714286,
      "grad_norm": 0.44930019974708557,
      "learning_rate": 1.1245714285714286e-05,
      "loss": 0.3261,
      "step": 22980
    },
    {
      "epoch": 6.5685714285714285,
      "grad_norm": 10.894770622253418,
      "learning_rate": 1.1241904761904763e-05,
      "loss": 0.4806,
      "step": 22990
    },
    {
      "epoch": 6.571428571428571,
      "grad_norm": 0.7439725995063782,
      "learning_rate": 1.1238095238095239e-05,
      "loss": 0.219,
      "step": 23000
    },
    {
      "epoch": 6.574285714285715,
      "grad_norm": 0.3443430960178375,
      "learning_rate": 1.1234285714285716e-05,
      "loss": 0.337,
      "step": 23010
    },
    {
      "epoch": 6.577142857142857,
      "grad_norm": 0.3829633891582489,
      "learning_rate": 1.1230476190476191e-05,
      "loss": 0.4047,
      "step": 23020
    },
    {
      "epoch": 6.58,
      "grad_norm": 0.32000234723091125,
      "learning_rate": 1.1226666666666669e-05,
      "loss": 0.503,
      "step": 23030
    },
    {
      "epoch": 6.582857142857143,
      "grad_norm": 0.3080655634403229,
      "learning_rate": 1.1222857142857144e-05,
      "loss": 0.3946,
      "step": 23040
    },
    {
      "epoch": 6.585714285714285,
      "grad_norm": 1.094484806060791,
      "learning_rate": 1.121904761904762e-05,
      "loss": 0.6152,
      "step": 23050
    },
    {
      "epoch": 6.588571428571429,
      "grad_norm": 11.31384563446045,
      "learning_rate": 1.1215238095238097e-05,
      "loss": 0.3857,
      "step": 23060
    },
    {
      "epoch": 6.591428571428572,
      "grad_norm": 0.26617273688316345,
      "learning_rate": 1.1211428571428573e-05,
      "loss": 0.6573,
      "step": 23070
    },
    {
      "epoch": 6.594285714285714,
      "grad_norm": 11.058876991271973,
      "learning_rate": 1.120761904761905e-05,
      "loss": 0.8628,
      "step": 23080
    },
    {
      "epoch": 6.597142857142857,
      "grad_norm": 0.3193170428276062,
      "learning_rate": 1.1203809523809526e-05,
      "loss": 0.6797,
      "step": 23090
    },
    {
      "epoch": 6.6,
      "grad_norm": 12.200594902038574,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 0.6066,
      "step": 23100
    },
    {
      "epoch": 6.602857142857143,
      "grad_norm": 11.253469467163086,
      "learning_rate": 1.1196190476190477e-05,
      "loss": 0.3999,
      "step": 23110
    },
    {
      "epoch": 6.605714285714286,
      "grad_norm": 0.3707987070083618,
      "learning_rate": 1.1192380952380952e-05,
      "loss": 0.4839,
      "step": 23120
    },
    {
      "epoch": 6.6085714285714285,
      "grad_norm": 0.37610939145088196,
      "learning_rate": 1.1188571428571428e-05,
      "loss": 0.9186,
      "step": 23130
    },
    {
      "epoch": 6.611428571428571,
      "grad_norm": 0.4984510540962219,
      "learning_rate": 1.1184761904761905e-05,
      "loss": 0.6887,
      "step": 23140
    },
    {
      "epoch": 6.614285714285714,
      "grad_norm": 0.41958656907081604,
      "learning_rate": 1.118095238095238e-05,
      "loss": 0.3298,
      "step": 23150
    },
    {
      "epoch": 6.617142857142857,
      "grad_norm": 0.39176440238952637,
      "learning_rate": 1.1177142857142858e-05,
      "loss": 0.4442,
      "step": 23160
    },
    {
      "epoch": 6.62,
      "grad_norm": 0.4107469320297241,
      "learning_rate": 1.1173333333333334e-05,
      "loss": 0.3626,
      "step": 23170
    },
    {
      "epoch": 6.622857142857143,
      "grad_norm": 0.9218759536743164,
      "learning_rate": 1.1169523809523811e-05,
      "loss": 0.2608,
      "step": 23180
    },
    {
      "epoch": 6.6257142857142854,
      "grad_norm": 0.39556416869163513,
      "learning_rate": 1.1165714285714287e-05,
      "loss": 0.7605,
      "step": 23190
    },
    {
      "epoch": 6.628571428571428,
      "grad_norm": 0.44199949502944946,
      "learning_rate": 1.1161904761904762e-05,
      "loss": 0.2573,
      "step": 23200
    },
    {
      "epoch": 6.631428571428572,
      "grad_norm": 0.3819334805011749,
      "learning_rate": 1.115809523809524e-05,
      "loss": 0.5297,
      "step": 23210
    },
    {
      "epoch": 6.634285714285714,
      "grad_norm": 0.45147719979286194,
      "learning_rate": 1.1154285714285715e-05,
      "loss": 0.5291,
      "step": 23220
    },
    {
      "epoch": 6.637142857142857,
      "grad_norm": 0.4282173216342926,
      "learning_rate": 1.1150476190476192e-05,
      "loss": 0.5293,
      "step": 23230
    },
    {
      "epoch": 6.64,
      "grad_norm": 11.873047828674316,
      "learning_rate": 1.1146666666666668e-05,
      "loss": 0.5605,
      "step": 23240
    },
    {
      "epoch": 6.642857142857143,
      "grad_norm": 10.8323974609375,
      "learning_rate": 1.1142857142857143e-05,
      "loss": 0.7106,
      "step": 23250
    },
    {
      "epoch": 6.645714285714286,
      "grad_norm": 0.5155794620513916,
      "learning_rate": 1.113904761904762e-05,
      "loss": 0.2924,
      "step": 23260
    },
    {
      "epoch": 6.648571428571429,
      "grad_norm": 11.373191833496094,
      "learning_rate": 1.1135238095238096e-05,
      "loss": 0.5459,
      "step": 23270
    },
    {
      "epoch": 6.651428571428571,
      "grad_norm": 0.3999955952167511,
      "learning_rate": 1.1131428571428574e-05,
      "loss": 0.0955,
      "step": 23280
    },
    {
      "epoch": 6.654285714285714,
      "grad_norm": 11.428614616394043,
      "learning_rate": 1.1127619047619049e-05,
      "loss": 0.1942,
      "step": 23290
    },
    {
      "epoch": 6.6571428571428575,
      "grad_norm": 11.273859977722168,
      "learning_rate": 1.1123809523809526e-05,
      "loss": 0.6077,
      "step": 23300
    },
    {
      "epoch": 6.66,
      "grad_norm": 0.3865775465965271,
      "learning_rate": 1.1120000000000002e-05,
      "loss": 0.5906,
      "step": 23310
    },
    {
      "epoch": 6.662857142857143,
      "grad_norm": 0.37662261724472046,
      "learning_rate": 1.1116190476190478e-05,
      "loss": 0.3448,
      "step": 23320
    },
    {
      "epoch": 6.6657142857142855,
      "grad_norm": 0.5938774347305298,
      "learning_rate": 1.1112380952380951e-05,
      "loss": 0.3089,
      "step": 23330
    },
    {
      "epoch": 6.668571428571429,
      "grad_norm": 12.217730522155762,
      "learning_rate": 1.1108571428571429e-05,
      "loss": 0.1923,
      "step": 23340
    },
    {
      "epoch": 6.671428571428572,
      "grad_norm": 0.27508410811424255,
      "learning_rate": 1.1104761904761904e-05,
      "loss": 0.4054,
      "step": 23350
    },
    {
      "epoch": 6.674285714285714,
      "grad_norm": 0.4414445161819458,
      "learning_rate": 1.1100952380952382e-05,
      "loss": 0.439,
      "step": 23360
    },
    {
      "epoch": 6.677142857142857,
      "grad_norm": 11.656944274902344,
      "learning_rate": 1.1097142857142857e-05,
      "loss": 0.7799,
      "step": 23370
    },
    {
      "epoch": 6.68,
      "grad_norm": 22.458410263061523,
      "learning_rate": 1.1093333333333334e-05,
      "loss": 0.8458,
      "step": 23380
    },
    {
      "epoch": 6.682857142857143,
      "grad_norm": 1.0065617561340332,
      "learning_rate": 1.108952380952381e-05,
      "loss": 0.5864,
      "step": 23390
    },
    {
      "epoch": 6.685714285714286,
      "grad_norm": 0.4018280506134033,
      "learning_rate": 1.1085714285714286e-05,
      "loss": 0.7591,
      "step": 23400
    },
    {
      "epoch": 6.688571428571429,
      "grad_norm": 0.42398712038993835,
      "learning_rate": 1.1081904761904763e-05,
      "loss": 0.32,
      "step": 23410
    },
    {
      "epoch": 6.691428571428571,
      "grad_norm": 22.762012481689453,
      "learning_rate": 1.1078095238095238e-05,
      "loss": 1.1786,
      "step": 23420
    },
    {
      "epoch": 6.694285714285714,
      "grad_norm": 0.6809502243995667,
      "learning_rate": 1.1074285714285716e-05,
      "loss": 0.6778,
      "step": 23430
    },
    {
      "epoch": 6.6971428571428575,
      "grad_norm": 0.6920964121818542,
      "learning_rate": 1.1070476190476191e-05,
      "loss": 0.416,
      "step": 23440
    },
    {
      "epoch": 6.7,
      "grad_norm": 11.636651039123535,
      "learning_rate": 1.1066666666666669e-05,
      "loss": 0.5598,
      "step": 23450
    },
    {
      "epoch": 6.702857142857143,
      "grad_norm": 0.8179654479026794,
      "learning_rate": 1.1062857142857144e-05,
      "loss": 0.5033,
      "step": 23460
    },
    {
      "epoch": 6.7057142857142855,
      "grad_norm": 0.5784241557121277,
      "learning_rate": 1.105904761904762e-05,
      "loss": 0.4282,
      "step": 23470
    },
    {
      "epoch": 6.708571428571428,
      "grad_norm": 22.03554916381836,
      "learning_rate": 1.1055238095238097e-05,
      "loss": 0.4112,
      "step": 23480
    },
    {
      "epoch": 6.711428571428572,
      "grad_norm": 11.561028480529785,
      "learning_rate": 1.1051428571428573e-05,
      "loss": 0.6469,
      "step": 23490
    },
    {
      "epoch": 6.714285714285714,
      "grad_norm": 11.35219955444336,
      "learning_rate": 1.104761904761905e-05,
      "loss": 0.4155,
      "step": 23500
    },
    {
      "epoch": 6.717142857142857,
      "grad_norm": 0.5859089493751526,
      "learning_rate": 1.1043809523809525e-05,
      "loss": 0.5338,
      "step": 23510
    },
    {
      "epoch": 6.72,
      "grad_norm": 0.5293574333190918,
      "learning_rate": 1.1040000000000001e-05,
      "loss": 0.2992,
      "step": 23520
    },
    {
      "epoch": 6.722857142857142,
      "grad_norm": 11.204992294311523,
      "learning_rate": 1.1036190476190478e-05,
      "loss": 0.66,
      "step": 23530
    },
    {
      "epoch": 6.725714285714286,
      "grad_norm": 10.723846435546875,
      "learning_rate": 1.1032380952380952e-05,
      "loss": 1.0293,
      "step": 23540
    },
    {
      "epoch": 6.728571428571429,
      "grad_norm": 0.8957434296607971,
      "learning_rate": 1.1028571428571428e-05,
      "loss": 0.6236,
      "step": 23550
    },
    {
      "epoch": 6.731428571428571,
      "grad_norm": 0.8384255766868591,
      "learning_rate": 1.1024761904761905e-05,
      "loss": 0.4081,
      "step": 23560
    },
    {
      "epoch": 6.734285714285714,
      "grad_norm": 10.616299629211426,
      "learning_rate": 1.102095238095238e-05,
      "loss": 0.6112,
      "step": 23570
    },
    {
      "epoch": 6.737142857142857,
      "grad_norm": 10.594162940979004,
      "learning_rate": 1.1017142857142858e-05,
      "loss": 0.8979,
      "step": 23580
    },
    {
      "epoch": 6.74,
      "grad_norm": 11.23258113861084,
      "learning_rate": 1.1013333333333333e-05,
      "loss": 0.5806,
      "step": 23590
    },
    {
      "epoch": 6.742857142857143,
      "grad_norm": 11.243413925170898,
      "learning_rate": 1.100952380952381e-05,
      "loss": 0.6018,
      "step": 23600
    },
    {
      "epoch": 6.7457142857142856,
      "grad_norm": 11.298041343688965,
      "learning_rate": 1.1005714285714286e-05,
      "loss": 0.8276,
      "step": 23610
    },
    {
      "epoch": 6.748571428571428,
      "grad_norm": 11.838216781616211,
      "learning_rate": 1.1001904761904762e-05,
      "loss": 0.7351,
      "step": 23620
    },
    {
      "epoch": 6.751428571428572,
      "grad_norm": 12.065067291259766,
      "learning_rate": 1.099809523809524e-05,
      "loss": 0.7659,
      "step": 23630
    },
    {
      "epoch": 6.7542857142857144,
      "grad_norm": 12.773154258728027,
      "learning_rate": 1.0994285714285715e-05,
      "loss": 0.6623,
      "step": 23640
    },
    {
      "epoch": 6.757142857142857,
      "grad_norm": 0.6798264980316162,
      "learning_rate": 1.0990476190476192e-05,
      "loss": 0.2827,
      "step": 23650
    },
    {
      "epoch": 6.76,
      "grad_norm": 1.0778225660324097,
      "learning_rate": 1.0986666666666668e-05,
      "loss": 0.7303,
      "step": 23660
    },
    {
      "epoch": 6.762857142857143,
      "grad_norm": 0.6779574751853943,
      "learning_rate": 1.0982857142857143e-05,
      "loss": 0.2996,
      "step": 23670
    },
    {
      "epoch": 6.765714285714286,
      "grad_norm": 0.6842542886734009,
      "learning_rate": 1.097904761904762e-05,
      "loss": 0.2817,
      "step": 23680
    },
    {
      "epoch": 6.768571428571429,
      "grad_norm": 11.038081169128418,
      "learning_rate": 1.0975238095238096e-05,
      "loss": 0.4944,
      "step": 23690
    },
    {
      "epoch": 6.771428571428571,
      "grad_norm": 11.791648864746094,
      "learning_rate": 1.0971428571428573e-05,
      "loss": 0.3529,
      "step": 23700
    },
    {
      "epoch": 6.774285714285714,
      "grad_norm": 0.45005425810813904,
      "learning_rate": 1.0967619047619049e-05,
      "loss": 0.4418,
      "step": 23710
    },
    {
      "epoch": 6.777142857142858,
      "grad_norm": 11.417305946350098,
      "learning_rate": 1.0963809523809526e-05,
      "loss": 0.5726,
      "step": 23720
    },
    {
      "epoch": 6.78,
      "grad_norm": 12.058972358703613,
      "learning_rate": 1.0960000000000002e-05,
      "loss": 0.7562,
      "step": 23730
    },
    {
      "epoch": 6.782857142857143,
      "grad_norm": 0.50653475522995,
      "learning_rate": 1.0956190476190477e-05,
      "loss": 0.2091,
      "step": 23740
    },
    {
      "epoch": 6.785714285714286,
      "grad_norm": 12.875147819519043,
      "learning_rate": 1.0952380952380955e-05,
      "loss": 0.7012,
      "step": 23750
    },
    {
      "epoch": 6.788571428571428,
      "grad_norm": 0.4620213806629181,
      "learning_rate": 1.0948571428571429e-05,
      "loss": 0.2018,
      "step": 23760
    },
    {
      "epoch": 6.791428571428572,
      "grad_norm": 11.003190040588379,
      "learning_rate": 1.0944761904761904e-05,
      "loss": 0.391,
      "step": 23770
    },
    {
      "epoch": 6.7942857142857145,
      "grad_norm": 0.39106300473213196,
      "learning_rate": 1.0940952380952381e-05,
      "loss": 0.6022,
      "step": 23780
    },
    {
      "epoch": 6.797142857142857,
      "grad_norm": 0.5844821929931641,
      "learning_rate": 1.0937142857142857e-05,
      "loss": 0.2809,
      "step": 23790
    },
    {
      "epoch": 6.8,
      "grad_norm": 1.145429253578186,
      "learning_rate": 1.0933333333333334e-05,
      "loss": 0.2074,
      "step": 23800
    },
    {
      "epoch": 6.8028571428571425,
      "grad_norm": 0.48275285959243774,
      "learning_rate": 1.092952380952381e-05,
      "loss": 0.5025,
      "step": 23810
    },
    {
      "epoch": 6.805714285714286,
      "grad_norm": 0.49527865648269653,
      "learning_rate": 1.0925714285714285e-05,
      "loss": 0.6051,
      "step": 23820
    },
    {
      "epoch": 6.808571428571429,
      "grad_norm": 11.168763160705566,
      "learning_rate": 1.0921904761904763e-05,
      "loss": 0.7807,
      "step": 23830
    },
    {
      "epoch": 6.811428571428571,
      "grad_norm": 0.5022673606872559,
      "learning_rate": 1.0918095238095238e-05,
      "loss": 0.7968,
      "step": 23840
    },
    {
      "epoch": 6.814285714285714,
      "grad_norm": 11.294453620910645,
      "learning_rate": 1.0914285714285716e-05,
      "loss": 0.7363,
      "step": 23850
    },
    {
      "epoch": 6.817142857142857,
      "grad_norm": 0.6041048765182495,
      "learning_rate": 1.0910476190476191e-05,
      "loss": 0.3434,
      "step": 23860
    },
    {
      "epoch": 6.82,
      "grad_norm": 0.6247476935386658,
      "learning_rate": 1.0906666666666668e-05,
      "loss": 0.3424,
      "step": 23870
    },
    {
      "epoch": 6.822857142857143,
      "grad_norm": 11.134449005126953,
      "learning_rate": 1.0902857142857144e-05,
      "loss": 0.8484,
      "step": 23880
    },
    {
      "epoch": 6.825714285714286,
      "grad_norm": 22.269920349121094,
      "learning_rate": 1.089904761904762e-05,
      "loss": 0.609,
      "step": 23890
    },
    {
      "epoch": 6.828571428571428,
      "grad_norm": 0.7637789845466614,
      "learning_rate": 1.0895238095238097e-05,
      "loss": 0.5199,
      "step": 23900
    },
    {
      "epoch": 6.831428571428571,
      "grad_norm": 11.029302597045898,
      "learning_rate": 1.0891428571428572e-05,
      "loss": 0.5359,
      "step": 23910
    },
    {
      "epoch": 6.8342857142857145,
      "grad_norm": 11.479162216186523,
      "learning_rate": 1.088761904761905e-05,
      "loss": 0.7267,
      "step": 23920
    },
    {
      "epoch": 6.837142857142857,
      "grad_norm": 13.144095420837402,
      "learning_rate": 1.0883809523809525e-05,
      "loss": 0.6006,
      "step": 23930
    },
    {
      "epoch": 6.84,
      "grad_norm": 11.069437980651855,
      "learning_rate": 1.0880000000000001e-05,
      "loss": 1.0995,
      "step": 23940
    },
    {
      "epoch": 6.8428571428571425,
      "grad_norm": 11.067228317260742,
      "learning_rate": 1.0876190476190478e-05,
      "loss": 0.7496,
      "step": 23950
    },
    {
      "epoch": 6.845714285714286,
      "grad_norm": 0.6360429525375366,
      "learning_rate": 1.0872380952380954e-05,
      "loss": 0.6899,
      "step": 23960
    },
    {
      "epoch": 6.848571428571429,
      "grad_norm": 0.9229058027267456,
      "learning_rate": 1.0868571428571431e-05,
      "loss": 0.6743,
      "step": 23970
    },
    {
      "epoch": 6.851428571428571,
      "grad_norm": 1.543018102645874,
      "learning_rate": 1.0864761904761905e-05,
      "loss": 0.4704,
      "step": 23980
    },
    {
      "epoch": 6.854285714285714,
      "grad_norm": 1.6055575609207153,
      "learning_rate": 1.086095238095238e-05,
      "loss": 0.4185,
      "step": 23990
    },
    {
      "epoch": 6.857142857142857,
      "grad_norm": 0.9786038398742676,
      "learning_rate": 1.0857142857142858e-05,
      "loss": 0.3562,
      "step": 24000
    },
    {
      "epoch": 6.86,
      "grad_norm": 0.5307798385620117,
      "learning_rate": 1.0853333333333333e-05,
      "loss": 0.1876,
      "step": 24010
    },
    {
      "epoch": 6.862857142857143,
      "grad_norm": 11.031403541564941,
      "learning_rate": 1.084952380952381e-05,
      "loss": 0.422,
      "step": 24020
    },
    {
      "epoch": 6.865714285714286,
      "grad_norm": 11.060455322265625,
      "learning_rate": 1.0845714285714286e-05,
      "loss": 0.5536,
      "step": 24030
    },
    {
      "epoch": 6.868571428571428,
      "grad_norm": 0.6494560241699219,
      "learning_rate": 1.0841904761904762e-05,
      "loss": 0.4034,
      "step": 24040
    },
    {
      "epoch": 6.871428571428572,
      "grad_norm": 11.37807846069336,
      "learning_rate": 1.0838095238095239e-05,
      "loss": 0.3148,
      "step": 24050
    },
    {
      "epoch": 6.8742857142857146,
      "grad_norm": 22.11855697631836,
      "learning_rate": 1.0834285714285715e-05,
      "loss": 0.6944,
      "step": 24060
    },
    {
      "epoch": 6.877142857142857,
      "grad_norm": 0.4533572196960449,
      "learning_rate": 1.0830476190476192e-05,
      "loss": 0.5207,
      "step": 24070
    },
    {
      "epoch": 6.88,
      "grad_norm": 11.200102806091309,
      "learning_rate": 1.0826666666666667e-05,
      "loss": 0.7864,
      "step": 24080
    },
    {
      "epoch": 6.882857142857143,
      "grad_norm": 0.4881586730480194,
      "learning_rate": 1.0822857142857143e-05,
      "loss": 0.3696,
      "step": 24090
    },
    {
      "epoch": 6.885714285714286,
      "grad_norm": 11.928275108337402,
      "learning_rate": 1.081904761904762e-05,
      "loss": 0.6451,
      "step": 24100
    },
    {
      "epoch": 6.888571428571429,
      "grad_norm": 0.7603151798248291,
      "learning_rate": 1.0815238095238096e-05,
      "loss": 0.4256,
      "step": 24110
    },
    {
      "epoch": 6.8914285714285715,
      "grad_norm": 2.2243425846099854,
      "learning_rate": 1.0811428571428573e-05,
      "loss": 0.5069,
      "step": 24120
    },
    {
      "epoch": 6.894285714285714,
      "grad_norm": 0.4570719003677368,
      "learning_rate": 1.0807619047619049e-05,
      "loss": 0.5898,
      "step": 24130
    },
    {
      "epoch": 6.897142857142857,
      "grad_norm": 0.8458859324455261,
      "learning_rate": 1.0803809523809526e-05,
      "loss": 0.3088,
      "step": 24140
    },
    {
      "epoch": 6.9,
      "grad_norm": 12.306435585021973,
      "learning_rate": 1.0800000000000002e-05,
      "loss": 0.5178,
      "step": 24150
    },
    {
      "epoch": 6.902857142857143,
      "grad_norm": 1.367967963218689,
      "learning_rate": 1.0796190476190477e-05,
      "loss": 0.3599,
      "step": 24160
    },
    {
      "epoch": 6.905714285714286,
      "grad_norm": 12.178885459899902,
      "learning_rate": 1.0792380952380954e-05,
      "loss": 0.3606,
      "step": 24170
    },
    {
      "epoch": 6.908571428571428,
      "grad_norm": 0.44042840600013733,
      "learning_rate": 1.078857142857143e-05,
      "loss": 0.4436,
      "step": 24180
    },
    {
      "epoch": 6.911428571428571,
      "grad_norm": 0.40535399317741394,
      "learning_rate": 1.0784761904761904e-05,
      "loss": 0.4046,
      "step": 24190
    },
    {
      "epoch": 6.914285714285715,
      "grad_norm": 0.41659146547317505,
      "learning_rate": 1.0780952380952381e-05,
      "loss": 0.6082,
      "step": 24200
    },
    {
      "epoch": 6.917142857142857,
      "grad_norm": 0.40654653310775757,
      "learning_rate": 1.0777142857142857e-05,
      "loss": 0.705,
      "step": 24210
    },
    {
      "epoch": 6.92,
      "grad_norm": 11.693504333496094,
      "learning_rate": 1.0773333333333334e-05,
      "loss": 0.5784,
      "step": 24220
    },
    {
      "epoch": 6.922857142857143,
      "grad_norm": 11.017367362976074,
      "learning_rate": 1.076952380952381e-05,
      "loss": 0.5305,
      "step": 24230
    },
    {
      "epoch": 6.925714285714285,
      "grad_norm": 0.49877437949180603,
      "learning_rate": 1.0765714285714285e-05,
      "loss": 0.3754,
      "step": 24240
    },
    {
      "epoch": 6.928571428571429,
      "grad_norm": 10.777687072753906,
      "learning_rate": 1.0761904761904763e-05,
      "loss": 0.6082,
      "step": 24250
    },
    {
      "epoch": 6.9314285714285715,
      "grad_norm": 12.473109245300293,
      "learning_rate": 1.0758095238095238e-05,
      "loss": 0.2966,
      "step": 24260
    },
    {
      "epoch": 6.934285714285714,
      "grad_norm": 0.5330758094787598,
      "learning_rate": 1.0754285714285715e-05,
      "loss": 0.4461,
      "step": 24270
    },
    {
      "epoch": 6.937142857142857,
      "grad_norm": 1.544336199760437,
      "learning_rate": 1.0750476190476191e-05,
      "loss": 0.6864,
      "step": 24280
    },
    {
      "epoch": 6.9399999999999995,
      "grad_norm": 0.3293323218822479,
      "learning_rate": 1.0746666666666668e-05,
      "loss": 0.424,
      "step": 24290
    },
    {
      "epoch": 6.942857142857143,
      "grad_norm": 0.39819571375846863,
      "learning_rate": 1.0742857142857144e-05,
      "loss": 0.8225,
      "step": 24300
    },
    {
      "epoch": 6.945714285714286,
      "grad_norm": 0.5046008229255676,
      "learning_rate": 1.073904761904762e-05,
      "loss": 0.5228,
      "step": 24310
    },
    {
      "epoch": 6.948571428571428,
      "grad_norm": 21.141294479370117,
      "learning_rate": 1.0735238095238097e-05,
      "loss": 1.0246,
      "step": 24320
    },
    {
      "epoch": 6.951428571428571,
      "grad_norm": 23.58829689025879,
      "learning_rate": 1.0731428571428572e-05,
      "loss": 0.8835,
      "step": 24330
    },
    {
      "epoch": 6.954285714285715,
      "grad_norm": 0.6234294772148132,
      "learning_rate": 1.072761904761905e-05,
      "loss": 0.7176,
      "step": 24340
    },
    {
      "epoch": 6.957142857142857,
      "grad_norm": 0.6084336042404175,
      "learning_rate": 1.0723809523809525e-05,
      "loss": 0.4802,
      "step": 24350
    },
    {
      "epoch": 6.96,
      "grad_norm": 0.603119969367981,
      "learning_rate": 1.072e-05,
      "loss": 0.2802,
      "step": 24360
    },
    {
      "epoch": 6.962857142857143,
      "grad_norm": 0.5569162964820862,
      "learning_rate": 1.0716190476190478e-05,
      "loss": 0.3186,
      "step": 24370
    },
    {
      "epoch": 6.965714285714286,
      "grad_norm": 10.877494812011719,
      "learning_rate": 1.0712380952380954e-05,
      "loss": 0.3681,
      "step": 24380
    },
    {
      "epoch": 6.968571428571429,
      "grad_norm": 0.782450258731842,
      "learning_rate": 1.070857142857143e-05,
      "loss": 0.4387,
      "step": 24390
    },
    {
      "epoch": 6.9714285714285715,
      "grad_norm": 0.4613773822784424,
      "learning_rate": 1.0704761904761906e-05,
      "loss": 0.6941,
      "step": 24400
    },
    {
      "epoch": 6.974285714285714,
      "grad_norm": 0.41043925285339355,
      "learning_rate": 1.070095238095238e-05,
      "loss": 0.1916,
      "step": 24410
    },
    {
      "epoch": 6.977142857142857,
      "grad_norm": 1.0835636854171753,
      "learning_rate": 1.0697142857142858e-05,
      "loss": 0.3605,
      "step": 24420
    },
    {
      "epoch": 6.98,
      "grad_norm": 0.4179254472255707,
      "learning_rate": 1.0693333333333333e-05,
      "loss": 0.6368,
      "step": 24430
    },
    {
      "epoch": 6.982857142857143,
      "grad_norm": 0.7948235273361206,
      "learning_rate": 1.068952380952381e-05,
      "loss": 0.3634,
      "step": 24440
    },
    {
      "epoch": 6.985714285714286,
      "grad_norm": 10.882400512695312,
      "learning_rate": 1.0685714285714286e-05,
      "loss": 0.4683,
      "step": 24450
    },
    {
      "epoch": 6.988571428571428,
      "grad_norm": 11.27000904083252,
      "learning_rate": 1.0681904761904762e-05,
      "loss": 0.4197,
      "step": 24460
    },
    {
      "epoch": 6.991428571428571,
      "grad_norm": 11.313857078552246,
      "learning_rate": 1.0678095238095239e-05,
      "loss": 0.5137,
      "step": 24470
    },
    {
      "epoch": 6.994285714285715,
      "grad_norm": 0.3665323555469513,
      "learning_rate": 1.0674285714285714e-05,
      "loss": 0.6117,
      "step": 24480
    },
    {
      "epoch": 6.997142857142857,
      "grad_norm": 11.189623832702637,
      "learning_rate": 1.0670476190476192e-05,
      "loss": 0.1083,
      "step": 24490
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.4113500416278839,
      "learning_rate": 1.0666666666666667e-05,
      "loss": 0.6016,
      "step": 24500
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.8709677419354839,
      "eval_f1": 0.0,
      "eval_loss": 0.5361905097961426,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 47.1796,
      "eval_samples_per_second": 63.587,
      "eval_steps_per_second": 1.992,
      "step": 24500
    },
    {
      "epoch": 7.002857142857143,
      "grad_norm": 11.672142028808594,
      "learning_rate": 1.0662857142857143e-05,
      "loss": 0.5377,
      "step": 24510
    },
    {
      "epoch": 7.005714285714285,
      "grad_norm": 12.640591621398926,
      "learning_rate": 1.065904761904762e-05,
      "loss": 0.7724,
      "step": 24520
    },
    {
      "epoch": 7.008571428571429,
      "grad_norm": 0.5197376608848572,
      "learning_rate": 1.0655238095238096e-05,
      "loss": 0.4349,
      "step": 24530
    },
    {
      "epoch": 7.011428571428572,
      "grad_norm": 0.4583507776260376,
      "learning_rate": 1.0651428571428573e-05,
      "loss": 0.3281,
      "step": 24540
    },
    {
      "epoch": 7.014285714285714,
      "grad_norm": 0.2912730574607849,
      "learning_rate": 1.0647619047619049e-05,
      "loss": 0.7676,
      "step": 24550
    },
    {
      "epoch": 7.017142857142857,
      "grad_norm": 11.327722549438477,
      "learning_rate": 1.0643809523809526e-05,
      "loss": 0.327,
      "step": 24560
    },
    {
      "epoch": 7.02,
      "grad_norm": 21.932659149169922,
      "learning_rate": 1.0640000000000001e-05,
      "loss": 1.1186,
      "step": 24570
    },
    {
      "epoch": 7.022857142857143,
      "grad_norm": 0.3649539053440094,
      "learning_rate": 1.0636190476190477e-05,
      "loss": 0.4387,
      "step": 24580
    },
    {
      "epoch": 7.025714285714286,
      "grad_norm": 0.37882813811302185,
      "learning_rate": 1.0632380952380954e-05,
      "loss": 0.2268,
      "step": 24590
    },
    {
      "epoch": 7.0285714285714285,
      "grad_norm": 0.35939592123031616,
      "learning_rate": 1.062857142857143e-05,
      "loss": 0.5882,
      "step": 24600
    },
    {
      "epoch": 7.031428571428571,
      "grad_norm": 0.3072660565376282,
      "learning_rate": 1.0624761904761907e-05,
      "loss": 0.2915,
      "step": 24610
    },
    {
      "epoch": 7.034285714285715,
      "grad_norm": 0.3533317744731903,
      "learning_rate": 1.0620952380952383e-05,
      "loss": 0.4418,
      "step": 24620
    },
    {
      "epoch": 7.037142857142857,
      "grad_norm": 11.708575248718262,
      "learning_rate": 1.0617142857142857e-05,
      "loss": 1.2572,
      "step": 24630
    },
    {
      "epoch": 7.04,
      "grad_norm": 0.49297064542770386,
      "learning_rate": 1.0613333333333334e-05,
      "loss": 0.3125,
      "step": 24640
    },
    {
      "epoch": 7.042857142857143,
      "grad_norm": 0.32636332511901855,
      "learning_rate": 1.060952380952381e-05,
      "loss": 0.3173,
      "step": 24650
    },
    {
      "epoch": 7.045714285714285,
      "grad_norm": 0.3367396593093872,
      "learning_rate": 1.0605714285714285e-05,
      "loss": 0.6227,
      "step": 24660
    },
    {
      "epoch": 7.048571428571429,
      "grad_norm": 0.3272627592086792,
      "learning_rate": 1.0601904761904762e-05,
      "loss": 0.437,
      "step": 24670
    },
    {
      "epoch": 7.051428571428572,
      "grad_norm": 0.3522507846355438,
      "learning_rate": 1.0598095238095238e-05,
      "loss": 0.711,
      "step": 24680
    },
    {
      "epoch": 7.054285714285714,
      "grad_norm": 11.372457504272461,
      "learning_rate": 1.0594285714285715e-05,
      "loss": 0.6172,
      "step": 24690
    },
    {
      "epoch": 7.057142857142857,
      "grad_norm": 0.4060685634613037,
      "learning_rate": 1.059047619047619e-05,
      "loss": 0.5665,
      "step": 24700
    },
    {
      "epoch": 7.06,
      "grad_norm": 0.8125633001327515,
      "learning_rate": 1.0586666666666668e-05,
      "loss": 0.6166,
      "step": 24710
    },
    {
      "epoch": 7.062857142857143,
      "grad_norm": 0.388110876083374,
      "learning_rate": 1.0582857142857144e-05,
      "loss": 0.5393,
      "step": 24720
    },
    {
      "epoch": 7.065714285714286,
      "grad_norm": 0.37231117486953735,
      "learning_rate": 1.057904761904762e-05,
      "loss": 0.3809,
      "step": 24730
    },
    {
      "epoch": 7.0685714285714285,
      "grad_norm": 11.129474639892578,
      "learning_rate": 1.0575238095238097e-05,
      "loss": 0.7954,
      "step": 24740
    },
    {
      "epoch": 7.071428571428571,
      "grad_norm": 0.40590959787368774,
      "learning_rate": 1.0571428571428572e-05,
      "loss": 0.181,
      "step": 24750
    },
    {
      "epoch": 7.074285714285715,
      "grad_norm": 22.48545265197754,
      "learning_rate": 1.056761904761905e-05,
      "loss": 0.3863,
      "step": 24760
    },
    {
      "epoch": 7.077142857142857,
      "grad_norm": 0.3543426990509033,
      "learning_rate": 1.0563809523809525e-05,
      "loss": 0.5444,
      "step": 24770
    },
    {
      "epoch": 7.08,
      "grad_norm": 11.727096557617188,
      "learning_rate": 1.056e-05,
      "loss": 0.5861,
      "step": 24780
    },
    {
      "epoch": 7.082857142857143,
      "grad_norm": 10.913558006286621,
      "learning_rate": 1.0556190476190478e-05,
      "loss": 0.1306,
      "step": 24790
    },
    {
      "epoch": 7.085714285714285,
      "grad_norm": 0.3104090392589569,
      "learning_rate": 1.0552380952380953e-05,
      "loss": 0.7691,
      "step": 24800
    },
    {
      "epoch": 7.088571428571429,
      "grad_norm": 0.3377473056316376,
      "learning_rate": 1.054857142857143e-05,
      "loss": 0.7595,
      "step": 24810
    },
    {
      "epoch": 7.091428571428572,
      "grad_norm": 11.162269592285156,
      "learning_rate": 1.0544761904761906e-05,
      "loss": 0.5889,
      "step": 24820
    },
    {
      "epoch": 7.094285714285714,
      "grad_norm": 0.3900856375694275,
      "learning_rate": 1.0540952380952384e-05,
      "loss": 0.3058,
      "step": 24830
    },
    {
      "epoch": 7.097142857142857,
      "grad_norm": 0.3369104564189911,
      "learning_rate": 1.0537142857142857e-05,
      "loss": 0.3082,
      "step": 24840
    },
    {
      "epoch": 7.1,
      "grad_norm": 12.022012710571289,
      "learning_rate": 1.0533333333333333e-05,
      "loss": 0.5621,
      "step": 24850
    },
    {
      "epoch": 7.102857142857143,
      "grad_norm": 0.4572135806083679,
      "learning_rate": 1.052952380952381e-05,
      "loss": 0.4652,
      "step": 24860
    },
    {
      "epoch": 7.105714285714286,
      "grad_norm": 0.3018821179866791,
      "learning_rate": 1.0525714285714286e-05,
      "loss": 0.1063,
      "step": 24870
    },
    {
      "epoch": 7.1085714285714285,
      "grad_norm": 11.191272735595703,
      "learning_rate": 1.0521904761904761e-05,
      "loss": 0.5889,
      "step": 24880
    },
    {
      "epoch": 7.111428571428571,
      "grad_norm": 11.204487800598145,
      "learning_rate": 1.0518095238095239e-05,
      "loss": 0.6636,
      "step": 24890
    },
    {
      "epoch": 7.114285714285714,
      "grad_norm": 11.597041130065918,
      "learning_rate": 1.0514285714285714e-05,
      "loss": 0.3915,
      "step": 24900
    },
    {
      "epoch": 7.117142857142857,
      "grad_norm": 10.60755729675293,
      "learning_rate": 1.0510476190476192e-05,
      "loss": 0.4117,
      "step": 24910
    },
    {
      "epoch": 7.12,
      "grad_norm": 0.33339038491249084,
      "learning_rate": 1.0506666666666667e-05,
      "loss": 0.4638,
      "step": 24920
    },
    {
      "epoch": 7.122857142857143,
      "grad_norm": 14.524789810180664,
      "learning_rate": 1.0502857142857143e-05,
      "loss": 0.7384,
      "step": 24930
    },
    {
      "epoch": 7.1257142857142854,
      "grad_norm": 0.34724435210227966,
      "learning_rate": 1.049904761904762e-05,
      "loss": 0.2496,
      "step": 24940
    },
    {
      "epoch": 7.128571428571428,
      "grad_norm": 0.3783978521823883,
      "learning_rate": 1.0495238095238096e-05,
      "loss": 0.8091,
      "step": 24950
    },
    {
      "epoch": 7.131428571428572,
      "grad_norm": 10.70423412322998,
      "learning_rate": 1.0491428571428573e-05,
      "loss": 0.6141,
      "step": 24960
    },
    {
      "epoch": 7.134285714285714,
      "grad_norm": 11.195477485656738,
      "learning_rate": 1.0487619047619048e-05,
      "loss": 0.6627,
      "step": 24970
    },
    {
      "epoch": 7.137142857142857,
      "grad_norm": 12.040627479553223,
      "learning_rate": 1.0483809523809526e-05,
      "loss": 0.6474,
      "step": 24980
    },
    {
      "epoch": 7.14,
      "grad_norm": 21.232330322265625,
      "learning_rate": 1.0480000000000001e-05,
      "loss": 0.2945,
      "step": 24990
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 0.5042281150817871,
      "learning_rate": 1.0476190476190477e-05,
      "loss": 0.7313,
      "step": 25000
    },
    {
      "epoch": 7.145714285714286,
      "grad_norm": 11.82262134552002,
      "learning_rate": 1.0472380952380954e-05,
      "loss": 0.4258,
      "step": 25010
    },
    {
      "epoch": 7.148571428571429,
      "grad_norm": 10.876683235168457,
      "learning_rate": 1.046857142857143e-05,
      "loss": 0.6653,
      "step": 25020
    },
    {
      "epoch": 7.151428571428571,
      "grad_norm": 11.278397560119629,
      "learning_rate": 1.0464761904761907e-05,
      "loss": 0.6359,
      "step": 25030
    },
    {
      "epoch": 7.154285714285714,
      "grad_norm": 9.95541000366211,
      "learning_rate": 1.0460952380952383e-05,
      "loss": 0.6645,
      "step": 25040
    },
    {
      "epoch": 7.1571428571428575,
      "grad_norm": 0.464584618806839,
      "learning_rate": 1.045714285714286e-05,
      "loss": 0.2459,
      "step": 25050
    },
    {
      "epoch": 7.16,
      "grad_norm": 11.043827056884766,
      "learning_rate": 1.0453333333333334e-05,
      "loss": 0.4646,
      "step": 25060
    },
    {
      "epoch": 7.162857142857143,
      "grad_norm": 1.2806556224822998,
      "learning_rate": 1.044952380952381e-05,
      "loss": 0.4295,
      "step": 25070
    },
    {
      "epoch": 7.1657142857142855,
      "grad_norm": 0.3734808564186096,
      "learning_rate": 1.0445714285714285e-05,
      "loss": 0.3763,
      "step": 25080
    },
    {
      "epoch": 7.168571428571428,
      "grad_norm": 0.7145366072654724,
      "learning_rate": 1.0441904761904762e-05,
      "loss": 0.2103,
      "step": 25090
    },
    {
      "epoch": 7.171428571428572,
      "grad_norm": 11.043814659118652,
      "learning_rate": 1.0438095238095238e-05,
      "loss": 0.7195,
      "step": 25100
    },
    {
      "epoch": 7.174285714285714,
      "grad_norm": 0.6413182020187378,
      "learning_rate": 1.0434285714285715e-05,
      "loss": 0.5667,
      "step": 25110
    },
    {
      "epoch": 7.177142857142857,
      "grad_norm": 11.05261516571045,
      "learning_rate": 1.043047619047619e-05,
      "loss": 0.5691,
      "step": 25120
    },
    {
      "epoch": 7.18,
      "grad_norm": 12.375407218933105,
      "learning_rate": 1.0426666666666668e-05,
      "loss": 0.801,
      "step": 25130
    },
    {
      "epoch": 7.182857142857143,
      "grad_norm": 0.5193557739257812,
      "learning_rate": 1.0422857142857143e-05,
      "loss": 0.521,
      "step": 25140
    },
    {
      "epoch": 7.185714285714286,
      "grad_norm": 0.44721975922584534,
      "learning_rate": 1.0419047619047619e-05,
      "loss": 0.0955,
      "step": 25150
    },
    {
      "epoch": 7.188571428571429,
      "grad_norm": 11.188943862915039,
      "learning_rate": 1.0415238095238096e-05,
      "loss": 0.6,
      "step": 25160
    },
    {
      "epoch": 7.191428571428571,
      "grad_norm": 0.46443724632263184,
      "learning_rate": 1.0411428571428572e-05,
      "loss": 0.7554,
      "step": 25170
    },
    {
      "epoch": 7.194285714285714,
      "grad_norm": 0.46646058559417725,
      "learning_rate": 1.040761904761905e-05,
      "loss": 0.4582,
      "step": 25180
    },
    {
      "epoch": 7.1971428571428575,
      "grad_norm": 0.4817187786102295,
      "learning_rate": 1.0403809523809525e-05,
      "loss": 0.8238,
      "step": 25190
    },
    {
      "epoch": 7.2,
      "grad_norm": 0.4263814389705658,
      "learning_rate": 1.04e-05,
      "loss": 0.796,
      "step": 25200
    },
    {
      "epoch": 7.202857142857143,
      "grad_norm": 1.2167201042175293,
      "learning_rate": 1.0396190476190478e-05,
      "loss": 0.5167,
      "step": 25210
    },
    {
      "epoch": 7.2057142857142855,
      "grad_norm": 10.666938781738281,
      "learning_rate": 1.0392380952380953e-05,
      "loss": 0.2954,
      "step": 25220
    },
    {
      "epoch": 7.208571428571428,
      "grad_norm": 0.35175877809524536,
      "learning_rate": 1.038857142857143e-05,
      "loss": 0.2178,
      "step": 25230
    },
    {
      "epoch": 7.211428571428572,
      "grad_norm": 10.988987922668457,
      "learning_rate": 1.0384761904761906e-05,
      "loss": 0.6568,
      "step": 25240
    },
    {
      "epoch": 7.214285714285714,
      "grad_norm": 0.331217497587204,
      "learning_rate": 1.0380952380952383e-05,
      "loss": 0.2585,
      "step": 25250
    },
    {
      "epoch": 7.217142857142857,
      "grad_norm": 0.35044777393341064,
      "learning_rate": 1.0377142857142859e-05,
      "loss": 0.4225,
      "step": 25260
    },
    {
      "epoch": 7.22,
      "grad_norm": 11.280311584472656,
      "learning_rate": 1.0373333333333335e-05,
      "loss": 0.354,
      "step": 25270
    },
    {
      "epoch": 7.222857142857142,
      "grad_norm": 12.366482734680176,
      "learning_rate": 1.036952380952381e-05,
      "loss": 0.5518,
      "step": 25280
    },
    {
      "epoch": 7.225714285714286,
      "grad_norm": 11.842536926269531,
      "learning_rate": 1.0365714285714286e-05,
      "loss": 0.877,
      "step": 25290
    },
    {
      "epoch": 7.228571428571429,
      "grad_norm": 0.4666704833507538,
      "learning_rate": 1.0361904761904761e-05,
      "loss": 0.8224,
      "step": 25300
    },
    {
      "epoch": 7.231428571428571,
      "grad_norm": 11.540544509887695,
      "learning_rate": 1.0358095238095239e-05,
      "loss": 0.6198,
      "step": 25310
    },
    {
      "epoch": 7.234285714285714,
      "grad_norm": 0.3907938301563263,
      "learning_rate": 1.0354285714285714e-05,
      "loss": 0.6175,
      "step": 25320
    },
    {
      "epoch": 7.2371428571428575,
      "grad_norm": 0.44052037596702576,
      "learning_rate": 1.0350476190476191e-05,
      "loss": 0.5331,
      "step": 25330
    },
    {
      "epoch": 7.24,
      "grad_norm": 0.43981173634529114,
      "learning_rate": 1.0346666666666667e-05,
      "loss": 0.5317,
      "step": 25340
    },
    {
      "epoch": 7.242857142857143,
      "grad_norm": 11.553994178771973,
      "learning_rate": 1.0342857142857143e-05,
      "loss": 0.6593,
      "step": 25350
    },
    {
      "epoch": 7.2457142857142856,
      "grad_norm": 0.4733116328716278,
      "learning_rate": 1.033904761904762e-05,
      "loss": 0.5358,
      "step": 25360
    },
    {
      "epoch": 7.248571428571428,
      "grad_norm": 11.194687843322754,
      "learning_rate": 1.0335238095238095e-05,
      "loss": 0.4939,
      "step": 25370
    },
    {
      "epoch": 7.251428571428572,
      "grad_norm": 0.7604016661643982,
      "learning_rate": 1.0331428571428573e-05,
      "loss": 0.1833,
      "step": 25380
    },
    {
      "epoch": 7.2542857142857144,
      "grad_norm": 0.7232478857040405,
      "learning_rate": 1.0327619047619048e-05,
      "loss": 0.7842,
      "step": 25390
    },
    {
      "epoch": 7.257142857142857,
      "grad_norm": 10.95949935913086,
      "learning_rate": 1.0323809523809526e-05,
      "loss": 0.5382,
      "step": 25400
    },
    {
      "epoch": 7.26,
      "grad_norm": 10.826215744018555,
      "learning_rate": 1.0320000000000001e-05,
      "loss": 0.3837,
      "step": 25410
    },
    {
      "epoch": 7.2628571428571425,
      "grad_norm": 0.47642046213150024,
      "learning_rate": 1.0316190476190477e-05,
      "loss": 0.2192,
      "step": 25420
    },
    {
      "epoch": 7.265714285714286,
      "grad_norm": 0.5370681881904602,
      "learning_rate": 1.0312380952380954e-05,
      "loss": 0.4291,
      "step": 25430
    },
    {
      "epoch": 7.268571428571429,
      "grad_norm": 0.3948240876197815,
      "learning_rate": 1.030857142857143e-05,
      "loss": 0.3691,
      "step": 25440
    },
    {
      "epoch": 7.271428571428571,
      "grad_norm": 11.363727569580078,
      "learning_rate": 1.0304761904761907e-05,
      "loss": 0.7681,
      "step": 25450
    },
    {
      "epoch": 7.274285714285714,
      "grad_norm": 0.5148797035217285,
      "learning_rate": 1.0300952380952382e-05,
      "loss": 0.3002,
      "step": 25460
    },
    {
      "epoch": 7.277142857142858,
      "grad_norm": 11.534239768981934,
      "learning_rate": 1.029714285714286e-05,
      "loss": 0.4172,
      "step": 25470
    },
    {
      "epoch": 7.28,
      "grad_norm": 0.5888867378234863,
      "learning_rate": 1.0293333333333335e-05,
      "loss": 0.3171,
      "step": 25480
    },
    {
      "epoch": 7.282857142857143,
      "grad_norm": 11.366876602172852,
      "learning_rate": 1.0289523809523811e-05,
      "loss": 0.314,
      "step": 25490
    },
    {
      "epoch": 7.285714285714286,
      "grad_norm": 22.185951232910156,
      "learning_rate": 1.0285714285714285e-05,
      "loss": 1.0331,
      "step": 25500
    },
    {
      "epoch": 7.288571428571428,
      "grad_norm": 0.3369565010070801,
      "learning_rate": 1.0281904761904762e-05,
      "loss": 0.7078,
      "step": 25510
    },
    {
      "epoch": 7.291428571428572,
      "grad_norm": 0.38793057203292847,
      "learning_rate": 1.0278095238095238e-05,
      "loss": 0.4985,
      "step": 25520
    },
    {
      "epoch": 7.2942857142857145,
      "grad_norm": 0.36520880460739136,
      "learning_rate": 1.0274285714285715e-05,
      "loss": 0.7232,
      "step": 25530
    },
    {
      "epoch": 7.297142857142857,
      "grad_norm": 11.132359504699707,
      "learning_rate": 1.027047619047619e-05,
      "loss": 0.8323,
      "step": 25540
    },
    {
      "epoch": 7.3,
      "grad_norm": 0.40649330615997314,
      "learning_rate": 1.0266666666666668e-05,
      "loss": 0.4399,
      "step": 25550
    },
    {
      "epoch": 7.3028571428571425,
      "grad_norm": 0.39678195118904114,
      "learning_rate": 1.0262857142857143e-05,
      "loss": 0.465,
      "step": 25560
    },
    {
      "epoch": 7.305714285714286,
      "grad_norm": 11.418906211853027,
      "learning_rate": 1.0259047619047619e-05,
      "loss": 0.1969,
      "step": 25570
    },
    {
      "epoch": 7.308571428571429,
      "grad_norm": 12.121987342834473,
      "learning_rate": 1.0255238095238096e-05,
      "loss": 0.4894,
      "step": 25580
    },
    {
      "epoch": 7.311428571428571,
      "grad_norm": 0.3936806917190552,
      "learning_rate": 1.0251428571428572e-05,
      "loss": 0.4184,
      "step": 25590
    },
    {
      "epoch": 7.314285714285714,
      "grad_norm": 0.38250890374183655,
      "learning_rate": 1.0247619047619049e-05,
      "loss": 0.1956,
      "step": 25600
    },
    {
      "epoch": 7.317142857142857,
      "grad_norm": 11.422852516174316,
      "learning_rate": 1.0243809523809525e-05,
      "loss": 0.4067,
      "step": 25610
    },
    {
      "epoch": 7.32,
      "grad_norm": 0.3951893150806427,
      "learning_rate": 1.024e-05,
      "loss": 0.6433,
      "step": 25620
    },
    {
      "epoch": 7.322857142857143,
      "grad_norm": 0.37124544382095337,
      "learning_rate": 1.0236190476190477e-05,
      "loss": 0.3054,
      "step": 25630
    },
    {
      "epoch": 7.325714285714286,
      "grad_norm": 0.9023582935333252,
      "learning_rate": 1.0232380952380953e-05,
      "loss": 0.5446,
      "step": 25640
    },
    {
      "epoch": 7.328571428571428,
      "grad_norm": 10.530847549438477,
      "learning_rate": 1.022857142857143e-05,
      "loss": 0.6676,
      "step": 25650
    },
    {
      "epoch": 7.331428571428571,
      "grad_norm": 11.23420238494873,
      "learning_rate": 1.0224761904761906e-05,
      "loss": 0.7024,
      "step": 25660
    },
    {
      "epoch": 7.3342857142857145,
      "grad_norm": 10.963661193847656,
      "learning_rate": 1.0220952380952383e-05,
      "loss": 0.3757,
      "step": 25670
    },
    {
      "epoch": 7.337142857142857,
      "grad_norm": 11.526782035827637,
      "learning_rate": 1.0217142857142859e-05,
      "loss": 0.7459,
      "step": 25680
    },
    {
      "epoch": 7.34,
      "grad_norm": 12.889144897460938,
      "learning_rate": 1.0213333333333334e-05,
      "loss": 0.6086,
      "step": 25690
    },
    {
      "epoch": 7.3428571428571425,
      "grad_norm": 11.22962760925293,
      "learning_rate": 1.0209523809523812e-05,
      "loss": 0.4404,
      "step": 25700
    },
    {
      "epoch": 7.345714285714286,
      "grad_norm": 0.7358370423316956,
      "learning_rate": 1.0205714285714286e-05,
      "loss": 0.3527,
      "step": 25710
    },
    {
      "epoch": 7.348571428571429,
      "grad_norm": 1.141513466835022,
      "learning_rate": 1.0201904761904761e-05,
      "loss": 0.351,
      "step": 25720
    },
    {
      "epoch": 7.351428571428571,
      "grad_norm": 0.5569818019866943,
      "learning_rate": 1.0198095238095238e-05,
      "loss": 0.4653,
      "step": 25730
    },
    {
      "epoch": 7.354285714285714,
      "grad_norm": 10.597068786621094,
      "learning_rate": 1.0194285714285714e-05,
      "loss": 0.3886,
      "step": 25740
    },
    {
      "epoch": 7.357142857142857,
      "grad_norm": 22.434268951416016,
      "learning_rate": 1.0190476190476191e-05,
      "loss": 0.8273,
      "step": 25750
    },
    {
      "epoch": 7.36,
      "grad_norm": 0.5804871320724487,
      "learning_rate": 1.0186666666666667e-05,
      "loss": 0.4374,
      "step": 25760
    },
    {
      "epoch": 7.362857142857143,
      "grad_norm": 11.418940544128418,
      "learning_rate": 1.0182857142857142e-05,
      "loss": 0.2493,
      "step": 25770
    },
    {
      "epoch": 7.365714285714286,
      "grad_norm": 11.594308853149414,
      "learning_rate": 1.017904761904762e-05,
      "loss": 0.2275,
      "step": 25780
    },
    {
      "epoch": 7.368571428571428,
      "grad_norm": 0.7462331652641296,
      "learning_rate": 1.0175238095238095e-05,
      "loss": 0.2643,
      "step": 25790
    },
    {
      "epoch": 7.371428571428572,
      "grad_norm": 12.527749061584473,
      "learning_rate": 1.0171428571428573e-05,
      "loss": 0.4338,
      "step": 25800
    },
    {
      "epoch": 7.3742857142857146,
      "grad_norm": 0.7666763663291931,
      "learning_rate": 1.0167619047619048e-05,
      "loss": 0.692,
      "step": 25810
    },
    {
      "epoch": 7.377142857142857,
      "grad_norm": 0.37216758728027344,
      "learning_rate": 1.0163809523809525e-05,
      "loss": 0.0148,
      "step": 25820
    },
    {
      "epoch": 7.38,
      "grad_norm": 0.3552798628807068,
      "learning_rate": 1.0160000000000001e-05,
      "loss": 0.7392,
      "step": 25830
    },
    {
      "epoch": 7.382857142857143,
      "grad_norm": 11.09640121459961,
      "learning_rate": 1.0156190476190477e-05,
      "loss": 0.4056,
      "step": 25840
    },
    {
      "epoch": 7.385714285714286,
      "grad_norm": 12.98358154296875,
      "learning_rate": 1.0152380952380954e-05,
      "loss": 0.7369,
      "step": 25850
    },
    {
      "epoch": 7.388571428571429,
      "grad_norm": 10.78476619720459,
      "learning_rate": 1.014857142857143e-05,
      "loss": 0.6833,
      "step": 25860
    },
    {
      "epoch": 7.3914285714285715,
      "grad_norm": 12.73511028289795,
      "learning_rate": 1.0144761904761907e-05,
      "loss": 0.8502,
      "step": 25870
    },
    {
      "epoch": 7.394285714285714,
      "grad_norm": 0.44071418046951294,
      "learning_rate": 1.0140952380952382e-05,
      "loss": 0.4612,
      "step": 25880
    },
    {
      "epoch": 7.397142857142857,
      "grad_norm": 10.740044593811035,
      "learning_rate": 1.013714285714286e-05,
      "loss": 0.294,
      "step": 25890
    },
    {
      "epoch": 7.4,
      "grad_norm": 11.45717716217041,
      "learning_rate": 1.0133333333333335e-05,
      "loss": 0.6706,
      "step": 25900
    },
    {
      "epoch": 7.402857142857143,
      "grad_norm": 0.42337411642074585,
      "learning_rate": 1.012952380952381e-05,
      "loss": 0.459,
      "step": 25910
    },
    {
      "epoch": 7.405714285714286,
      "grad_norm": 0.452737957239151,
      "learning_rate": 1.0125714285714288e-05,
      "loss": 0.7166,
      "step": 25920
    },
    {
      "epoch": 7.408571428571428,
      "grad_norm": 0.5860047340393066,
      "learning_rate": 1.0121904761904762e-05,
      "loss": 0.2328,
      "step": 25930
    },
    {
      "epoch": 7.411428571428571,
      "grad_norm": 1.335995078086853,
      "learning_rate": 1.0118095238095237e-05,
      "loss": 0.6525,
      "step": 25940
    },
    {
      "epoch": 7.414285714285715,
      "grad_norm": 0.9698708057403564,
      "learning_rate": 1.0114285714285715e-05,
      "loss": 0.3336,
      "step": 25950
    },
    {
      "epoch": 7.417142857142857,
      "grad_norm": 0.8813188076019287,
      "learning_rate": 1.011047619047619e-05,
      "loss": 0.4113,
      "step": 25960
    },
    {
      "epoch": 7.42,
      "grad_norm": 11.465841293334961,
      "learning_rate": 1.0106666666666668e-05,
      "loss": 0.2984,
      "step": 25970
    },
    {
      "epoch": 7.422857142857143,
      "grad_norm": 0.37588927149772644,
      "learning_rate": 1.0102857142857143e-05,
      "loss": 0.457,
      "step": 25980
    },
    {
      "epoch": 7.425714285714285,
      "grad_norm": 0.3536720871925354,
      "learning_rate": 1.0099047619047619e-05,
      "loss": 0.4991,
      "step": 25990
    },
    {
      "epoch": 7.428571428571429,
      "grad_norm": 1.4237887859344482,
      "learning_rate": 1.0095238095238096e-05,
      "loss": 0.5787,
      "step": 26000
    },
    {
      "epoch": 7.4314285714285715,
      "grad_norm": 0.45887356996536255,
      "learning_rate": 1.0091428571428572e-05,
      "loss": 0.5773,
      "step": 26010
    },
    {
      "epoch": 7.434285714285714,
      "grad_norm": 10.268448829650879,
      "learning_rate": 1.0087619047619049e-05,
      "loss": 0.3399,
      "step": 26020
    },
    {
      "epoch": 7.437142857142857,
      "grad_norm": 0.38321569561958313,
      "learning_rate": 1.0083809523809524e-05,
      "loss": 0.4135,
      "step": 26030
    },
    {
      "epoch": 7.44,
      "grad_norm": 0.3824180066585541,
      "learning_rate": 1.008e-05,
      "loss": 0.2981,
      "step": 26040
    },
    {
      "epoch": 7.442857142857143,
      "grad_norm": 21.431106567382812,
      "learning_rate": 1.0076190476190477e-05,
      "loss": 0.593,
      "step": 26050
    },
    {
      "epoch": 7.445714285714286,
      "grad_norm": 0.44288110733032227,
      "learning_rate": 1.0072380952380953e-05,
      "loss": 0.4871,
      "step": 26060
    },
    {
      "epoch": 7.448571428571428,
      "grad_norm": 11.045392036437988,
      "learning_rate": 1.006857142857143e-05,
      "loss": 0.6093,
      "step": 26070
    },
    {
      "epoch": 7.451428571428571,
      "grad_norm": 0.3743206560611725,
      "learning_rate": 1.0064761904761906e-05,
      "loss": 0.6296,
      "step": 26080
    },
    {
      "epoch": 7.454285714285715,
      "grad_norm": 0.38545167446136475,
      "learning_rate": 1.0060952380952383e-05,
      "loss": 0.5548,
      "step": 26090
    },
    {
      "epoch": 7.457142857142857,
      "grad_norm": 0.3927094042301178,
      "learning_rate": 1.0057142857142859e-05,
      "loss": 0.9525,
      "step": 26100
    },
    {
      "epoch": 7.46,
      "grad_norm": 0.6669925451278687,
      "learning_rate": 1.0053333333333334e-05,
      "loss": 0.4816,
      "step": 26110
    },
    {
      "epoch": 7.462857142857143,
      "grad_norm": 21.99234962463379,
      "learning_rate": 1.0049523809523811e-05,
      "loss": 0.4497,
      "step": 26120
    },
    {
      "epoch": 7.465714285714285,
      "grad_norm": 11.411349296569824,
      "learning_rate": 1.0045714285714287e-05,
      "loss": 0.432,
      "step": 26130
    },
    {
      "epoch": 7.468571428571429,
      "grad_norm": 0.4793974459171295,
      "learning_rate": 1.0041904761904764e-05,
      "loss": 0.6218,
      "step": 26140
    },
    {
      "epoch": 7.4714285714285715,
      "grad_norm": 10.204940795898438,
      "learning_rate": 1.0038095238095238e-05,
      "loss": 0.5046,
      "step": 26150
    },
    {
      "epoch": 7.474285714285714,
      "grad_norm": 0.389893501996994,
      "learning_rate": 1.0034285714285714e-05,
      "loss": 0.3453,
      "step": 26160
    },
    {
      "epoch": 7.477142857142857,
      "grad_norm": 0.3516864776611328,
      "learning_rate": 1.0030476190476191e-05,
      "loss": 0.3881,
      "step": 26170
    },
    {
      "epoch": 7.48,
      "grad_norm": 10.914860725402832,
      "learning_rate": 1.0026666666666667e-05,
      "loss": 0.3254,
      "step": 26180
    },
    {
      "epoch": 7.482857142857143,
      "grad_norm": 11.540685653686523,
      "learning_rate": 1.0022857142857142e-05,
      "loss": 0.3214,
      "step": 26190
    },
    {
      "epoch": 7.485714285714286,
      "grad_norm": 0.31146422028541565,
      "learning_rate": 1.001904761904762e-05,
      "loss": 0.5186,
      "step": 26200
    },
    {
      "epoch": 7.488571428571428,
      "grad_norm": 0.3390185236930847,
      "learning_rate": 1.0015238095238095e-05,
      "loss": 1.0842,
      "step": 26210
    },
    {
      "epoch": 7.491428571428571,
      "grad_norm": 0.3978179097175598,
      "learning_rate": 1.0011428571428572e-05,
      "loss": 0.7399,
      "step": 26220
    },
    {
      "epoch": 7.494285714285715,
      "grad_norm": 22.25877571105957,
      "learning_rate": 1.0007619047619048e-05,
      "loss": 0.615,
      "step": 26230
    },
    {
      "epoch": 7.497142857142857,
      "grad_norm": 1.0769025087356567,
      "learning_rate": 1.0003809523809525e-05,
      "loss": 0.2258,
      "step": 26240
    },
    {
      "epoch": 7.5,
      "grad_norm": 11.148493766784668,
      "learning_rate": 1e-05,
      "loss": 1.0885,
      "step": 26250
    },
    {
      "epoch": 7.502857142857143,
      "grad_norm": 0.48647403717041016,
      "learning_rate": 9.996190476190476e-06,
      "loss": 0.4085,
      "step": 26260
    },
    {
      "epoch": 7.505714285714285,
      "grad_norm": 0.408001184463501,
      "learning_rate": 9.992380952380954e-06,
      "loss": 0.7299,
      "step": 26270
    },
    {
      "epoch": 7.508571428571429,
      "grad_norm": 11.721542358398438,
      "learning_rate": 9.98857142857143e-06,
      "loss": 0.7598,
      "step": 26280
    },
    {
      "epoch": 7.511428571428572,
      "grad_norm": 12.029026985168457,
      "learning_rate": 9.984761904761907e-06,
      "loss": 0.8085,
      "step": 26290
    },
    {
      "epoch": 7.514285714285714,
      "grad_norm": 10.899916648864746,
      "learning_rate": 9.980952380952382e-06,
      "loss": 0.4938,
      "step": 26300
    },
    {
      "epoch": 7.517142857142857,
      "grad_norm": 11.732985496520996,
      "learning_rate": 9.977142857142858e-06,
      "loss": 1.0691,
      "step": 26310
    },
    {
      "epoch": 7.52,
      "grad_norm": 0.9651616215705872,
      "learning_rate": 9.973333333333333e-06,
      "loss": 0.7336,
      "step": 26320
    },
    {
      "epoch": 7.522857142857143,
      "grad_norm": 12.375213623046875,
      "learning_rate": 9.96952380952381e-06,
      "loss": 0.4779,
      "step": 26330
    },
    {
      "epoch": 7.525714285714286,
      "grad_norm": 11.036665916442871,
      "learning_rate": 9.965714285714286e-06,
      "loss": 0.5563,
      "step": 26340
    },
    {
      "epoch": 7.5285714285714285,
      "grad_norm": 13.53304386138916,
      "learning_rate": 9.961904761904763e-06,
      "loss": 0.6721,
      "step": 26350
    },
    {
      "epoch": 7.531428571428571,
      "grad_norm": 11.166887283325195,
      "learning_rate": 9.958095238095239e-06,
      "loss": 0.4047,
      "step": 26360
    },
    {
      "epoch": 7.534285714285714,
      "grad_norm": 0.5349740982055664,
      "learning_rate": 9.954285714285715e-06,
      "loss": 0.6154,
      "step": 26370
    },
    {
      "epoch": 7.537142857142857,
      "grad_norm": 12.758709907531738,
      "learning_rate": 9.950476190476192e-06,
      "loss": 0.6728,
      "step": 26380
    },
    {
      "epoch": 7.54,
      "grad_norm": 11.481327056884766,
      "learning_rate": 9.946666666666667e-06,
      "loss": 0.1939,
      "step": 26390
    },
    {
      "epoch": 7.542857142857143,
      "grad_norm": 11.536762237548828,
      "learning_rate": 9.942857142857145e-06,
      "loss": 0.3346,
      "step": 26400
    },
    {
      "epoch": 7.545714285714285,
      "grad_norm": 11.466500282287598,
      "learning_rate": 9.93904761904762e-06,
      "loss": 0.7528,
      "step": 26410
    },
    {
      "epoch": 7.548571428571429,
      "grad_norm": 11.013792991638184,
      "learning_rate": 9.935238095238096e-06,
      "loss": 0.5956,
      "step": 26420
    },
    {
      "epoch": 7.551428571428572,
      "grad_norm": 0.42800313234329224,
      "learning_rate": 9.931428571428571e-06,
      "loss": 0.5906,
      "step": 26430
    },
    {
      "epoch": 7.554285714285714,
      "grad_norm": 0.4063590466976166,
      "learning_rate": 9.927619047619049e-06,
      "loss": 0.381,
      "step": 26440
    },
    {
      "epoch": 7.557142857142857,
      "grad_norm": 21.564788818359375,
      "learning_rate": 9.923809523809524e-06,
      "loss": 0.2702,
      "step": 26450
    },
    {
      "epoch": 7.5600000000000005,
      "grad_norm": 11.352788925170898,
      "learning_rate": 9.920000000000002e-06,
      "loss": 0.6234,
      "step": 26460
    },
    {
      "epoch": 7.562857142857143,
      "grad_norm": 11.020533561706543,
      "learning_rate": 9.916190476190477e-06,
      "loss": 0.9883,
      "step": 26470
    },
    {
      "epoch": 7.565714285714286,
      "grad_norm": 0.41195669770240784,
      "learning_rate": 9.912380952380953e-06,
      "loss": 0.3831,
      "step": 26480
    },
    {
      "epoch": 7.5685714285714285,
      "grad_norm": 0.5127567648887634,
      "learning_rate": 9.90857142857143e-06,
      "loss": 0.5272,
      "step": 26490
    },
    {
      "epoch": 7.571428571428571,
      "grad_norm": 11.294098854064941,
      "learning_rate": 9.904761904761906e-06,
      "loss": 0.6825,
      "step": 26500
    },
    {
      "epoch": 7.574285714285715,
      "grad_norm": 11.777993202209473,
      "learning_rate": 9.900952380952383e-06,
      "loss": 0.5192,
      "step": 26510
    },
    {
      "epoch": 7.577142857142857,
      "grad_norm": 10.9274263381958,
      "learning_rate": 9.897142857142858e-06,
      "loss": 0.3449,
      "step": 26520
    },
    {
      "epoch": 7.58,
      "grad_norm": 9.62920093536377,
      "learning_rate": 9.893333333333334e-06,
      "loss": 0.2964,
      "step": 26530
    },
    {
      "epoch": 7.582857142857143,
      "grad_norm": 0.652846097946167,
      "learning_rate": 9.88952380952381e-06,
      "loss": 0.2126,
      "step": 26540
    },
    {
      "epoch": 7.585714285714285,
      "grad_norm": 0.4878654181957245,
      "learning_rate": 9.885714285714287e-06,
      "loss": 0.4612,
      "step": 26550
    },
    {
      "epoch": 7.588571428571429,
      "grad_norm": 11.77935791015625,
      "learning_rate": 9.881904761904762e-06,
      "loss": 0.4637,
      "step": 26560
    },
    {
      "epoch": 7.591428571428572,
      "grad_norm": 0.2895779311656952,
      "learning_rate": 9.878095238095238e-06,
      "loss": 0.6062,
      "step": 26570
    },
    {
      "epoch": 7.594285714285714,
      "grad_norm": 14.025834083557129,
      "learning_rate": 9.874285714285715e-06,
      "loss": 0.5945,
      "step": 26580
    },
    {
      "epoch": 7.597142857142857,
      "grad_norm": 11.055749893188477,
      "learning_rate": 9.870476190476191e-06,
      "loss": 0.557,
      "step": 26590
    },
    {
      "epoch": 7.6,
      "grad_norm": 10.9612398147583,
      "learning_rate": 9.866666666666668e-06,
      "loss": 0.4528,
      "step": 26600
    },
    {
      "epoch": 7.602857142857143,
      "grad_norm": 0.4883657991886139,
      "learning_rate": 9.862857142857144e-06,
      "loss": 1.0519,
      "step": 26610
    },
    {
      "epoch": 7.605714285714286,
      "grad_norm": 12.954290390014648,
      "learning_rate": 9.859047619047621e-06,
      "loss": 0.7073,
      "step": 26620
    },
    {
      "epoch": 7.6085714285714285,
      "grad_norm": 21.75700569152832,
      "learning_rate": 9.855238095238095e-06,
      "loss": 0.374,
      "step": 26630
    },
    {
      "epoch": 7.611428571428571,
      "grad_norm": 0.3359083831310272,
      "learning_rate": 9.851428571428572e-06,
      "loss": 0.1791,
      "step": 26640
    },
    {
      "epoch": 7.614285714285714,
      "grad_norm": 12.410211563110352,
      "learning_rate": 9.847619047619048e-06,
      "loss": 0.825,
      "step": 26650
    },
    {
      "epoch": 7.617142857142857,
      "grad_norm": 0.4721418023109436,
      "learning_rate": 9.843809523809525e-06,
      "loss": 0.6492,
      "step": 26660
    },
    {
      "epoch": 7.62,
      "grad_norm": 11.27936840057373,
      "learning_rate": 9.84e-06,
      "loss": 0.6964,
      "step": 26670
    },
    {
      "epoch": 7.622857142857143,
      "grad_norm": 0.3843247592449188,
      "learning_rate": 9.836190476190476e-06,
      "loss": 0.2155,
      "step": 26680
    },
    {
      "epoch": 7.6257142857142854,
      "grad_norm": 0.4147169291973114,
      "learning_rate": 9.832380952380954e-06,
      "loss": 0.2579,
      "step": 26690
    },
    {
      "epoch": 7.628571428571428,
      "grad_norm": 0.40729111433029175,
      "learning_rate": 9.828571428571429e-06,
      "loss": 0.4195,
      "step": 26700
    },
    {
      "epoch": 7.631428571428572,
      "grad_norm": 0.3897785246372223,
      "learning_rate": 9.824761904761906e-06,
      "loss": 0.6316,
      "step": 26710
    },
    {
      "epoch": 7.634285714285714,
      "grad_norm": 10.902016639709473,
      "learning_rate": 9.820952380952382e-06,
      "loss": 0.5767,
      "step": 26720
    },
    {
      "epoch": 7.637142857142857,
      "grad_norm": 0.35391420125961304,
      "learning_rate": 9.81714285714286e-06,
      "loss": 0.5543,
      "step": 26730
    },
    {
      "epoch": 7.64,
      "grad_norm": 0.5681731700897217,
      "learning_rate": 9.813333333333333e-06,
      "loss": 0.3498,
      "step": 26740
    },
    {
      "epoch": 7.642857142857143,
      "grad_norm": 0.8299800157546997,
      "learning_rate": 9.80952380952381e-06,
      "loss": 0.9515,
      "step": 26750
    },
    {
      "epoch": 7.645714285714286,
      "grad_norm": 0.33900412917137146,
      "learning_rate": 9.805714285714286e-06,
      "loss": 0.5473,
      "step": 26760
    },
    {
      "epoch": 7.648571428571429,
      "grad_norm": 11.328084945678711,
      "learning_rate": 9.801904761904763e-06,
      "loss": 0.7668,
      "step": 26770
    },
    {
      "epoch": 7.651428571428571,
      "grad_norm": 0.9496234655380249,
      "learning_rate": 9.798095238095239e-06,
      "loss": 0.2243,
      "step": 26780
    },
    {
      "epoch": 7.654285714285714,
      "grad_norm": 1.241032361984253,
      "learning_rate": 9.794285714285714e-06,
      "loss": 0.313,
      "step": 26790
    },
    {
      "epoch": 7.6571428571428575,
      "grad_norm": 11.101065635681152,
      "learning_rate": 9.790476190476192e-06,
      "loss": 0.6478,
      "step": 26800
    },
    {
      "epoch": 7.66,
      "grad_norm": 0.36135533452033997,
      "learning_rate": 9.786666666666667e-06,
      "loss": 0.3456,
      "step": 26810
    },
    {
      "epoch": 7.662857142857143,
      "grad_norm": 0.5398092269897461,
      "learning_rate": 9.782857142857145e-06,
      "loss": 0.4709,
      "step": 26820
    },
    {
      "epoch": 7.6657142857142855,
      "grad_norm": 0.5743739604949951,
      "learning_rate": 9.77904761904762e-06,
      "loss": 0.8899,
      "step": 26830
    },
    {
      "epoch": 7.668571428571429,
      "grad_norm": 0.36018863320350647,
      "learning_rate": 9.775238095238096e-06,
      "loss": 0.7855,
      "step": 26840
    },
    {
      "epoch": 7.671428571428572,
      "grad_norm": 10.602255821228027,
      "learning_rate": 9.771428571428571e-06,
      "loss": 0.9182,
      "step": 26850
    },
    {
      "epoch": 7.674285714285714,
      "grad_norm": 10.997488021850586,
      "learning_rate": 9.767619047619049e-06,
      "loss": 0.9916,
      "step": 26860
    },
    {
      "epoch": 7.677142857142857,
      "grad_norm": 0.3806665241718292,
      "learning_rate": 9.763809523809524e-06,
      "loss": 0.2576,
      "step": 26870
    },
    {
      "epoch": 7.68,
      "grad_norm": 0.42861559987068176,
      "learning_rate": 9.760000000000001e-06,
      "loss": 0.6128,
      "step": 26880
    },
    {
      "epoch": 7.682857142857143,
      "grad_norm": 0.4200586676597595,
      "learning_rate": 9.756190476190477e-06,
      "loss": 0.0946,
      "step": 26890
    },
    {
      "epoch": 7.685714285714286,
      "grad_norm": 1.191732406616211,
      "learning_rate": 9.752380952380953e-06,
      "loss": 0.3832,
      "step": 26900
    },
    {
      "epoch": 7.688571428571429,
      "grad_norm": 11.107605934143066,
      "learning_rate": 9.74857142857143e-06,
      "loss": 0.4231,
      "step": 26910
    },
    {
      "epoch": 7.691428571428571,
      "grad_norm": 0.3682541251182556,
      "learning_rate": 9.744761904761905e-06,
      "loss": 0.2731,
      "step": 26920
    },
    {
      "epoch": 7.694285714285714,
      "grad_norm": 10.128433227539062,
      "learning_rate": 9.740952380952383e-06,
      "loss": 0.3955,
      "step": 26930
    },
    {
      "epoch": 7.6971428571428575,
      "grad_norm": 11.190449714660645,
      "learning_rate": 9.737142857142858e-06,
      "loss": 0.527,
      "step": 26940
    },
    {
      "epoch": 7.7,
      "grad_norm": 10.592103004455566,
      "learning_rate": 9.733333333333334e-06,
      "loss": 0.642,
      "step": 26950
    },
    {
      "epoch": 7.702857142857143,
      "grad_norm": 0.717879056930542,
      "learning_rate": 9.72952380952381e-06,
      "loss": 0.5451,
      "step": 26960
    },
    {
      "epoch": 7.7057142857142855,
      "grad_norm": 11.05611801147461,
      "learning_rate": 9.725714285714287e-06,
      "loss": 0.5052,
      "step": 26970
    },
    {
      "epoch": 7.708571428571428,
      "grad_norm": 0.7610574960708618,
      "learning_rate": 9.721904761904762e-06,
      "loss": 0.3931,
      "step": 26980
    },
    {
      "epoch": 7.711428571428572,
      "grad_norm": 0.8301079869270325,
      "learning_rate": 9.718095238095238e-06,
      "loss": 0.673,
      "step": 26990
    },
    {
      "epoch": 7.714285714285714,
      "grad_norm": 11.778473854064941,
      "learning_rate": 9.714285714285715e-06,
      "loss": 0.5499,
      "step": 27000
    },
    {
      "epoch": 7.717142857142857,
      "grad_norm": 0.8640084266662598,
      "learning_rate": 9.71047619047619e-06,
      "loss": 0.3461,
      "step": 27010
    },
    {
      "epoch": 7.72,
      "grad_norm": 10.209656715393066,
      "learning_rate": 9.706666666666668e-06,
      "loss": 0.6295,
      "step": 27020
    },
    {
      "epoch": 7.722857142857142,
      "grad_norm": 0.711882472038269,
      "learning_rate": 9.702857142857144e-06,
      "loss": 0.2226,
      "step": 27030
    },
    {
      "epoch": 7.725714285714286,
      "grad_norm": 0.3611539900302887,
      "learning_rate": 9.699047619047621e-06,
      "loss": 0.5678,
      "step": 27040
    },
    {
      "epoch": 7.728571428571429,
      "grad_norm": 0.3681522607803345,
      "learning_rate": 9.695238095238096e-06,
      "loss": 0.7663,
      "step": 27050
    },
    {
      "epoch": 7.731428571428571,
      "grad_norm": 0.6447820663452148,
      "learning_rate": 9.691428571428572e-06,
      "loss": 0.2785,
      "step": 27060
    },
    {
      "epoch": 7.734285714285714,
      "grad_norm": 12.919611930847168,
      "learning_rate": 9.687619047619048e-06,
      "loss": 0.4328,
      "step": 27070
    },
    {
      "epoch": 7.737142857142857,
      "grad_norm": 11.423077583312988,
      "learning_rate": 9.683809523809525e-06,
      "loss": 0.7622,
      "step": 27080
    },
    {
      "epoch": 7.74,
      "grad_norm": 12.650296211242676,
      "learning_rate": 9.68e-06,
      "loss": 0.7877,
      "step": 27090
    },
    {
      "epoch": 7.742857142857143,
      "grad_norm": 10.723018646240234,
      "learning_rate": 9.676190476190476e-06,
      "loss": 0.8005,
      "step": 27100
    },
    {
      "epoch": 7.7457142857142856,
      "grad_norm": 10.919678688049316,
      "learning_rate": 9.672380952380953e-06,
      "loss": 0.7279,
      "step": 27110
    },
    {
      "epoch": 7.748571428571428,
      "grad_norm": 1.2698137760162354,
      "learning_rate": 9.668571428571429e-06,
      "loss": 0.2251,
      "step": 27120
    },
    {
      "epoch": 7.751428571428572,
      "grad_norm": 0.8354511857032776,
      "learning_rate": 9.664761904761906e-06,
      "loss": 0.6288,
      "step": 27130
    },
    {
      "epoch": 7.7542857142857144,
      "grad_norm": 0.4213261306285858,
      "learning_rate": 9.660952380952382e-06,
      "loss": 0.7208,
      "step": 27140
    },
    {
      "epoch": 7.757142857142857,
      "grad_norm": 0.8481855988502502,
      "learning_rate": 9.657142857142859e-06,
      "loss": 0.3892,
      "step": 27150
    },
    {
      "epoch": 7.76,
      "grad_norm": 21.30076026916504,
      "learning_rate": 9.653333333333335e-06,
      "loss": 0.5051,
      "step": 27160
    },
    {
      "epoch": 7.762857142857143,
      "grad_norm": 0.42021164298057556,
      "learning_rate": 9.64952380952381e-06,
      "loss": 0.0163,
      "step": 27170
    },
    {
      "epoch": 7.765714285714286,
      "grad_norm": 11.620157241821289,
      "learning_rate": 9.645714285714286e-06,
      "loss": 0.5548,
      "step": 27180
    },
    {
      "epoch": 7.768571428571429,
      "grad_norm": 0.7144141793251038,
      "learning_rate": 9.641904761904763e-06,
      "loss": 0.4399,
      "step": 27190
    },
    {
      "epoch": 7.771428571428571,
      "grad_norm": 0.3357909023761749,
      "learning_rate": 9.638095238095239e-06,
      "loss": 0.5197,
      "step": 27200
    },
    {
      "epoch": 7.774285714285714,
      "grad_norm": 0.350864976644516,
      "learning_rate": 9.634285714285714e-06,
      "loss": 0.8262,
      "step": 27210
    },
    {
      "epoch": 7.777142857142858,
      "grad_norm": 12.90579605102539,
      "learning_rate": 9.630476190476192e-06,
      "loss": 0.9564,
      "step": 27220
    },
    {
      "epoch": 7.78,
      "grad_norm": 0.5484108328819275,
      "learning_rate": 9.626666666666667e-06,
      "loss": 0.7125,
      "step": 27230
    },
    {
      "epoch": 7.782857142857143,
      "grad_norm": 0.4566280245780945,
      "learning_rate": 9.622857142857144e-06,
      "loss": 0.4135,
      "step": 27240
    },
    {
      "epoch": 7.785714285714286,
      "grad_norm": 0.7194508910179138,
      "learning_rate": 9.61904761904762e-06,
      "loss": 0.5224,
      "step": 27250
    },
    {
      "epoch": 7.788571428571428,
      "grad_norm": 10.896098136901855,
      "learning_rate": 9.615238095238096e-06,
      "loss": 0.5242,
      "step": 27260
    },
    {
      "epoch": 7.791428571428572,
      "grad_norm": 0.4145919978618622,
      "learning_rate": 9.611428571428573e-06,
      "loss": 0.684,
      "step": 27270
    },
    {
      "epoch": 7.7942857142857145,
      "grad_norm": 0.42236030101776123,
      "learning_rate": 9.607619047619048e-06,
      "loss": 0.3921,
      "step": 27280
    },
    {
      "epoch": 7.797142857142857,
      "grad_norm": 0.6864573359489441,
      "learning_rate": 9.603809523809524e-06,
      "loss": 0.3029,
      "step": 27290
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.4077701270580292,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.4678,
      "step": 27300
    },
    {
      "epoch": 7.8028571428571425,
      "grad_norm": 0.7274964451789856,
      "learning_rate": 9.596190476190477e-06,
      "loss": 0.3048,
      "step": 27310
    },
    {
      "epoch": 7.805714285714286,
      "grad_norm": 0.34027352929115295,
      "learning_rate": 9.592380952380952e-06,
      "loss": 0.4248,
      "step": 27320
    },
    {
      "epoch": 7.808571428571429,
      "grad_norm": 0.3703790307044983,
      "learning_rate": 9.58857142857143e-06,
      "loss": 0.6351,
      "step": 27330
    },
    {
      "epoch": 7.811428571428571,
      "grad_norm": 11.17155933380127,
      "learning_rate": 9.584761904761905e-06,
      "loss": 0.3557,
      "step": 27340
    },
    {
      "epoch": 7.814285714285714,
      "grad_norm": 0.3284202218055725,
      "learning_rate": 9.580952380952383e-06,
      "loss": 0.5065,
      "step": 27350
    },
    {
      "epoch": 7.817142857142857,
      "grad_norm": 0.32827839255332947,
      "learning_rate": 9.577142857142858e-06,
      "loss": 0.6416,
      "step": 27360
    },
    {
      "epoch": 7.82,
      "grad_norm": 0.38879498839378357,
      "learning_rate": 9.573333333333334e-06,
      "loss": 0.8125,
      "step": 27370
    },
    {
      "epoch": 7.822857142857143,
      "grad_norm": 21.744417190551758,
      "learning_rate": 9.569523809523811e-06,
      "loss": 0.5972,
      "step": 27380
    },
    {
      "epoch": 7.825714285714286,
      "grad_norm": 13.56470775604248,
      "learning_rate": 9.565714285714287e-06,
      "loss": 0.6944,
      "step": 27390
    },
    {
      "epoch": 7.828571428571428,
      "grad_norm": 1.6956617832183838,
      "learning_rate": 9.561904761904762e-06,
      "loss": 0.2181,
      "step": 27400
    },
    {
      "epoch": 7.831428571428571,
      "grad_norm": 0.4543227255344391,
      "learning_rate": 9.558095238095238e-06,
      "loss": 0.4107,
      "step": 27410
    },
    {
      "epoch": 7.8342857142857145,
      "grad_norm": 0.48190438747406006,
      "learning_rate": 9.554285714285715e-06,
      "loss": 0.6901,
      "step": 27420
    },
    {
      "epoch": 7.837142857142857,
      "grad_norm": 0.44176146388053894,
      "learning_rate": 9.55047619047619e-06,
      "loss": 0.534,
      "step": 27430
    },
    {
      "epoch": 7.84,
      "grad_norm": 0.40261372923851013,
      "learning_rate": 9.546666666666668e-06,
      "loss": 0.8386,
      "step": 27440
    },
    {
      "epoch": 7.8428571428571425,
      "grad_norm": 0.4200987219810486,
      "learning_rate": 9.542857142857143e-06,
      "loss": 0.1334,
      "step": 27450
    },
    {
      "epoch": 7.845714285714286,
      "grad_norm": 11.682440757751465,
      "learning_rate": 9.53904761904762e-06,
      "loss": 0.5296,
      "step": 27460
    },
    {
      "epoch": 7.848571428571429,
      "grad_norm": 11.339618682861328,
      "learning_rate": 9.535238095238096e-06,
      "loss": 0.4228,
      "step": 27470
    },
    {
      "epoch": 7.851428571428571,
      "grad_norm": 10.711322784423828,
      "learning_rate": 9.531428571428572e-06,
      "loss": 0.8369,
      "step": 27480
    },
    {
      "epoch": 7.854285714285714,
      "grad_norm": 10.823739051818848,
      "learning_rate": 9.52761904761905e-06,
      "loss": 0.2417,
      "step": 27490
    },
    {
      "epoch": 7.857142857142857,
      "grad_norm": 0.4464944899082184,
      "learning_rate": 9.523809523809525e-06,
      "loss": 0.4129,
      "step": 27500
    },
    {
      "epoch": 7.86,
      "grad_norm": 13.267253875732422,
      "learning_rate": 9.52e-06,
      "loss": 0.6136,
      "step": 27510
    },
    {
      "epoch": 7.862857142857143,
      "grad_norm": 13.170926094055176,
      "learning_rate": 9.516190476190476e-06,
      "loss": 0.4447,
      "step": 27520
    },
    {
      "epoch": 7.865714285714286,
      "grad_norm": 10.638651847839355,
      "learning_rate": 9.512380952380953e-06,
      "loss": 0.7221,
      "step": 27530
    },
    {
      "epoch": 7.868571428571428,
      "grad_norm": 0.5136826038360596,
      "learning_rate": 9.508571428571429e-06,
      "loss": 0.3545,
      "step": 27540
    },
    {
      "epoch": 7.871428571428572,
      "grad_norm": 10.779214859008789,
      "learning_rate": 9.504761904761906e-06,
      "loss": 0.5229,
      "step": 27550
    },
    {
      "epoch": 7.8742857142857146,
      "grad_norm": 21.895780563354492,
      "learning_rate": 9.500952380952382e-06,
      "loss": 0.4284,
      "step": 27560
    },
    {
      "epoch": 7.877142857142857,
      "grad_norm": 11.19291877746582,
      "learning_rate": 9.497142857142859e-06,
      "loss": 0.1334,
      "step": 27570
    },
    {
      "epoch": 7.88,
      "grad_norm": 22.780132293701172,
      "learning_rate": 9.493333333333334e-06,
      "loss": 0.7614,
      "step": 27580
    },
    {
      "epoch": 7.882857142857143,
      "grad_norm": 11.181410789489746,
      "learning_rate": 9.48952380952381e-06,
      "loss": 0.297,
      "step": 27590
    },
    {
      "epoch": 7.885714285714286,
      "grad_norm": 22.47191047668457,
      "learning_rate": 9.485714285714287e-06,
      "loss": 0.5841,
      "step": 27600
    },
    {
      "epoch": 7.888571428571429,
      "grad_norm": 0.38992902636528015,
      "learning_rate": 9.481904761904763e-06,
      "loss": 0.4169,
      "step": 27610
    },
    {
      "epoch": 7.8914285714285715,
      "grad_norm": 0.5413650870323181,
      "learning_rate": 9.478095238095239e-06,
      "loss": 0.8519,
      "step": 27620
    },
    {
      "epoch": 7.894285714285714,
      "grad_norm": 0.39066216349601746,
      "learning_rate": 9.474285714285714e-06,
      "loss": 0.6909,
      "step": 27630
    },
    {
      "epoch": 7.897142857142857,
      "grad_norm": 10.463276863098145,
      "learning_rate": 9.470476190476191e-06,
      "loss": 0.784,
      "step": 27640
    },
    {
      "epoch": 7.9,
      "grad_norm": 1.282692551612854,
      "learning_rate": 9.466666666666667e-06,
      "loss": 0.3971,
      "step": 27650
    },
    {
      "epoch": 7.902857142857143,
      "grad_norm": 22.357192993164062,
      "learning_rate": 9.462857142857144e-06,
      "loss": 1.02,
      "step": 27660
    },
    {
      "epoch": 7.905714285714286,
      "grad_norm": 10.82511043548584,
      "learning_rate": 9.45904761904762e-06,
      "loss": 0.5092,
      "step": 27670
    },
    {
      "epoch": 7.908571428571428,
      "grad_norm": 1.4480830430984497,
      "learning_rate": 9.455238095238095e-06,
      "loss": 0.4601,
      "step": 27680
    },
    {
      "epoch": 7.911428571428571,
      "grad_norm": 0.6830936074256897,
      "learning_rate": 9.451428571428573e-06,
      "loss": 0.1502,
      "step": 27690
    },
    {
      "epoch": 7.914285714285715,
      "grad_norm": 0.4607793688774109,
      "learning_rate": 9.447619047619048e-06,
      "loss": 0.3608,
      "step": 27700
    },
    {
      "epoch": 7.917142857142857,
      "grad_norm": 10.945055961608887,
      "learning_rate": 9.443809523809526e-06,
      "loss": 0.8823,
      "step": 27710
    },
    {
      "epoch": 7.92,
      "grad_norm": 0.6172373294830322,
      "learning_rate": 9.440000000000001e-06,
      "loss": 0.4585,
      "step": 27720
    },
    {
      "epoch": 7.922857142857143,
      "grad_norm": 0.4321131706237793,
      "learning_rate": 9.436190476190477e-06,
      "loss": 0.2052,
      "step": 27730
    },
    {
      "epoch": 7.925714285714285,
      "grad_norm": 0.6566587090492249,
      "learning_rate": 9.432380952380952e-06,
      "loss": 0.3212,
      "step": 27740
    },
    {
      "epoch": 7.928571428571429,
      "grad_norm": 22.24820899963379,
      "learning_rate": 9.42857142857143e-06,
      "loss": 0.79,
      "step": 27750
    },
    {
      "epoch": 7.9314285714285715,
      "grad_norm": 11.046506881713867,
      "learning_rate": 9.424761904761905e-06,
      "loss": 0.5936,
      "step": 27760
    },
    {
      "epoch": 7.934285714285714,
      "grad_norm": 0.542632520198822,
      "learning_rate": 9.420952380952382e-06,
      "loss": 0.5951,
      "step": 27770
    },
    {
      "epoch": 7.937142857142857,
      "grad_norm": 2107.901123046875,
      "learning_rate": 9.417142857142858e-06,
      "loss": 0.5193,
      "step": 27780
    },
    {
      "epoch": 7.9399999999999995,
      "grad_norm": 1.2295842170715332,
      "learning_rate": 9.413333333333334e-06,
      "loss": 0.7381,
      "step": 27790
    },
    {
      "epoch": 7.942857142857143,
      "grad_norm": 1.263010859489441,
      "learning_rate": 9.40952380952381e-06,
      "loss": 0.5334,
      "step": 27800
    },
    {
      "epoch": 7.945714285714286,
      "grad_norm": 10.854804039001465,
      "learning_rate": 9.405714285714286e-06,
      "loss": 0.5151,
      "step": 27810
    },
    {
      "epoch": 7.948571428571428,
      "grad_norm": 0.41081568598747253,
      "learning_rate": 9.401904761904764e-06,
      "loss": 0.5106,
      "step": 27820
    },
    {
      "epoch": 7.951428571428571,
      "grad_norm": 1.1686331033706665,
      "learning_rate": 9.398095238095238e-06,
      "loss": 0.624,
      "step": 27830
    },
    {
      "epoch": 7.954285714285715,
      "grad_norm": 12.375892639160156,
      "learning_rate": 9.394285714285715e-06,
      "loss": 0.5227,
      "step": 27840
    },
    {
      "epoch": 7.957142857142857,
      "grad_norm": 22.076061248779297,
      "learning_rate": 9.39047619047619e-06,
      "loss": 0.8943,
      "step": 27850
    },
    {
      "epoch": 7.96,
      "grad_norm": 10.954917907714844,
      "learning_rate": 9.386666666666668e-06,
      "loss": 0.3911,
      "step": 27860
    },
    {
      "epoch": 7.962857142857143,
      "grad_norm": 11.345417976379395,
      "learning_rate": 9.382857142857143e-06,
      "loss": 0.4314,
      "step": 27870
    },
    {
      "epoch": 7.965714285714286,
      "grad_norm": 0.5669245719909668,
      "learning_rate": 9.37904761904762e-06,
      "loss": 0.5355,
      "step": 27880
    },
    {
      "epoch": 7.968571428571429,
      "grad_norm": 0.5201793313026428,
      "learning_rate": 9.375238095238096e-06,
      "loss": 0.6677,
      "step": 27890
    },
    {
      "epoch": 7.9714285714285715,
      "grad_norm": 22.311676025390625,
      "learning_rate": 9.371428571428572e-06,
      "loss": 0.5433,
      "step": 27900
    },
    {
      "epoch": 7.974285714285714,
      "grad_norm": 0.5020434856414795,
      "learning_rate": 9.367619047619049e-06,
      "loss": 0.0984,
      "step": 27910
    },
    {
      "epoch": 7.977142857142857,
      "grad_norm": 11.202775001525879,
      "learning_rate": 9.363809523809525e-06,
      "loss": 0.6753,
      "step": 27920
    },
    {
      "epoch": 7.98,
      "grad_norm": 0.5589452981948853,
      "learning_rate": 9.360000000000002e-06,
      "loss": 0.7649,
      "step": 27930
    },
    {
      "epoch": 7.982857142857143,
      "grad_norm": 10.941716194152832,
      "learning_rate": 9.356190476190476e-06,
      "loss": 0.8204,
      "step": 27940
    },
    {
      "epoch": 7.985714285714286,
      "grad_norm": 11.08979320526123,
      "learning_rate": 9.352380952380953e-06,
      "loss": 0.3987,
      "step": 27950
    },
    {
      "epoch": 7.988571428571428,
      "grad_norm": 11.343077659606934,
      "learning_rate": 9.348571428571429e-06,
      "loss": 0.5885,
      "step": 27960
    },
    {
      "epoch": 7.991428571428571,
      "grad_norm": 0.5866004824638367,
      "learning_rate": 9.344761904761906e-06,
      "loss": 0.1693,
      "step": 27970
    },
    {
      "epoch": 7.994285714285715,
      "grad_norm": 0.40274113416671753,
      "learning_rate": 9.340952380952381e-06,
      "loss": 0.3796,
      "step": 27980
    },
    {
      "epoch": 7.997142857142857,
      "grad_norm": 0.8165167570114136,
      "learning_rate": 9.337142857142859e-06,
      "loss": 0.8681,
      "step": 27990
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.3869147002696991,
      "learning_rate": 9.333333333333334e-06,
      "loss": 0.5313,
      "step": 28000
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.8709677419354839,
      "eval_f1": 0.0,
      "eval_loss": 0.5157354474067688,
      "eval_precision": 0.0,
      "eval_recall": 0.0,
      "eval_runtime": 45.0366,
      "eval_samples_per_second": 66.613,
      "eval_steps_per_second": 2.087,
      "step": 28000
    },
    {
      "epoch": 8.002857142857144,
      "grad_norm": 0.8866907358169556,
      "learning_rate": 9.32952380952381e-06,
      "loss": 0.4959,
      "step": 28010
    },
    {
      "epoch": 8.005714285714285,
      "grad_norm": 0.4304828345775604,
      "learning_rate": 9.325714285714287e-06,
      "loss": 0.5315,
      "step": 28020
    },
    {
      "epoch": 8.008571428571429,
      "grad_norm": 0.39442968368530273,
      "learning_rate": 9.321904761904763e-06,
      "loss": 0.3375,
      "step": 28030
    },
    {
      "epoch": 8.01142857142857,
      "grad_norm": 0.5226112008094788,
      "learning_rate": 9.318095238095238e-06,
      "loss": 0.5747,
      "step": 28040
    },
    {
      "epoch": 8.014285714285714,
      "grad_norm": 0.37173476815223694,
      "learning_rate": 9.314285714285714e-06,
      "loss": 0.6112,
      "step": 28050
    },
    {
      "epoch": 8.017142857142858,
      "grad_norm": 0.8980893492698669,
      "learning_rate": 9.310476190476191e-06,
      "loss": 0.3391,
      "step": 28060
    },
    {
      "epoch": 8.02,
      "grad_norm": 0.38679301738739014,
      "learning_rate": 9.306666666666667e-06,
      "loss": 0.5786,
      "step": 28070
    },
    {
      "epoch": 8.022857142857143,
      "grad_norm": 0.4003441333770752,
      "learning_rate": 9.302857142857144e-06,
      "loss": 0.4498,
      "step": 28080
    },
    {
      "epoch": 8.025714285714285,
      "grad_norm": 10.673492431640625,
      "learning_rate": 9.29904761904762e-06,
      "loss": 0.4985,
      "step": 28090
    },
    {
      "epoch": 8.028571428571428,
      "grad_norm": 0.4197043180465698,
      "learning_rate": 9.295238095238095e-06,
      "loss": 0.3808,
      "step": 28100
    },
    {
      "epoch": 8.031428571428572,
      "grad_norm": 10.455636978149414,
      "learning_rate": 9.291428571428572e-06,
      "loss": 0.5765,
      "step": 28110
    },
    {
      "epoch": 8.034285714285714,
      "grad_norm": 0.4984806478023529,
      "learning_rate": 9.287619047619048e-06,
      "loss": 0.0999,
      "step": 28120
    },
    {
      "epoch": 8.037142857142857,
      "grad_norm": 0.45156964659690857,
      "learning_rate": 9.283809523809525e-06,
      "loss": 0.4979,
      "step": 28130
    },
    {
      "epoch": 8.04,
      "grad_norm": 10.872578620910645,
      "learning_rate": 9.280000000000001e-06,
      "loss": 0.3816,
      "step": 28140
    },
    {
      "epoch": 8.042857142857143,
      "grad_norm": 1.6037189960479736,
      "learning_rate": 9.276190476190477e-06,
      "loss": 0.5713,
      "step": 28150
    },
    {
      "epoch": 8.045714285714286,
      "grad_norm": 0.3476611375808716,
      "learning_rate": 9.272380952380952e-06,
      "loss": 0.5852,
      "step": 28160
    },
    {
      "epoch": 8.048571428571428,
      "grad_norm": 10.247657775878906,
      "learning_rate": 9.26857142857143e-06,
      "loss": 0.5347,
      "step": 28170
    },
    {
      "epoch": 8.051428571428572,
      "grad_norm": 11.799468040466309,
      "learning_rate": 9.264761904761905e-06,
      "loss": 0.6525,
      "step": 28180
    },
    {
      "epoch": 8.054285714285715,
      "grad_norm": 11.467902183532715,
      "learning_rate": 9.260952380952382e-06,
      "loss": 0.7794,
      "step": 28190
    },
    {
      "epoch": 8.057142857142857,
      "grad_norm": 12.369013786315918,
      "learning_rate": 9.257142857142858e-06,
      "loss": 0.7336,
      "step": 28200
    },
    {
      "epoch": 8.06,
      "grad_norm": 0.8728904724121094,
      "learning_rate": 9.253333333333333e-06,
      "loss": 0.3798,
      "step": 28210
    },
    {
      "epoch": 8.062857142857142,
      "grad_norm": 0.43073299527168274,
      "learning_rate": 9.24952380952381e-06,
      "loss": 0.3496,
      "step": 28220
    },
    {
      "epoch": 8.065714285714286,
      "grad_norm": 0.39614924788475037,
      "learning_rate": 9.245714285714286e-06,
      "loss": 0.2164,
      "step": 28230
    },
    {
      "epoch": 8.06857142857143,
      "grad_norm": 0.8460494875907898,
      "learning_rate": 9.241904761904764e-06,
      "loss": 0.5809,
      "step": 28240
    },
    {
      "epoch": 8.071428571428571,
      "grad_norm": 11.197649002075195,
      "learning_rate": 9.238095238095239e-06,
      "loss": 0.2163,
      "step": 28250
    },
    {
      "epoch": 8.074285714285715,
      "grad_norm": 0.34370166063308716,
      "learning_rate": 9.234285714285715e-06,
      "loss": 0.3702,
      "step": 28260
    },
    {
      "epoch": 8.077142857142857,
      "grad_norm": 11.099089622497559,
      "learning_rate": 9.23047619047619e-06,
      "loss": 0.3365,
      "step": 28270
    },
    {
      "epoch": 8.08,
      "grad_norm": 10.825971603393555,
      "learning_rate": 9.226666666666668e-06,
      "loss": 0.222,
      "step": 28280
    },
    {
      "epoch": 8.082857142857144,
      "grad_norm": 0.6520130634307861,
      "learning_rate": 9.222857142857143e-06,
      "loss": 0.3418,
      "step": 28290
    },
    {
      "epoch": 8.085714285714285,
      "grad_norm": 0.57682865858078,
      "learning_rate": 9.21904761904762e-06,
      "loss": 0.4644,
      "step": 28300
    },
    {
      "epoch": 8.088571428571429,
      "grad_norm": 0.32762935757637024,
      "learning_rate": 9.215238095238096e-06,
      "loss": 0.2559,
      "step": 28310
    },
    {
      "epoch": 8.09142857142857,
      "grad_norm": 11.342520713806152,
      "learning_rate": 9.211428571428572e-06,
      "loss": 0.7157,
      "step": 28320
    },
    {
      "epoch": 8.094285714285714,
      "grad_norm": 11.910995483398438,
      "learning_rate": 9.207619047619049e-06,
      "loss": 0.2008,
      "step": 28330
    },
    {
      "epoch": 8.097142857142858,
      "grad_norm": 0.3103678524494171,
      "learning_rate": 9.203809523809524e-06,
      "loss": 0.5046,
      "step": 28340
    },
    {
      "epoch": 8.1,
      "grad_norm": 0.3249813914299011,
      "learning_rate": 9.200000000000002e-06,
      "loss": 0.28,
      "step": 28350
    },
    {
      "epoch": 8.102857142857143,
      "grad_norm": 0.2732454240322113,
      "learning_rate": 9.196190476190477e-06,
      "loss": 0.3128,
      "step": 28360
    },
    {
      "epoch": 8.105714285714285,
      "grad_norm": 0.34297654032707214,
      "learning_rate": 9.192380952380953e-06,
      "loss": 0.3161,
      "step": 28370
    },
    {
      "epoch": 8.108571428571429,
      "grad_norm": 14.077583312988281,
      "learning_rate": 9.188571428571428e-06,
      "loss": 0.7051,
      "step": 28380
    },
    {
      "epoch": 8.111428571428572,
      "grad_norm": 0.22971008718013763,
      "learning_rate": 9.184761904761906e-06,
      "loss": 0.2694,
      "step": 28390
    },
    {
      "epoch": 8.114285714285714,
      "grad_norm": 0.9054556488990784,
      "learning_rate": 9.180952380952381e-06,
      "loss": 0.9236,
      "step": 28400
    },
    {
      "epoch": 8.117142857142857,
      "grad_norm": 0.4710524380207062,
      "learning_rate": 9.177142857142859e-06,
      "loss": 0.8989,
      "step": 28410
    },
    {
      "epoch": 8.12,
      "grad_norm": 0.7782903909683228,
      "learning_rate": 9.173333333333334e-06,
      "loss": 0.5516,
      "step": 28420
    },
    {
      "epoch": 8.122857142857143,
      "grad_norm": 11.401959419250488,
      "learning_rate": 9.16952380952381e-06,
      "loss": 0.6349,
      "step": 28430
    },
    {
      "epoch": 8.125714285714286,
      "grad_norm": 11.356770515441895,
      "learning_rate": 9.165714285714287e-06,
      "loss": 0.5456,
      "step": 28440
    },
    {
      "epoch": 8.128571428571428,
      "grad_norm": 11.082507133483887,
      "learning_rate": 9.161904761904763e-06,
      "loss": 0.8882,
      "step": 28450
    },
    {
      "epoch": 8.131428571428572,
      "grad_norm": 10.959794998168945,
      "learning_rate": 9.15809523809524e-06,
      "loss": 0.3508,
      "step": 28460
    },
    {
      "epoch": 8.134285714285713,
      "grad_norm": 0.4380697011947632,
      "learning_rate": 9.154285714285715e-06,
      "loss": 0.4717,
      "step": 28470
    },
    {
      "epoch": 8.137142857142857,
      "grad_norm": 1.3750877380371094,
      "learning_rate": 9.150476190476191e-06,
      "loss": 0.81,
      "step": 28480
    },
    {
      "epoch": 8.14,
      "grad_norm": 11.195035934448242,
      "learning_rate": 9.146666666666667e-06,
      "loss": 0.4937,
      "step": 28490
    },
    {
      "epoch": 8.142857142857142,
      "grad_norm": 0.4926091730594635,
      "learning_rate": 9.142857142857144e-06,
      "loss": 0.4868,
      "step": 28500
    },
    {
      "epoch": 8.145714285714286,
      "grad_norm": 11.756317138671875,
      "learning_rate": 9.13904761904762e-06,
      "loss": 0.4744,
      "step": 28510
    },
    {
      "epoch": 8.14857142857143,
      "grad_norm": 0.4460048973560333,
      "learning_rate": 9.135238095238095e-06,
      "loss": 0.324,
      "step": 28520
    },
    {
      "epoch": 8.151428571428571,
      "grad_norm": 10.665892601013184,
      "learning_rate": 9.131428571428572e-06,
      "loss": 0.5676,
      "step": 28530
    },
    {
      "epoch": 8.154285714285715,
      "grad_norm": 0.41735222935676575,
      "learning_rate": 9.127619047619048e-06,
      "loss": 0.1835,
      "step": 28540
    },
    {
      "epoch": 8.157142857142857,
      "grad_norm": 11.193963050842285,
      "learning_rate": 9.123809523809525e-06,
      "loss": 0.6158,
      "step": 28550
    },
    {
      "epoch": 8.16,
      "grad_norm": 0.3847380578517914,
      "learning_rate": 9.12e-06,
      "loss": 0.3777,
      "step": 28560
    },
    {
      "epoch": 8.162857142857144,
      "grad_norm": 10.979222297668457,
      "learning_rate": 9.116190476190478e-06,
      "loss": 0.5043,
      "step": 28570
    },
    {
      "epoch": 8.165714285714285,
      "grad_norm": 0.39524465799331665,
      "learning_rate": 9.112380952380954e-06,
      "loss": 0.4567,
      "step": 28580
    },
    {
      "epoch": 8.168571428571429,
      "grad_norm": 0.3934597373008728,
      "learning_rate": 9.10857142857143e-06,
      "loss": 0.7804,
      "step": 28590
    },
    {
      "epoch": 8.17142857142857,
      "grad_norm": 0.38886722922325134,
      "learning_rate": 9.104761904761905e-06,
      "loss": 0.19,
      "step": 28600
    },
    {
      "epoch": 8.174285714285714,
      "grad_norm": 0.7719364762306213,
      "learning_rate": 9.100952380952382e-06,
      "loss": 0.6985,
      "step": 28610
    },
    {
      "epoch": 8.177142857142858,
      "grad_norm": 12.5477876663208,
      "learning_rate": 9.097142857142858e-06,
      "loss": 0.7438,
      "step": 28620
    },
    {
      "epoch": 8.18,
      "grad_norm": 0.46765097975730896,
      "learning_rate": 9.093333333333333e-06,
      "loss": 0.646,
      "step": 28630
    },
    {
      "epoch": 8.182857142857143,
      "grad_norm": 0.4053014814853668,
      "learning_rate": 9.08952380952381e-06,
      "loss": 0.6685,
      "step": 28640
    },
    {
      "epoch": 8.185714285714285,
      "grad_norm": 0.5164862275123596,
      "learning_rate": 9.085714285714286e-06,
      "loss": 0.5318,
      "step": 28650
    },
    {
      "epoch": 8.188571428571429,
      "grad_norm": 0.3667260408401489,
      "learning_rate": 9.081904761904763e-06,
      "loss": 0.7339,
      "step": 28660
    },
    {
      "epoch": 8.191428571428572,
      "grad_norm": 0.35408005118370056,
      "learning_rate": 9.078095238095239e-06,
      "loss": 0.4468,
      "step": 28670
    },
    {
      "epoch": 8.194285714285714,
      "grad_norm": 0.3497224450111389,
      "learning_rate": 9.074285714285716e-06,
      "loss": 0.4298,
      "step": 28680
    },
    {
      "epoch": 8.197142857142858,
      "grad_norm": 0.3850945234298706,
      "learning_rate": 9.070476190476192e-06,
      "loss": 1.0518,
      "step": 28690
    },
    {
      "epoch": 8.2,
      "grad_norm": 0.48987531661987305,
      "learning_rate": 9.066666666666667e-06,
      "loss": 0.7175,
      "step": 28700
    },
    {
      "epoch": 8.202857142857143,
      "grad_norm": 0.4793627858161926,
      "learning_rate": 9.062857142857143e-06,
      "loss": 0.601,
      "step": 28710
    },
    {
      "epoch": 8.205714285714286,
      "grad_norm": 0.4532695412635803,
      "learning_rate": 9.05904761904762e-06,
      "loss": 0.213,
      "step": 28720
    },
    {
      "epoch": 8.208571428571428,
      "grad_norm": 11.135258674621582,
      "learning_rate": 9.055238095238096e-06,
      "loss": 0.9327,
      "step": 28730
    },
    {
      "epoch": 8.211428571428572,
      "grad_norm": 10.844048500061035,
      "learning_rate": 9.051428571428571e-06,
      "loss": 0.6162,
      "step": 28740
    },
    {
      "epoch": 8.214285714285714,
      "grad_norm": 0.45864346623420715,
      "learning_rate": 9.047619047619049e-06,
      "loss": 0.4622,
      "step": 28750
    },
    {
      "epoch": 8.217142857142857,
      "grad_norm": 0.45563778281211853,
      "learning_rate": 9.043809523809524e-06,
      "loss": 0.2987,
      "step": 28760
    },
    {
      "epoch": 8.22,
      "grad_norm": 0.38833698630332947,
      "learning_rate": 9.040000000000002e-06,
      "loss": 0.1004,
      "step": 28770
    },
    {
      "epoch": 8.222857142857142,
      "grad_norm": 22.350133895874023,
      "learning_rate": 9.036190476190477e-06,
      "loss": 0.5895,
      "step": 28780
    },
    {
      "epoch": 8.225714285714286,
      "grad_norm": 10.586430549621582,
      "learning_rate": 9.032380952380954e-06,
      "loss": 1.1688,
      "step": 28790
    },
    {
      "epoch": 8.228571428571428,
      "grad_norm": 0.4512902498245239,
      "learning_rate": 9.028571428571428e-06,
      "loss": 0.6157,
      "step": 28800
    },
    {
      "epoch": 8.231428571428571,
      "grad_norm": 0.7217273712158203,
      "learning_rate": 9.024761904761906e-06,
      "loss": 0.3555,
      "step": 28810
    },
    {
      "epoch": 8.234285714285715,
      "grad_norm": 10.330818176269531,
      "learning_rate": 9.020952380952381e-06,
      "loss": 0.5241,
      "step": 28820
    },
    {
      "epoch": 8.237142857142857,
      "grad_norm": 22.015377044677734,
      "learning_rate": 9.017142857142858e-06,
      "loss": 0.9683,
      "step": 28830
    },
    {
      "epoch": 8.24,
      "grad_norm": 10.908635139465332,
      "learning_rate": 9.013333333333334e-06,
      "loss": 0.5532,
      "step": 28840
    },
    {
      "epoch": 8.242857142857142,
      "grad_norm": 1.0880409479141235,
      "learning_rate": 9.00952380952381e-06,
      "loss": 0.5489,
      "step": 28850
    },
    {
      "epoch": 8.245714285714286,
      "grad_norm": 11.22098159790039,
      "learning_rate": 9.005714285714287e-06,
      "loss": 1.1755,
      "step": 28860
    },
    {
      "epoch": 8.248571428571429,
      "grad_norm": 11.112303733825684,
      "learning_rate": 9.001904761904762e-06,
      "loss": 0.6846,
      "step": 28870
    },
    {
      "epoch": 8.251428571428571,
      "grad_norm": 10.34815788269043,
      "learning_rate": 8.99809523809524e-06,
      "loss": 0.6446,
      "step": 28880
    },
    {
      "epoch": 8.254285714285714,
      "grad_norm": 9.770468711853027,
      "learning_rate": 8.994285714285715e-06,
      "loss": 0.4963,
      "step": 28890
    },
    {
      "epoch": 8.257142857142856,
      "grad_norm": 21.636281967163086,
      "learning_rate": 8.990476190476191e-06,
      "loss": 0.7353,
      "step": 28900
    },
    {
      "epoch": 8.26,
      "grad_norm": 0.6143141388893127,
      "learning_rate": 8.986666666666666e-06,
      "loss": 0.357,
      "step": 28910
    },
    {
      "epoch": 8.262857142857143,
      "grad_norm": 0.5988360047340393,
      "learning_rate": 8.982857142857144e-06,
      "loss": 0.4608,
      "step": 28920
    },
    {
      "epoch": 8.265714285714285,
      "grad_norm": 0.6162686944007874,
      "learning_rate": 8.97904761904762e-06,
      "loss": 0.2021,
      "step": 28930
    },
    {
      "epoch": 8.268571428571429,
      "grad_norm": 2.279447078704834,
      "learning_rate": 8.975238095238097e-06,
      "loss": 0.3933,
      "step": 28940
    },
    {
      "epoch": 8.271428571428572,
      "grad_norm": 1.6588184833526611,
      "learning_rate": 8.971428571428572e-06,
      "loss": 0.5738,
      "step": 28950
    },
    {
      "epoch": 8.274285714285714,
      "grad_norm": 11.06672477722168,
      "learning_rate": 8.967619047619048e-06,
      "loss": 0.908,
      "step": 28960
    },
    {
      "epoch": 8.277142857142858,
      "grad_norm": 1.4583930969238281,
      "learning_rate": 8.963809523809525e-06,
      "loss": 0.1015,
      "step": 28970
    },
    {
      "epoch": 8.28,
      "grad_norm": 0.4758633077144623,
      "learning_rate": 8.96e-06,
      "loss": 0.3475,
      "step": 28980
    },
    {
      "epoch": 8.282857142857143,
      "grad_norm": 0.6589037179946899,
      "learning_rate": 8.956190476190478e-06,
      "loss": 0.2132,
      "step": 28990
    },
    {
      "epoch": 8.285714285714286,
      "grad_norm": 0.5706764459609985,
      "learning_rate": 8.952380952380953e-06,
      "loss": 0.2156,
      "step": 29000
    },
    {
      "epoch": 8.288571428571428,
      "grad_norm": 10.730775833129883,
      "learning_rate": 8.948571428571429e-06,
      "loss": 0.2016,
      "step": 29010
    },
    {
      "epoch": 8.291428571428572,
      "grad_norm": 0.5360599756240845,
      "learning_rate": 8.944761904761905e-06,
      "loss": 0.3493,
      "step": 29020
    },
    {
      "epoch": 8.294285714285714,
      "grad_norm": 0.5003685355186462,
      "learning_rate": 8.940952380952382e-06,
      "loss": 0.311,
      "step": 29030
    },
    {
      "epoch": 8.297142857142857,
      "grad_norm": 11.802980422973633,
      "learning_rate": 8.937142857142857e-06,
      "loss": 0.3373,
      "step": 29040
    },
    {
      "epoch": 8.3,
      "grad_norm": 22.160930633544922,
      "learning_rate": 8.933333333333333e-06,
      "loss": 0.5509,
      "step": 29050
    },
    {
      "epoch": 8.302857142857142,
      "grad_norm": 0.4811325669288635,
      "learning_rate": 8.92952380952381e-06,
      "loss": 0.51,
      "step": 29060
    },
    {
      "epoch": 8.305714285714286,
      "grad_norm": 22.248180389404297,
      "learning_rate": 8.925714285714286e-06,
      "loss": 0.6951,
      "step": 29070
    },
    {
      "epoch": 8.308571428571428,
      "grad_norm": 0.5854206085205078,
      "learning_rate": 8.921904761904763e-06,
      "loss": 0.5289,
      "step": 29080
    },
    {
      "epoch": 8.311428571428571,
      "grad_norm": 0.33593782782554626,
      "learning_rate": 8.918095238095239e-06,
      "loss": 0.0126,
      "step": 29090
    },
    {
      "epoch": 8.314285714285715,
      "grad_norm": 0.33396467566490173,
      "learning_rate": 8.914285714285716e-06,
      "loss": 0.3809,
      "step": 29100
    },
    {
      "epoch": 8.317142857142857,
      "grad_norm": 10.870004653930664,
      "learning_rate": 8.910476190476192e-06,
      "loss": 0.4452,
      "step": 29110
    },
    {
      "epoch": 8.32,
      "grad_norm": 0.30013617873191833,
      "learning_rate": 8.906666666666667e-06,
      "loss": 0.2578,
      "step": 29120
    },
    {
      "epoch": 8.322857142857142,
      "grad_norm": 10.73234748840332,
      "learning_rate": 8.902857142857143e-06,
      "loss": 0.6105,
      "step": 29130
    },
    {
      "epoch": 8.325714285714286,
      "grad_norm": 11.128899574279785,
      "learning_rate": 8.89904761904762e-06,
      "loss": 0.8495,
      "step": 29140
    },
    {
      "epoch": 8.32857142857143,
      "grad_norm": 0.34425902366638184,
      "learning_rate": 8.895238095238096e-06,
      "loss": 0.2285,
      "step": 29150
    },
    {
      "epoch": 8.331428571428571,
      "grad_norm": 0.305045485496521,
      "learning_rate": 8.891428571428571e-06,
      "loss": 0.4116,
      "step": 29160
    },
    {
      "epoch": 8.334285714285715,
      "grad_norm": 11.29869556427002,
      "learning_rate": 8.887619047619049e-06,
      "loss": 0.2259,
      "step": 29170
    },
    {
      "epoch": 8.337142857142856,
      "grad_norm": 0.31190887093544006,
      "learning_rate": 8.883809523809524e-06,
      "loss": 0.6284,
      "step": 29180
    },
    {
      "epoch": 8.34,
      "grad_norm": 11.192618370056152,
      "learning_rate": 8.880000000000001e-06,
      "loss": 0.1981,
      "step": 29190
    },
    {
      "epoch": 8.342857142857143,
      "grad_norm": 11.756946563720703,
      "learning_rate": 8.876190476190477e-06,
      "loss": 0.3943,
      "step": 29200
    },
    {
      "epoch": 8.345714285714285,
      "grad_norm": 11.45611572265625,
      "learning_rate": 8.872380952380954e-06,
      "loss": 0.7724,
      "step": 29210
    },
    {
      "epoch": 8.348571428571429,
      "grad_norm": 0.2786214351654053,
      "learning_rate": 8.86857142857143e-06,
      "loss": 0.2961,
      "step": 29220
    },
    {
      "epoch": 8.35142857142857,
      "grad_norm": 13.088817596435547,
      "learning_rate": 8.864761904761905e-06,
      "loss": 0.6423,
      "step": 29230
    },
    {
      "epoch": 8.354285714285714,
      "grad_norm": 0.3031889796257019,
      "learning_rate": 8.860952380952381e-06,
      "loss": 0.4558,
      "step": 29240
    },
    {
      "epoch": 8.357142857142858,
      "grad_norm": 0.531970739364624,
      "learning_rate": 8.857142857142858e-06,
      "loss": 0.5391,
      "step": 29250
    },
    {
      "epoch": 8.36,
      "grad_norm": 10.83565616607666,
      "learning_rate": 8.853333333333334e-06,
      "loss": 0.2251,
      "step": 29260
    },
    {
      "epoch": 8.362857142857143,
      "grad_norm": 0.5269580483436584,
      "learning_rate": 8.84952380952381e-06,
      "loss": 0.1417,
      "step": 29270
    },
    {
      "epoch": 8.365714285714287,
      "grad_norm": 0.2150622457265854,
      "learning_rate": 8.845714285714287e-06,
      "loss": 0.7839,
      "step": 29280
    },
    {
      "epoch": 8.368571428571428,
      "grad_norm": 22.366960525512695,
      "learning_rate": 8.841904761904762e-06,
      "loss": 0.7549,
      "step": 29290
    },
    {
      "epoch": 8.371428571428572,
      "grad_norm": 10.538464546203613,
      "learning_rate": 8.83809523809524e-06,
      "loss": 0.5695,
      "step": 29300
    },
    {
      "epoch": 8.374285714285714,
      "grad_norm": 0.8405140042304993,
      "learning_rate": 8.834285714285715e-06,
      "loss": 0.4121,
      "step": 29310
    },
    {
      "epoch": 8.377142857142857,
      "grad_norm": 12.631850242614746,
      "learning_rate": 8.83047619047619e-06,
      "loss": 0.6069,
      "step": 29320
    },
    {
      "epoch": 8.38,
      "grad_norm": 0.22645160555839539,
      "learning_rate": 8.826666666666668e-06,
      "loss": 0.5557,
      "step": 29330
    },
    {
      "epoch": 8.382857142857143,
      "grad_norm": 13.179092407226562,
      "learning_rate": 8.822857142857144e-06,
      "loss": 0.6188,
      "step": 29340
    },
    {
      "epoch": 8.385714285714286,
      "grad_norm": 0.4630742073059082,
      "learning_rate": 8.819047619047619e-06,
      "loss": 0.4795,
      "step": 29350
    },
    {
      "epoch": 8.388571428571428,
      "grad_norm": 0.7800259590148926,
      "learning_rate": 8.815238095238096e-06,
      "loss": 0.3978,
      "step": 29360
    },
    {
      "epoch": 8.391428571428571,
      "grad_norm": 0.22156836092472076,
      "learning_rate": 8.811428571428572e-06,
      "loss": 0.4771,
      "step": 29370
    },
    {
      "epoch": 8.394285714285715,
      "grad_norm": 0.20347967743873596,
      "learning_rate": 8.807619047619048e-06,
      "loss": 0.3402,
      "step": 29380
    },
    {
      "epoch": 8.397142857142857,
      "grad_norm": 0.9312033653259277,
      "learning_rate": 8.803809523809525e-06,
      "loss": 0.6898,
      "step": 29390
    },
    {
      "epoch": 8.4,
      "grad_norm": 0.46628573536872864,
      "learning_rate": 8.8e-06,
      "loss": 1.1502,
      "step": 29400
    },
    {
      "epoch": 8.402857142857142,
      "grad_norm": 0.24387353658676147,
      "learning_rate": 8.796190476190478e-06,
      "loss": 0.5944,
      "step": 29410
    },
    {
      "epoch": 8.405714285714286,
      "grad_norm": 1.1272673606872559,
      "learning_rate": 8.792380952380953e-06,
      "loss": 0.7439,
      "step": 29420
    },
    {
      "epoch": 8.40857142857143,
      "grad_norm": 13.138053894042969,
      "learning_rate": 8.788571428571429e-06,
      "loss": 0.6891,
      "step": 29430
    },
    {
      "epoch": 8.411428571428571,
      "grad_norm": 0.5500555038452148,
      "learning_rate": 8.784761904761906e-06,
      "loss": 0.6492,
      "step": 29440
    },
    {
      "epoch": 8.414285714285715,
      "grad_norm": 11.368160247802734,
      "learning_rate": 8.780952380952382e-06,
      "loss": 0.2215,
      "step": 29450
    },
    {
      "epoch": 8.417142857142856,
      "grad_norm": 1.7028228044509888,
      "learning_rate": 8.777142857142857e-06,
      "loss": 0.5186,
      "step": 29460
    },
    {
      "epoch": 8.42,
      "grad_norm": 0.507819414138794,
      "learning_rate": 8.773333333333333e-06,
      "loss": 0.1622,
      "step": 29470
    },
    {
      "epoch": 8.422857142857143,
      "grad_norm": 11.066230773925781,
      "learning_rate": 8.76952380952381e-06,
      "loss": 0.4283,
      "step": 29480
    },
    {
      "epoch": 8.425714285714285,
      "grad_norm": 0.8595981597900391,
      "learning_rate": 8.765714285714286e-06,
      "loss": 0.3461,
      "step": 29490
    },
    {
      "epoch": 8.428571428571429,
      "grad_norm": 0.27196401357650757,
      "learning_rate": 8.761904761904763e-06,
      "loss": 0.6421,
      "step": 29500
    },
    {
      "epoch": 8.43142857142857,
      "grad_norm": 0.458332359790802,
      "learning_rate": 8.758095238095239e-06,
      "loss": 0.4303,
      "step": 29510
    },
    {
      "epoch": 8.434285714285714,
      "grad_norm": 0.7728680372238159,
      "learning_rate": 8.754285714285716e-06,
      "loss": 0.8829,
      "step": 29520
    },
    {
      "epoch": 8.437142857142858,
      "grad_norm": 0.32402536273002625,
      "learning_rate": 8.750476190476191e-06,
      "loss": 0.5401,
      "step": 29530
    },
    {
      "epoch": 8.44,
      "grad_norm": 0.28933218121528625,
      "learning_rate": 8.746666666666667e-06,
      "loss": 0.0936,
      "step": 29540
    },
    {
      "epoch": 8.442857142857143,
      "grad_norm": 13.001442909240723,
      "learning_rate": 8.742857142857144e-06,
      "loss": 0.6391,
      "step": 29550
    },
    {
      "epoch": 8.445714285714285,
      "grad_norm": 10.922881126403809,
      "learning_rate": 8.73904761904762e-06,
      "loss": 1.0435,
      "step": 29560
    },
    {
      "epoch": 8.448571428571428,
      "grad_norm": 0.3336573541164398,
      "learning_rate": 8.735238095238096e-06,
      "loss": 0.5311,
      "step": 29570
    },
    {
      "epoch": 8.451428571428572,
      "grad_norm": 0.8472896218299866,
      "learning_rate": 8.731428571428571e-06,
      "loss": 0.4322,
      "step": 29580
    },
    {
      "epoch": 8.454285714285714,
      "grad_norm": 13.760578155517578,
      "learning_rate": 8.727619047619048e-06,
      "loss": 0.3504,
      "step": 29590
    },
    {
      "epoch": 8.457142857142857,
      "grad_norm": 135.216796875,
      "learning_rate": 8.723809523809524e-06,
      "loss": 0.5142,
      "step": 29600
    },
    {
      "epoch": 8.46,
      "grad_norm": 1.1239893436431885,
      "learning_rate": 8.720000000000001e-06,
      "loss": 0.5286,
      "step": 29610
    },
    {
      "epoch": 8.462857142857143,
      "grad_norm": 0.48033490777015686,
      "learning_rate": 8.716190476190477e-06,
      "loss": 0.5969,
      "step": 29620
    },
    {
      "epoch": 8.465714285714286,
      "grad_norm": 0.4860091209411621,
      "learning_rate": 8.712380952380954e-06,
      "loss": 0.3491,
      "step": 29630
    },
    {
      "epoch": 8.468571428571428,
      "grad_norm": 13.447052001953125,
      "learning_rate": 8.70857142857143e-06,
      "loss": 0.3894,
      "step": 29640
    },
    {
      "epoch": 8.471428571428572,
      "grad_norm": 2.1010241508483887,
      "learning_rate": 8.704761904761905e-06,
      "loss": 0.5174,
      "step": 29650
    },
    {
      "epoch": 8.474285714285715,
      "grad_norm": 12.13523006439209,
      "learning_rate": 8.700952380952383e-06,
      "loss": 0.5982,
      "step": 29660
    },
    {
      "epoch": 8.477142857142857,
      "grad_norm": 11.172664642333984,
      "learning_rate": 8.697142857142858e-06,
      "loss": 0.4311,
      "step": 29670
    },
    {
      "epoch": 8.48,
      "grad_norm": 2.385869264602661,
      "learning_rate": 8.693333333333334e-06,
      "loss": 0.4145,
      "step": 29680
    },
    {
      "epoch": 8.482857142857142,
      "grad_norm": 33.92063903808594,
      "learning_rate": 8.68952380952381e-06,
      "loss": 0.5863,
      "step": 29690
    },
    {
      "epoch": 8.485714285714286,
      "grad_norm": 0.8201861381530762,
      "learning_rate": 8.685714285714287e-06,
      "loss": 0.6924,
      "step": 29700
    },
    {
      "epoch": 8.48857142857143,
      "grad_norm": 11.61481761932373,
      "learning_rate": 8.681904761904762e-06,
      "loss": 0.4967,
      "step": 29710
    },
    {
      "epoch": 8.491428571428571,
      "grad_norm": 13.858259201049805,
      "learning_rate": 8.67809523809524e-06,
      "loss": 0.7773,
      "step": 29720
    },
    {
      "epoch": 8.494285714285715,
      "grad_norm": 0.8501216769218445,
      "learning_rate": 8.674285714285715e-06,
      "loss": 0.3648,
      "step": 29730
    },
    {
      "epoch": 8.497142857142856,
      "grad_norm": 13.082605361938477,
      "learning_rate": 8.67047619047619e-06,
      "loss": 0.7882,
      "step": 29740
    },
    {
      "epoch": 8.5,
      "grad_norm": 12.186149597167969,
      "learning_rate": 8.666666666666668e-06,
      "loss": 0.423,
      "step": 29750
    },
    {
      "epoch": 8.502857142857144,
      "grad_norm": 0.6915271878242493,
      "learning_rate": 8.662857142857143e-06,
      "loss": 0.2489,
      "step": 29760
    },
    {
      "epoch": 8.505714285714285,
      "grad_norm": 0.5746052265167236,
      "learning_rate": 8.65904761904762e-06,
      "loss": 0.4184,
      "step": 29770
    },
    {
      "epoch": 8.508571428571429,
      "grad_norm": 14.781352996826172,
      "learning_rate": 8.655238095238096e-06,
      "loss": 0.5792,
      "step": 29780
    },
    {
      "epoch": 8.51142857142857,
      "grad_norm": 11.317091941833496,
      "learning_rate": 8.651428571428572e-06,
      "loss": 0.9282,
      "step": 29790
    },
    {
      "epoch": 8.514285714285714,
      "grad_norm": 11.999377250671387,
      "learning_rate": 8.647619047619047e-06,
      "loss": 0.4518,
      "step": 29800
    },
    {
      "epoch": 8.517142857142858,
      "grad_norm": 0.5995094180107117,
      "learning_rate": 8.643809523809525e-06,
      "loss": 0.1684,
      "step": 29810
    },
    {
      "epoch": 8.52,
      "grad_norm": 0.6285431385040283,
      "learning_rate": 8.64e-06,
      "loss": 0.2998,
      "step": 29820
    },
    {
      "epoch": 8.522857142857143,
      "grad_norm": 0.6919007301330566,
      "learning_rate": 8.636190476190478e-06,
      "loss": 0.0705,
      "step": 29830
    },
    {
      "epoch": 8.525714285714285,
      "grad_norm": 12.650032043457031,
      "learning_rate": 8.632380952380953e-06,
      "loss": 0.6113,
      "step": 29840
    },
    {
      "epoch": 8.528571428571428,
      "grad_norm": 0.3737347424030304,
      "learning_rate": 8.628571428571429e-06,
      "loss": 0.4743,
      "step": 29850
    },
    {
      "epoch": 8.531428571428572,
      "grad_norm": 0.3654617667198181,
      "learning_rate": 8.624761904761906e-06,
      "loss": 0.4966,
      "step": 29860
    },
    {
      "epoch": 8.534285714285714,
      "grad_norm": 0.3325916528701782,
      "learning_rate": 8.620952380952382e-06,
      "loss": 0.2138,
      "step": 29870
    },
    {
      "epoch": 8.537142857142857,
      "grad_norm": 0.3962394595146179,
      "learning_rate": 8.617142857142859e-06,
      "loss": 1.032,
      "step": 29880
    },
    {
      "epoch": 8.54,
      "grad_norm": 0.5570785403251648,
      "learning_rate": 8.613333333333333e-06,
      "loss": 0.6651,
      "step": 29890
    },
    {
      "epoch": 8.542857142857143,
      "grad_norm": 11.009188652038574,
      "learning_rate": 8.60952380952381e-06,
      "loss": 0.3398,
      "step": 29900
    },
    {
      "epoch": 8.545714285714286,
      "grad_norm": 0.4326082170009613,
      "learning_rate": 8.605714285714286e-06,
      "loss": 0.6241,
      "step": 29910
    },
    {
      "epoch": 8.548571428571428,
      "grad_norm": 11.038167953491211,
      "learning_rate": 8.601904761904763e-06,
      "loss": 0.5035,
      "step": 29920
    },
    {
      "epoch": 8.551428571428572,
      "grad_norm": 0.4935643970966339,
      "learning_rate": 8.598095238095238e-06,
      "loss": 0.9027,
      "step": 29930
    },
    {
      "epoch": 8.554285714285715,
      "grad_norm": 0.5925140976905823,
      "learning_rate": 8.594285714285716e-06,
      "loss": 0.5752,
      "step": 29940
    },
    {
      "epoch": 8.557142857142857,
      "grad_norm": 0.6958273649215698,
      "learning_rate": 8.590476190476191e-06,
      "loss": 0.679,
      "step": 29950
    },
    {
      "epoch": 8.56,
      "grad_norm": 10.76803207397461,
      "learning_rate": 8.586666666666667e-06,
      "loss": 0.4247,
      "step": 29960
    },
    {
      "epoch": 8.562857142857142,
      "grad_norm": 10.833380699157715,
      "learning_rate": 8.582857142857144e-06,
      "loss": 0.817,
      "step": 29970
    },
    {
      "epoch": 8.565714285714286,
      "grad_norm": 0.6782263517379761,
      "learning_rate": 8.57904761904762e-06,
      "loss": 0.6611,
      "step": 29980
    },
    {
      "epoch": 8.56857142857143,
      "grad_norm": 0.530023455619812,
      "learning_rate": 8.575238095238097e-06,
      "loss": 0.0138,
      "step": 29990
    },
    {
      "epoch": 8.571428571428571,
      "grad_norm": 11.009644508361816,
      "learning_rate": 8.571428571428571e-06,
      "loss": 0.5524,
      "step": 30000
    },
    {
      "epoch": 8.574285714285715,
      "grad_norm": 0.4270225167274475,
      "learning_rate": 8.567619047619048e-06,
      "loss": 0.6435,
      "step": 30010
    },
    {
      "epoch": 8.577142857142857,
      "grad_norm": 11.166247367858887,
      "learning_rate": 8.563809523809524e-06,
      "loss": 0.5497,
      "step": 30020
    },
    {
      "epoch": 8.58,
      "grad_norm": 11.056060791015625,
      "learning_rate": 8.560000000000001e-06,
      "loss": 0.3601,
      "step": 30030
    },
    {
      "epoch": 8.582857142857144,
      "grad_norm": 1.36297607421875,
      "learning_rate": 8.556190476190477e-06,
      "loss": 0.634,
      "step": 30040
    },
    {
      "epoch": 8.585714285714285,
      "grad_norm": 22.090099334716797,
      "learning_rate": 8.552380952380954e-06,
      "loss": 0.5681,
      "step": 30050
    },
    {
      "epoch": 8.588571428571429,
      "grad_norm": 0.501749575138092,
      "learning_rate": 8.54857142857143e-06,
      "loss": 0.6095,
      "step": 30060
    },
    {
      "epoch": 8.59142857142857,
      "grad_norm": 0.7873939275741577,
      "learning_rate": 8.544761904761905e-06,
      "loss": 0.4496,
      "step": 30070
    },
    {
      "epoch": 8.594285714285714,
      "grad_norm": 0.6872701048851013,
      "learning_rate": 8.540952380952382e-06,
      "loss": 0.3341,
      "step": 30080
    },
    {
      "epoch": 8.597142857142858,
      "grad_norm": 0.5029446482658386,
      "learning_rate": 8.537142857142858e-06,
      "loss": 0.7711,
      "step": 30090
    },
    {
      "epoch": 8.6,
      "grad_norm": 11.194643020629883,
      "learning_rate": 8.533333333333335e-06,
      "loss": 0.4518,
      "step": 30100
    },
    {
      "epoch": 8.602857142857143,
      "grad_norm": 0.4735928177833557,
      "learning_rate": 8.529523809523809e-06,
      "loss": 0.509,
      "step": 30110
    },
    {
      "epoch": 8.605714285714285,
      "grad_norm": 15.018264770507812,
      "learning_rate": 8.525714285714286e-06,
      "loss": 0.5028,
      "step": 30120
    },
    {
      "epoch": 8.608571428571429,
      "grad_norm": 0.4517279863357544,
      "learning_rate": 8.521904761904762e-06,
      "loss": 0.1882,
      "step": 30130
    },
    {
      "epoch": 8.611428571428572,
      "grad_norm": 11.855854988098145,
      "learning_rate": 8.51809523809524e-06,
      "loss": 0.4773,
      "step": 30140
    },
    {
      "epoch": 8.614285714285714,
      "grad_norm": 11.231346130371094,
      "learning_rate": 8.514285714285715e-06,
      "loss": 0.3781,
      "step": 30150
    },
    {
      "epoch": 8.617142857142857,
      "grad_norm": 0.34952208399772644,
      "learning_rate": 8.51047619047619e-06,
      "loss": 0.6111,
      "step": 30160
    },
    {
      "epoch": 8.62,
      "grad_norm": 59.55949401855469,
      "learning_rate": 8.506666666666668e-06,
      "loss": 0.8969,
      "step": 30170
    },
    {
      "epoch": 8.622857142857143,
      "grad_norm": 12.700138092041016,
      "learning_rate": 8.502857142857143e-06,
      "loss": 0.8826,
      "step": 30180
    },
    {
      "epoch": 8.625714285714286,
      "grad_norm": 0.5907236337661743,
      "learning_rate": 8.49904761904762e-06,
      "loss": 0.8287,
      "step": 30190
    },
    {
      "epoch": 8.628571428571428,
      "grad_norm": 24.647966384887695,
      "learning_rate": 8.495238095238096e-06,
      "loss": 0.7285,
      "step": 30200
    },
    {
      "epoch": 8.631428571428572,
      "grad_norm": 0.609438419342041,
      "learning_rate": 8.491428571428572e-06,
      "loss": 0.8686,
      "step": 30210
    },
    {
      "epoch": 8.634285714285713,
      "grad_norm": 32.26472854614258,
      "learning_rate": 8.487619047619047e-06,
      "loss": 0.5913,
      "step": 30220
    },
    {
      "epoch": 8.637142857142857,
      "grad_norm": 21.66818618774414,
      "learning_rate": 8.483809523809525e-06,
      "loss": 0.706,
      "step": 30230
    },
    {
      "epoch": 8.64,
      "grad_norm": 13.232300758361816,
      "learning_rate": 8.48e-06,
      "loss": 0.4817,
      "step": 30240
    },
    {
      "epoch": 8.642857142857142,
      "grad_norm": 10.749130249023438,
      "learning_rate": 8.476190476190477e-06,
      "loss": 0.3004,
      "step": 30250
    },
    {
      "epoch": 8.645714285714286,
      "grad_norm": 25.087493896484375,
      "learning_rate": 8.472380952380953e-06,
      "loss": 0.5491,
      "step": 30260
    },
    {
      "epoch": 8.64857142857143,
      "grad_norm": 0.8485902547836304,
      "learning_rate": 8.468571428571429e-06,
      "loss": 0.083,
      "step": 30270
    },
    {
      "epoch": 8.651428571428571,
      "grad_norm": 24.19097900390625,
      "learning_rate": 8.464761904761906e-06,
      "loss": 0.3135,
      "step": 30280
    },
    {
      "epoch": 8.654285714285715,
      "grad_norm": 13.977376937866211,
      "learning_rate": 8.460952380952381e-06,
      "loss": 0.6477,
      "step": 30290
    },
    {
      "epoch": 8.657142857142857,
      "grad_norm": 0.5038153529167175,
      "learning_rate": 8.457142857142859e-06,
      "loss": 0.4495,
      "step": 30300
    },
    {
      "epoch": 8.66,
      "grad_norm": 11.093771934509277,
      "learning_rate": 8.453333333333334e-06,
      "loss": 0.4764,
      "step": 30310
    },
    {
      "epoch": 8.662857142857142,
      "grad_norm": 12.204660415649414,
      "learning_rate": 8.44952380952381e-06,
      "loss": 1.0688,
      "step": 30320
    },
    {
      "epoch": 8.665714285714285,
      "grad_norm": 0.6996908783912659,
      "learning_rate": 8.445714285714285e-06,
      "loss": 0.5382,
      "step": 30330
    },
    {
      "epoch": 8.668571428571429,
      "grad_norm": 0.6753948926925659,
      "learning_rate": 8.441904761904763e-06,
      "loss": 0.2722,
      "step": 30340
    },
    {
      "epoch": 8.67142857142857,
      "grad_norm": 22.192052841186523,
      "learning_rate": 8.438095238095238e-06,
      "loss": 0.676,
      "step": 30350
    },
    {
      "epoch": 8.674285714285714,
      "grad_norm": 0.7816388607025146,
      "learning_rate": 8.434285714285716e-06,
      "loss": 0.7908,
      "step": 30360
    },
    {
      "epoch": 8.677142857142858,
      "grad_norm": 0.8522191047668457,
      "learning_rate": 8.430476190476191e-06,
      "loss": 0.6531,
      "step": 30370
    },
    {
      "epoch": 8.68,
      "grad_norm": 0.779651403427124,
      "learning_rate": 8.426666666666667e-06,
      "loss": 0.2564,
      "step": 30380
    },
    {
      "epoch": 8.682857142857143,
      "grad_norm": 20.331405639648438,
      "learning_rate": 8.422857142857144e-06,
      "loss": 0.582,
      "step": 30390
    },
    {
      "epoch": 8.685714285714285,
      "grad_norm": 10.649762153625488,
      "learning_rate": 8.41904761904762e-06,
      "loss": 0.9324,
      "step": 30400
    },
    {
      "epoch": 8.688571428571429,
      "grad_norm": 0.7585724592208862,
      "learning_rate": 8.415238095238097e-06,
      "loss": 0.4281,
      "step": 30410
    },
    {
      "epoch": 8.691428571428572,
      "grad_norm": 0.6862541437149048,
      "learning_rate": 8.411428571428572e-06,
      "loss": 0.3675,
      "step": 30420
    },
    {
      "epoch": 8.694285714285714,
      "grad_norm": 33.09425354003906,
      "learning_rate": 8.407619047619048e-06,
      "loss": 0.2597,
      "step": 30430
    },
    {
      "epoch": 8.697142857142858,
      "grad_norm": 10.742305755615234,
      "learning_rate": 8.403809523809524e-06,
      "loss": 0.5782,
      "step": 30440
    },
    {
      "epoch": 8.7,
      "grad_norm": 12.91296100616455,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.5528,
      "step": 30450
    },
    {
      "epoch": 8.702857142857143,
      "grad_norm": 11.28366756439209,
      "learning_rate": 8.396190476190476e-06,
      "loss": 0.8204,
      "step": 30460
    },
    {
      "epoch": 8.705714285714286,
      "grad_norm": 22.889638900756836,
      "learning_rate": 8.392380952380954e-06,
      "loss": 0.7266,
      "step": 30470
    },
    {
      "epoch": 8.708571428571428,
      "grad_norm": 0.7912735342979431,
      "learning_rate": 8.38857142857143e-06,
      "loss": 0.5706,
      "step": 30480
    },
    {
      "epoch": 8.711428571428572,
      "grad_norm": 11.397714614868164,
      "learning_rate": 8.384761904761905e-06,
      "loss": 0.4392,
      "step": 30490
    },
    {
      "epoch": 8.714285714285714,
      "grad_norm": 0.7471539378166199,
      "learning_rate": 8.380952380952382e-06,
      "loss": 0.2674,
      "step": 30500
    },
    {
      "epoch": 8.717142857142857,
      "grad_norm": 0.749125599861145,
      "learning_rate": 8.377142857142858e-06,
      "loss": 0.6327,
      "step": 30510
    },
    {
      "epoch": 8.72,
      "grad_norm": 0.7047986388206482,
      "learning_rate": 8.373333333333335e-06,
      "loss": 0.325,
      "step": 30520
    },
    {
      "epoch": 8.722857142857142,
      "grad_norm": 0.63201504945755,
      "learning_rate": 8.36952380952381e-06,
      "loss": 0.4332,
      "step": 30530
    },
    {
      "epoch": 8.725714285714286,
      "grad_norm": 10.96857738494873,
      "learning_rate": 8.365714285714286e-06,
      "loss": 0.7344,
      "step": 30540
    },
    {
      "epoch": 8.728571428571428,
      "grad_norm": 11.082847595214844,
      "learning_rate": 8.361904761904762e-06,
      "loss": 0.461,
      "step": 30550
    },
    {
      "epoch": 8.731428571428571,
      "grad_norm": 0.49433189630508423,
      "learning_rate": 8.358095238095239e-06,
      "loss": 0.2889,
      "step": 30560
    },
    {
      "epoch": 8.734285714285715,
      "grad_norm": 10.90075397491455,
      "learning_rate": 8.354285714285715e-06,
      "loss": 0.4373,
      "step": 30570
    },
    {
      "epoch": 8.737142857142857,
      "grad_norm": 0.5042359828948975,
      "learning_rate": 8.35047619047619e-06,
      "loss": 0.6201,
      "step": 30580
    },
    {
      "epoch": 8.74,
      "grad_norm": 0.5406646132469177,
      "learning_rate": 8.346666666666668e-06,
      "loss": 0.9023,
      "step": 30590
    },
    {
      "epoch": 8.742857142857144,
      "grad_norm": 0.5552993416786194,
      "learning_rate": 8.342857142857143e-06,
      "loss": 0.4485,
      "step": 30600
    },
    {
      "epoch": 8.745714285714286,
      "grad_norm": 0.583816647529602,
      "learning_rate": 8.33904761904762e-06,
      "loss": 0.5573,
      "step": 30610
    },
    {
      "epoch": 8.748571428571429,
      "grad_norm": 0.5463262796401978,
      "learning_rate": 8.335238095238096e-06,
      "loss": 0.6738,
      "step": 30620
    },
    {
      "epoch": 8.751428571428571,
      "grad_norm": 0.5371657013893127,
      "learning_rate": 8.331428571428573e-06,
      "loss": 0.595,
      "step": 30630
    },
    {
      "epoch": 8.754285714285714,
      "grad_norm": 0.4908334016799927,
      "learning_rate": 8.327619047619049e-06,
      "loss": 0.1246,
      "step": 30640
    },
    {
      "epoch": 8.757142857142856,
      "grad_norm": 0.5229637026786804,
      "learning_rate": 8.323809523809524e-06,
      "loss": 1.1312,
      "step": 30650
    },
    {
      "epoch": 8.76,
      "grad_norm": 0.596394419670105,
      "learning_rate": 8.32e-06,
      "loss": 0.5571,
      "step": 30660
    },
    {
      "epoch": 8.762857142857143,
      "grad_norm": 0.5854582786560059,
      "learning_rate": 8.316190476190477e-06,
      "loss": 0.3421,
      "step": 30670
    },
    {
      "epoch": 8.765714285714285,
      "grad_norm": 22.257444381713867,
      "learning_rate": 8.312380952380953e-06,
      "loss": 0.7758,
      "step": 30680
    },
    {
      "epoch": 8.768571428571429,
      "grad_norm": 0.6462774276733398,
      "learning_rate": 8.308571428571428e-06,
      "loss": 0.6419,
      "step": 30690
    },
    {
      "epoch": 8.771428571428572,
      "grad_norm": 10.896041870117188,
      "learning_rate": 8.304761904761906e-06,
      "loss": 0.7521,
      "step": 30700
    },
    {
      "epoch": 8.774285714285714,
      "grad_norm": 11.349225044250488,
      "learning_rate": 8.300952380952381e-06,
      "loss": 0.6344,
      "step": 30710
    },
    {
      "epoch": 8.777142857142858,
      "grad_norm": 33.42048263549805,
      "learning_rate": 8.297142857142859e-06,
      "loss": 0.7282,
      "step": 30720
    },
    {
      "epoch": 8.78,
      "grad_norm": 0.7383718490600586,
      "learning_rate": 8.293333333333334e-06,
      "loss": 0.417,
      "step": 30730
    },
    {
      "epoch": 8.782857142857143,
      "grad_norm": 0.7580348253250122,
      "learning_rate": 8.289523809523811e-06,
      "loss": 0.3271,
      "step": 30740
    },
    {
      "epoch": 8.785714285714286,
      "grad_norm": 0.6500111818313599,
      "learning_rate": 8.285714285714287e-06,
      "loss": 0.3245,
      "step": 30750
    },
    {
      "epoch": 8.788571428571428,
      "grad_norm": 10.863940238952637,
      "learning_rate": 8.281904761904763e-06,
      "loss": 0.2281,
      "step": 30760
    },
    {
      "epoch": 8.791428571428572,
      "grad_norm": 0.5540140867233276,
      "learning_rate": 8.278095238095238e-06,
      "loss": 0.5634,
      "step": 30770
    },
    {
      "epoch": 8.794285714285714,
      "grad_norm": 22.429365158081055,
      "learning_rate": 8.274285714285715e-06,
      "loss": 1.2208,
      "step": 30780
    },
    {
      "epoch": 8.797142857142857,
      "grad_norm": 10.85138988494873,
      "learning_rate": 8.270476190476191e-06,
      "loss": 0.8815,
      "step": 30790
    },
    {
      "epoch": 8.8,
      "grad_norm": 10.90804386138916,
      "learning_rate": 8.266666666666667e-06,
      "loss": 0.8539,
      "step": 30800
    },
    {
      "epoch": 8.802857142857142,
      "grad_norm": 10.619771957397461,
      "learning_rate": 8.262857142857144e-06,
      "loss": 0.9312,
      "step": 30810
    },
    {
      "epoch": 8.805714285714286,
      "grad_norm": 10.440189361572266,
      "learning_rate": 8.25904761904762e-06,
      "loss": 0.9956,
      "step": 30820
    },
    {
      "epoch": 8.808571428571428,
      "grad_norm": 1.1462209224700928,
      "learning_rate": 8.255238095238097e-06,
      "loss": 0.493,
      "step": 30830
    },
    {
      "epoch": 8.811428571428571,
      "grad_norm": 1.0012879371643066,
      "learning_rate": 8.251428571428572e-06,
      "loss": 0.6972,
      "step": 30840
    },
    {
      "epoch": 8.814285714285715,
      "grad_norm": 1.0448137521743774,
      "learning_rate": 8.24761904761905e-06,
      "loss": 0.7892,
      "step": 30850
    },
    {
      "epoch": 8.817142857142857,
      "grad_norm": 10.400901794433594,
      "learning_rate": 8.243809523809525e-06,
      "loss": 0.5877,
      "step": 30860
    },
    {
      "epoch": 8.82,
      "grad_norm": 21.58885383605957,
      "learning_rate": 8.24e-06,
      "loss": 0.8671,
      "step": 30870
    },
    {
      "epoch": 8.822857142857142,
      "grad_norm": 1.141076683998108,
      "learning_rate": 8.236190476190476e-06,
      "loss": 0.3945,
      "step": 30880
    },
    {
      "epoch": 8.825714285714286,
      "grad_norm": 10.212576866149902,
      "learning_rate": 8.232380952380954e-06,
      "loss": 0.4878,
      "step": 30890
    },
    {
      "epoch": 8.82857142857143,
      "grad_norm": 1.0181820392608643,
      "learning_rate": 8.22857142857143e-06,
      "loss": 0.5859,
      "step": 30900
    },
    {
      "epoch": 8.831428571428571,
      "grad_norm": 10.816805839538574,
      "learning_rate": 8.224761904761905e-06,
      "loss": 0.1494,
      "step": 30910
    },
    {
      "epoch": 8.834285714285715,
      "grad_norm": 0.8834896683692932,
      "learning_rate": 8.220952380952382e-06,
      "loss": 0.902,
      "step": 30920
    },
    {
      "epoch": 8.837142857142858,
      "grad_norm": 0.9169544577598572,
      "learning_rate": 8.217142857142858e-06,
      "loss": 0.3162,
      "step": 30930
    },
    {
      "epoch": 8.84,
      "grad_norm": 0.7459352016448975,
      "learning_rate": 8.213333333333335e-06,
      "loss": 0.5376,
      "step": 30940
    },
    {
      "epoch": 8.842857142857143,
      "grad_norm": 11.477849960327148,
      "learning_rate": 8.20952380952381e-06,
      "loss": 0.7364,
      "step": 30950
    },
    {
      "epoch": 8.845714285714285,
      "grad_norm": 11.696667671203613,
      "learning_rate": 8.205714285714286e-06,
      "loss": 0.6353,
      "step": 30960
    },
    {
      "epoch": 8.848571428571429,
      "grad_norm": 10.931235313415527,
      "learning_rate": 8.201904761904762e-06,
      "loss": 0.5181,
      "step": 30970
    },
    {
      "epoch": 8.85142857142857,
      "grad_norm": 0.849296510219574,
      "learning_rate": 8.198095238095239e-06,
      "loss": 0.417,
      "step": 30980
    },
    {
      "epoch": 8.854285714285714,
      "grad_norm": 22.47024154663086,
      "learning_rate": 8.194285714285714e-06,
      "loss": 0.6145,
      "step": 30990
    },
    {
      "epoch": 8.857142857142858,
      "grad_norm": 0.9257625937461853,
      "learning_rate": 8.190476190476192e-06,
      "loss": 0.6258,
      "step": 31000
    },
    {
      "epoch": 8.86,
      "grad_norm": 0.9575245380401611,
      "learning_rate": 8.186666666666667e-06,
      "loss": 0.6113,
      "step": 31010
    },
    {
      "epoch": 8.862857142857143,
      "grad_norm": 11.391207695007324,
      "learning_rate": 8.182857142857143e-06,
      "loss": 0.2226,
      "step": 31020
    },
    {
      "epoch": 8.865714285714287,
      "grad_norm": 10.841400146484375,
      "learning_rate": 8.17904761904762e-06,
      "loss": 0.9456,
      "step": 31030
    },
    {
      "epoch": 8.868571428571428,
      "grad_norm": 0.7506211996078491,
      "learning_rate": 8.175238095238096e-06,
      "loss": 0.4272,
      "step": 31040
    },
    {
      "epoch": 8.871428571428572,
      "grad_norm": 11.147732734680176,
      "learning_rate": 8.171428571428573e-06,
      "loss": 0.7504,
      "step": 31050
    },
    {
      "epoch": 8.874285714285714,
      "grad_norm": 0.6728619933128357,
      "learning_rate": 8.167619047619049e-06,
      "loss": 0.6437,
      "step": 31060
    },
    {
      "epoch": 8.877142857142857,
      "grad_norm": 0.7129612565040588,
      "learning_rate": 8.163809523809524e-06,
      "loss": 0.6503,
      "step": 31070
    },
    {
      "epoch": 8.88,
      "grad_norm": 0.6780422329902649,
      "learning_rate": 8.16e-06,
      "loss": 0.4323,
      "step": 31080
    },
    {
      "epoch": 8.882857142857143,
      "grad_norm": 0.6207046508789062,
      "learning_rate": 8.156190476190477e-06,
      "loss": 0.1203,
      "step": 31090
    },
    {
      "epoch": 8.885714285714286,
      "grad_norm": 10.866724014282227,
      "learning_rate": 8.152380952380953e-06,
      "loss": 0.7798,
      "step": 31100
    },
    {
      "epoch": 8.888571428571428,
      "grad_norm": 10.98454761505127,
      "learning_rate": 8.148571428571428e-06,
      "loss": 0.4448,
      "step": 31110
    },
    {
      "epoch": 8.891428571428571,
      "grad_norm": 0.5805070400238037,
      "learning_rate": 8.144761904761906e-06,
      "loss": 0.5049,
      "step": 31120
    },
    {
      "epoch": 8.894285714285715,
      "grad_norm": 22.031373977661133,
      "learning_rate": 8.140952380952381e-06,
      "loss": 0.6648,
      "step": 31130
    },
    {
      "epoch": 8.897142857142857,
      "grad_norm": 0.6312711238861084,
      "learning_rate": 8.137142857142858e-06,
      "loss": 0.3327,
      "step": 31140
    },
    {
      "epoch": 8.9,
      "grad_norm": 0.6120714545249939,
      "learning_rate": 8.133333333333334e-06,
      "loss": 0.6547,
      "step": 31150
    },
    {
      "epoch": 8.902857142857142,
      "grad_norm": 0.6257786154747009,
      "learning_rate": 8.129523809523811e-06,
      "loss": 0.437,
      "step": 31160
    },
    {
      "epoch": 8.905714285714286,
      "grad_norm": 11.127211570739746,
      "learning_rate": 8.125714285714287e-06,
      "loss": 0.866,
      "step": 31170
    },
    {
      "epoch": 8.90857142857143,
      "grad_norm": 0.7632820010185242,
      "learning_rate": 8.121904761904762e-06,
      "loss": 0.8562,
      "step": 31180
    },
    {
      "epoch": 8.911428571428571,
      "grad_norm": 10.813750267028809,
      "learning_rate": 8.118095238095238e-06,
      "loss": 0.5873,
      "step": 31190
    },
    {
      "epoch": 8.914285714285715,
      "grad_norm": 22.36198616027832,
      "learning_rate": 8.114285714285715e-06,
      "loss": 0.8308,
      "step": 31200
    },
    {
      "epoch": 8.917142857142856,
      "grad_norm": 11.291003227233887,
      "learning_rate": 8.11047619047619e-06,
      "loss": 0.7644,
      "step": 31210
    },
    {
      "epoch": 8.92,
      "grad_norm": 0.8312732577323914,
      "learning_rate": 8.106666666666666e-06,
      "loss": 0.0193,
      "step": 31220
    },
    {
      "epoch": 8.922857142857143,
      "grad_norm": 10.736679077148438,
      "learning_rate": 8.102857142857144e-06,
      "loss": 0.5203,
      "step": 31230
    },
    {
      "epoch": 8.925714285714285,
      "grad_norm": 0.7436619400978088,
      "learning_rate": 8.09904761904762e-06,
      "loss": 0.6283,
      "step": 31240
    },
    {
      "epoch": 8.928571428571429,
      "grad_norm": 0.7383474111557007,
      "learning_rate": 8.095238095238097e-06,
      "loss": 0.3154,
      "step": 31250
    },
    {
      "epoch": 8.93142857142857,
      "grad_norm": 11.011147499084473,
      "learning_rate": 8.091428571428572e-06,
      "loss": 0.836,
      "step": 31260
    },
    {
      "epoch": 8.934285714285714,
      "grad_norm": 22.09630012512207,
      "learning_rate": 8.08761904761905e-06,
      "loss": 0.9408,
      "step": 31270
    },
    {
      "epoch": 8.937142857142858,
      "grad_norm": 0.8535085320472717,
      "learning_rate": 8.083809523809525e-06,
      "loss": 0.6146,
      "step": 31280
    },
    {
      "epoch": 8.94,
      "grad_norm": 11.052160263061523,
      "learning_rate": 8.08e-06,
      "loss": 0.7111,
      "step": 31290
    },
    {
      "epoch": 8.942857142857143,
      "grad_norm": 0.8430439233779907,
      "learning_rate": 8.076190476190476e-06,
      "loss": 0.4053,
      "step": 31300
    },
    {
      "epoch": 8.945714285714285,
      "grad_norm": 0.8639585375785828,
      "learning_rate": 8.072380952380953e-06,
      "loss": 0.609,
      "step": 31310
    },
    {
      "epoch": 8.948571428571428,
      "grad_norm": 0.8935556411743164,
      "learning_rate": 8.068571428571429e-06,
      "loss": 0.4081,
      "step": 31320
    },
    {
      "epoch": 8.951428571428572,
      "grad_norm": 0.8573184609413147,
      "learning_rate": 8.064761904761905e-06,
      "loss": 0.3139,
      "step": 31330
    },
    {
      "epoch": 8.954285714285714,
      "grad_norm": 0.8333479166030884,
      "learning_rate": 8.060952380952382e-06,
      "loss": 0.8235,
      "step": 31340
    },
    {
      "epoch": 8.957142857142857,
      "grad_norm": 10.73119831085205,
      "learning_rate": 8.057142857142857e-06,
      "loss": 0.2198,
      "step": 31350
    },
    {
      "epoch": 8.96,
      "grad_norm": 0.6076321005821228,
      "learning_rate": 8.053333333333335e-06,
      "loss": 0.3315,
      "step": 31360
    },
    {
      "epoch": 8.962857142857143,
      "grad_norm": 10.801459312438965,
      "learning_rate": 8.04952380952381e-06,
      "loss": 0.548,
      "step": 31370
    },
    {
      "epoch": 8.965714285714286,
      "grad_norm": 0.6127169728279114,
      "learning_rate": 8.045714285714286e-06,
      "loss": 0.9868,
      "step": 31380
    },
    {
      "epoch": 8.968571428571428,
      "grad_norm": 10.882803916931152,
      "learning_rate": 8.041904761904763e-06,
      "loss": 0.6501,
      "step": 31390
    },
    {
      "epoch": 8.971428571428572,
      "grad_norm": 10.740518569946289,
      "learning_rate": 8.038095238095239e-06,
      "loss": 0.3338,
      "step": 31400
    },
    {
      "epoch": 8.974285714285715,
      "grad_norm": 22.58867835998535,
      "learning_rate": 8.034285714285714e-06,
      "loss": 0.656,
      "step": 31410
    },
    {
      "epoch": 8.977142857142857,
      "grad_norm": 10.805560111999512,
      "learning_rate": 8.030476190476192e-06,
      "loss": 0.8656,
      "step": 31420
    },
    {
      "epoch": 8.98,
      "grad_norm": 0.6698180437088013,
      "learning_rate": 8.026666666666667e-06,
      "loss": 0.6458,
      "step": 31430
    },
    {
      "epoch": 8.982857142857142,
      "grad_norm": 0.6702859997749329,
      "learning_rate": 8.022857142857143e-06,
      "loss": 0.2253,
      "step": 31440
    },
    {
      "epoch": 8.985714285714286,
      "grad_norm": 0.7094405889511108,
      "learning_rate": 8.01904761904762e-06,
      "loss": 0.6449,
      "step": 31450
    },
    {
      "epoch": 8.98857142857143,
      "grad_norm": 0.6919853687286377,
      "learning_rate": 8.015238095238096e-06,
      "loss": 0.6447,
      "step": 31460
    },
    {
      "epoch": 8.991428571428571,
      "grad_norm": 0.6772711277008057,
      "learning_rate": 8.011428571428573e-06,
      "loss": 0.6418,
      "step": 31470
    },
    {
      "epoch": 8.994285714285715,
      "grad_norm": 1.9254423379898071,
      "learning_rate": 8.007619047619048e-06,
      "loss": 0.5385,
      "step": 31480
    },
    {
      "epoch": 8.997142857142856,
      "grad_norm": 21.958599090576172,
      "learning_rate": 8.003809523809524e-06,
      "loss": 0.4308,
      "step": 31490
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.6673271656036377,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.4741,
      "step": 31500
    },
    {
      "epoch": 9.0,
      "eval_accuracy": 0.8716397849462365,
      "eval_f1": 0.010362694300518135,
      "eval_loss": 0.5435313582420349,
      "eval_precision": 1.0,
      "eval_recall": 0.005208333333333333,
      "eval_runtime": 46.3536,
      "eval_samples_per_second": 64.72,
      "eval_steps_per_second": 2.028,
      "step": 31500
    },
    {
      "epoch": 9.002857142857144,
      "grad_norm": 22.56405258178711,
      "learning_rate": 7.996190476190477e-06,
      "loss": 0.5493,
      "step": 31510
    },
    {
      "epoch": 9.005714285714285,
      "grad_norm": 0.5865819454193115,
      "learning_rate": 7.992380952380952e-06,
      "loss": 0.8958,
      "step": 31520
    },
    {
      "epoch": 9.008571428571429,
      "grad_norm": 10.697793960571289,
      "learning_rate": 7.988571428571428e-06,
      "loss": 0.7648,
      "step": 31530
    },
    {
      "epoch": 9.01142857142857,
      "grad_norm": 0.658691942691803,
      "learning_rate": 7.984761904761905e-06,
      "loss": 0.4386,
      "step": 31540
    },
    {
      "epoch": 9.014285714285714,
      "grad_norm": 21.979873657226562,
      "learning_rate": 7.980952380952381e-06,
      "loss": 0.7457,
      "step": 31550
    },
    {
      "epoch": 9.017142857142858,
      "grad_norm": 11.186820030212402,
      "learning_rate": 7.977142857142858e-06,
      "loss": 0.8354,
      "step": 31560
    },
    {
      "epoch": 9.02,
      "grad_norm": 10.651748657226562,
      "learning_rate": 7.973333333333334e-06,
      "loss": 0.3201,
      "step": 31570
    },
    {
      "epoch": 9.022857142857143,
      "grad_norm": 0.7029895186424255,
      "learning_rate": 7.969523809523811e-06,
      "loss": 0.6359,
      "step": 31580
    },
    {
      "epoch": 9.025714285714285,
      "grad_norm": 21.820987701416016,
      "learning_rate": 7.965714285714287e-06,
      "loss": 1.0818,
      "step": 31590
    },
    {
      "epoch": 9.028571428571428,
      "grad_norm": 0.8242877125740051,
      "learning_rate": 7.961904761904762e-06,
      "loss": 0.4687,
      "step": 31600
    },
    {
      "epoch": 9.031428571428572,
      "grad_norm": 0.7607361674308777,
      "learning_rate": 7.95809523809524e-06,
      "loss": 0.4158,
      "step": 31610
    },
    {
      "epoch": 9.034285714285714,
      "grad_norm": 0.6675125956535339,
      "learning_rate": 7.954285714285715e-06,
      "loss": 0.2213,
      "step": 31620
    },
    {
      "epoch": 9.037142857142857,
      "grad_norm": 11.867975234985352,
      "learning_rate": 7.95047619047619e-06,
      "loss": 0.229,
      "step": 31630
    },
    {
      "epoch": 9.04,
      "grad_norm": 0.578530490398407,
      "learning_rate": 7.946666666666666e-06,
      "loss": 0.7758,
      "step": 31640
    },
    {
      "epoch": 9.042857142857143,
      "grad_norm": 0.6191504001617432,
      "learning_rate": 7.942857142857144e-06,
      "loss": 0.6913,
      "step": 31650
    },
    {
      "epoch": 9.045714285714286,
      "grad_norm": 0.5703847408294678,
      "learning_rate": 7.939047619047619e-06,
      "loss": 0.1755,
      "step": 31660
    },
    {
      "epoch": 9.048571428571428,
      "grad_norm": 11.10621452331543,
      "learning_rate": 7.935238095238096e-06,
      "loss": 0.5559,
      "step": 31670
    },
    {
      "epoch": 9.051428571428572,
      "grad_norm": 0.6133795976638794,
      "learning_rate": 7.931428571428572e-06,
      "loss": 0.5874,
      "step": 31680
    },
    {
      "epoch": 9.054285714285715,
      "grad_norm": 10.70000171661377,
      "learning_rate": 7.92761904761905e-06,
      "loss": 0.547,
      "step": 31690
    },
    {
      "epoch": 9.057142857142857,
      "grad_norm": 0.6466131806373596,
      "learning_rate": 7.923809523809525e-06,
      "loss": 0.5471,
      "step": 31700
    },
    {
      "epoch": 9.06,
      "grad_norm": 22.059917449951172,
      "learning_rate": 7.92e-06,
      "loss": 0.6632,
      "step": 31710
    },
    {
      "epoch": 9.062857142857142,
      "grad_norm": 10.890615463256836,
      "learning_rate": 7.916190476190478e-06,
      "loss": 0.7483,
      "step": 31720
    },
    {
      "epoch": 9.065714285714286,
      "grad_norm": 11.250321388244629,
      "learning_rate": 7.912380952380953e-06,
      "loss": 0.5308,
      "step": 31730
    },
    {
      "epoch": 9.06857142857143,
      "grad_norm": 10.728778839111328,
      "learning_rate": 7.908571428571429e-06,
      "loss": 0.3237,
      "step": 31740
    },
    {
      "epoch": 9.071428571428571,
      "grad_norm": 10.681297302246094,
      "learning_rate": 7.904761904761904e-06,
      "loss": 0.4248,
      "step": 31750
    },
    {
      "epoch": 9.074285714285715,
      "grad_norm": 0.6810404658317566,
      "learning_rate": 7.900952380952382e-06,
      "loss": 0.322,
      "step": 31760
    },
    {
      "epoch": 9.077142857142857,
      "grad_norm": 10.992050170898438,
      "learning_rate": 7.897142857142857e-06,
      "loss": 0.2247,
      "step": 31770
    },
    {
      "epoch": 9.08,
      "grad_norm": 0.6206357479095459,
      "learning_rate": 7.893333333333335e-06,
      "loss": 0.6736,
      "step": 31780
    },
    {
      "epoch": 9.082857142857144,
      "grad_norm": 0.5854122638702393,
      "learning_rate": 7.88952380952381e-06,
      "loss": 0.6178,
      "step": 31790
    },
    {
      "epoch": 9.085714285714285,
      "grad_norm": 0.6086991429328918,
      "learning_rate": 7.885714285714286e-06,
      "loss": 0.6991,
      "step": 31800
    },
    {
      "epoch": 9.088571428571429,
      "grad_norm": 0.6414876580238342,
      "learning_rate": 7.881904761904763e-06,
      "loss": 0.5499,
      "step": 31810
    },
    {
      "epoch": 9.09142857142857,
      "grad_norm": 11.137236595153809,
      "learning_rate": 7.878095238095239e-06,
      "loss": 0.4448,
      "step": 31820
    },
    {
      "epoch": 9.094285714285714,
      "grad_norm": 0.7445172667503357,
      "learning_rate": 7.874285714285716e-06,
      "loss": 0.6428,
      "step": 31830
    },
    {
      "epoch": 9.097142857142858,
      "grad_norm": 10.716113090515137,
      "learning_rate": 7.870476190476191e-06,
      "loss": 0.845,
      "step": 31840
    },
    {
      "epoch": 9.1,
      "grad_norm": 1.0180164575576782,
      "learning_rate": 7.866666666666667e-06,
      "loss": 0.4049,
      "step": 31850
    },
    {
      "epoch": 9.102857142857143,
      "grad_norm": 11.07705307006836,
      "learning_rate": 7.862857142857143e-06,
      "loss": 0.4126,
      "step": 31860
    },
    {
      "epoch": 9.105714285714285,
      "grad_norm": 22.22710418701172,
      "learning_rate": 7.85904761904762e-06,
      "loss": 0.7056,
      "step": 31870
    },
    {
      "epoch": 9.108571428571429,
      "grad_norm": 0.9472475647926331,
      "learning_rate": 7.855238095238095e-06,
      "loss": 0.7071,
      "step": 31880
    },
    {
      "epoch": 9.111428571428572,
      "grad_norm": 11.059910774230957,
      "learning_rate": 7.851428571428573e-06,
      "loss": 0.6936,
      "step": 31890
    },
    {
      "epoch": 9.114285714285714,
      "grad_norm": 1.0468628406524658,
      "learning_rate": 7.847619047619048e-06,
      "loss": 0.5083,
      "step": 31900
    },
    {
      "epoch": 9.117142857142857,
      "grad_norm": 0.8355057835578918,
      "learning_rate": 7.843809523809524e-06,
      "loss": 0.3203,
      "step": 31910
    },
    {
      "epoch": 9.12,
      "grad_norm": 1.0354037284851074,
      "learning_rate": 7.840000000000001e-06,
      "loss": 0.3486,
      "step": 31920
    },
    {
      "epoch": 9.122857142857143,
      "grad_norm": 0.6758739352226257,
      "learning_rate": 7.836190476190477e-06,
      "loss": 0.3269,
      "step": 31930
    },
    {
      "epoch": 9.125714285714286,
      "grad_norm": 22.36406135559082,
      "learning_rate": 7.832380952380954e-06,
      "loss": 0.6889,
      "step": 31940
    },
    {
      "epoch": 9.128571428571428,
      "grad_norm": 0.5883726477622986,
      "learning_rate": 7.828571428571428e-06,
      "loss": 0.1211,
      "step": 31950
    },
    {
      "epoch": 9.131428571428572,
      "grad_norm": 0.5973055958747864,
      "learning_rate": 7.824761904761905e-06,
      "loss": 0.4513,
      "step": 31960
    },
    {
      "epoch": 9.134285714285713,
      "grad_norm": 11.146756172180176,
      "learning_rate": 7.82095238095238e-06,
      "loss": 0.1365,
      "step": 31970
    },
    {
      "epoch": 9.137142857142857,
      "grad_norm": 0.5212480425834656,
      "learning_rate": 7.817142857142858e-06,
      "loss": 0.5757,
      "step": 31980
    },
    {
      "epoch": 9.14,
      "grad_norm": 0.5166849493980408,
      "learning_rate": 7.813333333333334e-06,
      "loss": 0.3516,
      "step": 31990
    },
    {
      "epoch": 9.142857142857142,
      "grad_norm": 0.5347657799720764,
      "learning_rate": 7.809523809523811e-06,
      "loss": 0.5698,
      "step": 32000
    },
    {
      "epoch": 9.145714285714286,
      "grad_norm": 11.015217781066895,
      "learning_rate": 7.805714285714286e-06,
      "loss": 0.5431,
      "step": 32010
    },
    {
      "epoch": 9.14857142857143,
      "grad_norm": 0.6232923865318298,
      "learning_rate": 7.801904761904762e-06,
      "loss": 0.777,
      "step": 32020
    },
    {
      "epoch": 9.151428571428571,
      "grad_norm": 0.6015565395355225,
      "learning_rate": 7.79809523809524e-06,
      "loss": 0.4311,
      "step": 32030
    },
    {
      "epoch": 9.154285714285715,
      "grad_norm": 1.913623332977295,
      "learning_rate": 7.794285714285715e-06,
      "loss": 0.1737,
      "step": 32040
    },
    {
      "epoch": 9.157142857142857,
      "grad_norm": 10.861671447753906,
      "learning_rate": 7.790476190476192e-06,
      "loss": 0.6942,
      "step": 32050
    },
    {
      "epoch": 9.16,
      "grad_norm": 1.0197829008102417,
      "learning_rate": 7.786666666666666e-06,
      "loss": 0.4991,
      "step": 32060
    },
    {
      "epoch": 9.162857142857144,
      "grad_norm": 13.719260215759277,
      "learning_rate": 7.782857142857143e-06,
      "loss": 0.2834,
      "step": 32070
    },
    {
      "epoch": 9.165714285714285,
      "grad_norm": 10.797807693481445,
      "learning_rate": 7.779047619047619e-06,
      "loss": 0.8951,
      "step": 32080
    },
    {
      "epoch": 9.168571428571429,
      "grad_norm": 0.509127140045166,
      "learning_rate": 7.775238095238096e-06,
      "loss": 0.3568,
      "step": 32090
    },
    {
      "epoch": 9.17142857142857,
      "grad_norm": 10.831008911132812,
      "learning_rate": 7.771428571428572e-06,
      "loss": 0.4003,
      "step": 32100
    },
    {
      "epoch": 9.174285714285714,
      "grad_norm": 11.251815795898438,
      "learning_rate": 7.767619047619049e-06,
      "loss": 0.8702,
      "step": 32110
    },
    {
      "epoch": 9.177142857142858,
      "grad_norm": 10.800811767578125,
      "learning_rate": 7.763809523809525e-06,
      "loss": 0.3487,
      "step": 32120
    },
    {
      "epoch": 9.18,
      "grad_norm": 10.840115547180176,
      "learning_rate": 7.76e-06,
      "loss": 0.4745,
      "step": 32130
    },
    {
      "epoch": 9.182857142857143,
      "grad_norm": 22.265987396240234,
      "learning_rate": 7.756190476190478e-06,
      "loss": 0.9723,
      "step": 32140
    },
    {
      "epoch": 9.185714285714285,
      "grad_norm": 0.7516437768936157,
      "learning_rate": 7.752380952380953e-06,
      "loss": 0.6292,
      "step": 32150
    },
    {
      "epoch": 9.188571428571429,
      "grad_norm": 0.8082651495933533,
      "learning_rate": 7.74857142857143e-06,
      "loss": 0.378,
      "step": 32160
    },
    {
      "epoch": 9.191428571428572,
      "grad_norm": 21.93168830871582,
      "learning_rate": 7.744761904761904e-06,
      "loss": 0.5317,
      "step": 32170
    },
    {
      "epoch": 9.194285714285714,
      "grad_norm": 0.7675169110298157,
      "learning_rate": 7.740952380952382e-06,
      "loss": 0.4409,
      "step": 32180
    },
    {
      "epoch": 9.197142857142858,
      "grad_norm": 0.7329204082489014,
      "learning_rate": 7.737142857142857e-06,
      "loss": 0.4271,
      "step": 32190
    },
    {
      "epoch": 9.2,
      "grad_norm": 0.6827196478843689,
      "learning_rate": 7.733333333333334e-06,
      "loss": 0.3225,
      "step": 32200
    },
    {
      "epoch": 9.202857142857143,
      "grad_norm": 0.6808779835700989,
      "learning_rate": 7.72952380952381e-06,
      "loss": 0.7399,
      "step": 32210
    },
    {
      "epoch": 9.205714285714286,
      "grad_norm": 0.7497907876968384,
      "learning_rate": 7.725714285714286e-06,
      "loss": 0.6287,
      "step": 32220
    },
    {
      "epoch": 9.208571428571428,
      "grad_norm": 0.8262796998023987,
      "learning_rate": 7.721904761904763e-06,
      "loss": 0.6255,
      "step": 32230
    },
    {
      "epoch": 9.211428571428572,
      "grad_norm": 21.966957092285156,
      "learning_rate": 7.718095238095238e-06,
      "loss": 0.5211,
      "step": 32240
    },
    {
      "epoch": 9.214285714285714,
      "grad_norm": 0.7935459613800049,
      "learning_rate": 7.714285714285716e-06,
      "loss": 0.5155,
      "step": 32250
    },
    {
      "epoch": 9.217142857142857,
      "grad_norm": 0.7941822409629822,
      "learning_rate": 7.710476190476191e-06,
      "loss": 0.611,
      "step": 32260
    },
    {
      "epoch": 9.22,
      "grad_norm": 0.7665414810180664,
      "learning_rate": 7.706666666666669e-06,
      "loss": 0.3507,
      "step": 32270
    },
    {
      "epoch": 9.222857142857142,
      "grad_norm": 0.6369873881340027,
      "learning_rate": 7.702857142857142e-06,
      "loss": 0.4632,
      "step": 32280
    },
    {
      "epoch": 9.225714285714286,
      "grad_norm": 6.785785675048828,
      "learning_rate": 7.69904761904762e-06,
      "loss": 0.3477,
      "step": 32290
    },
    {
      "epoch": 9.228571428571428,
      "grad_norm": 11.041769027709961,
      "learning_rate": 7.695238095238095e-06,
      "loss": 0.2321,
      "step": 32300
    },
    {
      "epoch": 9.231428571428571,
      "grad_norm": 11.67003059387207,
      "learning_rate": 7.691428571428573e-06,
      "loss": 0.4512,
      "step": 32310
    },
    {
      "epoch": 9.234285714285715,
      "grad_norm": 0.5017713904380798,
      "learning_rate": 7.687619047619048e-06,
      "loss": 0.3961,
      "step": 32320
    },
    {
      "epoch": 9.237142857142857,
      "grad_norm": 0.5669755935668945,
      "learning_rate": 7.683809523809524e-06,
      "loss": 0.9056,
      "step": 32330
    },
    {
      "epoch": 9.24,
      "grad_norm": 0.5423455834388733,
      "learning_rate": 7.680000000000001e-06,
      "loss": 0.2344,
      "step": 32340
    },
    {
      "epoch": 9.242857142857142,
      "grad_norm": 22.307126998901367,
      "learning_rate": 7.676190476190477e-06,
      "loss": 0.7897,
      "step": 32350
    },
    {
      "epoch": 9.245714285714286,
      "grad_norm": 0.5578608512878418,
      "learning_rate": 7.672380952380954e-06,
      "loss": 0.4519,
      "step": 32360
    },
    {
      "epoch": 9.248571428571429,
      "grad_norm": 11.08634090423584,
      "learning_rate": 7.66857142857143e-06,
      "loss": 0.7804,
      "step": 32370
    },
    {
      "epoch": 9.251428571428571,
      "grad_norm": 22.673431396484375,
      "learning_rate": 7.664761904761905e-06,
      "loss": 1.072,
      "step": 32380
    },
    {
      "epoch": 9.254285714285714,
      "grad_norm": 0.7292876243591309,
      "learning_rate": 7.66095238095238e-06,
      "loss": 0.7385,
      "step": 32390
    },
    {
      "epoch": 9.257142857142856,
      "grad_norm": 10.621859550476074,
      "learning_rate": 7.657142857142858e-06,
      "loss": 0.6225,
      "step": 32400
    },
    {
      "epoch": 9.26,
      "grad_norm": 11.334683418273926,
      "learning_rate": 7.653333333333333e-06,
      "loss": 0.4177,
      "step": 32410
    },
    {
      "epoch": 9.262857142857143,
      "grad_norm": 11.700187683105469,
      "learning_rate": 7.64952380952381e-06,
      "loss": 0.7263,
      "step": 32420
    },
    {
      "epoch": 9.265714285714285,
      "grad_norm": 0.8620011806488037,
      "learning_rate": 7.645714285714286e-06,
      "loss": 0.3107,
      "step": 32430
    },
    {
      "epoch": 9.268571428571429,
      "grad_norm": 10.732436180114746,
      "learning_rate": 7.641904761904762e-06,
      "loss": 0.5151,
      "step": 32440
    },
    {
      "epoch": 9.271428571428572,
      "grad_norm": 11.381473541259766,
      "learning_rate": 7.63809523809524e-06,
      "loss": 0.3151,
      "step": 32450
    },
    {
      "epoch": 9.274285714285714,
      "grad_norm": 0.7579271793365479,
      "learning_rate": 7.634285714285715e-06,
      "loss": 0.5244,
      "step": 32460
    },
    {
      "epoch": 9.277142857142858,
      "grad_norm": 0.7087310552597046,
      "learning_rate": 7.630476190476192e-06,
      "loss": 0.3294,
      "step": 32470
    },
    {
      "epoch": 9.28,
      "grad_norm": 10.719786643981934,
      "learning_rate": 7.626666666666668e-06,
      "loss": 0.4448,
      "step": 32480
    },
    {
      "epoch": 9.282857142857143,
      "grad_norm": 10.865911483764648,
      "learning_rate": 7.622857142857143e-06,
      "loss": 0.7606,
      "step": 32490
    },
    {
      "epoch": 9.285714285714286,
      "grad_norm": 10.840607643127441,
      "learning_rate": 7.61904761904762e-06,
      "loss": 0.3339,
      "step": 32500
    },
    {
      "epoch": 9.288571428571428,
      "grad_norm": 0.6620466709136963,
      "learning_rate": 7.615238095238095e-06,
      "loss": 0.4388,
      "step": 32510
    },
    {
      "epoch": 9.291428571428572,
      "grad_norm": 0.6497985124588013,
      "learning_rate": 7.611428571428572e-06,
      "loss": 0.4332,
      "step": 32520
    },
    {
      "epoch": 9.294285714285714,
      "grad_norm": 0.651700496673584,
      "learning_rate": 7.607619047619048e-06,
      "loss": 0.6454,
      "step": 32530
    },
    {
      "epoch": 9.297142857142857,
      "grad_norm": 10.83918285369873,
      "learning_rate": 7.6038095238095245e-06,
      "loss": 0.6499,
      "step": 32540
    },
    {
      "epoch": 9.3,
      "grad_norm": 11.004376411437988,
      "learning_rate": 7.600000000000001e-06,
      "loss": 0.6474,
      "step": 32550
    },
    {
      "epoch": 9.302857142857142,
      "grad_norm": 0.692957878112793,
      "learning_rate": 7.596190476190477e-06,
      "loss": 0.6447,
      "step": 32560
    },
    {
      "epoch": 9.305714285714286,
      "grad_norm": 0.6015384793281555,
      "learning_rate": 7.592380952380953e-06,
      "loss": 0.3332,
      "step": 32570
    },
    {
      "epoch": 9.308571428571428,
      "grad_norm": 0.5602625608444214,
      "learning_rate": 7.588571428571429e-06,
      "loss": 0.5528,
      "step": 32580
    },
    {
      "epoch": 9.311428571428571,
      "grad_norm": 11.040600776672363,
      "learning_rate": 7.584761904761906e-06,
      "loss": 0.3372,
      "step": 32590
    },
    {
      "epoch": 9.314285714285715,
      "grad_norm": 11.728968620300293,
      "learning_rate": 7.580952380952381e-06,
      "loss": 0.4461,
      "step": 32600
    },
    {
      "epoch": 9.317142857142857,
      "grad_norm": 0.5455137491226196,
      "learning_rate": 7.577142857142857e-06,
      "loss": 0.5566,
      "step": 32610
    },
    {
      "epoch": 9.32,
      "grad_norm": 10.974142074584961,
      "learning_rate": 7.573333333333333e-06,
      "loss": 0.5547,
      "step": 32620
    },
    {
      "epoch": 9.322857142857142,
      "grad_norm": 0.5521641969680786,
      "learning_rate": 7.56952380952381e-06,
      "loss": 0.2344,
      "step": 32630
    },
    {
      "epoch": 9.325714285714286,
      "grad_norm": 0.5653383731842041,
      "learning_rate": 7.565714285714286e-06,
      "loss": 0.6833,
      "step": 32640
    },
    {
      "epoch": 9.32857142857143,
      "grad_norm": 11.461095809936523,
      "learning_rate": 7.561904761904763e-06,
      "loss": 0.2298,
      "step": 32650
    },
    {
      "epoch": 9.331428571428571,
      "grad_norm": 10.952249526977539,
      "learning_rate": 7.558095238095239e-06,
      "loss": 0.7017,
      "step": 32660
    },
    {
      "epoch": 9.334285714285715,
      "grad_norm": 0.5124641060829163,
      "learning_rate": 7.5542857142857155e-06,
      "loss": 1.0197,
      "step": 32670
    },
    {
      "epoch": 9.337142857142856,
      "grad_norm": 10.90466022491455,
      "learning_rate": 7.550476190476191e-06,
      "loss": 0.6638,
      "step": 32680
    },
    {
      "epoch": 9.34,
      "grad_norm": 0.5346818566322327,
      "learning_rate": 7.5466666666666675e-06,
      "loss": 0.4443,
      "step": 32690
    },
    {
      "epoch": 9.342857142857143,
      "grad_norm": 11.131850242614746,
      "learning_rate": 7.542857142857144e-06,
      "loss": 0.3458,
      "step": 32700
    },
    {
      "epoch": 9.345714285714285,
      "grad_norm": 11.411260604858398,
      "learning_rate": 7.5390476190476195e-06,
      "loss": 0.683,
      "step": 32710
    },
    {
      "epoch": 9.348571428571429,
      "grad_norm": 15.011341094970703,
      "learning_rate": 7.535238095238095e-06,
      "loss": 0.8718,
      "step": 32720
    },
    {
      "epoch": 9.35142857142857,
      "grad_norm": 0.691810131072998,
      "learning_rate": 7.5314285714285716e-06,
      "loss": 0.7541,
      "step": 32730
    },
    {
      "epoch": 9.354285714285714,
      "grad_norm": 0.6202644109725952,
      "learning_rate": 7.527619047619048e-06,
      "loss": 0.221,
      "step": 32740
    },
    {
      "epoch": 9.357142857142858,
      "grad_norm": 10.72844123840332,
      "learning_rate": 7.523809523809524e-06,
      "loss": 0.5449,
      "step": 32750
    },
    {
      "epoch": 9.36,
      "grad_norm": 10.920177459716797,
      "learning_rate": 7.520000000000001e-06,
      "loss": 0.2249,
      "step": 32760
    },
    {
      "epoch": 9.362857142857143,
      "grad_norm": 0.5700662136077881,
      "learning_rate": 7.516190476190477e-06,
      "loss": 0.8637,
      "step": 32770
    },
    {
      "epoch": 9.365714285714287,
      "grad_norm": 10.785386085510254,
      "learning_rate": 7.512380952380953e-06,
      "loss": 0.2293,
      "step": 32780
    },
    {
      "epoch": 9.368571428571428,
      "grad_norm": 0.5717787146568298,
      "learning_rate": 7.508571428571429e-06,
      "loss": 0.5608,
      "step": 32790
    },
    {
      "epoch": 9.371428571428572,
      "grad_norm": 0.5831973552703857,
      "learning_rate": 7.504761904761906e-06,
      "loss": 0.6686,
      "step": 32800
    },
    {
      "epoch": 9.374285714285714,
      "grad_norm": 0.5706679821014404,
      "learning_rate": 7.500952380952382e-06,
      "loss": 0.4447,
      "step": 32810
    },
    {
      "epoch": 9.377142857142857,
      "grad_norm": 10.917252540588379,
      "learning_rate": 7.497142857142857e-06,
      "loss": 1.2112,
      "step": 32820
    },
    {
      "epoch": 9.38,
      "grad_norm": 0.6699965000152588,
      "learning_rate": 7.493333333333333e-06,
      "loss": 0.5415,
      "step": 32830
    },
    {
      "epoch": 9.382857142857143,
      "grad_norm": 0.7026315331459045,
      "learning_rate": 7.48952380952381e-06,
      "loss": 0.4357,
      "step": 32840
    },
    {
      "epoch": 9.385714285714286,
      "grad_norm": 10.759878158569336,
      "learning_rate": 7.485714285714286e-06,
      "loss": 0.434,
      "step": 32850
    },
    {
      "epoch": 9.388571428571428,
      "grad_norm": 22.121397018432617,
      "learning_rate": 7.481904761904763e-06,
      "loss": 0.7485,
      "step": 32860
    },
    {
      "epoch": 9.391428571428571,
      "grad_norm": 0.6694117188453674,
      "learning_rate": 7.478095238095239e-06,
      "loss": 0.3292,
      "step": 32870
    },
    {
      "epoch": 9.394285714285715,
      "grad_norm": 0.6986767053604126,
      "learning_rate": 7.4742857142857154e-06,
      "loss": 0.5404,
      "step": 32880
    },
    {
      "epoch": 9.397142857142857,
      "grad_norm": 10.68172550201416,
      "learning_rate": 7.470476190476191e-06,
      "loss": 0.9587,
      "step": 32890
    },
    {
      "epoch": 9.4,
      "grad_norm": 0.7076550126075745,
      "learning_rate": 7.4666666666666675e-06,
      "loss": 0.4292,
      "step": 32900
    },
    {
      "epoch": 9.402857142857142,
      "grad_norm": 0.6958231329917908,
      "learning_rate": 7.462857142857144e-06,
      "loss": 0.1194,
      "step": 32910
    },
    {
      "epoch": 9.405714285714286,
      "grad_norm": 10.745105743408203,
      "learning_rate": 7.45904761904762e-06,
      "loss": 0.549,
      "step": 32920
    },
    {
      "epoch": 9.40857142857143,
      "grad_norm": 22.00411033630371,
      "learning_rate": 7.455238095238095e-06,
      "loss": 0.7561,
      "step": 32930
    },
    {
      "epoch": 9.411428571428571,
      "grad_norm": 11.03339958190918,
      "learning_rate": 7.4514285714285715e-06,
      "loss": 0.4374,
      "step": 32940
    },
    {
      "epoch": 9.414285714285715,
      "grad_norm": 10.602239608764648,
      "learning_rate": 7.447619047619048e-06,
      "loss": 0.8191,
      "step": 32950
    },
    {
      "epoch": 9.417142857142856,
      "grad_norm": 0.777872622013092,
      "learning_rate": 7.443809523809524e-06,
      "loss": 0.3233,
      "step": 32960
    },
    {
      "epoch": 9.42,
      "grad_norm": 10.734274864196777,
      "learning_rate": 7.440000000000001e-06,
      "loss": 0.7354,
      "step": 32970
    },
    {
      "epoch": 9.422857142857143,
      "grad_norm": 10.687056541442871,
      "learning_rate": 7.436190476190477e-06,
      "loss": 0.5324,
      "step": 32980
    },
    {
      "epoch": 9.425714285714285,
      "grad_norm": 0.8412285447120667,
      "learning_rate": 7.432380952380953e-06,
      "loss": 0.726,
      "step": 32990
    },
    {
      "epoch": 9.428571428571429,
      "grad_norm": 10.929898262023926,
      "learning_rate": 7.428571428571429e-06,
      "loss": 0.5205,
      "step": 33000
    },
    {
      "epoch": 9.43142857142857,
      "grad_norm": 10.733569145202637,
      "learning_rate": 7.424761904761906e-06,
      "loss": 0.3172,
      "step": 33010
    },
    {
      "epoch": 9.434285714285714,
      "grad_norm": 0.7905034422874451,
      "learning_rate": 7.420952380952382e-06,
      "loss": 0.6183,
      "step": 33020
    },
    {
      "epoch": 9.437142857142858,
      "grad_norm": 0.5715756416320801,
      "learning_rate": 7.417142857142857e-06,
      "loss": 0.2188,
      "step": 33030
    },
    {
      "epoch": 9.44,
      "grad_norm": 10.773417472839355,
      "learning_rate": 7.413333333333333e-06,
      "loss": 0.6354,
      "step": 33040
    },
    {
      "epoch": 9.442857142857143,
      "grad_norm": 21.809646606445312,
      "learning_rate": 7.40952380952381e-06,
      "loss": 0.5353,
      "step": 33050
    },
    {
      "epoch": 9.445714285714285,
      "grad_norm": 0.6771706938743591,
      "learning_rate": 7.405714285714286e-06,
      "loss": 0.8454,
      "step": 33060
    },
    {
      "epoch": 9.448571428571428,
      "grad_norm": 0.7412337064743042,
      "learning_rate": 7.4019047619047625e-06,
      "loss": 0.5346,
      "step": 33070
    },
    {
      "epoch": 9.451428571428572,
      "grad_norm": 0.6785945892333984,
      "learning_rate": 7.398095238095239e-06,
      "loss": 0.1186,
      "step": 33080
    },
    {
      "epoch": 9.454285714285714,
      "grad_norm": 0.636785626411438,
      "learning_rate": 7.394285714285715e-06,
      "loss": 0.5458,
      "step": 33090
    },
    {
      "epoch": 9.457142857142857,
      "grad_norm": 10.711992263793945,
      "learning_rate": 7.390476190476191e-06,
      "loss": 1.0682,
      "step": 33100
    },
    {
      "epoch": 9.46,
      "grad_norm": 0.7052420973777771,
      "learning_rate": 7.386666666666667e-06,
      "loss": 0.2229,
      "step": 33110
    },
    {
      "epoch": 9.462857142857143,
      "grad_norm": 10.754658699035645,
      "learning_rate": 7.382857142857144e-06,
      "loss": 0.9417,
      "step": 33120
    },
    {
      "epoch": 9.465714285714286,
      "grad_norm": 11.297992706298828,
      "learning_rate": 7.37904761904762e-06,
      "loss": 0.7459,
      "step": 33130
    },
    {
      "epoch": 9.468571428571428,
      "grad_norm": 10.852835655212402,
      "learning_rate": 7.375238095238095e-06,
      "loss": 0.7323,
      "step": 33140
    },
    {
      "epoch": 9.471428571428572,
      "grad_norm": 0.7856447100639343,
      "learning_rate": 7.371428571428571e-06,
      "loss": 0.2194,
      "step": 33150
    },
    {
      "epoch": 9.474285714285715,
      "grad_norm": 10.664849281311035,
      "learning_rate": 7.367619047619048e-06,
      "loss": 0.9285,
      "step": 33160
    },
    {
      "epoch": 9.477142857142857,
      "grad_norm": 10.637886047363281,
      "learning_rate": 7.363809523809524e-06,
      "loss": 0.6203,
      "step": 33170
    },
    {
      "epoch": 9.48,
      "grad_norm": 10.492781639099121,
      "learning_rate": 7.360000000000001e-06,
      "loss": 0.518,
      "step": 33180
    },
    {
      "epoch": 9.482857142857142,
      "grad_norm": 10.536287307739258,
      "learning_rate": 7.356190476190477e-06,
      "loss": 0.9024,
      "step": 33190
    },
    {
      "epoch": 9.485714285714286,
      "grad_norm": 0.8309023976325989,
      "learning_rate": 7.352380952380953e-06,
      "loss": 0.4004,
      "step": 33200
    },
    {
      "epoch": 9.48857142857143,
      "grad_norm": 10.317628860473633,
      "learning_rate": 7.348571428571429e-06,
      "loss": 0.3944,
      "step": 33210
    },
    {
      "epoch": 9.491428571428571,
      "grad_norm": 10.683326721191406,
      "learning_rate": 7.3447619047619056e-06,
      "loss": 0.3078,
      "step": 33220
    },
    {
      "epoch": 9.494285714285715,
      "grad_norm": 0.7663276791572571,
      "learning_rate": 7.340952380952382e-06,
      "loss": 0.1179,
      "step": 33230
    },
    {
      "epoch": 9.497142857142856,
      "grad_norm": 0.6105944514274597,
      "learning_rate": 7.337142857142858e-06,
      "loss": 0.5242,
      "step": 33240
    },
    {
      "epoch": 9.5,
      "grad_norm": 0.6200832724571228,
      "learning_rate": 7.333333333333333e-06,
      "loss": 0.7577,
      "step": 33250
    },
    {
      "epoch": 9.502857142857144,
      "grad_norm": 0.5836860537528992,
      "learning_rate": 7.3295238095238096e-06,
      "loss": 0.014,
      "step": 33260
    },
    {
      "epoch": 9.505714285714285,
      "grad_norm": 0.5274277925491333,
      "learning_rate": 7.325714285714286e-06,
      "loss": 0.7857,
      "step": 33270
    },
    {
      "epoch": 9.508571428571429,
      "grad_norm": 0.5632822513580322,
      "learning_rate": 7.3219047619047624e-06,
      "loss": 0.6744,
      "step": 33280
    },
    {
      "epoch": 9.51142857142857,
      "grad_norm": 0.667793869972229,
      "learning_rate": 7.318095238095239e-06,
      "loss": 0.6666,
      "step": 33290
    },
    {
      "epoch": 9.514285714285714,
      "grad_norm": 0.654328465461731,
      "learning_rate": 7.314285714285715e-06,
      "loss": 0.9764,
      "step": 33300
    },
    {
      "epoch": 9.517142857142858,
      "grad_norm": 0.6656069755554199,
      "learning_rate": 7.310476190476191e-06,
      "loss": 0.4414,
      "step": 33310
    },
    {
      "epoch": 9.52,
      "grad_norm": 11.269054412841797,
      "learning_rate": 7.306666666666667e-06,
      "loss": 0.3315,
      "step": 33320
    },
    {
      "epoch": 9.522857142857143,
      "grad_norm": 0.623012363910675,
      "learning_rate": 7.302857142857144e-06,
      "loss": 0.2231,
      "step": 33330
    },
    {
      "epoch": 9.525714285714285,
      "grad_norm": 0.5796484351158142,
      "learning_rate": 7.29904761904762e-06,
      "loss": 0.2276,
      "step": 33340
    },
    {
      "epoch": 9.528571428571428,
      "grad_norm": 11.640262603759766,
      "learning_rate": 7.295238095238097e-06,
      "loss": 0.5598,
      "step": 33350
    },
    {
      "epoch": 9.531428571428572,
      "grad_norm": 0.5782290101051331,
      "learning_rate": 7.291428571428571e-06,
      "loss": 0.2303,
      "step": 33360
    },
    {
      "epoch": 9.534285714285714,
      "grad_norm": 10.904107093811035,
      "learning_rate": 7.287619047619048e-06,
      "loss": 0.5639,
      "step": 33370
    },
    {
      "epoch": 9.537142857142857,
      "grad_norm": 11.430927276611328,
      "learning_rate": 7.283809523809524e-06,
      "loss": 0.8971,
      "step": 33380
    },
    {
      "epoch": 9.54,
      "grad_norm": 10.725828170776367,
      "learning_rate": 7.280000000000001e-06,
      "loss": 0.6481,
      "step": 33390
    },
    {
      "epoch": 9.542857142857143,
      "grad_norm": 10.715934753417969,
      "learning_rate": 7.276190476190477e-06,
      "loss": 0.4394,
      "step": 33400
    },
    {
      "epoch": 9.545714285714286,
      "grad_norm": 0.6926630735397339,
      "learning_rate": 7.272380952380953e-06,
      "loss": 0.3313,
      "step": 33410
    },
    {
      "epoch": 9.548571428571428,
      "grad_norm": 10.712891578674316,
      "learning_rate": 7.268571428571429e-06,
      "loss": 0.3397,
      "step": 33420
    },
    {
      "epoch": 9.551428571428572,
      "grad_norm": 10.763704299926758,
      "learning_rate": 7.2647619047619055e-06,
      "loss": 0.4474,
      "step": 33430
    },
    {
      "epoch": 9.554285714285715,
      "grad_norm": 0.581488311290741,
      "learning_rate": 7.260952380952382e-06,
      "loss": 0.5229,
      "step": 33440
    },
    {
      "epoch": 9.557142857142857,
      "grad_norm": 0.5830856561660767,
      "learning_rate": 7.257142857142858e-06,
      "loss": 0.5561,
      "step": 33450
    },
    {
      "epoch": 9.56,
      "grad_norm": 0.726327657699585,
      "learning_rate": 7.253333333333335e-06,
      "loss": 0.4528,
      "step": 33460
    },
    {
      "epoch": 9.562857142857142,
      "grad_norm": 10.8035249710083,
      "learning_rate": 7.2495238095238095e-06,
      "loss": 0.5563,
      "step": 33470
    },
    {
      "epoch": 9.565714285714286,
      "grad_norm": 0.5746752619743347,
      "learning_rate": 7.245714285714286e-06,
      "loss": 0.336,
      "step": 33480
    },
    {
      "epoch": 9.56857142857143,
      "grad_norm": 0.5504197478294373,
      "learning_rate": 7.241904761904762e-06,
      "loss": 0.3389,
      "step": 33490
    },
    {
      "epoch": 9.571428571428571,
      "grad_norm": 0.5600464344024658,
      "learning_rate": 7.238095238095239e-06,
      "loss": 0.451,
      "step": 33500
    },
    {
      "epoch": 9.574285714285715,
      "grad_norm": 0.5330368876457214,
      "learning_rate": 7.234285714285715e-06,
      "loss": 0.7829,
      "step": 33510
    },
    {
      "epoch": 9.577142857142857,
      "grad_norm": 10.883785247802734,
      "learning_rate": 7.230476190476191e-06,
      "loss": 0.5618,
      "step": 33520
    },
    {
      "epoch": 9.58,
      "grad_norm": 10.95439624786377,
      "learning_rate": 7.226666666666667e-06,
      "loss": 0.6726,
      "step": 33530
    },
    {
      "epoch": 9.582857142857144,
      "grad_norm": 10.788823127746582,
      "learning_rate": 7.222857142857144e-06,
      "loss": 0.3397,
      "step": 33540
    },
    {
      "epoch": 9.585714285714285,
      "grad_norm": 0.5801153182983398,
      "learning_rate": 7.21904761904762e-06,
      "loss": 0.4513,
      "step": 33550
    },
    {
      "epoch": 9.588571428571429,
      "grad_norm": 0.5417077541351318,
      "learning_rate": 7.2152380952380965e-06,
      "loss": 0.3385,
      "step": 33560
    },
    {
      "epoch": 9.59142857142857,
      "grad_norm": 10.800278663635254,
      "learning_rate": 7.211428571428573e-06,
      "loss": 0.5725,
      "step": 33570
    },
    {
      "epoch": 9.594285714285714,
      "grad_norm": 11.924967765808105,
      "learning_rate": 7.207619047619048e-06,
      "loss": 0.6803,
      "step": 33580
    },
    {
      "epoch": 9.597142857142858,
      "grad_norm": 0.5276818871498108,
      "learning_rate": 7.203809523809524e-06,
      "loss": 0.6755,
      "step": 33590
    },
    {
      "epoch": 9.6,
      "grad_norm": 0.6009430289268494,
      "learning_rate": 7.2000000000000005e-06,
      "loss": 0.8917,
      "step": 33600
    },
    {
      "epoch": 9.602857142857143,
      "grad_norm": 0.6274697184562683,
      "learning_rate": 7.196190476190477e-06,
      "loss": 0.3688,
      "step": 33610
    },
    {
      "epoch": 9.605714285714285,
      "grad_norm": 10.718153953552246,
      "learning_rate": 7.1923809523809525e-06,
      "loss": 0.65,
      "step": 33620
    },
    {
      "epoch": 9.608571428571429,
      "grad_norm": 10.893115043640137,
      "learning_rate": 7.188571428571429e-06,
      "loss": 0.7473,
      "step": 33630
    },
    {
      "epoch": 9.611428571428572,
      "grad_norm": 10.730430603027344,
      "learning_rate": 7.184761904761905e-06,
      "loss": 0.6327,
      "step": 33640
    },
    {
      "epoch": 9.614285714285714,
      "grad_norm": 21.731826782226562,
      "learning_rate": 7.180952380952382e-06,
      "loss": 0.8221,
      "step": 33650
    },
    {
      "epoch": 9.617142857142857,
      "grad_norm": 0.8076942563056946,
      "learning_rate": 7.177142857142858e-06,
      "loss": 0.2176,
      "step": 33660
    },
    {
      "epoch": 9.62,
      "grad_norm": 10.594935417175293,
      "learning_rate": 7.173333333333335e-06,
      "loss": 0.7869,
      "step": 33670
    },
    {
      "epoch": 9.622857142857143,
      "grad_norm": 0.7019805312156677,
      "learning_rate": 7.16952380952381e-06,
      "loss": 0.118,
      "step": 33680
    },
    {
      "epoch": 9.625714285714286,
      "grad_norm": 10.616223335266113,
      "learning_rate": 7.165714285714286e-06,
      "loss": 0.5251,
      "step": 33690
    },
    {
      "epoch": 9.628571428571428,
      "grad_norm": 0.6347440481185913,
      "learning_rate": 7.161904761904762e-06,
      "loss": 0.1208,
      "step": 33700
    },
    {
      "epoch": 9.631428571428572,
      "grad_norm": 18.70922088623047,
      "learning_rate": 7.158095238095239e-06,
      "loss": 0.7691,
      "step": 33710
    },
    {
      "epoch": 9.634285714285713,
      "grad_norm": 0.5512351393699646,
      "learning_rate": 7.154285714285715e-06,
      "loss": 0.4483,
      "step": 33720
    },
    {
      "epoch": 9.637142857142857,
      "grad_norm": 0.665309488773346,
      "learning_rate": 7.150476190476191e-06,
      "loss": 0.6555,
      "step": 33730
    },
    {
      "epoch": 9.64,
      "grad_norm": 10.97432804107666,
      "learning_rate": 7.146666666666667e-06,
      "loss": 0.546,
      "step": 33740
    },
    {
      "epoch": 9.642857142857142,
      "grad_norm": 0.7113362550735474,
      "learning_rate": 7.1428571428571436e-06,
      "loss": 0.7512,
      "step": 33750
    },
    {
      "epoch": 9.645714285714286,
      "grad_norm": 11.009054183959961,
      "learning_rate": 7.13904761904762e-06,
      "loss": 0.7373,
      "step": 33760
    },
    {
      "epoch": 9.64857142857143,
      "grad_norm": 10.671205520629883,
      "learning_rate": 7.135238095238096e-06,
      "loss": 0.5231,
      "step": 33770
    },
    {
      "epoch": 9.651428571428571,
      "grad_norm": 0.8358359932899475,
      "learning_rate": 7.131428571428573e-06,
      "loss": 0.8238,
      "step": 33780
    },
    {
      "epoch": 9.654285714285715,
      "grad_norm": 10.544822692871094,
      "learning_rate": 7.127619047619048e-06,
      "loss": 0.6089,
      "step": 33790
    },
    {
      "epoch": 9.657142857142857,
      "grad_norm": 10.563644409179688,
      "learning_rate": 7.123809523809524e-06,
      "loss": 0.4141,
      "step": 33800
    },
    {
      "epoch": 9.66,
      "grad_norm": 0.7750056982040405,
      "learning_rate": 7.1200000000000004e-06,
      "loss": 0.3183,
      "step": 33810
    },
    {
      "epoch": 9.662857142857142,
      "grad_norm": 0.5543503165245056,
      "learning_rate": 7.116190476190477e-06,
      "loss": 0.3218,
      "step": 33820
    },
    {
      "epoch": 9.665714285714285,
      "grad_norm": 10.715750694274902,
      "learning_rate": 7.1123809523809525e-06,
      "loss": 0.6355,
      "step": 33830
    },
    {
      "epoch": 9.668571428571429,
      "grad_norm": 0.7385091185569763,
      "learning_rate": 7.108571428571429e-06,
      "loss": 0.6492,
      "step": 33840
    },
    {
      "epoch": 9.67142857142857,
      "grad_norm": 10.563421249389648,
      "learning_rate": 7.104761904761905e-06,
      "loss": 1.0408,
      "step": 33850
    },
    {
      "epoch": 9.674285714285714,
      "grad_norm": 0.738683819770813,
      "learning_rate": 7.100952380952382e-06,
      "loss": 0.1194,
      "step": 33860
    },
    {
      "epoch": 9.677142857142858,
      "grad_norm": 0.7512431740760803,
      "learning_rate": 7.097142857142858e-06,
      "loss": 0.8152,
      "step": 33870
    },
    {
      "epoch": 9.68,
      "grad_norm": 0.7736790180206299,
      "learning_rate": 7.093333333333335e-06,
      "loss": 0.5271,
      "step": 33880
    },
    {
      "epoch": 9.682857142857143,
      "grad_norm": 21.90036392211914,
      "learning_rate": 7.08952380952381e-06,
      "loss": 0.8422,
      "step": 33890
    },
    {
      "epoch": 9.685714285714285,
      "grad_norm": 0.6980336904525757,
      "learning_rate": 7.085714285714286e-06,
      "loss": 0.1185,
      "step": 33900
    },
    {
      "epoch": 9.688571428571429,
      "grad_norm": 12.165910720825195,
      "learning_rate": 7.081904761904762e-06,
      "loss": 0.5491,
      "step": 33910
    },
    {
      "epoch": 9.691428571428572,
      "grad_norm": 10.618627548217773,
      "learning_rate": 7.078095238095239e-06,
      "loss": 0.4286,
      "step": 33920
    },
    {
      "epoch": 9.694285714285714,
      "grad_norm": 10.678927421569824,
      "learning_rate": 7.074285714285715e-06,
      "loss": 0.7448,
      "step": 33930
    },
    {
      "epoch": 9.697142857142858,
      "grad_norm": 10.680093765258789,
      "learning_rate": 7.070476190476191e-06,
      "loss": 0.6356,
      "step": 33940
    },
    {
      "epoch": 9.7,
      "grad_norm": 10.579975128173828,
      "learning_rate": 7.066666666666667e-06,
      "loss": 0.4253,
      "step": 33950
    },
    {
      "epoch": 9.702857142857143,
      "grad_norm": 10.560576438903809,
      "learning_rate": 7.0628571428571435e-06,
      "loss": 0.7278,
      "step": 33960
    },
    {
      "epoch": 9.705714285714286,
      "grad_norm": 10.577019691467285,
      "learning_rate": 7.05904761904762e-06,
      "loss": 0.2253,
      "step": 33970
    },
    {
      "epoch": 9.708571428571428,
      "grad_norm": 0.7287392020225525,
      "learning_rate": 7.055238095238096e-06,
      "loss": 0.3234,
      "step": 33980
    },
    {
      "epoch": 9.711428571428572,
      "grad_norm": 10.631731033325195,
      "learning_rate": 7.051428571428573e-06,
      "loss": 0.4315,
      "step": 33990
    },
    {
      "epoch": 9.714285714285714,
      "grad_norm": 0.6947550773620605,
      "learning_rate": 7.047619047619048e-06,
      "loss": 0.537,
      "step": 34000
    },
    {
      "epoch": 9.717142857142857,
      "grad_norm": 0.7043502330780029,
      "learning_rate": 7.043809523809524e-06,
      "loss": 0.4294,
      "step": 34010
    },
    {
      "epoch": 9.72,
      "grad_norm": 10.626554489135742,
      "learning_rate": 7.04e-06,
      "loss": 0.8387,
      "step": 34020
    },
    {
      "epoch": 9.722857142857142,
      "grad_norm": 10.690196990966797,
      "learning_rate": 7.036190476190477e-06,
      "loss": 0.5263,
      "step": 34030
    },
    {
      "epoch": 9.725714285714286,
      "grad_norm": 22.07050323486328,
      "learning_rate": 7.032380952380952e-06,
      "loss": 0.8137,
      "step": 34040
    },
    {
      "epoch": 9.728571428571428,
      "grad_norm": 0.9771542549133301,
      "learning_rate": 7.028571428571429e-06,
      "loss": 0.6016,
      "step": 34050
    },
    {
      "epoch": 9.731428571428571,
      "grad_norm": 0.9188412427902222,
      "learning_rate": 7.024761904761905e-06,
      "loss": 0.4053,
      "step": 34060
    },
    {
      "epoch": 9.734285714285715,
      "grad_norm": 0.8520846366882324,
      "learning_rate": 7.020952380952382e-06,
      "loss": 0.411,
      "step": 34070
    },
    {
      "epoch": 9.737142857142857,
      "grad_norm": 0.8141540288925171,
      "learning_rate": 7.017142857142858e-06,
      "loss": 0.7103,
      "step": 34080
    },
    {
      "epoch": 9.74,
      "grad_norm": 0.7575310468673706,
      "learning_rate": 7.0133333333333345e-06,
      "loss": 0.7224,
      "step": 34090
    },
    {
      "epoch": 9.742857142857144,
      "grad_norm": 0.7180566787719727,
      "learning_rate": 7.00952380952381e-06,
      "loss": 0.3196,
      "step": 34100
    },
    {
      "epoch": 9.745714285714286,
      "grad_norm": 0.7208151817321777,
      "learning_rate": 7.0057142857142865e-06,
      "loss": 0.7389,
      "step": 34110
    },
    {
      "epoch": 9.748571428571429,
      "grad_norm": 0.6927156448364258,
      "learning_rate": 7.001904761904762e-06,
      "loss": 0.4248,
      "step": 34120
    },
    {
      "epoch": 9.751428571428571,
      "grad_norm": 0.674981415271759,
      "learning_rate": 6.9980952380952385e-06,
      "loss": 0.3233,
      "step": 34130
    },
    {
      "epoch": 9.754285714285714,
      "grad_norm": 0.7163017988204956,
      "learning_rate": 6.994285714285715e-06,
      "loss": 0.5375,
      "step": 34140
    },
    {
      "epoch": 9.757142857142856,
      "grad_norm": 0.6975812315940857,
      "learning_rate": 6.9904761904761905e-06,
      "loss": 0.6491,
      "step": 34150
    },
    {
      "epoch": 9.76,
      "grad_norm": 0.6206368207931519,
      "learning_rate": 6.986666666666667e-06,
      "loss": 0.3386,
      "step": 34160
    },
    {
      "epoch": 9.762857142857143,
      "grad_norm": 0.5614076256752014,
      "learning_rate": 6.982857142857143e-06,
      "loss": 0.4483,
      "step": 34170
    },
    {
      "epoch": 9.765714285714285,
      "grad_norm": 0.4935466945171356,
      "learning_rate": 6.97904761904762e-06,
      "loss": 0.4396,
      "step": 34180
    },
    {
      "epoch": 9.768571428571429,
      "grad_norm": 0.5312113165855408,
      "learning_rate": 6.975238095238096e-06,
      "loss": 0.508,
      "step": 34190
    },
    {
      "epoch": 9.771428571428572,
      "grad_norm": 0.5239582657814026,
      "learning_rate": 6.971428571428573e-06,
      "loss": 0.5657,
      "step": 34200
    },
    {
      "epoch": 9.774285714285714,
      "grad_norm": 0.5732532143592834,
      "learning_rate": 6.967619047619048e-06,
      "loss": 0.4547,
      "step": 34210
    },
    {
      "epoch": 9.777142857142858,
      "grad_norm": 0.5880659222602844,
      "learning_rate": 6.963809523809525e-06,
      "loss": 0.5915,
      "step": 34220
    },
    {
      "epoch": 9.78,
      "grad_norm": 0.6043433547019958,
      "learning_rate": 6.96e-06,
      "loss": 0.4491,
      "step": 34230
    },
    {
      "epoch": 9.782857142857143,
      "grad_norm": 11.013160705566406,
      "learning_rate": 6.956190476190477e-06,
      "loss": 0.6641,
      "step": 34240
    },
    {
      "epoch": 9.785714285714286,
      "grad_norm": 0.6268718838691711,
      "learning_rate": 6.952380952380952e-06,
      "loss": 0.8649,
      "step": 34250
    },
    {
      "epoch": 9.788571428571428,
      "grad_norm": 22.38608169555664,
      "learning_rate": 6.948571428571429e-06,
      "loss": 0.5418,
      "step": 34260
    },
    {
      "epoch": 9.791428571428572,
      "grad_norm": 11.430458068847656,
      "learning_rate": 6.944761904761905e-06,
      "loss": 0.4295,
      "step": 34270
    },
    {
      "epoch": 9.794285714285714,
      "grad_norm": 0.7329259514808655,
      "learning_rate": 6.9409523809523816e-06,
      "loss": 0.6402,
      "step": 34280
    },
    {
      "epoch": 9.797142857142857,
      "grad_norm": 0.7943970561027527,
      "learning_rate": 6.937142857142858e-06,
      "loss": 0.513,
      "step": 34290
    },
    {
      "epoch": 9.8,
      "grad_norm": 11.498963356018066,
      "learning_rate": 6.9333333333333344e-06,
      "loss": 1.3459,
      "step": 34300
    },
    {
      "epoch": 9.802857142857142,
      "grad_norm": 11.447809219360352,
      "learning_rate": 6.92952380952381e-06,
      "loss": 0.3139,
      "step": 34310
    },
    {
      "epoch": 9.805714285714286,
      "grad_norm": 22.306636810302734,
      "learning_rate": 6.9257142857142864e-06,
      "loss": 0.8008,
      "step": 34320
    },
    {
      "epoch": 9.808571428571428,
      "grad_norm": 0.9520202279090881,
      "learning_rate": 6.921904761904763e-06,
      "loss": 0.5047,
      "step": 34330
    },
    {
      "epoch": 9.811428571428571,
      "grad_norm": 10.419708251953125,
      "learning_rate": 6.9180952380952385e-06,
      "loss": 0.5978,
      "step": 34340
    },
    {
      "epoch": 9.814285714285715,
      "grad_norm": 10.40768051147461,
      "learning_rate": 6.914285714285715e-06,
      "loss": 0.4053,
      "step": 34350
    },
    {
      "epoch": 9.817142857142857,
      "grad_norm": 0.9492949843406677,
      "learning_rate": 6.9104761904761905e-06,
      "loss": 0.3156,
      "step": 34360
    },
    {
      "epoch": 9.82,
      "grad_norm": 10.558073043823242,
      "learning_rate": 6.906666666666667e-06,
      "loss": 0.6137,
      "step": 34370
    },
    {
      "epoch": 9.822857142857142,
      "grad_norm": 0.8205754160881042,
      "learning_rate": 6.902857142857143e-06,
      "loss": 0.915,
      "step": 34380
    },
    {
      "epoch": 9.825714285714286,
      "grad_norm": 10.670025825500488,
      "learning_rate": 6.89904761904762e-06,
      "loss": 0.7062,
      "step": 34390
    },
    {
      "epoch": 9.82857142857143,
      "grad_norm": 10.54991340637207,
      "learning_rate": 6.895238095238096e-06,
      "loss": 0.5021,
      "step": 34400
    },
    {
      "epoch": 9.831428571428571,
      "grad_norm": 0.8489302396774292,
      "learning_rate": 6.891428571428573e-06,
      "loss": 0.4112,
      "step": 34410
    },
    {
      "epoch": 9.834285714285715,
      "grad_norm": 0.8611817359924316,
      "learning_rate": 6.887619047619048e-06,
      "loss": 0.6158,
      "step": 34420
    },
    {
      "epoch": 9.837142857142858,
      "grad_norm": 10.655155181884766,
      "learning_rate": 6.883809523809525e-06,
      "loss": 0.2215,
      "step": 34430
    },
    {
      "epoch": 9.84,
      "grad_norm": 10.671825408935547,
      "learning_rate": 6.88e-06,
      "loss": 0.5338,
      "step": 34440
    },
    {
      "epoch": 9.842857142857143,
      "grad_norm": 0.6779638528823853,
      "learning_rate": 6.876190476190477e-06,
      "loss": 0.4305,
      "step": 34450
    },
    {
      "epoch": 9.845714285714285,
      "grad_norm": 0.6799265742301941,
      "learning_rate": 6.872380952380952e-06,
      "loss": 0.54,
      "step": 34460
    },
    {
      "epoch": 9.848571428571429,
      "grad_norm": 0.6764227151870728,
      "learning_rate": 6.868571428571429e-06,
      "loss": 0.328,
      "step": 34470
    },
    {
      "epoch": 9.85142857142857,
      "grad_norm": 21.964719772338867,
      "learning_rate": 6.864761904761905e-06,
      "loss": 0.7559,
      "step": 34480
    },
    {
      "epoch": 9.854285714285714,
      "grad_norm": 10.659134864807129,
      "learning_rate": 6.8609523809523815e-06,
      "loss": 0.6454,
      "step": 34490
    },
    {
      "epoch": 9.857142857142858,
      "grad_norm": 10.648510932922363,
      "learning_rate": 6.857142857142858e-06,
      "loss": 0.5304,
      "step": 34500
    },
    {
      "epoch": 9.86,
      "grad_norm": 10.973090171813965,
      "learning_rate": 6.853333333333334e-06,
      "loss": 0.6327,
      "step": 34510
    },
    {
      "epoch": 9.862857142857143,
      "grad_norm": 0.7699109315872192,
      "learning_rate": 6.84952380952381e-06,
      "loss": 0.3198,
      "step": 34520
    },
    {
      "epoch": 9.865714285714287,
      "grad_norm": 0.7636353373527527,
      "learning_rate": 6.845714285714286e-06,
      "loss": 0.4276,
      "step": 34530
    },
    {
      "epoch": 9.868571428571428,
      "grad_norm": 21.80774688720703,
      "learning_rate": 6.841904761904763e-06,
      "loss": 0.9411,
      "step": 34540
    },
    {
      "epoch": 9.871428571428572,
      "grad_norm": 10.5852689743042,
      "learning_rate": 6.838095238095238e-06,
      "loss": 0.7221,
      "step": 34550
    },
    {
      "epoch": 9.874285714285714,
      "grad_norm": 10.469950675964355,
      "learning_rate": 6.834285714285715e-06,
      "loss": 0.615,
      "step": 34560
    },
    {
      "epoch": 9.877142857142857,
      "grad_norm": 0.8233832716941833,
      "learning_rate": 6.83047619047619e-06,
      "loss": 0.5099,
      "step": 34570
    },
    {
      "epoch": 9.88,
      "grad_norm": 0.8721160888671875,
      "learning_rate": 6.826666666666667e-06,
      "loss": 0.7977,
      "step": 34580
    },
    {
      "epoch": 9.882857142857143,
      "grad_norm": 0.8335566520690918,
      "learning_rate": 6.822857142857143e-06,
      "loss": 0.2168,
      "step": 34590
    },
    {
      "epoch": 9.885714285714286,
      "grad_norm": 0.7924942970275879,
      "learning_rate": 6.81904761904762e-06,
      "loss": 0.4219,
      "step": 34600
    },
    {
      "epoch": 9.888571428571428,
      "grad_norm": 0.8452296853065491,
      "learning_rate": 6.815238095238096e-06,
      "loss": 0.5281,
      "step": 34610
    },
    {
      "epoch": 9.891428571428571,
      "grad_norm": 21.954139709472656,
      "learning_rate": 6.8114285714285725e-06,
      "loss": 0.7436,
      "step": 34620
    },
    {
      "epoch": 9.894285714285715,
      "grad_norm": 22.021997451782227,
      "learning_rate": 6.807619047619048e-06,
      "loss": 0.6428,
      "step": 34630
    },
    {
      "epoch": 9.897142857142857,
      "grad_norm": 0.703289806842804,
      "learning_rate": 6.8038095238095245e-06,
      "loss": 0.9285,
      "step": 34640
    },
    {
      "epoch": 9.9,
      "grad_norm": 0.7333263158798218,
      "learning_rate": 6.800000000000001e-06,
      "loss": 0.4311,
      "step": 34650
    },
    {
      "epoch": 9.902857142857142,
      "grad_norm": 10.576308250427246,
      "learning_rate": 6.7961904761904765e-06,
      "loss": 0.7335,
      "step": 34660
    },
    {
      "epoch": 9.905714285714286,
      "grad_norm": 0.8050382137298584,
      "learning_rate": 6.792380952380952e-06,
      "loss": 0.7313,
      "step": 34670
    },
    {
      "epoch": 9.90857142857143,
      "grad_norm": 10.746527671813965,
      "learning_rate": 6.7885714285714286e-06,
      "loss": 0.2889,
      "step": 34680
    },
    {
      "epoch": 9.911428571428571,
      "grad_norm": 0.7953391075134277,
      "learning_rate": 6.784761904761905e-06,
      "loss": 0.8265,
      "step": 34690
    },
    {
      "epoch": 9.914285714285715,
      "grad_norm": 0.7702640295028687,
      "learning_rate": 6.780952380952381e-06,
      "loss": 0.5221,
      "step": 34700
    },
    {
      "epoch": 9.917142857142856,
      "grad_norm": 10.590744018554688,
      "learning_rate": 6.777142857142858e-06,
      "loss": 0.3181,
      "step": 34710
    },
    {
      "epoch": 9.92,
      "grad_norm": 10.591684341430664,
      "learning_rate": 6.773333333333334e-06,
      "loss": 0.6265,
      "step": 34720
    },
    {
      "epoch": 9.922857142857143,
      "grad_norm": 10.78818416595459,
      "learning_rate": 6.769523809523811e-06,
      "loss": 0.3214,
      "step": 34730
    },
    {
      "epoch": 9.925714285714285,
      "grad_norm": 0.8245396018028259,
      "learning_rate": 6.765714285714286e-06,
      "loss": 0.6216,
      "step": 34740
    },
    {
      "epoch": 9.928571428571429,
      "grad_norm": 0.7881397604942322,
      "learning_rate": 6.761904761904763e-06,
      "loss": 0.2173,
      "step": 34750
    },
    {
      "epoch": 9.93142857142857,
      "grad_norm": 10.702491760253906,
      "learning_rate": 6.758095238095239e-06,
      "loss": 0.6365,
      "step": 34760
    },
    {
      "epoch": 9.934285714285714,
      "grad_norm": 10.969123840332031,
      "learning_rate": 6.754285714285715e-06,
      "loss": 0.7379,
      "step": 34770
    },
    {
      "epoch": 9.937142857142858,
      "grad_norm": 0.7928082346916199,
      "learning_rate": 6.75047619047619e-06,
      "loss": 0.5194,
      "step": 34780
    },
    {
      "epoch": 9.94,
      "grad_norm": 21.8894100189209,
      "learning_rate": 6.746666666666667e-06,
      "loss": 0.4186,
      "step": 34790
    },
    {
      "epoch": 9.942857142857143,
      "grad_norm": 0.761562168598175,
      "learning_rate": 6.742857142857143e-06,
      "loss": 0.1193,
      "step": 34800
    },
    {
      "epoch": 9.945714285714285,
      "grad_norm": 11.051441192626953,
      "learning_rate": 6.73904761904762e-06,
      "loss": 0.6361,
      "step": 34810
    },
    {
      "epoch": 9.948571428571428,
      "grad_norm": 0.6960853338241577,
      "learning_rate": 6.735238095238096e-06,
      "loss": 0.3235,
      "step": 34820
    },
    {
      "epoch": 9.951428571428572,
      "grad_norm": 22.046226501464844,
      "learning_rate": 6.7314285714285724e-06,
      "loss": 1.1476,
      "step": 34830
    },
    {
      "epoch": 9.954285714285714,
      "grad_norm": 1.015513300895691,
      "learning_rate": 6.727619047619048e-06,
      "loss": 0.73,
      "step": 34840
    },
    {
      "epoch": 9.957142857142857,
      "grad_norm": 10.619818687438965,
      "learning_rate": 6.7238095238095245e-06,
      "loss": 0.3988,
      "step": 34850
    },
    {
      "epoch": 9.96,
      "grad_norm": 0.7580776810646057,
      "learning_rate": 6.720000000000001e-06,
      "loss": 0.3196,
      "step": 34860
    },
    {
      "epoch": 9.962857142857143,
      "grad_norm": 10.626029968261719,
      "learning_rate": 6.716190476190477e-06,
      "loss": 0.7347,
      "step": 34870
    },
    {
      "epoch": 9.965714285714286,
      "grad_norm": 0.5406931042671204,
      "learning_rate": 6.712380952380952e-06,
      "loss": 0.5262,
      "step": 34880
    },
    {
      "epoch": 9.968571428571428,
      "grad_norm": 0.6818724870681763,
      "learning_rate": 6.7085714285714285e-06,
      "loss": 0.3309,
      "step": 34890
    },
    {
      "epoch": 9.971428571428572,
      "grad_norm": 0.640069842338562,
      "learning_rate": 6.704761904761905e-06,
      "loss": 0.3255,
      "step": 34900
    },
    {
      "epoch": 9.974285714285715,
      "grad_norm": 0.642927348613739,
      "learning_rate": 6.700952380952381e-06,
      "loss": 0.3326,
      "step": 34910
    },
    {
      "epoch": 9.977142857142857,
      "grad_norm": 0.6172720193862915,
      "learning_rate": 6.697142857142858e-06,
      "loss": 0.5464,
      "step": 34920
    },
    {
      "epoch": 9.98,
      "grad_norm": 10.944265365600586,
      "learning_rate": 6.693333333333334e-06,
      "loss": 0.4448,
      "step": 34930
    },
    {
      "epoch": 9.982857142857142,
      "grad_norm": 0.6386846899986267,
      "learning_rate": 6.689523809523811e-06,
      "loss": 0.9755,
      "step": 34940
    },
    {
      "epoch": 9.985714285714286,
      "grad_norm": 0.6290990114212036,
      "learning_rate": 6.685714285714286e-06,
      "loss": 0.5451,
      "step": 34950
    },
    {
      "epoch": 9.98857142857143,
      "grad_norm": 0.6639038324356079,
      "learning_rate": 6.681904761904763e-06,
      "loss": 0.5378,
      "step": 34960
    },
    {
      "epoch": 9.991428571428571,
      "grad_norm": 10.716215133666992,
      "learning_rate": 6.678095238095239e-06,
      "loss": 0.5392,
      "step": 34970
    },
    {
      "epoch": 9.994285714285715,
      "grad_norm": 0.733978271484375,
      "learning_rate": 6.6742857142857155e-06,
      "loss": 0.6414,
      "step": 34980
    },
    {
      "epoch": 9.997142857142856,
      "grad_norm": 33.17207336425781,
      "learning_rate": 6.67047619047619e-06,
      "loss": 0.6415,
      "step": 34990
    },
    {
      "epoch": 10.0,
      "grad_norm": 10.876444816589355,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.7409,
      "step": 35000
    },
    {
      "epoch": 10.0,
      "eval_accuracy": 0.8763440860215054,
      "eval_f1": 0.09359605911330049,
      "eval_loss": 0.5303295254707336,
      "eval_precision": 0.8636363636363636,
      "eval_recall": 0.049479166666666664,
      "eval_runtime": 47.2388,
      "eval_samples_per_second": 63.507,
      "eval_steps_per_second": 1.99,
      "step": 35000
    },
    {
      "epoch": 10.002857142857144,
      "grad_norm": 10.686507225036621,
      "learning_rate": 6.662857142857143e-06,
      "loss": 0.4125,
      "step": 35010
    },
    {
      "epoch": 10.005714285714285,
      "grad_norm": 0.7671018242835999,
      "learning_rate": 6.6590476190476195e-06,
      "loss": 0.914,
      "step": 35020
    },
    {
      "epoch": 10.008571428571429,
      "grad_norm": 0.7508463859558105,
      "learning_rate": 6.655238095238096e-06,
      "loss": 0.2225,
      "step": 35030
    },
    {
      "epoch": 10.01142857142857,
      "grad_norm": 0.6982596516609192,
      "learning_rate": 6.651428571428572e-06,
      "loss": 0.7395,
      "step": 35040
    },
    {
      "epoch": 10.014285714285714,
      "grad_norm": 10.646403312683105,
      "learning_rate": 6.647619047619048e-06,
      "loss": 0.6395,
      "step": 35050
    },
    {
      "epoch": 10.017142857142858,
      "grad_norm": 22.011594772338867,
      "learning_rate": 6.643809523809524e-06,
      "loss": 0.6336,
      "step": 35060
    },
    {
      "epoch": 10.02,
      "grad_norm": 10.882304191589355,
      "learning_rate": 6.640000000000001e-06,
      "loss": 0.4248,
      "step": 35070
    },
    {
      "epoch": 10.022857142857143,
      "grad_norm": 11.007084846496582,
      "learning_rate": 6.636190476190477e-06,
      "loss": 0.3221,
      "step": 35080
    },
    {
      "epoch": 10.025714285714285,
      "grad_norm": 0.759311854839325,
      "learning_rate": 6.632380952380954e-06,
      "loss": 1.1276,
      "step": 35090
    },
    {
      "epoch": 10.028571428571428,
      "grad_norm": 0.7825380563735962,
      "learning_rate": 6.628571428571428e-06,
      "loss": 0.3155,
      "step": 35100
    },
    {
      "epoch": 10.031428571428572,
      "grad_norm": 0.7376511096954346,
      "learning_rate": 6.624761904761905e-06,
      "loss": 0.5146,
      "step": 35110
    },
    {
      "epoch": 10.034285714285714,
      "grad_norm": 21.935827255249023,
      "learning_rate": 6.620952380952381e-06,
      "loss": 0.5167,
      "step": 35120
    },
    {
      "epoch": 10.037142857142857,
      "grad_norm": 0.8893026113510132,
      "learning_rate": 6.617142857142858e-06,
      "loss": 0.317,
      "step": 35130
    },
    {
      "epoch": 10.04,
      "grad_norm": 0.8204153776168823,
      "learning_rate": 6.613333333333334e-06,
      "loss": 0.52,
      "step": 35140
    },
    {
      "epoch": 10.042857142857143,
      "grad_norm": 0.7588776350021362,
      "learning_rate": 6.6095238095238105e-06,
      "loss": 0.3243,
      "step": 35150
    },
    {
      "epoch": 10.045714285714286,
      "grad_norm": 10.68127155303955,
      "learning_rate": 6.605714285714286e-06,
      "loss": 0.8306,
      "step": 35160
    },
    {
      "epoch": 10.048571428571428,
      "grad_norm": 10.61782169342041,
      "learning_rate": 6.6019047619047625e-06,
      "loss": 0.3208,
      "step": 35170
    },
    {
      "epoch": 10.051428571428572,
      "grad_norm": 11.351297378540039,
      "learning_rate": 6.598095238095239e-06,
      "loss": 0.6281,
      "step": 35180
    },
    {
      "epoch": 10.054285714285715,
      "grad_norm": 0.7585065364837646,
      "learning_rate": 6.594285714285715e-06,
      "loss": 0.3223,
      "step": 35190
    },
    {
      "epoch": 10.057142857142857,
      "grad_norm": 21.83631706237793,
      "learning_rate": 6.59047619047619e-06,
      "loss": 0.5361,
      "step": 35200
    },
    {
      "epoch": 10.06,
      "grad_norm": 10.724566459655762,
      "learning_rate": 6.5866666666666666e-06,
      "loss": 0.6363,
      "step": 35210
    },
    {
      "epoch": 10.062857142857142,
      "grad_norm": 11.100167274475098,
      "learning_rate": 6.582857142857143e-06,
      "loss": 0.5332,
      "step": 35220
    },
    {
      "epoch": 10.065714285714286,
      "grad_norm": 0.7227843999862671,
      "learning_rate": 6.579047619047619e-06,
      "loss": 0.533,
      "step": 35230
    },
    {
      "epoch": 10.06857142857143,
      "grad_norm": 0.556303083896637,
      "learning_rate": 6.575238095238096e-06,
      "loss": 0.737,
      "step": 35240
    },
    {
      "epoch": 10.071428571428571,
      "grad_norm": 0.708189845085144,
      "learning_rate": 6.571428571428572e-06,
      "loss": 0.7319,
      "step": 35250
    },
    {
      "epoch": 10.074285714285715,
      "grad_norm": 0.7795339226722717,
      "learning_rate": 6.567619047619048e-06,
      "loss": 0.4258,
      "step": 35260
    },
    {
      "epoch": 10.077142857142857,
      "grad_norm": 0.7769452333450317,
      "learning_rate": 6.563809523809524e-06,
      "loss": 0.842,
      "step": 35270
    },
    {
      "epoch": 10.08,
      "grad_norm": 0.7731917500495911,
      "learning_rate": 6.560000000000001e-06,
      "loss": 0.4204,
      "step": 35280
    },
    {
      "epoch": 10.082857142857144,
      "grad_norm": 10.659892082214355,
      "learning_rate": 6.556190476190477e-06,
      "loss": 0.647,
      "step": 35290
    },
    {
      "epoch": 10.085714285714285,
      "grad_norm": 10.984210968017578,
      "learning_rate": 6.552380952380954e-06,
      "loss": 0.6391,
      "step": 35300
    },
    {
      "epoch": 10.088571428571429,
      "grad_norm": 0.6667490601539612,
      "learning_rate": 6.548571428571428e-06,
      "loss": 0.117,
      "step": 35310
    },
    {
      "epoch": 10.09142857142857,
      "grad_norm": 0.7027953267097473,
      "learning_rate": 6.544761904761905e-06,
      "loss": 0.6471,
      "step": 35320
    },
    {
      "epoch": 10.094285714285714,
      "grad_norm": 11.029997825622559,
      "learning_rate": 6.540952380952381e-06,
      "loss": 0.3231,
      "step": 35330
    },
    {
      "epoch": 10.097142857142858,
      "grad_norm": 10.986313819885254,
      "learning_rate": 6.537142857142858e-06,
      "loss": 0.6435,
      "step": 35340
    },
    {
      "epoch": 10.1,
      "grad_norm": 10.53135871887207,
      "learning_rate": 6.533333333333334e-06,
      "loss": 1.1223,
      "step": 35350
    },
    {
      "epoch": 10.102857142857143,
      "grad_norm": 12.219483375549316,
      "learning_rate": 6.5295238095238105e-06,
      "loss": 0.6044,
      "step": 35360
    },
    {
      "epoch": 10.105714285714285,
      "grad_norm": 0.9079957008361816,
      "learning_rate": 6.525714285714286e-06,
      "loss": 0.5127,
      "step": 35370
    },
    {
      "epoch": 10.108571428571429,
      "grad_norm": 10.974324226379395,
      "learning_rate": 6.5219047619047625e-06,
      "loss": 0.4008,
      "step": 35380
    },
    {
      "epoch": 10.111428571428572,
      "grad_norm": 0.7492117881774902,
      "learning_rate": 6.518095238095239e-06,
      "loss": 0.4135,
      "step": 35390
    },
    {
      "epoch": 10.114285714285714,
      "grad_norm": 10.958372116088867,
      "learning_rate": 6.514285714285715e-06,
      "loss": 1.024,
      "step": 35400
    },
    {
      "epoch": 10.117142857142857,
      "grad_norm": 0.7513748407363892,
      "learning_rate": 6.510476190476192e-06,
      "loss": 0.4206,
      "step": 35410
    },
    {
      "epoch": 10.12,
      "grad_norm": 10.845064163208008,
      "learning_rate": 6.5066666666666665e-06,
      "loss": 0.4333,
      "step": 35420
    },
    {
      "epoch": 10.122857142857143,
      "grad_norm": 0.5920554399490356,
      "learning_rate": 6.502857142857143e-06,
      "loss": 0.4878,
      "step": 35430
    },
    {
      "epoch": 10.125714285714286,
      "grad_norm": 0.580541729927063,
      "learning_rate": 6.499047619047619e-06,
      "loss": 0.4356,
      "step": 35440
    },
    {
      "epoch": 10.128571428571428,
      "grad_norm": 12.56118106842041,
      "learning_rate": 6.495238095238096e-06,
      "loss": 0.6661,
      "step": 35450
    },
    {
      "epoch": 10.131428571428572,
      "grad_norm": 12.710707664489746,
      "learning_rate": 6.491428571428572e-06,
      "loss": 0.4373,
      "step": 35460
    },
    {
      "epoch": 10.134285714285713,
      "grad_norm": 11.73436164855957,
      "learning_rate": 6.487619047619048e-06,
      "loss": 0.3365,
      "step": 35470
    },
    {
      "epoch": 10.137142857142857,
      "grad_norm": 0.6088956594467163,
      "learning_rate": 6.483809523809524e-06,
      "loss": 0.4425,
      "step": 35480
    },
    {
      "epoch": 10.14,
      "grad_norm": 12.092243194580078,
      "learning_rate": 6.480000000000001e-06,
      "loss": 0.3915,
      "step": 35490
    },
    {
      "epoch": 10.142857142857142,
      "grad_norm": 0.483193576335907,
      "learning_rate": 6.476190476190477e-06,
      "loss": 0.2354,
      "step": 35500
    },
    {
      "epoch": 10.145714285714286,
      "grad_norm": 11.179762840270996,
      "learning_rate": 6.4723809523809535e-06,
      "loss": 0.5896,
      "step": 35510
    },
    {
      "epoch": 10.14857142857143,
      "grad_norm": 0.4453590512275696,
      "learning_rate": 6.46857142857143e-06,
      "loss": 0.237,
      "step": 35520
    },
    {
      "epoch": 10.151428571428571,
      "grad_norm": 10.874674797058105,
      "learning_rate": 6.464761904761905e-06,
      "loss": 0.5761,
      "step": 35530
    },
    {
      "epoch": 10.154285714285715,
      "grad_norm": 0.475159615278244,
      "learning_rate": 6.460952380952381e-06,
      "loss": 0.1242,
      "step": 35540
    },
    {
      "epoch": 10.157142857142857,
      "grad_norm": 22.269811630249023,
      "learning_rate": 6.4571428571428575e-06,
      "loss": 0.6884,
      "step": 35550
    },
    {
      "epoch": 10.16,
      "grad_norm": 11.107063293457031,
      "learning_rate": 6.453333333333334e-06,
      "loss": 0.6937,
      "step": 35560
    },
    {
      "epoch": 10.162857142857144,
      "grad_norm": 0.510758638381958,
      "learning_rate": 6.44952380952381e-06,
      "loss": 0.7874,
      "step": 35570
    },
    {
      "epoch": 10.165714285714285,
      "grad_norm": 0.5315493941307068,
      "learning_rate": 6.445714285714286e-06,
      "loss": 0.3414,
      "step": 35580
    },
    {
      "epoch": 10.168571428571429,
      "grad_norm": 10.957413673400879,
      "learning_rate": 6.441904761904762e-06,
      "loss": 0.6778,
      "step": 35590
    },
    {
      "epoch": 10.17142857142857,
      "grad_norm": 11.036252975463867,
      "learning_rate": 6.438095238095239e-06,
      "loss": 1.0855,
      "step": 35600
    },
    {
      "epoch": 10.174285714285714,
      "grad_norm": 0.8457436561584473,
      "learning_rate": 6.434285714285715e-06,
      "loss": 0.7303,
      "step": 35610
    },
    {
      "epoch": 10.177142857142858,
      "grad_norm": 11.555706977844238,
      "learning_rate": 6.430476190476192e-06,
      "loss": 0.4907,
      "step": 35620
    },
    {
      "epoch": 10.18,
      "grad_norm": 1.0978766679763794,
      "learning_rate": 6.426666666666668e-06,
      "loss": 0.5718,
      "step": 35630
    },
    {
      "epoch": 10.182857142857143,
      "grad_norm": 0.8907181620597839,
      "learning_rate": 6.422857142857143e-06,
      "loss": 0.6017,
      "step": 35640
    },
    {
      "epoch": 10.185714285714285,
      "grad_norm": 14.244499206542969,
      "learning_rate": 6.419047619047619e-06,
      "loss": 0.7411,
      "step": 35650
    },
    {
      "epoch": 10.188571428571429,
      "grad_norm": 11.585423469543457,
      "learning_rate": 6.415238095238096e-06,
      "loss": 0.4366,
      "step": 35660
    },
    {
      "epoch": 10.191428571428572,
      "grad_norm": 10.9750394821167,
      "learning_rate": 6.411428571428572e-06,
      "loss": 0.759,
      "step": 35670
    },
    {
      "epoch": 10.194285714285714,
      "grad_norm": 0.5233992338180542,
      "learning_rate": 6.407619047619048e-06,
      "loss": 0.4347,
      "step": 35680
    },
    {
      "epoch": 10.197142857142858,
      "grad_norm": 0.6349893808364868,
      "learning_rate": 6.403809523809524e-06,
      "loss": 0.2299,
      "step": 35690
    },
    {
      "epoch": 10.2,
      "grad_norm": 0.6411933302879333,
      "learning_rate": 6.4000000000000006e-06,
      "loss": 0.9421,
      "step": 35700
    },
    {
      "epoch": 10.202857142857143,
      "grad_norm": 10.791762351989746,
      "learning_rate": 6.396190476190477e-06,
      "loss": 0.6521,
      "step": 35710
    },
    {
      "epoch": 10.205714285714286,
      "grad_norm": 11.63447093963623,
      "learning_rate": 6.392380952380953e-06,
      "loss": 0.4441,
      "step": 35720
    },
    {
      "epoch": 10.208571428571428,
      "grad_norm": 0.6266125440597534,
      "learning_rate": 6.38857142857143e-06,
      "loss": 0.5448,
      "step": 35730
    },
    {
      "epoch": 10.211428571428572,
      "grad_norm": 0.6615601181983948,
      "learning_rate": 6.3847619047619054e-06,
      "loss": 0.5496,
      "step": 35740
    },
    {
      "epoch": 10.214285714285714,
      "grad_norm": 0.6286749243736267,
      "learning_rate": 6.380952380952381e-06,
      "loss": 0.3391,
      "step": 35750
    },
    {
      "epoch": 10.217142857142857,
      "grad_norm": 0.5799189805984497,
      "learning_rate": 6.3771428571428574e-06,
      "loss": 0.4427,
      "step": 35760
    },
    {
      "epoch": 10.22,
      "grad_norm": 0.5938915610313416,
      "learning_rate": 6.373333333333334e-06,
      "loss": 0.6645,
      "step": 35770
    },
    {
      "epoch": 10.222857142857142,
      "grad_norm": 0.6050066351890564,
      "learning_rate": 6.36952380952381e-06,
      "loss": 0.2324,
      "step": 35780
    },
    {
      "epoch": 10.225714285714286,
      "grad_norm": 10.879090309143066,
      "learning_rate": 6.365714285714286e-06,
      "loss": 0.2318,
      "step": 35790
    },
    {
      "epoch": 10.228571428571428,
      "grad_norm": 0.5414546132087708,
      "learning_rate": 6.361904761904762e-06,
      "loss": 0.347,
      "step": 35800
    },
    {
      "epoch": 10.231428571428571,
      "grad_norm": 0.5215004086494446,
      "learning_rate": 6.358095238095239e-06,
      "loss": 1.0071,
      "step": 35810
    },
    {
      "epoch": 10.234285714285715,
      "grad_norm": 0.5960937738418579,
      "learning_rate": 6.354285714285715e-06,
      "loss": 0.9906,
      "step": 35820
    },
    {
      "epoch": 10.237142857142857,
      "grad_norm": 0.6057766079902649,
      "learning_rate": 6.350476190476192e-06,
      "loss": 0.4391,
      "step": 35830
    },
    {
      "epoch": 10.24,
      "grad_norm": 0.6104954481124878,
      "learning_rate": 6.346666666666668e-06,
      "loss": 0.6531,
      "step": 35840
    },
    {
      "epoch": 10.242857142857142,
      "grad_norm": 0.630115270614624,
      "learning_rate": 6.342857142857143e-06,
      "loss": 0.5478,
      "step": 35850
    },
    {
      "epoch": 10.245714285714286,
      "grad_norm": 22.185314178466797,
      "learning_rate": 6.339047619047619e-06,
      "loss": 0.4346,
      "step": 35860
    },
    {
      "epoch": 10.248571428571429,
      "grad_norm": 23.0756778717041,
      "learning_rate": 6.335238095238096e-06,
      "loss": 0.2354,
      "step": 35870
    },
    {
      "epoch": 10.251428571428571,
      "grad_norm": 0.5379551649093628,
      "learning_rate": 6.331428571428572e-06,
      "loss": 0.9033,
      "step": 35880
    },
    {
      "epoch": 10.254285714285714,
      "grad_norm": 0.6498675346374512,
      "learning_rate": 6.327619047619048e-06,
      "loss": 0.7846,
      "step": 35890
    },
    {
      "epoch": 10.257142857142856,
      "grad_norm": 10.802745819091797,
      "learning_rate": 6.323809523809524e-06,
      "loss": 0.8669,
      "step": 35900
    },
    {
      "epoch": 10.26,
      "grad_norm": 11.023687362670898,
      "learning_rate": 6.3200000000000005e-06,
      "loss": 1.1677,
      "step": 35910
    },
    {
      "epoch": 10.262857142857143,
      "grad_norm": 1.0264039039611816,
      "learning_rate": 6.316190476190477e-06,
      "loss": 0.42,
      "step": 35920
    },
    {
      "epoch": 10.265714285714285,
      "grad_norm": 1.2766761779785156,
      "learning_rate": 6.312380952380953e-06,
      "loss": 0.4055,
      "step": 35930
    },
    {
      "epoch": 10.268571428571429,
      "grad_norm": 1.0842479467391968,
      "learning_rate": 6.30857142857143e-06,
      "loss": 0.4069,
      "step": 35940
    },
    {
      "epoch": 10.271428571428572,
      "grad_norm": 12.043462753295898,
      "learning_rate": 6.304761904761905e-06,
      "loss": 0.7723,
      "step": 35950
    },
    {
      "epoch": 10.274285714285714,
      "grad_norm": 10.57329273223877,
      "learning_rate": 6.300952380952381e-06,
      "loss": 0.3877,
      "step": 35960
    },
    {
      "epoch": 10.277142857142858,
      "grad_norm": 0.769170343875885,
      "learning_rate": 6.297142857142857e-06,
      "loss": 0.1095,
      "step": 35970
    },
    {
      "epoch": 10.28,
      "grad_norm": 1.256497859954834,
      "learning_rate": 6.293333333333334e-06,
      "loss": 0.7112,
      "step": 35980
    },
    {
      "epoch": 10.282857142857143,
      "grad_norm": 23.805513381958008,
      "learning_rate": 6.28952380952381e-06,
      "loss": 0.8185,
      "step": 35990
    },
    {
      "epoch": 10.285714285714286,
      "grad_norm": 0.7028524875640869,
      "learning_rate": 6.285714285714286e-06,
      "loss": 0.4404,
      "step": 36000
    },
    {
      "epoch": 10.288571428571428,
      "grad_norm": 11.38224983215332,
      "learning_rate": 6.281904761904762e-06,
      "loss": 0.4954,
      "step": 36010
    },
    {
      "epoch": 10.291428571428572,
      "grad_norm": 11.448366165161133,
      "learning_rate": 6.278095238095239e-06,
      "loss": 0.3549,
      "step": 36020
    },
    {
      "epoch": 10.294285714285714,
      "grad_norm": 0.42805007100105286,
      "learning_rate": 6.274285714285715e-06,
      "loss": 0.6792,
      "step": 36030
    },
    {
      "epoch": 10.297142857142857,
      "grad_norm": 0.37932896614074707,
      "learning_rate": 6.2704761904761915e-06,
      "loss": 0.3754,
      "step": 36040
    },
    {
      "epoch": 10.3,
      "grad_norm": 0.5328060388565063,
      "learning_rate": 6.266666666666668e-06,
      "loss": 0.7325,
      "step": 36050
    },
    {
      "epoch": 10.302857142857142,
      "grad_norm": 0.3718118965625763,
      "learning_rate": 6.2628571428571435e-06,
      "loss": 0.2456,
      "step": 36060
    },
    {
      "epoch": 10.305714285714286,
      "grad_norm": 25.18290901184082,
      "learning_rate": 6.259047619047619e-06,
      "loss": 0.7254,
      "step": 36070
    },
    {
      "epoch": 10.308571428571428,
      "grad_norm": 12.314961433410645,
      "learning_rate": 6.2552380952380955e-06,
      "loss": 0.5725,
      "step": 36080
    },
    {
      "epoch": 10.311428571428571,
      "grad_norm": 0.4824628233909607,
      "learning_rate": 6.251428571428572e-06,
      "loss": 0.5633,
      "step": 36090
    },
    {
      "epoch": 10.314285714285715,
      "grad_norm": 0.5541091561317444,
      "learning_rate": 6.2476190476190475e-06,
      "loss": 0.6855,
      "step": 36100
    },
    {
      "epoch": 10.317142857142857,
      "grad_norm": 11.410863876342773,
      "learning_rate": 6.243809523809524e-06,
      "loss": 0.8937,
      "step": 36110
    },
    {
      "epoch": 10.32,
      "grad_norm": 11.162955284118652,
      "learning_rate": 6.24e-06,
      "loss": 0.8468,
      "step": 36120
    },
    {
      "epoch": 10.322857142857142,
      "grad_norm": 0.7992362976074219,
      "learning_rate": 6.236190476190477e-06,
      "loss": 0.6238,
      "step": 36130
    },
    {
      "epoch": 10.325714285714286,
      "grad_norm": 0.8131570219993591,
      "learning_rate": 6.232380952380953e-06,
      "loss": 0.5195,
      "step": 36140
    },
    {
      "epoch": 10.32857142857143,
      "grad_norm": 11.748147964477539,
      "learning_rate": 6.22857142857143e-06,
      "loss": 0.5253,
      "step": 36150
    },
    {
      "epoch": 10.331428571428571,
      "grad_norm": 10.924013137817383,
      "learning_rate": 6.224761904761905e-06,
      "loss": 0.6043,
      "step": 36160
    },
    {
      "epoch": 10.334285714285715,
      "grad_norm": 11.484956741333008,
      "learning_rate": 6.220952380952382e-06,
      "loss": 0.3228,
      "step": 36170
    },
    {
      "epoch": 10.337142857142856,
      "grad_norm": 0.8673660159111023,
      "learning_rate": 6.217142857142857e-06,
      "loss": 0.4196,
      "step": 36180
    },
    {
      "epoch": 10.34,
      "grad_norm": 10.63166332244873,
      "learning_rate": 6.213333333333334e-06,
      "loss": 0.8443,
      "step": 36190
    },
    {
      "epoch": 10.342857142857143,
      "grad_norm": 0.6656327247619629,
      "learning_rate": 6.20952380952381e-06,
      "loss": 0.3322,
      "step": 36200
    },
    {
      "epoch": 10.345714285714285,
      "grad_norm": 0.7492939233779907,
      "learning_rate": 6.205714285714286e-06,
      "loss": 0.5336,
      "step": 36210
    },
    {
      "epoch": 10.348571428571429,
      "grad_norm": 0.6830840706825256,
      "learning_rate": 6.201904761904762e-06,
      "loss": 0.7383,
      "step": 36220
    },
    {
      "epoch": 10.35142857142857,
      "grad_norm": 10.756827354431152,
      "learning_rate": 6.1980952380952386e-06,
      "loss": 1.0284,
      "step": 36230
    },
    {
      "epoch": 10.354285714285714,
      "grad_norm": 1.3817479610443115,
      "learning_rate": 6.194285714285715e-06,
      "loss": 0.4203,
      "step": 36240
    },
    {
      "epoch": 10.357142857142858,
      "grad_norm": 0.7459380626678467,
      "learning_rate": 6.1904761904761914e-06,
      "loss": 0.3218,
      "step": 36250
    },
    {
      "epoch": 10.36,
      "grad_norm": 0.7597331404685974,
      "learning_rate": 6.186666666666668e-06,
      "loss": 0.6242,
      "step": 36260
    },
    {
      "epoch": 10.362857142857143,
      "grad_norm": 11.045201301574707,
      "learning_rate": 6.1828571428571434e-06,
      "loss": 0.5316,
      "step": 36270
    },
    {
      "epoch": 10.365714285714287,
      "grad_norm": 0.5141777992248535,
      "learning_rate": 6.17904761904762e-06,
      "loss": 0.6476,
      "step": 36280
    },
    {
      "epoch": 10.368571428571428,
      "grad_norm": 0.6817981600761414,
      "learning_rate": 6.1752380952380954e-06,
      "loss": 0.5328,
      "step": 36290
    },
    {
      "epoch": 10.371428571428572,
      "grad_norm": 11.186361312866211,
      "learning_rate": 6.171428571428572e-06,
      "loss": 0.5257,
      "step": 36300
    },
    {
      "epoch": 10.374285714285714,
      "grad_norm": 11.069297790527344,
      "learning_rate": 6.1676190476190475e-06,
      "loss": 0.7416,
      "step": 36310
    },
    {
      "epoch": 10.377142857142857,
      "grad_norm": 0.7540646195411682,
      "learning_rate": 6.163809523809524e-06,
      "loss": 0.4299,
      "step": 36320
    },
    {
      "epoch": 10.38,
      "grad_norm": 0.6655927300453186,
      "learning_rate": 6.16e-06,
      "loss": 0.5282,
      "step": 36330
    },
    {
      "epoch": 10.382857142857143,
      "grad_norm": 0.6273042559623718,
      "learning_rate": 6.156190476190477e-06,
      "loss": 0.1188,
      "step": 36340
    },
    {
      "epoch": 10.385714285714286,
      "grad_norm": 10.871830940246582,
      "learning_rate": 6.152380952380953e-06,
      "loss": 0.4488,
      "step": 36350
    },
    {
      "epoch": 10.388571428571428,
      "grad_norm": 0.5172252058982849,
      "learning_rate": 6.14857142857143e-06,
      "loss": 0.2283,
      "step": 36360
    },
    {
      "epoch": 10.391428571428571,
      "grad_norm": 11.25632381439209,
      "learning_rate": 6.144761904761905e-06,
      "loss": 0.5741,
      "step": 36370
    },
    {
      "epoch": 10.394285714285715,
      "grad_norm": 11.234978675842285,
      "learning_rate": 6.140952380952382e-06,
      "loss": 0.6873,
      "step": 36380
    },
    {
      "epoch": 10.397142857142857,
      "grad_norm": 0.5225406885147095,
      "learning_rate": 6.137142857142858e-06,
      "loss": 0.3468,
      "step": 36390
    },
    {
      "epoch": 10.4,
      "grad_norm": 0.5239641666412354,
      "learning_rate": 6.133333333333334e-06,
      "loss": 0.2337,
      "step": 36400
    },
    {
      "epoch": 10.402857142857142,
      "grad_norm": 10.93270492553711,
      "learning_rate": 6.12952380952381e-06,
      "loss": 0.7906,
      "step": 36410
    },
    {
      "epoch": 10.405714285714286,
      "grad_norm": 0.4997934103012085,
      "learning_rate": 6.125714285714286e-06,
      "loss": 0.5782,
      "step": 36420
    },
    {
      "epoch": 10.40857142857143,
      "grad_norm": 11.177103996276855,
      "learning_rate": 6.121904761904762e-06,
      "loss": 0.6794,
      "step": 36430
    },
    {
      "epoch": 10.411428571428571,
      "grad_norm": 0.5611933469772339,
      "learning_rate": 6.1180952380952385e-06,
      "loss": 0.338,
      "step": 36440
    },
    {
      "epoch": 10.414285714285715,
      "grad_norm": 0.5862415432929993,
      "learning_rate": 6.114285714285715e-06,
      "loss": 0.5617,
      "step": 36450
    },
    {
      "epoch": 10.417142857142856,
      "grad_norm": 0.5389019250869751,
      "learning_rate": 6.110476190476191e-06,
      "loss": 0.3594,
      "step": 36460
    },
    {
      "epoch": 10.42,
      "grad_norm": 0.5508801341056824,
      "learning_rate": 6.106666666666668e-06,
      "loss": 0.4485,
      "step": 36470
    },
    {
      "epoch": 10.422857142857143,
      "grad_norm": 10.736215591430664,
      "learning_rate": 6.102857142857143e-06,
      "loss": 0.7712,
      "step": 36480
    },
    {
      "epoch": 10.425714285714285,
      "grad_norm": 0.6295856833457947,
      "learning_rate": 6.09904761904762e-06,
      "loss": 0.339,
      "step": 36490
    },
    {
      "epoch": 10.428571428571429,
      "grad_norm": 10.960558891296387,
      "learning_rate": 6.095238095238096e-06,
      "loss": 0.7668,
      "step": 36500
    },
    {
      "epoch": 10.43142857142857,
      "grad_norm": 0.6732896566390991,
      "learning_rate": 6.091428571428572e-06,
      "loss": 0.3248,
      "step": 36510
    },
    {
      "epoch": 10.434285714285714,
      "grad_norm": 0.6396888494491577,
      "learning_rate": 6.087619047619047e-06,
      "loss": 0.4359,
      "step": 36520
    },
    {
      "epoch": 10.437142857142858,
      "grad_norm": 0.6163698434829712,
      "learning_rate": 6.083809523809524e-06,
      "loss": 0.3374,
      "step": 36530
    },
    {
      "epoch": 10.44,
      "grad_norm": 0.5686874985694885,
      "learning_rate": 6.08e-06,
      "loss": 0.7758,
      "step": 36540
    },
    {
      "epoch": 10.442857142857143,
      "grad_norm": 11.231892585754395,
      "learning_rate": 6.076190476190477e-06,
      "loss": 0.7676,
      "step": 36550
    },
    {
      "epoch": 10.445714285714285,
      "grad_norm": 0.5810129642486572,
      "learning_rate": 6.072380952380953e-06,
      "loss": 0.3382,
      "step": 36560
    },
    {
      "epoch": 10.448571428571428,
      "grad_norm": 11.479879379272461,
      "learning_rate": 6.0685714285714295e-06,
      "loss": 0.8987,
      "step": 36570
    },
    {
      "epoch": 10.451428571428572,
      "grad_norm": 11.083184242248535,
      "learning_rate": 6.064761904761905e-06,
      "loss": 0.442,
      "step": 36580
    },
    {
      "epoch": 10.454285714285714,
      "grad_norm": 0.6962906718254089,
      "learning_rate": 6.0609523809523815e-06,
      "loss": 0.329,
      "step": 36590
    },
    {
      "epoch": 10.457142857142857,
      "grad_norm": 10.706816673278809,
      "learning_rate": 6.057142857142858e-06,
      "loss": 0.847,
      "step": 36600
    },
    {
      "epoch": 10.46,
      "grad_norm": 11.660431861877441,
      "learning_rate": 6.0533333333333335e-06,
      "loss": 0.4298,
      "step": 36610
    },
    {
      "epoch": 10.462857142857143,
      "grad_norm": 0.6234182715415955,
      "learning_rate": 6.04952380952381e-06,
      "loss": 0.5576,
      "step": 36620
    },
    {
      "epoch": 10.465714285714286,
      "grad_norm": 23.314367294311523,
      "learning_rate": 6.0457142857142855e-06,
      "loss": 0.4438,
      "step": 36630
    },
    {
      "epoch": 10.468571428571428,
      "grad_norm": 34.58290100097656,
      "learning_rate": 6.041904761904762e-06,
      "loss": 0.7809,
      "step": 36640
    },
    {
      "epoch": 10.471428571428572,
      "grad_norm": 0.7010390162467957,
      "learning_rate": 6.038095238095238e-06,
      "loss": 0.5496,
      "step": 36650
    },
    {
      "epoch": 10.474285714285715,
      "grad_norm": 21.92831802368164,
      "learning_rate": 6.034285714285715e-06,
      "loss": 0.966,
      "step": 36660
    },
    {
      "epoch": 10.477142857142857,
      "grad_norm": 0.7391495108604431,
      "learning_rate": 6.030476190476191e-06,
      "loss": 0.8657,
      "step": 36670
    },
    {
      "epoch": 10.48,
      "grad_norm": 0.7046994566917419,
      "learning_rate": 6.026666666666668e-06,
      "loss": 0.3228,
      "step": 36680
    },
    {
      "epoch": 10.482857142857142,
      "grad_norm": 0.6380530595779419,
      "learning_rate": 6.022857142857143e-06,
      "loss": 0.5319,
      "step": 36690
    },
    {
      "epoch": 10.485714285714286,
      "grad_norm": 0.7737598419189453,
      "learning_rate": 6.01904761904762e-06,
      "loss": 0.7443,
      "step": 36700
    },
    {
      "epoch": 10.48857142857143,
      "grad_norm": 0.7071029543876648,
      "learning_rate": 6.015238095238096e-06,
      "loss": 0.3184,
      "step": 36710
    },
    {
      "epoch": 10.491428571428571,
      "grad_norm": 0.7009769082069397,
      "learning_rate": 6.011428571428572e-06,
      "loss": 0.4266,
      "step": 36720
    },
    {
      "epoch": 10.494285714285715,
      "grad_norm": 0.6205650568008423,
      "learning_rate": 6.007619047619047e-06,
      "loss": 0.1223,
      "step": 36730
    },
    {
      "epoch": 10.497142857142856,
      "grad_norm": 10.924768447875977,
      "learning_rate": 6.003809523809524e-06,
      "loss": 0.7682,
      "step": 36740
    },
    {
      "epoch": 10.5,
      "grad_norm": 0.6165189146995544,
      "learning_rate": 6e-06,
      "loss": 0.8721,
      "step": 36750
    },
    {
      "epoch": 10.502857142857144,
      "grad_norm": 0.4776657223701477,
      "learning_rate": 5.996190476190477e-06,
      "loss": 0.538,
      "step": 36760
    },
    {
      "epoch": 10.505714285714285,
      "grad_norm": 11.192938804626465,
      "learning_rate": 5.992380952380953e-06,
      "loss": 0.4373,
      "step": 36770
    },
    {
      "epoch": 10.508571428571429,
      "grad_norm": 0.6376149654388428,
      "learning_rate": 5.9885714285714294e-06,
      "loss": 0.2283,
      "step": 36780
    },
    {
      "epoch": 10.51142857142857,
      "grad_norm": 14.039860725402832,
      "learning_rate": 5.984761904761905e-06,
      "loss": 0.5617,
      "step": 36790
    },
    {
      "epoch": 10.514285714285714,
      "grad_norm": 0.5719514489173889,
      "learning_rate": 5.9809523809523814e-06,
      "loss": 0.7781,
      "step": 36800
    },
    {
      "epoch": 10.517142857142858,
      "grad_norm": 0.6232759952545166,
      "learning_rate": 5.977142857142858e-06,
      "loss": 0.334,
      "step": 36810
    },
    {
      "epoch": 10.52,
      "grad_norm": 10.934674263000488,
      "learning_rate": 5.973333333333334e-06,
      "loss": 0.3318,
      "step": 36820
    },
    {
      "epoch": 10.522857142857143,
      "grad_norm": 0.6168274879455566,
      "learning_rate": 5.96952380952381e-06,
      "loss": 0.5394,
      "step": 36830
    },
    {
      "epoch": 10.525714285714285,
      "grad_norm": 0.6052142977714539,
      "learning_rate": 5.9657142857142855e-06,
      "loss": 0.6515,
      "step": 36840
    },
    {
      "epoch": 10.528571428571428,
      "grad_norm": 11.614324569702148,
      "learning_rate": 5.961904761904762e-06,
      "loss": 0.4414,
      "step": 36850
    },
    {
      "epoch": 10.531428571428572,
      "grad_norm": 0.6944302320480347,
      "learning_rate": 5.958095238095238e-06,
      "loss": 0.6545,
      "step": 36860
    },
    {
      "epoch": 10.534285714285714,
      "grad_norm": 0.7813045382499695,
      "learning_rate": 5.954285714285715e-06,
      "loss": 0.5438,
      "step": 36870
    },
    {
      "epoch": 10.537142857142857,
      "grad_norm": 10.713788032531738,
      "learning_rate": 5.950476190476191e-06,
      "loss": 0.5191,
      "step": 36880
    },
    {
      "epoch": 10.54,
      "grad_norm": 0.848310112953186,
      "learning_rate": 5.946666666666668e-06,
      "loss": 0.6309,
      "step": 36890
    },
    {
      "epoch": 10.542857142857143,
      "grad_norm": 0.6825160980224609,
      "learning_rate": 5.942857142857143e-06,
      "loss": 0.4318,
      "step": 36900
    },
    {
      "epoch": 10.545714285714286,
      "grad_norm": 0.717736005783081,
      "learning_rate": 5.93904761904762e-06,
      "loss": 0.531,
      "step": 36910
    },
    {
      "epoch": 10.548571428571428,
      "grad_norm": 22.45639991760254,
      "learning_rate": 5.935238095238096e-06,
      "loss": 0.6467,
      "step": 36920
    },
    {
      "epoch": 10.551428571428572,
      "grad_norm": 10.927699089050293,
      "learning_rate": 5.9314285714285725e-06,
      "loss": 0.7614,
      "step": 36930
    },
    {
      "epoch": 10.554285714285715,
      "grad_norm": 0.6530511379241943,
      "learning_rate": 5.927619047619047e-06,
      "loss": 0.7516,
      "step": 36940
    },
    {
      "epoch": 10.557142857142857,
      "grad_norm": 10.93723201751709,
      "learning_rate": 5.923809523809524e-06,
      "loss": 0.4371,
      "step": 36950
    },
    {
      "epoch": 10.56,
      "grad_norm": 0.7202932238578796,
      "learning_rate": 5.92e-06,
      "loss": 0.5342,
      "step": 36960
    },
    {
      "epoch": 10.562857142857142,
      "grad_norm": 0.7611075639724731,
      "learning_rate": 5.9161904761904765e-06,
      "loss": 0.4359,
      "step": 36970
    },
    {
      "epoch": 10.565714285714286,
      "grad_norm": 0.6629348993301392,
      "learning_rate": 5.912380952380953e-06,
      "loss": 1.1581,
      "step": 36980
    },
    {
      "epoch": 10.56857142857143,
      "grad_norm": 0.7069562673568726,
      "learning_rate": 5.908571428571429e-06,
      "loss": 0.2237,
      "step": 36990
    },
    {
      "epoch": 10.571428571428571,
      "grad_norm": 11.216225624084473,
      "learning_rate": 5.904761904761905e-06,
      "loss": 0.4321,
      "step": 37000
    },
    {
      "epoch": 10.574285714285715,
      "grad_norm": 0.6007823348045349,
      "learning_rate": 5.900952380952381e-06,
      "loss": 0.2298,
      "step": 37010
    },
    {
      "epoch": 10.577142857142857,
      "grad_norm": 11.119811058044434,
      "learning_rate": 5.897142857142858e-06,
      "loss": 0.4542,
      "step": 37020
    },
    {
      "epoch": 10.58,
      "grad_norm": 0.5258204936981201,
      "learning_rate": 5.893333333333334e-06,
      "loss": 0.4488,
      "step": 37030
    },
    {
      "epoch": 10.582857142857144,
      "grad_norm": 0.5865366458892822,
      "learning_rate": 5.889523809523811e-06,
      "loss": 0.4553,
      "step": 37040
    },
    {
      "epoch": 10.585714285714285,
      "grad_norm": 11.095192909240723,
      "learning_rate": 5.885714285714285e-06,
      "loss": 0.2368,
      "step": 37050
    },
    {
      "epoch": 10.588571428571429,
      "grad_norm": 0.46850383281707764,
      "learning_rate": 5.881904761904762e-06,
      "loss": 0.4595,
      "step": 37060
    },
    {
      "epoch": 10.59142857142857,
      "grad_norm": 21.94506072998047,
      "learning_rate": 5.878095238095238e-06,
      "loss": 1.0996,
      "step": 37070
    },
    {
      "epoch": 10.594285714285714,
      "grad_norm": 11.509613990783691,
      "learning_rate": 5.874285714285715e-06,
      "loss": 0.3312,
      "step": 37080
    },
    {
      "epoch": 10.597142857142858,
      "grad_norm": 0.5087239146232605,
      "learning_rate": 5.870476190476191e-06,
      "loss": 0.4416,
      "step": 37090
    },
    {
      "epoch": 10.6,
      "grad_norm": 0.62945556640625,
      "learning_rate": 5.8666666666666675e-06,
      "loss": 0.3418,
      "step": 37100
    },
    {
      "epoch": 10.602857142857143,
      "grad_norm": 0.5383846759796143,
      "learning_rate": 5.862857142857143e-06,
      "loss": 0.3303,
      "step": 37110
    },
    {
      "epoch": 10.605714285714285,
      "grad_norm": 45.049766540527344,
      "learning_rate": 5.8590476190476195e-06,
      "loss": 1.012,
      "step": 37120
    },
    {
      "epoch": 10.608571428571429,
      "grad_norm": 0.5520920753479004,
      "learning_rate": 5.855238095238096e-06,
      "loss": 0.4594,
      "step": 37130
    },
    {
      "epoch": 10.611428571428572,
      "grad_norm": 11.157779693603516,
      "learning_rate": 5.851428571428572e-06,
      "loss": 0.5345,
      "step": 37140
    },
    {
      "epoch": 10.614285714285714,
      "grad_norm": 0.7693145871162415,
      "learning_rate": 5.847619047619049e-06,
      "loss": 0.5343,
      "step": 37150
    },
    {
      "epoch": 10.617142857142857,
      "grad_norm": 0.5802348852157593,
      "learning_rate": 5.8438095238095236e-06,
      "loss": 0.4193,
      "step": 37160
    },
    {
      "epoch": 10.62,
      "grad_norm": 22.135929107666016,
      "learning_rate": 5.84e-06,
      "loss": 0.6444,
      "step": 37170
    },
    {
      "epoch": 10.622857142857143,
      "grad_norm": 10.934996604919434,
      "learning_rate": 5.836190476190476e-06,
      "loss": 0.5718,
      "step": 37180
    },
    {
      "epoch": 10.625714285714286,
      "grad_norm": 11.243279457092285,
      "learning_rate": 5.832380952380953e-06,
      "loss": 0.5576,
      "step": 37190
    },
    {
      "epoch": 10.628571428571428,
      "grad_norm": 0.6159957051277161,
      "learning_rate": 5.828571428571429e-06,
      "loss": 0.4449,
      "step": 37200
    },
    {
      "epoch": 10.631428571428572,
      "grad_norm": 0.5167646408081055,
      "learning_rate": 5.824761904761906e-06,
      "loss": 0.2267,
      "step": 37210
    },
    {
      "epoch": 10.634285714285713,
      "grad_norm": 11.383153915405273,
      "learning_rate": 5.820952380952381e-06,
      "loss": 0.7864,
      "step": 37220
    },
    {
      "epoch": 10.637142857142857,
      "grad_norm": 11.09321403503418,
      "learning_rate": 5.817142857142858e-06,
      "loss": 0.6606,
      "step": 37230
    },
    {
      "epoch": 10.64,
      "grad_norm": 33.22905731201172,
      "learning_rate": 5.813333333333334e-06,
      "loss": 0.6435,
      "step": 37240
    },
    {
      "epoch": 10.642857142857142,
      "grad_norm": 0.6977831721305847,
      "learning_rate": 5.8095238095238106e-06,
      "loss": 0.549,
      "step": 37250
    },
    {
      "epoch": 10.645714285714286,
      "grad_norm": 22.259414672851562,
      "learning_rate": 5.805714285714285e-06,
      "loss": 0.4325,
      "step": 37260
    },
    {
      "epoch": 10.64857142857143,
      "grad_norm": 11.254487037658691,
      "learning_rate": 5.801904761904762e-06,
      "loss": 1.0348,
      "step": 37270
    },
    {
      "epoch": 10.651428571428571,
      "grad_norm": 0.9277485609054565,
      "learning_rate": 5.798095238095238e-06,
      "loss": 0.6083,
      "step": 37280
    },
    {
      "epoch": 10.654285714285715,
      "grad_norm": 0.5997840762138367,
      "learning_rate": 5.794285714285715e-06,
      "loss": 0.0167,
      "step": 37290
    },
    {
      "epoch": 10.657142857142857,
      "grad_norm": 0.48234859108924866,
      "learning_rate": 5.790476190476191e-06,
      "loss": 0.2131,
      "step": 37300
    },
    {
      "epoch": 10.66,
      "grad_norm": 0.461925745010376,
      "learning_rate": 5.7866666666666674e-06,
      "loss": 0.5534,
      "step": 37310
    },
    {
      "epoch": 10.662857142857142,
      "grad_norm": 0.5213112831115723,
      "learning_rate": 5.782857142857143e-06,
      "loss": 0.4617,
      "step": 37320
    },
    {
      "epoch": 10.665714285714285,
      "grad_norm": 0.3972892761230469,
      "learning_rate": 5.7790476190476195e-06,
      "loss": 0.4477,
      "step": 37330
    },
    {
      "epoch": 10.668571428571429,
      "grad_norm": 0.5222291946411133,
      "learning_rate": 5.775238095238096e-06,
      "loss": 0.5822,
      "step": 37340
    },
    {
      "epoch": 10.67142857142857,
      "grad_norm": 0.42727598547935486,
      "learning_rate": 5.771428571428572e-06,
      "loss": 0.5503,
      "step": 37350
    },
    {
      "epoch": 10.674285714285714,
      "grad_norm": 11.345460891723633,
      "learning_rate": 5.767619047619049e-06,
      "loss": 1.1083,
      "step": 37360
    },
    {
      "epoch": 10.677142857142858,
      "grad_norm": 0.47520381212234497,
      "learning_rate": 5.7638095238095235e-06,
      "loss": 0.121,
      "step": 37370
    },
    {
      "epoch": 10.68,
      "grad_norm": 22.450685501098633,
      "learning_rate": 5.76e-06,
      "loss": 0.4676,
      "step": 37380
    },
    {
      "epoch": 10.682857142857143,
      "grad_norm": 0.4553874135017395,
      "learning_rate": 5.756190476190476e-06,
      "loss": 0.7515,
      "step": 37390
    },
    {
      "epoch": 10.685714285714285,
      "grad_norm": 0.8074752688407898,
      "learning_rate": 5.752380952380953e-06,
      "loss": 0.3374,
      "step": 37400
    },
    {
      "epoch": 10.688571428571429,
      "grad_norm": 0.6905653476715088,
      "learning_rate": 5.748571428571429e-06,
      "loss": 0.1259,
      "step": 37410
    },
    {
      "epoch": 10.691428571428572,
      "grad_norm": 0.61201411485672,
      "learning_rate": 5.744761904761906e-06,
      "loss": 0.4376,
      "step": 37420
    },
    {
      "epoch": 10.694285714285714,
      "grad_norm": 22.72640037536621,
      "learning_rate": 5.740952380952381e-06,
      "loss": 0.8699,
      "step": 37430
    },
    {
      "epoch": 10.697142857142858,
      "grad_norm": 11.221851348876953,
      "learning_rate": 5.737142857142858e-06,
      "loss": 0.4423,
      "step": 37440
    },
    {
      "epoch": 10.7,
      "grad_norm": 0.7306726574897766,
      "learning_rate": 5.733333333333334e-06,
      "loss": 0.76,
      "step": 37450
    },
    {
      "epoch": 10.702857142857143,
      "grad_norm": 0.7145862579345703,
      "learning_rate": 5.7295238095238105e-06,
      "loss": 0.6296,
      "step": 37460
    },
    {
      "epoch": 10.705714285714286,
      "grad_norm": 0.7085518836975098,
      "learning_rate": 5.725714285714287e-06,
      "loss": 0.4292,
      "step": 37470
    },
    {
      "epoch": 10.708571428571428,
      "grad_norm": 13.78434944152832,
      "learning_rate": 5.721904761904762e-06,
      "loss": 0.6245,
      "step": 37480
    },
    {
      "epoch": 10.711428571428572,
      "grad_norm": 0.7561516165733337,
      "learning_rate": 5.718095238095238e-06,
      "loss": 0.3216,
      "step": 37490
    },
    {
      "epoch": 10.714285714285714,
      "grad_norm": 22.47251319885254,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 0.9258,
      "step": 37500
    },
    {
      "epoch": 10.717142857142857,
      "grad_norm": 22.522348403930664,
      "learning_rate": 5.710476190476191e-06,
      "loss": 0.5081,
      "step": 37510
    },
    {
      "epoch": 10.72,
      "grad_norm": 0.6742369532585144,
      "learning_rate": 5.706666666666667e-06,
      "loss": 0.5904,
      "step": 37520
    },
    {
      "epoch": 10.722857142857142,
      "grad_norm": 0.625930666923523,
      "learning_rate": 5.702857142857143e-06,
      "loss": 0.6901,
      "step": 37530
    },
    {
      "epoch": 10.725714285714286,
      "grad_norm": 0.884345531463623,
      "learning_rate": 5.699047619047619e-06,
      "loss": 0.322,
      "step": 37540
    },
    {
      "epoch": 10.728571428571428,
      "grad_norm": 11.164645195007324,
      "learning_rate": 5.695238095238096e-06,
      "loss": 0.5851,
      "step": 37550
    },
    {
      "epoch": 10.731428571428571,
      "grad_norm": 0.5077316164970398,
      "learning_rate": 5.691428571428572e-06,
      "loss": 0.3233,
      "step": 37560
    },
    {
      "epoch": 10.734285714285715,
      "grad_norm": 12.415600776672363,
      "learning_rate": 5.687619047619049e-06,
      "loss": 0.5595,
      "step": 37570
    },
    {
      "epoch": 10.737142857142857,
      "grad_norm": 24.091737747192383,
      "learning_rate": 5.683809523809525e-06,
      "loss": 0.9749,
      "step": 37580
    },
    {
      "epoch": 10.74,
      "grad_norm": 0.9148748517036438,
      "learning_rate": 5.68e-06,
      "loss": 0.4087,
      "step": 37590
    },
    {
      "epoch": 10.742857142857144,
      "grad_norm": 13.621708869934082,
      "learning_rate": 5.676190476190476e-06,
      "loss": 0.538,
      "step": 37600
    },
    {
      "epoch": 10.745714285714286,
      "grad_norm": 0.8887361288070679,
      "learning_rate": 5.672380952380953e-06,
      "loss": 0.6935,
      "step": 37610
    },
    {
      "epoch": 10.748571428571429,
      "grad_norm": 0.7113077044487,
      "learning_rate": 5.668571428571429e-06,
      "loss": 0.4513,
      "step": 37620
    },
    {
      "epoch": 10.751428571428571,
      "grad_norm": 11.076364517211914,
      "learning_rate": 5.6647619047619055e-06,
      "loss": 0.6445,
      "step": 37630
    },
    {
      "epoch": 10.754285714285714,
      "grad_norm": 0.896485447883606,
      "learning_rate": 5.660952380952381e-06,
      "loss": 0.3179,
      "step": 37640
    },
    {
      "epoch": 10.757142857142856,
      "grad_norm": 0.6134293079376221,
      "learning_rate": 5.6571428571428576e-06,
      "loss": 0.4561,
      "step": 37650
    },
    {
      "epoch": 10.76,
      "grad_norm": 11.03581428527832,
      "learning_rate": 5.653333333333334e-06,
      "loss": 0.6389,
      "step": 37660
    },
    {
      "epoch": 10.762857142857143,
      "grad_norm": 0.9207119941711426,
      "learning_rate": 5.64952380952381e-06,
      "loss": 0.6725,
      "step": 37670
    },
    {
      "epoch": 10.765714285714285,
      "grad_norm": 1.1514257192611694,
      "learning_rate": 5.645714285714287e-06,
      "loss": 0.3166,
      "step": 37680
    },
    {
      "epoch": 10.768571428571429,
      "grad_norm": 0.6202377676963806,
      "learning_rate": 5.641904761904763e-06,
      "loss": 0.4031,
      "step": 37690
    },
    {
      "epoch": 10.771428571428572,
      "grad_norm": 0.5777508616447449,
      "learning_rate": 5.638095238095238e-06,
      "loss": 0.3189,
      "step": 37700
    },
    {
      "epoch": 10.774285714285714,
      "grad_norm": 0.45256099104881287,
      "learning_rate": 5.6342857142857144e-06,
      "loss": 0.4346,
      "step": 37710
    },
    {
      "epoch": 10.777142857142858,
      "grad_norm": 0.37362051010131836,
      "learning_rate": 5.630476190476191e-06,
      "loss": 0.8027,
      "step": 37720
    },
    {
      "epoch": 10.78,
      "grad_norm": 0.6835682392120361,
      "learning_rate": 5.626666666666667e-06,
      "loss": 0.2264,
      "step": 37730
    },
    {
      "epoch": 10.782857142857143,
      "grad_norm": 0.5728029608726501,
      "learning_rate": 5.622857142857143e-06,
      "loss": 0.5172,
      "step": 37740
    },
    {
      "epoch": 10.785714285714286,
      "grad_norm": 11.091123580932617,
      "learning_rate": 5.619047619047619e-06,
      "loss": 0.5349,
      "step": 37750
    },
    {
      "epoch": 10.788571428571428,
      "grad_norm": 0.551307201385498,
      "learning_rate": 5.615238095238096e-06,
      "loss": 0.7273,
      "step": 37760
    },
    {
      "epoch": 10.791428571428572,
      "grad_norm": 0.5497938990592957,
      "learning_rate": 5.611428571428572e-06,
      "loss": 0.3419,
      "step": 37770
    },
    {
      "epoch": 10.794285714285714,
      "grad_norm": 23.373130798339844,
      "learning_rate": 5.607619047619049e-06,
      "loss": 0.8189,
      "step": 37780
    },
    {
      "epoch": 10.797142857142857,
      "grad_norm": 0.5617454051971436,
      "learning_rate": 5.603809523809525e-06,
      "loss": 0.2266,
      "step": 37790
    },
    {
      "epoch": 10.8,
      "grad_norm": 0.503106415271759,
      "learning_rate": 5.600000000000001e-06,
      "loss": 0.7242,
      "step": 37800
    },
    {
      "epoch": 10.802857142857142,
      "grad_norm": 8.986324310302734,
      "learning_rate": 5.596190476190476e-06,
      "loss": 0.5672,
      "step": 37810
    },
    {
      "epoch": 10.805714285714286,
      "grad_norm": 0.32881101965904236,
      "learning_rate": 5.592380952380953e-06,
      "loss": 0.3188,
      "step": 37820
    },
    {
      "epoch": 10.808571428571428,
      "grad_norm": 23.20331382751465,
      "learning_rate": 5.588571428571429e-06,
      "loss": 0.4476,
      "step": 37830
    },
    {
      "epoch": 10.811428571428571,
      "grad_norm": 0.36021775007247925,
      "learning_rate": 5.5847619047619055e-06,
      "loss": 0.4299,
      "step": 37840
    },
    {
      "epoch": 10.814285714285715,
      "grad_norm": 0.4505966603755951,
      "learning_rate": 5.580952380952381e-06,
      "loss": 0.9245,
      "step": 37850
    },
    {
      "epoch": 10.817142857142857,
      "grad_norm": 11.609687805175781,
      "learning_rate": 5.5771428571428575e-06,
      "loss": 0.328,
      "step": 37860
    },
    {
      "epoch": 10.82,
      "grad_norm": 10.985881805419922,
      "learning_rate": 5.573333333333334e-06,
      "loss": 0.6907,
      "step": 37870
    },
    {
      "epoch": 10.822857142857142,
      "grad_norm": 22.752277374267578,
      "learning_rate": 5.56952380952381e-06,
      "loss": 0.5144,
      "step": 37880
    },
    {
      "epoch": 10.825714285714286,
      "grad_norm": 0.7595087289810181,
      "learning_rate": 5.565714285714287e-06,
      "loss": 0.1245,
      "step": 37890
    },
    {
      "epoch": 10.82857142857143,
      "grad_norm": 0.44703954458236694,
      "learning_rate": 5.561904761904763e-06,
      "loss": 0.3076,
      "step": 37900
    },
    {
      "epoch": 10.831428571428571,
      "grad_norm": 12.35688304901123,
      "learning_rate": 5.558095238095239e-06,
      "loss": 0.4103,
      "step": 37910
    },
    {
      "epoch": 10.834285714285715,
      "grad_norm": 0.1725739985704422,
      "learning_rate": 5.554285714285714e-06,
      "loss": 0.5995,
      "step": 37920
    },
    {
      "epoch": 10.837142857142858,
      "grad_norm": 0.8467824459075928,
      "learning_rate": 5.550476190476191e-06,
      "loss": 0.2187,
      "step": 37930
    },
    {
      "epoch": 10.84,
      "grad_norm": 15.949745178222656,
      "learning_rate": 5.546666666666667e-06,
      "loss": 0.3865,
      "step": 37940
    },
    {
      "epoch": 10.842857142857143,
      "grad_norm": 14.58506965637207,
      "learning_rate": 5.542857142857143e-06,
      "loss": 0.7339,
      "step": 37950
    },
    {
      "epoch": 10.845714285714285,
      "grad_norm": 12.802056312561035,
      "learning_rate": 5.539047619047619e-06,
      "loss": 0.2756,
      "step": 37960
    },
    {
      "epoch": 10.848571428571429,
      "grad_norm": 13.413228988647461,
      "learning_rate": 5.535238095238096e-06,
      "loss": 0.9533,
      "step": 37970
    },
    {
      "epoch": 10.85142857142857,
      "grad_norm": 0.5564278960227966,
      "learning_rate": 5.531428571428572e-06,
      "loss": 0.3774,
      "step": 37980
    },
    {
      "epoch": 10.854285714285714,
      "grad_norm": 0.33860158920288086,
      "learning_rate": 5.5276190476190485e-06,
      "loss": 0.2948,
      "step": 37990
    },
    {
      "epoch": 10.857142857142858,
      "grad_norm": 0.32904109358787537,
      "learning_rate": 5.523809523809525e-06,
      "loss": 0.3721,
      "step": 38000
    },
    {
      "epoch": 10.86,
      "grad_norm": 0.3405836820602417,
      "learning_rate": 5.5200000000000005e-06,
      "loss": 0.4708,
      "step": 38010
    },
    {
      "epoch": 10.862857142857143,
      "grad_norm": 0.17591500282287598,
      "learning_rate": 5.516190476190476e-06,
      "loss": 0.2765,
      "step": 38020
    },
    {
      "epoch": 10.865714285714287,
      "grad_norm": 35.325260162353516,
      "learning_rate": 5.5123809523809525e-06,
      "loss": 0.6004,
      "step": 38030
    },
    {
      "epoch": 10.868571428571428,
      "grad_norm": 14.38736629486084,
      "learning_rate": 5.508571428571429e-06,
      "loss": 0.5797,
      "step": 38040
    },
    {
      "epoch": 10.871428571428572,
      "grad_norm": 22.758312225341797,
      "learning_rate": 5.504761904761905e-06,
      "loss": 0.9828,
      "step": 38050
    },
    {
      "epoch": 10.874285714285714,
      "grad_norm": 0.5302552580833435,
      "learning_rate": 5.500952380952381e-06,
      "loss": 0.5609,
      "step": 38060
    },
    {
      "epoch": 10.877142857142857,
      "grad_norm": 13.588251113891602,
      "learning_rate": 5.497142857142857e-06,
      "loss": 0.7818,
      "step": 38070
    },
    {
      "epoch": 10.88,
      "grad_norm": 1.496385931968689,
      "learning_rate": 5.493333333333334e-06,
      "loss": 0.4944,
      "step": 38080
    },
    {
      "epoch": 10.882857142857143,
      "grad_norm": 0.9843507409095764,
      "learning_rate": 5.48952380952381e-06,
      "loss": 0.5294,
      "step": 38090
    },
    {
      "epoch": 10.885714285714286,
      "grad_norm": 0.6136276125907898,
      "learning_rate": 5.485714285714287e-06,
      "loss": 0.5728,
      "step": 38100
    },
    {
      "epoch": 10.888571428571428,
      "grad_norm": 0.48635947704315186,
      "learning_rate": 5.481904761904763e-06,
      "loss": 0.4945,
      "step": 38110
    },
    {
      "epoch": 10.891428571428571,
      "grad_norm": 0.45133325457572937,
      "learning_rate": 5.478095238095239e-06,
      "loss": 0.6798,
      "step": 38120
    },
    {
      "epoch": 10.894285714285715,
      "grad_norm": 0.3246406614780426,
      "learning_rate": 5.474285714285714e-06,
      "loss": 0.4656,
      "step": 38130
    },
    {
      "epoch": 10.897142857142857,
      "grad_norm": 0.6335424184799194,
      "learning_rate": 5.470476190476191e-06,
      "loss": 0.4309,
      "step": 38140
    },
    {
      "epoch": 10.9,
      "grad_norm": 13.720682144165039,
      "learning_rate": 5.466666666666667e-06,
      "loss": 0.8767,
      "step": 38150
    },
    {
      "epoch": 10.902857142857142,
      "grad_norm": 0.8158618807792664,
      "learning_rate": 5.462857142857143e-06,
      "loss": 0.3158,
      "step": 38160
    },
    {
      "epoch": 10.905714285714286,
      "grad_norm": 0.6962303519248962,
      "learning_rate": 5.459047619047619e-06,
      "loss": 0.2134,
      "step": 38170
    },
    {
      "epoch": 10.90857142857143,
      "grad_norm": 84.4632339477539,
      "learning_rate": 5.4552380952380956e-06,
      "loss": 0.4753,
      "step": 38180
    },
    {
      "epoch": 10.911428571428571,
      "grad_norm": 12.621045112609863,
      "learning_rate": 5.451428571428572e-06,
      "loss": 0.521,
      "step": 38190
    },
    {
      "epoch": 10.914285714285715,
      "grad_norm": 0.2763499617576599,
      "learning_rate": 5.447619047619048e-06,
      "loss": 0.6337,
      "step": 38200
    },
    {
      "epoch": 10.917142857142856,
      "grad_norm": 12.676579475402832,
      "learning_rate": 5.443809523809525e-06,
      "loss": 0.6149,
      "step": 38210
    },
    {
      "epoch": 10.92,
      "grad_norm": 13.733885765075684,
      "learning_rate": 5.4400000000000004e-06,
      "loss": 0.9069,
      "step": 38220
    },
    {
      "epoch": 10.922857142857143,
      "grad_norm": 45.9985237121582,
      "learning_rate": 5.436190476190477e-06,
      "loss": 0.7455,
      "step": 38230
    },
    {
      "epoch": 10.925714285714285,
      "grad_norm": 11.602790832519531,
      "learning_rate": 5.4323809523809524e-06,
      "loss": 0.1918,
      "step": 38240
    },
    {
      "epoch": 10.928571428571429,
      "grad_norm": 8.908960342407227,
      "learning_rate": 5.428571428571429e-06,
      "loss": 0.5717,
      "step": 38250
    },
    {
      "epoch": 10.93142857142857,
      "grad_norm": 0.42934101819992065,
      "learning_rate": 5.424761904761905e-06,
      "loss": 0.1692,
      "step": 38260
    },
    {
      "epoch": 10.934285714285714,
      "grad_norm": 14.261900901794434,
      "learning_rate": 5.420952380952381e-06,
      "loss": 0.5718,
      "step": 38270
    },
    {
      "epoch": 10.937142857142858,
      "grad_norm": 0.21526281535625458,
      "learning_rate": 5.417142857142857e-06,
      "loss": 0.4545,
      "step": 38280
    },
    {
      "epoch": 10.94,
      "grad_norm": 31.659221649169922,
      "learning_rate": 5.413333333333334e-06,
      "loss": 0.3524,
      "step": 38290
    },
    {
      "epoch": 10.942857142857143,
      "grad_norm": 22.85505485534668,
      "learning_rate": 5.40952380952381e-06,
      "loss": 0.6553,
      "step": 38300
    },
    {
      "epoch": 10.945714285714285,
      "grad_norm": 14.597108840942383,
      "learning_rate": 5.405714285714287e-06,
      "loss": 0.635,
      "step": 38310
    },
    {
      "epoch": 10.948571428571428,
      "grad_norm": 0.5932289958000183,
      "learning_rate": 5.401904761904763e-06,
      "loss": 0.1892,
      "step": 38320
    },
    {
      "epoch": 10.951428571428572,
      "grad_norm": 0.28886842727661133,
      "learning_rate": 5.398095238095239e-06,
      "loss": 0.3763,
      "step": 38330
    },
    {
      "epoch": 10.954285714285714,
      "grad_norm": 12.72752857208252,
      "learning_rate": 5.394285714285715e-06,
      "loss": 0.7256,
      "step": 38340
    },
    {
      "epoch": 10.957142857142857,
      "grad_norm": 1.5509991645812988,
      "learning_rate": 5.390476190476191e-06,
      "loss": 0.5961,
      "step": 38350
    },
    {
      "epoch": 10.96,
      "grad_norm": 1.5814465284347534,
      "learning_rate": 5.386666666666667e-06,
      "loss": 0.3833,
      "step": 38360
    },
    {
      "epoch": 10.962857142857143,
      "grad_norm": 0.7939358353614807,
      "learning_rate": 5.382857142857143e-06,
      "loss": 0.2696,
      "step": 38370
    },
    {
      "epoch": 10.965714285714286,
      "grad_norm": 13.24892520904541,
      "learning_rate": 5.379047619047619e-06,
      "loss": 0.5113,
      "step": 38380
    },
    {
      "epoch": 10.968571428571428,
      "grad_norm": 0.9554345011711121,
      "learning_rate": 5.3752380952380955e-06,
      "loss": 0.197,
      "step": 38390
    },
    {
      "epoch": 10.971428571428572,
      "grad_norm": 0.4447493553161621,
      "learning_rate": 5.371428571428572e-06,
      "loss": 0.4268,
      "step": 38400
    },
    {
      "epoch": 10.974285714285715,
      "grad_norm": 20.300371170043945,
      "learning_rate": 5.367619047619048e-06,
      "loss": 0.3339,
      "step": 38410
    },
    {
      "epoch": 10.977142857142857,
      "grad_norm": 0.31414350867271423,
      "learning_rate": 5.363809523809525e-06,
      "loss": 0.5776,
      "step": 38420
    },
    {
      "epoch": 10.98,
      "grad_norm": 0.12261809408664703,
      "learning_rate": 5.36e-06,
      "loss": 0.7437,
      "step": 38430
    },
    {
      "epoch": 10.982857142857142,
      "grad_norm": 17.548778533935547,
      "learning_rate": 5.356190476190477e-06,
      "loss": 0.3992,
      "step": 38440
    },
    {
      "epoch": 10.985714285714286,
      "grad_norm": 0.8031891584396362,
      "learning_rate": 5.352380952380953e-06,
      "loss": 0.4023,
      "step": 38450
    },
    {
      "epoch": 10.98857142857143,
      "grad_norm": 0.634562075138092,
      "learning_rate": 5.348571428571429e-06,
      "loss": 0.4085,
      "step": 38460
    },
    {
      "epoch": 10.991428571428571,
      "grad_norm": 0.7908397912979126,
      "learning_rate": 5.344761904761905e-06,
      "loss": 0.2497,
      "step": 38470
    },
    {
      "epoch": 10.994285714285715,
      "grad_norm": 0.4675135314464569,
      "learning_rate": 5.340952380952381e-06,
      "loss": 0.1858,
      "step": 38480
    },
    {
      "epoch": 10.997142857142856,
      "grad_norm": 0.4374168813228607,
      "learning_rate": 5.337142857142857e-06,
      "loss": 0.3637,
      "step": 38490
    },
    {
      "epoch": 11.0,
      "grad_norm": 17.146223068237305,
      "learning_rate": 5.333333333333334e-06,
      "loss": 0.4616,
      "step": 38500
    },
    {
      "epoch": 11.0,
      "eval_accuracy": 0.8860887096774194,
      "eval_f1": 0.3957219251336898,
      "eval_loss": 0.5322086811065674,
      "eval_precision": 0.6271186440677966,
      "eval_recall": 0.2890625,
      "eval_runtime": 46.7485,
      "eval_samples_per_second": 64.173,
      "eval_steps_per_second": 2.011,
      "step": 38500
    },
    {
      "epoch": 11.002857142857144,
      "grad_norm": 0.38629430532455444,
      "learning_rate": 5.32952380952381e-06,
      "loss": 0.343,
      "step": 38510
    },
    {
      "epoch": 11.005714285714285,
      "grad_norm": 0.15391391515731812,
      "learning_rate": 5.3257142857142865e-06,
      "loss": 0.255,
      "step": 38520
    },
    {
      "epoch": 11.008571428571429,
      "grad_norm": 0.4051533043384552,
      "learning_rate": 5.321904761904763e-06,
      "loss": 0.3818,
      "step": 38530
    },
    {
      "epoch": 11.01142857142857,
      "grad_norm": 0.11753874272108078,
      "learning_rate": 5.3180952380952385e-06,
      "loss": 0.3406,
      "step": 38540
    },
    {
      "epoch": 11.014285714285714,
      "grad_norm": 80.70840454101562,
      "learning_rate": 5.314285714285715e-06,
      "loss": 0.4613,
      "step": 38550
    },
    {
      "epoch": 11.017142857142858,
      "grad_norm": 15.385042190551758,
      "learning_rate": 5.310476190476191e-06,
      "loss": 0.3002,
      "step": 38560
    },
    {
      "epoch": 11.02,
      "grad_norm": 0.895301103591919,
      "learning_rate": 5.306666666666667e-06,
      "loss": 0.4524,
      "step": 38570
    },
    {
      "epoch": 11.022857142857143,
      "grad_norm": 0.19298908114433289,
      "learning_rate": 5.3028571428571425e-06,
      "loss": 0.1145,
      "step": 38580
    },
    {
      "epoch": 11.025714285714285,
      "grad_norm": 0.1032259538769722,
      "learning_rate": 5.299047619047619e-06,
      "loss": 0.7854,
      "step": 38590
    },
    {
      "epoch": 11.028571428571428,
      "grad_norm": 0.4505808353424072,
      "learning_rate": 5.295238095238095e-06,
      "loss": 0.8929,
      "step": 38600
    },
    {
      "epoch": 11.031428571428572,
      "grad_norm": 38.58187484741211,
      "learning_rate": 5.291428571428572e-06,
      "loss": 0.3992,
      "step": 38610
    },
    {
      "epoch": 11.034285714285714,
      "grad_norm": 11.826395034790039,
      "learning_rate": 5.287619047619048e-06,
      "loss": 0.2674,
      "step": 38620
    },
    {
      "epoch": 11.037142857142857,
      "grad_norm": 24.624107360839844,
      "learning_rate": 5.283809523809525e-06,
      "loss": 0.3606,
      "step": 38630
    },
    {
      "epoch": 11.04,
      "grad_norm": 1.729508638381958,
      "learning_rate": 5.28e-06,
      "loss": 0.7954,
      "step": 38640
    },
    {
      "epoch": 11.042857142857143,
      "grad_norm": 5.537888526916504,
      "learning_rate": 5.276190476190477e-06,
      "loss": 0.7424,
      "step": 38650
    },
    {
      "epoch": 11.045714285714286,
      "grad_norm": 16.604310989379883,
      "learning_rate": 5.272380952380953e-06,
      "loss": 0.3154,
      "step": 38660
    },
    {
      "epoch": 11.048571428571428,
      "grad_norm": 18.534391403198242,
      "learning_rate": 5.268571428571429e-06,
      "loss": 0.4421,
      "step": 38670
    },
    {
      "epoch": 11.051428571428572,
      "grad_norm": 24.022138595581055,
      "learning_rate": 5.264761904761905e-06,
      "loss": 0.2625,
      "step": 38680
    },
    {
      "epoch": 11.054285714285715,
      "grad_norm": 13.391802787780762,
      "learning_rate": 5.260952380952381e-06,
      "loss": 0.3466,
      "step": 38690
    },
    {
      "epoch": 11.057142857142857,
      "grad_norm": 0.28452977538108826,
      "learning_rate": 5.257142857142857e-06,
      "loss": 0.4319,
      "step": 38700
    },
    {
      "epoch": 11.06,
      "grad_norm": 0.3111429512500763,
      "learning_rate": 5.2533333333333336e-06,
      "loss": 0.4012,
      "step": 38710
    },
    {
      "epoch": 11.062857142857142,
      "grad_norm": 0.5894052386283875,
      "learning_rate": 5.24952380952381e-06,
      "loss": 0.6325,
      "step": 38720
    },
    {
      "epoch": 11.065714285714286,
      "grad_norm": 12.491127967834473,
      "learning_rate": 5.2457142857142864e-06,
      "loss": 0.5047,
      "step": 38730
    },
    {
      "epoch": 11.06857142857143,
      "grad_norm": 0.8819729685783386,
      "learning_rate": 5.241904761904763e-06,
      "loss": 0.2502,
      "step": 38740
    },
    {
      "epoch": 11.071428571428571,
      "grad_norm": 34.36791229248047,
      "learning_rate": 5.2380952380952384e-06,
      "loss": 0.6827,
      "step": 38750
    },
    {
      "epoch": 11.074285714285715,
      "grad_norm": 279.1576843261719,
      "learning_rate": 5.234285714285715e-06,
      "loss": 0.7018,
      "step": 38760
    },
    {
      "epoch": 11.077142857142857,
      "grad_norm": 0.32488471269607544,
      "learning_rate": 5.230476190476191e-06,
      "loss": 0.6402,
      "step": 38770
    },
    {
      "epoch": 11.08,
      "grad_norm": 0.3150205612182617,
      "learning_rate": 5.226666666666667e-06,
      "loss": 0.3887,
      "step": 38780
    },
    {
      "epoch": 11.082857142857144,
      "grad_norm": 0.4591093361377716,
      "learning_rate": 5.2228571428571425e-06,
      "loss": 0.3053,
      "step": 38790
    },
    {
      "epoch": 11.085714285714285,
      "grad_norm": 0.31551796197891235,
      "learning_rate": 5.219047619047619e-06,
      "loss": 0.6308,
      "step": 38800
    },
    {
      "epoch": 11.088571428571429,
      "grad_norm": 1.7689436674118042,
      "learning_rate": 5.215238095238095e-06,
      "loss": 0.2022,
      "step": 38810
    },
    {
      "epoch": 11.09142857142857,
      "grad_norm": 0.23394867777824402,
      "learning_rate": 5.211428571428572e-06,
      "loss": 0.2514,
      "step": 38820
    },
    {
      "epoch": 11.094285714285714,
      "grad_norm": 53.25083923339844,
      "learning_rate": 5.207619047619048e-06,
      "loss": 0.5249,
      "step": 38830
    },
    {
      "epoch": 11.097142857142858,
      "grad_norm": 0.2236832082271576,
      "learning_rate": 5.203809523809525e-06,
      "loss": 0.8659,
      "step": 38840
    },
    {
      "epoch": 11.1,
      "grad_norm": 0.512660026550293,
      "learning_rate": 5.2e-06,
      "loss": 0.4757,
      "step": 38850
    },
    {
      "epoch": 11.102857142857143,
      "grad_norm": 19.21662712097168,
      "learning_rate": 5.196190476190477e-06,
      "loss": 0.8757,
      "step": 38860
    },
    {
      "epoch": 11.105714285714285,
      "grad_norm": 13.324861526489258,
      "learning_rate": 5.192380952380953e-06,
      "loss": 0.7684,
      "step": 38870
    },
    {
      "epoch": 11.108571428571429,
      "grad_norm": 33.66212463378906,
      "learning_rate": 5.1885714285714295e-06,
      "loss": 0.4508,
      "step": 38880
    },
    {
      "epoch": 11.111428571428572,
      "grad_norm": 16.62167739868164,
      "learning_rate": 5.184761904761905e-06,
      "loss": 0.4469,
      "step": 38890
    },
    {
      "epoch": 11.114285714285714,
      "grad_norm": 17.752422332763672,
      "learning_rate": 5.180952380952381e-06,
      "loss": 0.5522,
      "step": 38900
    },
    {
      "epoch": 11.117142857142857,
      "grad_norm": 0.573344886302948,
      "learning_rate": 5.177142857142857e-06,
      "loss": 0.2968,
      "step": 38910
    },
    {
      "epoch": 11.12,
      "grad_norm": 23.69547462463379,
      "learning_rate": 5.1733333333333335e-06,
      "loss": 0.5852,
      "step": 38920
    },
    {
      "epoch": 11.122857142857143,
      "grad_norm": 0.22143317759037018,
      "learning_rate": 5.16952380952381e-06,
      "loss": 0.5995,
      "step": 38930
    },
    {
      "epoch": 11.125714285714286,
      "grad_norm": 15.992713928222656,
      "learning_rate": 5.165714285714286e-06,
      "loss": 0.5414,
      "step": 38940
    },
    {
      "epoch": 11.128571428571428,
      "grad_norm": 11.962413787841797,
      "learning_rate": 5.161904761904763e-06,
      "loss": 0.7199,
      "step": 38950
    },
    {
      "epoch": 11.131428571428572,
      "grad_norm": 0.4529930055141449,
      "learning_rate": 5.158095238095238e-06,
      "loss": 0.6824,
      "step": 38960
    },
    {
      "epoch": 11.134285714285713,
      "grad_norm": 15.610542297363281,
      "learning_rate": 5.154285714285715e-06,
      "loss": 0.4938,
      "step": 38970
    },
    {
      "epoch": 11.137142857142857,
      "grad_norm": 0.5007578134536743,
      "learning_rate": 5.150476190476191e-06,
      "loss": 0.0919,
      "step": 38980
    },
    {
      "epoch": 11.14,
      "grad_norm": 0.23796314001083374,
      "learning_rate": 5.146666666666668e-06,
      "loss": 0.5563,
      "step": 38990
    },
    {
      "epoch": 11.142857142857142,
      "grad_norm": 0.34753134846687317,
      "learning_rate": 5.142857142857142e-06,
      "loss": 0.3181,
      "step": 39000
    },
    {
      "epoch": 11.145714285714286,
      "grad_norm": 12.963444709777832,
      "learning_rate": 5.139047619047619e-06,
      "loss": 0.6261,
      "step": 39010
    },
    {
      "epoch": 11.14857142857143,
      "grad_norm": 0.17267408967018127,
      "learning_rate": 5.135238095238095e-06,
      "loss": 0.0405,
      "step": 39020
    },
    {
      "epoch": 11.151428571428571,
      "grad_norm": 0.1226118803024292,
      "learning_rate": 5.131428571428572e-06,
      "loss": 0.3337,
      "step": 39030
    },
    {
      "epoch": 11.154285714285715,
      "grad_norm": 1.4995495080947876,
      "learning_rate": 5.127619047619048e-06,
      "loss": 0.4166,
      "step": 39040
    },
    {
      "epoch": 11.157142857142857,
      "grad_norm": 14.964204788208008,
      "learning_rate": 5.1238095238095245e-06,
      "loss": 0.4562,
      "step": 39050
    },
    {
      "epoch": 11.16,
      "grad_norm": 0.9547725319862366,
      "learning_rate": 5.12e-06,
      "loss": 0.1241,
      "step": 39060
    },
    {
      "epoch": 11.162857142857144,
      "grad_norm": 21.776533126831055,
      "learning_rate": 5.1161904761904765e-06,
      "loss": 0.9031,
      "step": 39070
    },
    {
      "epoch": 11.165714285714285,
      "grad_norm": 0.15516503155231476,
      "learning_rate": 5.112380952380953e-06,
      "loss": 0.4719,
      "step": 39080
    },
    {
      "epoch": 11.168571428571429,
      "grad_norm": 12.229915618896484,
      "learning_rate": 5.108571428571429e-06,
      "loss": 0.5575,
      "step": 39090
    },
    {
      "epoch": 11.17142857142857,
      "grad_norm": 0.47629645466804504,
      "learning_rate": 5.104761904761906e-06,
      "loss": 0.6382,
      "step": 39100
    },
    {
      "epoch": 11.174285714285714,
      "grad_norm": 0.3273174464702606,
      "learning_rate": 5.1009523809523806e-06,
      "loss": 0.4343,
      "step": 39110
    },
    {
      "epoch": 11.177142857142858,
      "grad_norm": 21.263160705566406,
      "learning_rate": 5.097142857142857e-06,
      "loss": 0.6822,
      "step": 39120
    },
    {
      "epoch": 11.18,
      "grad_norm": 11.912022590637207,
      "learning_rate": 5.093333333333333e-06,
      "loss": 0.4707,
      "step": 39130
    },
    {
      "epoch": 11.182857142857143,
      "grad_norm": 0.2655075788497925,
      "learning_rate": 5.08952380952381e-06,
      "loss": 0.2146,
      "step": 39140
    },
    {
      "epoch": 11.185714285714285,
      "grad_norm": 14.200439453125,
      "learning_rate": 5.085714285714286e-06,
      "loss": 0.4733,
      "step": 39150
    },
    {
      "epoch": 11.188571428571429,
      "grad_norm": 12.224308013916016,
      "learning_rate": 5.081904761904763e-06,
      "loss": 0.3712,
      "step": 39160
    },
    {
      "epoch": 11.191428571428572,
      "grad_norm": 0.5155190825462341,
      "learning_rate": 5.078095238095238e-06,
      "loss": 0.1611,
      "step": 39170
    },
    {
      "epoch": 11.194285714285714,
      "grad_norm": 0.38670891523361206,
      "learning_rate": 5.074285714285715e-06,
      "loss": 0.6392,
      "step": 39180
    },
    {
      "epoch": 11.197142857142858,
      "grad_norm": 0.5754042267799377,
      "learning_rate": 5.070476190476191e-06,
      "loss": 0.4362,
      "step": 39190
    },
    {
      "epoch": 11.2,
      "grad_norm": 0.6342265009880066,
      "learning_rate": 5.0666666666666676e-06,
      "loss": 0.5894,
      "step": 39200
    },
    {
      "epoch": 11.202857142857143,
      "grad_norm": 0.4689787030220032,
      "learning_rate": 5.062857142857144e-06,
      "loss": 0.402,
      "step": 39210
    },
    {
      "epoch": 11.205714285714286,
      "grad_norm": 12.648930549621582,
      "learning_rate": 5.059047619047619e-06,
      "loss": 0.3452,
      "step": 39220
    },
    {
      "epoch": 11.208571428571428,
      "grad_norm": 14.462146759033203,
      "learning_rate": 5.055238095238095e-06,
      "loss": 0.2136,
      "step": 39230
    },
    {
      "epoch": 11.211428571428572,
      "grad_norm": 14.414682388305664,
      "learning_rate": 5.051428571428572e-06,
      "loss": 0.2952,
      "step": 39240
    },
    {
      "epoch": 11.214285714285714,
      "grad_norm": 106.11787414550781,
      "learning_rate": 5.047619047619048e-06,
      "loss": 0.8517,
      "step": 39250
    },
    {
      "epoch": 11.217142857142857,
      "grad_norm": 13.261298179626465,
      "learning_rate": 5.0438095238095244e-06,
      "loss": 0.452,
      "step": 39260
    },
    {
      "epoch": 11.22,
      "grad_norm": 1.8765650987625122,
      "learning_rate": 5.04e-06,
      "loss": 0.2432,
      "step": 39270
    },
    {
      "epoch": 11.222857142857142,
      "grad_norm": 1.2370266914367676,
      "learning_rate": 5.0361904761904765e-06,
      "loss": 0.2246,
      "step": 39280
    },
    {
      "epoch": 11.225714285714286,
      "grad_norm": 0.6344723701477051,
      "learning_rate": 5.032380952380953e-06,
      "loss": 0.2317,
      "step": 39290
    },
    {
      "epoch": 11.228571428571428,
      "grad_norm": 15.32237720489502,
      "learning_rate": 5.028571428571429e-06,
      "loss": 0.4574,
      "step": 39300
    },
    {
      "epoch": 11.231428571428571,
      "grad_norm": 0.3255465030670166,
      "learning_rate": 5.024761904761906e-06,
      "loss": 0.7132,
      "step": 39310
    },
    {
      "epoch": 11.234285714285715,
      "grad_norm": 0.18021459877490997,
      "learning_rate": 5.020952380952382e-06,
      "loss": 0.2145,
      "step": 39320
    },
    {
      "epoch": 11.237142857142857,
      "grad_norm": 0.25818029046058655,
      "learning_rate": 5.017142857142857e-06,
      "loss": 0.2222,
      "step": 39330
    },
    {
      "epoch": 11.24,
      "grad_norm": 0.14826145768165588,
      "learning_rate": 5.013333333333333e-06,
      "loss": 0.2835,
      "step": 39340
    },
    {
      "epoch": 11.242857142857142,
      "grad_norm": 0.11473909765481949,
      "learning_rate": 5.00952380952381e-06,
      "loss": 0.5503,
      "step": 39350
    },
    {
      "epoch": 11.245714285714286,
      "grad_norm": 0.10229716449975967,
      "learning_rate": 5.005714285714286e-06,
      "loss": 0.0946,
      "step": 39360
    },
    {
      "epoch": 11.248571428571429,
      "grad_norm": 6.138926982879639,
      "learning_rate": 5.001904761904763e-06,
      "loss": 0.5242,
      "step": 39370
    },
    {
      "epoch": 11.251428571428571,
      "grad_norm": 14.007132530212402,
      "learning_rate": 4.998095238095238e-06,
      "loss": 0.3481,
      "step": 39380
    },
    {
      "epoch": 11.254285714285714,
      "grad_norm": 14.788908958435059,
      "learning_rate": 4.994285714285715e-06,
      "loss": 0.2208,
      "step": 39390
    },
    {
      "epoch": 11.257142857142856,
      "grad_norm": 0.4114815294742584,
      "learning_rate": 4.990476190476191e-06,
      "loss": 0.3253,
      "step": 39400
    },
    {
      "epoch": 11.26,
      "grad_norm": 16.514678955078125,
      "learning_rate": 4.986666666666667e-06,
      "loss": 0.9487,
      "step": 39410
    },
    {
      "epoch": 11.262857142857143,
      "grad_norm": 0.40501394867897034,
      "learning_rate": 4.982857142857143e-06,
      "loss": 0.3384,
      "step": 39420
    },
    {
      "epoch": 11.265714285714285,
      "grad_norm": 1.0167739391326904,
      "learning_rate": 4.9790476190476195e-06,
      "loss": 0.1353,
      "step": 39430
    },
    {
      "epoch": 11.268571428571429,
      "grad_norm": 0.4599153399467468,
      "learning_rate": 4.975238095238096e-06,
      "loss": 0.3603,
      "step": 39440
    },
    {
      "epoch": 11.271428571428572,
      "grad_norm": 1.3091788291931152,
      "learning_rate": 4.971428571428572e-06,
      "loss": 0.4362,
      "step": 39450
    },
    {
      "epoch": 11.274285714285714,
      "grad_norm": 0.21384161710739136,
      "learning_rate": 4.967619047619048e-06,
      "loss": 0.4514,
      "step": 39460
    },
    {
      "epoch": 11.277142857142858,
      "grad_norm": 13.879666328430176,
      "learning_rate": 4.963809523809524e-06,
      "loss": 0.4777,
      "step": 39470
    },
    {
      "epoch": 11.28,
      "grad_norm": 0.20859931409358978,
      "learning_rate": 4.960000000000001e-06,
      "loss": 0.1922,
      "step": 39480
    },
    {
      "epoch": 11.282857142857143,
      "grad_norm": 16.642091751098633,
      "learning_rate": 4.956190476190476e-06,
      "loss": 0.3738,
      "step": 39490
    },
    {
      "epoch": 11.285714285714286,
      "grad_norm": 0.19726713001728058,
      "learning_rate": 4.952380952380953e-06,
      "loss": 0.4861,
      "step": 39500
    },
    {
      "epoch": 11.288571428571428,
      "grad_norm": 0.0880105197429657,
      "learning_rate": 4.948571428571429e-06,
      "loss": 0.5009,
      "step": 39510
    },
    {
      "epoch": 11.291428571428572,
      "grad_norm": 0.2539651393890381,
      "learning_rate": 4.944761904761905e-06,
      "loss": 0.5076,
      "step": 39520
    },
    {
      "epoch": 11.294285714285714,
      "grad_norm": 13.6282320022583,
      "learning_rate": 4.940952380952381e-06,
      "loss": 0.5241,
      "step": 39530
    },
    {
      "epoch": 11.297142857142857,
      "grad_norm": 27.034944534301758,
      "learning_rate": 4.937142857142858e-06,
      "loss": 0.5778,
      "step": 39540
    },
    {
      "epoch": 11.3,
      "grad_norm": 26.70742416381836,
      "learning_rate": 4.933333333333334e-06,
      "loss": 0.6718,
      "step": 39550
    },
    {
      "epoch": 11.302857142857142,
      "grad_norm": 14.123342514038086,
      "learning_rate": 4.9295238095238105e-06,
      "loss": 0.5918,
      "step": 39560
    },
    {
      "epoch": 11.305714285714286,
      "grad_norm": 0.05123887583613396,
      "learning_rate": 4.925714285714286e-06,
      "loss": 0.5284,
      "step": 39570
    },
    {
      "epoch": 11.308571428571428,
      "grad_norm": 32.185543060302734,
      "learning_rate": 4.9219047619047625e-06,
      "loss": 0.6166,
      "step": 39580
    },
    {
      "epoch": 11.311428571428571,
      "grad_norm": 1.054368495941162,
      "learning_rate": 4.918095238095238e-06,
      "loss": 0.5288,
      "step": 39590
    },
    {
      "epoch": 11.314285714285715,
      "grad_norm": 22.246288299560547,
      "learning_rate": 4.9142857142857145e-06,
      "loss": 0.5299,
      "step": 39600
    },
    {
      "epoch": 11.317142857142857,
      "grad_norm": 0.5456336140632629,
      "learning_rate": 4.910476190476191e-06,
      "loss": 0.5133,
      "step": 39610
    },
    {
      "epoch": 11.32,
      "grad_norm": 0.49526339769363403,
      "learning_rate": 4.9066666666666666e-06,
      "loss": 0.2652,
      "step": 39620
    },
    {
      "epoch": 11.322857142857142,
      "grad_norm": 0.11980047076940536,
      "learning_rate": 4.902857142857143e-06,
      "loss": 0.0131,
      "step": 39630
    },
    {
      "epoch": 11.325714285714286,
      "grad_norm": 0.8251940608024597,
      "learning_rate": 4.899047619047619e-06,
      "loss": 0.747,
      "step": 39640
    },
    {
      "epoch": 11.32857142857143,
      "grad_norm": 15.157613754272461,
      "learning_rate": 4.895238095238096e-06,
      "loss": 0.4242,
      "step": 39650
    },
    {
      "epoch": 11.331428571428571,
      "grad_norm": 0.1682012379169464,
      "learning_rate": 4.891428571428572e-06,
      "loss": 0.39,
      "step": 39660
    },
    {
      "epoch": 11.334285714285715,
      "grad_norm": 0.6850187182426453,
      "learning_rate": 4.887619047619048e-06,
      "loss": 0.2547,
      "step": 39670
    },
    {
      "epoch": 11.337142857142856,
      "grad_norm": 0.4155489206314087,
      "learning_rate": 4.883809523809524e-06,
      "loss": 0.1972,
      "step": 39680
    },
    {
      "epoch": 11.34,
      "grad_norm": 0.22596241533756256,
      "learning_rate": 4.880000000000001e-06,
      "loss": 0.3507,
      "step": 39690
    },
    {
      "epoch": 11.342857142857143,
      "grad_norm": 14.63659954071045,
      "learning_rate": 4.876190476190476e-06,
      "loss": 0.6808,
      "step": 39700
    },
    {
      "epoch": 11.345714285714285,
      "grad_norm": 19.992671966552734,
      "learning_rate": 4.872380952380953e-06,
      "loss": 0.728,
      "step": 39710
    },
    {
      "epoch": 11.348571428571429,
      "grad_norm": 1.155616044998169,
      "learning_rate": 4.868571428571429e-06,
      "loss": 0.5619,
      "step": 39720
    },
    {
      "epoch": 11.35142857142857,
      "grad_norm": 12.85268783569336,
      "learning_rate": 4.864761904761905e-06,
      "loss": 0.2059,
      "step": 39730
    },
    {
      "epoch": 11.354285714285714,
      "grad_norm": 4.476174831390381,
      "learning_rate": 4.860952380952381e-06,
      "loss": 0.5989,
      "step": 39740
    },
    {
      "epoch": 11.357142857142858,
      "grad_norm": 0.4655679166316986,
      "learning_rate": 4.857142857142858e-06,
      "loss": 0.2911,
      "step": 39750
    },
    {
      "epoch": 11.36,
      "grad_norm": 0.7095836400985718,
      "learning_rate": 4.853333333333334e-06,
      "loss": 0.6895,
      "step": 39760
    },
    {
      "epoch": 11.362857142857143,
      "grad_norm": 0.2997702658176422,
      "learning_rate": 4.8495238095238104e-06,
      "loss": 0.2969,
      "step": 39770
    },
    {
      "epoch": 11.365714285714287,
      "grad_norm": 1.447865605354309,
      "learning_rate": 4.845714285714286e-06,
      "loss": 0.3898,
      "step": 39780
    },
    {
      "epoch": 11.368571428571428,
      "grad_norm": 1.208366870880127,
      "learning_rate": 4.8419047619047625e-06,
      "loss": 0.0144,
      "step": 39790
    },
    {
      "epoch": 11.371428571428572,
      "grad_norm": 20.31526756286621,
      "learning_rate": 4.838095238095238e-06,
      "loss": 0.1696,
      "step": 39800
    },
    {
      "epoch": 11.374285714285714,
      "grad_norm": 0.43403056263923645,
      "learning_rate": 4.8342857142857145e-06,
      "loss": 0.4949,
      "step": 39810
    },
    {
      "epoch": 11.377142857142857,
      "grad_norm": 0.11926542967557907,
      "learning_rate": 4.830476190476191e-06,
      "loss": 0.2609,
      "step": 39820
    },
    {
      "epoch": 11.38,
      "grad_norm": 0.03612673655152321,
      "learning_rate": 4.826666666666667e-06,
      "loss": 0.4958,
      "step": 39830
    },
    {
      "epoch": 11.382857142857143,
      "grad_norm": 18.521026611328125,
      "learning_rate": 4.822857142857143e-06,
      "loss": 0.325,
      "step": 39840
    },
    {
      "epoch": 11.385714285714286,
      "grad_norm": 21.013511657714844,
      "learning_rate": 4.819047619047619e-06,
      "loss": 0.7454,
      "step": 39850
    },
    {
      "epoch": 11.388571428571428,
      "grad_norm": 0.1232675313949585,
      "learning_rate": 4.815238095238096e-06,
      "loss": 0.6837,
      "step": 39860
    },
    {
      "epoch": 11.391428571428571,
      "grad_norm": 1.076316475868225,
      "learning_rate": 4.811428571428572e-06,
      "loss": 0.2272,
      "step": 39870
    },
    {
      "epoch": 11.394285714285715,
      "grad_norm": 23.19366455078125,
      "learning_rate": 4.807619047619048e-06,
      "loss": 0.574,
      "step": 39880
    },
    {
      "epoch": 11.397142857142857,
      "grad_norm": 0.11344573646783829,
      "learning_rate": 4.803809523809524e-06,
      "loss": 0.3585,
      "step": 39890
    },
    {
      "epoch": 11.4,
      "grad_norm": 0.19716773927211761,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.0944,
      "step": 39900
    },
    {
      "epoch": 11.402857142857142,
      "grad_norm": 0.07434801012277603,
      "learning_rate": 4.796190476190476e-06,
      "loss": 0.5996,
      "step": 39910
    },
    {
      "epoch": 11.405714285714286,
      "grad_norm": 0.23425033688545227,
      "learning_rate": 4.792380952380953e-06,
      "loss": 0.758,
      "step": 39920
    },
    {
      "epoch": 11.40857142857143,
      "grad_norm": 31.038278579711914,
      "learning_rate": 4.788571428571429e-06,
      "loss": 0.634,
      "step": 39930
    },
    {
      "epoch": 11.411428571428571,
      "grad_norm": 0.980040431022644,
      "learning_rate": 4.7847619047619055e-06,
      "loss": 0.263,
      "step": 39940
    },
    {
      "epoch": 11.414285714285715,
      "grad_norm": 0.2484193742275238,
      "learning_rate": 4.780952380952381e-06,
      "loss": 0.3602,
      "step": 39950
    },
    {
      "epoch": 11.417142857142856,
      "grad_norm": 0.25592026114463806,
      "learning_rate": 4.7771428571428575e-06,
      "loss": 0.1578,
      "step": 39960
    },
    {
      "epoch": 11.42,
      "grad_norm": 13.91031265258789,
      "learning_rate": 4.773333333333334e-06,
      "loss": 0.3009,
      "step": 39970
    },
    {
      "epoch": 11.422857142857143,
      "grad_norm": 0.547231912612915,
      "learning_rate": 4.76952380952381e-06,
      "loss": 0.6237,
      "step": 39980
    },
    {
      "epoch": 11.425714285714285,
      "grad_norm": 0.11405381560325623,
      "learning_rate": 4.765714285714286e-06,
      "loss": 0.1916,
      "step": 39990
    },
    {
      "epoch": 11.428571428571429,
      "grad_norm": 17.379962921142578,
      "learning_rate": 4.761904761904762e-06,
      "loss": 0.6463,
      "step": 40000
    },
    {
      "epoch": 11.43142857142857,
      "grad_norm": 0.5466387867927551,
      "learning_rate": 4.758095238095238e-06,
      "loss": 0.5307,
      "step": 40010
    },
    {
      "epoch": 11.434285714285714,
      "grad_norm": 3.3946592807769775,
      "learning_rate": 4.754285714285714e-06,
      "loss": 0.4169,
      "step": 40020
    },
    {
      "epoch": 11.437142857142858,
      "grad_norm": 0.4166390895843506,
      "learning_rate": 4.750476190476191e-06,
      "loss": 0.2616,
      "step": 40030
    },
    {
      "epoch": 11.44,
      "grad_norm": 0.19615641236305237,
      "learning_rate": 4.746666666666667e-06,
      "loss": 0.4458,
      "step": 40040
    },
    {
      "epoch": 11.442857142857143,
      "grad_norm": 0.22741878032684326,
      "learning_rate": 4.742857142857144e-06,
      "loss": 0.5422,
      "step": 40050
    },
    {
      "epoch": 11.445714285714285,
      "grad_norm": 0.05577961727976799,
      "learning_rate": 4.739047619047619e-06,
      "loss": 0.2279,
      "step": 40060
    },
    {
      "epoch": 11.448571428571428,
      "grad_norm": 0.206847682595253,
      "learning_rate": 4.735238095238096e-06,
      "loss": 0.2059,
      "step": 40070
    },
    {
      "epoch": 11.451428571428572,
      "grad_norm": 88.3373794555664,
      "learning_rate": 4.731428571428572e-06,
      "loss": 0.4266,
      "step": 40080
    },
    {
      "epoch": 11.454285714285714,
      "grad_norm": 0.18321727216243744,
      "learning_rate": 4.727619047619048e-06,
      "loss": 0.4598,
      "step": 40090
    },
    {
      "epoch": 11.457142857142857,
      "grad_norm": 0.051961060613393784,
      "learning_rate": 4.723809523809524e-06,
      "loss": 0.4639,
      "step": 40100
    },
    {
      "epoch": 11.46,
      "grad_norm": 23.915483474731445,
      "learning_rate": 4.7200000000000005e-06,
      "loss": 0.8284,
      "step": 40110
    },
    {
      "epoch": 11.462857142857143,
      "grad_norm": 0.3510664701461792,
      "learning_rate": 4.716190476190476e-06,
      "loss": 0.2987,
      "step": 40120
    },
    {
      "epoch": 11.465714285714286,
      "grad_norm": 15.53415298461914,
      "learning_rate": 4.7123809523809526e-06,
      "loss": 0.4911,
      "step": 40130
    },
    {
      "epoch": 11.468571428571428,
      "grad_norm": 0.3916047215461731,
      "learning_rate": 4.708571428571429e-06,
      "loss": 0.1712,
      "step": 40140
    },
    {
      "epoch": 11.471428571428572,
      "grad_norm": 15.019845962524414,
      "learning_rate": 4.704761904761905e-06,
      "loss": 0.3761,
      "step": 40150
    },
    {
      "epoch": 11.474285714285715,
      "grad_norm": 14.228103637695312,
      "learning_rate": 4.700952380952382e-06,
      "loss": 0.2236,
      "step": 40160
    },
    {
      "epoch": 11.477142857142857,
      "grad_norm": 0.159830704331398,
      "learning_rate": 4.6971428571428574e-06,
      "loss": 0.4448,
      "step": 40170
    },
    {
      "epoch": 11.48,
      "grad_norm": 41.45811462402344,
      "learning_rate": 4.693333333333334e-06,
      "loss": 0.87,
      "step": 40180
    },
    {
      "epoch": 11.482857142857142,
      "grad_norm": 0.41576629877090454,
      "learning_rate": 4.68952380952381e-06,
      "loss": 0.1868,
      "step": 40190
    },
    {
      "epoch": 11.485714285714286,
      "grad_norm": 0.2727440893650055,
      "learning_rate": 4.685714285714286e-06,
      "loss": 0.0653,
      "step": 40200
    },
    {
      "epoch": 11.48857142857143,
      "grad_norm": 0.271106481552124,
      "learning_rate": 4.681904761904762e-06,
      "loss": 0.1719,
      "step": 40210
    },
    {
      "epoch": 11.491428571428571,
      "grad_norm": 0.4508534371852875,
      "learning_rate": 4.678095238095238e-06,
      "loss": 0.3223,
      "step": 40220
    },
    {
      "epoch": 11.494285714285715,
      "grad_norm": 2.2686452865600586,
      "learning_rate": 4.674285714285714e-06,
      "loss": 0.4495,
      "step": 40230
    },
    {
      "epoch": 11.497142857142856,
      "grad_norm": 0.13180942833423615,
      "learning_rate": 4.670476190476191e-06,
      "loss": 0.772,
      "step": 40240
    },
    {
      "epoch": 11.5,
      "grad_norm": 0.3234667479991913,
      "learning_rate": 4.666666666666667e-06,
      "loss": 0.4044,
      "step": 40250
    },
    {
      "epoch": 11.502857142857144,
      "grad_norm": 0.1922946721315384,
      "learning_rate": 4.662857142857144e-06,
      "loss": 0.2878,
      "step": 40260
    },
    {
      "epoch": 11.505714285714285,
      "grad_norm": 0.2791931927204132,
      "learning_rate": 4.659047619047619e-06,
      "loss": 0.4238,
      "step": 40270
    },
    {
      "epoch": 11.508571428571429,
      "grad_norm": 2.6734426021575928,
      "learning_rate": 4.655238095238096e-06,
      "loss": 0.174,
      "step": 40280
    },
    {
      "epoch": 11.51142857142857,
      "grad_norm": 0.28261035680770874,
      "learning_rate": 4.651428571428572e-06,
      "loss": 0.3784,
      "step": 40290
    },
    {
      "epoch": 11.514285714285714,
      "grad_norm": 1.4755014181137085,
      "learning_rate": 4.647619047619048e-06,
      "loss": 0.2197,
      "step": 40300
    },
    {
      "epoch": 11.517142857142858,
      "grad_norm": 25.408300399780273,
      "learning_rate": 4.643809523809524e-06,
      "loss": 0.4944,
      "step": 40310
    },
    {
      "epoch": 11.52,
      "grad_norm": 0.160518079996109,
      "learning_rate": 4.6400000000000005e-06,
      "loss": 0.1602,
      "step": 40320
    },
    {
      "epoch": 11.522857142857143,
      "grad_norm": 13.564806938171387,
      "learning_rate": 4.636190476190476e-06,
      "loss": 0.3493,
      "step": 40330
    },
    {
      "epoch": 11.525714285714285,
      "grad_norm": 0.24314665794372559,
      "learning_rate": 4.6323809523809525e-06,
      "loss": 0.3134,
      "step": 40340
    },
    {
      "epoch": 11.528571428571428,
      "grad_norm": 16.53590202331543,
      "learning_rate": 4.628571428571429e-06,
      "loss": 0.2383,
      "step": 40350
    },
    {
      "epoch": 11.531428571428572,
      "grad_norm": 0.2022753208875656,
      "learning_rate": 4.624761904761905e-06,
      "loss": 0.5695,
      "step": 40360
    },
    {
      "epoch": 11.534285714285714,
      "grad_norm": 14.121329307556152,
      "learning_rate": 4.620952380952382e-06,
      "loss": 0.2132,
      "step": 40370
    },
    {
      "epoch": 11.537142857142857,
      "grad_norm": 13.54965877532959,
      "learning_rate": 4.617142857142857e-06,
      "loss": 0.506,
      "step": 40380
    },
    {
      "epoch": 11.54,
      "grad_norm": 0.3446183502674103,
      "learning_rate": 4.613333333333334e-06,
      "loss": 0.6323,
      "step": 40390
    },
    {
      "epoch": 11.542857142857143,
      "grad_norm": 0.8093492388725281,
      "learning_rate": 4.60952380952381e-06,
      "loss": 0.611,
      "step": 40400
    },
    {
      "epoch": 11.545714285714286,
      "grad_norm": 0.2859642505645752,
      "learning_rate": 4.605714285714286e-06,
      "loss": 0.7853,
      "step": 40410
    },
    {
      "epoch": 11.548571428571428,
      "grad_norm": 14.08768367767334,
      "learning_rate": 4.601904761904762e-06,
      "loss": 0.2455,
      "step": 40420
    },
    {
      "epoch": 11.551428571428572,
      "grad_norm": 0.19375482201576233,
      "learning_rate": 4.598095238095239e-06,
      "loss": 0.1174,
      "step": 40430
    },
    {
      "epoch": 11.554285714285715,
      "grad_norm": 0.21759453415870667,
      "learning_rate": 4.594285714285714e-06,
      "loss": 0.6919,
      "step": 40440
    },
    {
      "epoch": 11.557142857142857,
      "grad_norm": 13.858524322509766,
      "learning_rate": 4.590476190476191e-06,
      "loss": 0.3626,
      "step": 40450
    },
    {
      "epoch": 11.56,
      "grad_norm": 226.92982482910156,
      "learning_rate": 4.586666666666667e-06,
      "loss": 0.4498,
      "step": 40460
    },
    {
      "epoch": 11.562857142857142,
      "grad_norm": 19.460336685180664,
      "learning_rate": 4.5828571428571435e-06,
      "loss": 0.6315,
      "step": 40470
    },
    {
      "epoch": 11.565714285714286,
      "grad_norm": 1.1968069076538086,
      "learning_rate": 4.57904761904762e-06,
      "loss": 0.5947,
      "step": 40480
    },
    {
      "epoch": 11.56857142857143,
      "grad_norm": 0.4451476037502289,
      "learning_rate": 4.5752380952380955e-06,
      "loss": 0.4976,
      "step": 40490
    },
    {
      "epoch": 11.571428571428571,
      "grad_norm": 1.4795522689819336,
      "learning_rate": 4.571428571428572e-06,
      "loss": 0.4557,
      "step": 40500
    },
    {
      "epoch": 11.574285714285715,
      "grad_norm": 0.42599475383758545,
      "learning_rate": 4.5676190476190475e-06,
      "loss": 0.0106,
      "step": 40510
    },
    {
      "epoch": 11.577142857142857,
      "grad_norm": 0.15071797370910645,
      "learning_rate": 4.563809523809524e-06,
      "loss": 0.1857,
      "step": 40520
    },
    {
      "epoch": 11.58,
      "grad_norm": 32.567020416259766,
      "learning_rate": 4.56e-06,
      "loss": 0.655,
      "step": 40530
    },
    {
      "epoch": 11.582857142857144,
      "grad_norm": 24.537002563476562,
      "learning_rate": 4.556190476190477e-06,
      "loss": 0.85,
      "step": 40540
    },
    {
      "epoch": 11.585714285714285,
      "grad_norm": 16.581586837768555,
      "learning_rate": 4.552380952380952e-06,
      "loss": 0.5916,
      "step": 40550
    },
    {
      "epoch": 11.588571428571429,
      "grad_norm": 0.9586451649665833,
      "learning_rate": 4.548571428571429e-06,
      "loss": 0.2601,
      "step": 40560
    },
    {
      "epoch": 11.59142857142857,
      "grad_norm": 0.4252161681652069,
      "learning_rate": 4.544761904761905e-06,
      "loss": 0.4588,
      "step": 40570
    },
    {
      "epoch": 11.594285714285714,
      "grad_norm": 0.34135884046554565,
      "learning_rate": 4.540952380952382e-06,
      "loss": 0.0928,
      "step": 40580
    },
    {
      "epoch": 11.597142857142858,
      "grad_norm": 0.3605310916900635,
      "learning_rate": 4.537142857142858e-06,
      "loss": 0.5617,
      "step": 40590
    },
    {
      "epoch": 11.6,
      "grad_norm": 0.23481449484825134,
      "learning_rate": 4.533333333333334e-06,
      "loss": 0.5716,
      "step": 40600
    },
    {
      "epoch": 11.602857142857143,
      "grad_norm": 0.4890773296356201,
      "learning_rate": 4.52952380952381e-06,
      "loss": 0.6667,
      "step": 40610
    },
    {
      "epoch": 11.605714285714285,
      "grad_norm": 2.6525843143463135,
      "learning_rate": 4.525714285714286e-06,
      "loss": 0.4341,
      "step": 40620
    },
    {
      "epoch": 11.608571428571429,
      "grad_norm": 0.4881201684474945,
      "learning_rate": 4.521904761904762e-06,
      "loss": 0.5201,
      "step": 40630
    },
    {
      "epoch": 11.611428571428572,
      "grad_norm": 0.8526688814163208,
      "learning_rate": 4.5180952380952386e-06,
      "loss": 0.4512,
      "step": 40640
    },
    {
      "epoch": 11.614285714285714,
      "grad_norm": 15.188516616821289,
      "learning_rate": 4.514285714285714e-06,
      "loss": 0.5018,
      "step": 40650
    },
    {
      "epoch": 11.617142857142857,
      "grad_norm": 0.5180122256278992,
      "learning_rate": 4.5104761904761906e-06,
      "loss": 0.2493,
      "step": 40660
    },
    {
      "epoch": 11.62,
      "grad_norm": 0.7193624973297119,
      "learning_rate": 4.506666666666667e-06,
      "loss": 0.1875,
      "step": 40670
    },
    {
      "epoch": 11.622857142857143,
      "grad_norm": 0.31337758898735046,
      "learning_rate": 4.5028571428571434e-06,
      "loss": 0.3699,
      "step": 40680
    },
    {
      "epoch": 11.625714285714286,
      "grad_norm": 0.3351084291934967,
      "learning_rate": 4.49904761904762e-06,
      "loss": 0.6504,
      "step": 40690
    },
    {
      "epoch": 11.628571428571428,
      "grad_norm": 0.6974260210990906,
      "learning_rate": 4.4952380952380954e-06,
      "loss": 0.5013,
      "step": 40700
    },
    {
      "epoch": 11.631428571428572,
      "grad_norm": 0.24715465307235718,
      "learning_rate": 4.491428571428572e-06,
      "loss": 0.3391,
      "step": 40710
    },
    {
      "epoch": 11.634285714285713,
      "grad_norm": 0.2543594241142273,
      "learning_rate": 4.487619047619048e-06,
      "loss": 0.5882,
      "step": 40720
    },
    {
      "epoch": 11.637142857142857,
      "grad_norm": 0.3960145115852356,
      "learning_rate": 4.483809523809524e-06,
      "loss": 0.408,
      "step": 40730
    },
    {
      "epoch": 11.64,
      "grad_norm": 13.660852432250977,
      "learning_rate": 4.48e-06,
      "loss": 0.4806,
      "step": 40740
    },
    {
      "epoch": 11.642857142857142,
      "grad_norm": 0.40067926049232483,
      "learning_rate": 4.476190476190477e-06,
      "loss": 0.8512,
      "step": 40750
    },
    {
      "epoch": 11.645714285714286,
      "grad_norm": 0.7863921523094177,
      "learning_rate": 4.472380952380952e-06,
      "loss": 0.4578,
      "step": 40760
    },
    {
      "epoch": 11.64857142857143,
      "grad_norm": 12.964154243469238,
      "learning_rate": 4.468571428571429e-06,
      "loss": 0.4543,
      "step": 40770
    },
    {
      "epoch": 11.651428571428571,
      "grad_norm": 0.38701844215393066,
      "learning_rate": 4.464761904761905e-06,
      "loss": 0.2734,
      "step": 40780
    },
    {
      "epoch": 11.654285714285715,
      "grad_norm": 2.419198989868164,
      "learning_rate": 4.460952380952382e-06,
      "loss": 0.0413,
      "step": 40790
    },
    {
      "epoch": 11.657142857142857,
      "grad_norm": 13.552431106567383,
      "learning_rate": 4.457142857142858e-06,
      "loss": 0.3583,
      "step": 40800
    },
    {
      "epoch": 11.66,
      "grad_norm": 0.8522095680236816,
      "learning_rate": 4.453333333333334e-06,
      "loss": 0.5086,
      "step": 40810
    },
    {
      "epoch": 11.662857142857142,
      "grad_norm": 48.04620361328125,
      "learning_rate": 4.44952380952381e-06,
      "loss": 0.6256,
      "step": 40820
    },
    {
      "epoch": 11.665714285714285,
      "grad_norm": 0.18589094281196594,
      "learning_rate": 4.445714285714286e-06,
      "loss": 0.496,
      "step": 40830
    },
    {
      "epoch": 11.668571428571429,
      "grad_norm": 24.690998077392578,
      "learning_rate": 4.441904761904762e-06,
      "loss": 0.8222,
      "step": 40840
    },
    {
      "epoch": 11.67142857142857,
      "grad_norm": 0.5094448924064636,
      "learning_rate": 4.4380952380952385e-06,
      "loss": 0.8721,
      "step": 40850
    },
    {
      "epoch": 11.674285714285714,
      "grad_norm": 0.34653425216674805,
      "learning_rate": 4.434285714285715e-06,
      "loss": 0.3329,
      "step": 40860
    },
    {
      "epoch": 11.677142857142858,
      "grad_norm": 0.4752092659473419,
      "learning_rate": 4.4304761904761905e-06,
      "loss": 0.3027,
      "step": 40870
    },
    {
      "epoch": 11.68,
      "grad_norm": 47.15058135986328,
      "learning_rate": 4.426666666666667e-06,
      "loss": 0.4148,
      "step": 40880
    },
    {
      "epoch": 11.682857142857143,
      "grad_norm": 0.2691717743873596,
      "learning_rate": 4.422857142857143e-06,
      "loss": 0.4553,
      "step": 40890
    },
    {
      "epoch": 11.685714285714285,
      "grad_norm": 14.712244033813477,
      "learning_rate": 4.41904761904762e-06,
      "loss": 0.4901,
      "step": 40900
    },
    {
      "epoch": 11.688571428571429,
      "grad_norm": 0.25638633966445923,
      "learning_rate": 4.415238095238095e-06,
      "loss": 0.4306,
      "step": 40910
    },
    {
      "epoch": 11.691428571428572,
      "grad_norm": 0.2966870367527008,
      "learning_rate": 4.411428571428572e-06,
      "loss": 0.3431,
      "step": 40920
    },
    {
      "epoch": 11.694285714285714,
      "grad_norm": 0.9117028117179871,
      "learning_rate": 4.407619047619048e-06,
      "loss": 0.2236,
      "step": 40930
    },
    {
      "epoch": 11.697142857142858,
      "grad_norm": 1.245977759361267,
      "learning_rate": 4.403809523809524e-06,
      "loss": 0.4326,
      "step": 40940
    },
    {
      "epoch": 11.7,
      "grad_norm": 24.133377075195312,
      "learning_rate": 4.4e-06,
      "loss": 0.9198,
      "step": 40950
    },
    {
      "epoch": 11.702857142857143,
      "grad_norm": 61.77004623413086,
      "learning_rate": 4.396190476190477e-06,
      "loss": 0.4837,
      "step": 40960
    },
    {
      "epoch": 11.705714285714286,
      "grad_norm": 0.5072124600410461,
      "learning_rate": 4.392380952380953e-06,
      "loss": 0.3247,
      "step": 40970
    },
    {
      "epoch": 11.708571428571428,
      "grad_norm": 13.79956340789795,
      "learning_rate": 4.388571428571429e-06,
      "loss": 0.2873,
      "step": 40980
    },
    {
      "epoch": 11.711428571428572,
      "grad_norm": 0.4317510426044464,
      "learning_rate": 4.384761904761905e-06,
      "loss": 0.2247,
      "step": 40990
    },
    {
      "epoch": 11.714285714285714,
      "grad_norm": 0.8249155879020691,
      "learning_rate": 4.3809523809523815e-06,
      "loss": 0.3342,
      "step": 41000
    },
    {
      "epoch": 11.717142857142857,
      "grad_norm": 0.27572283148765564,
      "learning_rate": 4.377142857142858e-06,
      "loss": 0.5067,
      "step": 41010
    },
    {
      "epoch": 11.72,
      "grad_norm": 37.70621871948242,
      "learning_rate": 4.3733333333333335e-06,
      "loss": 0.5005,
      "step": 41020
    },
    {
      "epoch": 11.722857142857142,
      "grad_norm": 1.799126148223877,
      "learning_rate": 4.36952380952381e-06,
      "loss": 0.3602,
      "step": 41030
    },
    {
      "epoch": 11.725714285714286,
      "grad_norm": 0.2503150701522827,
      "learning_rate": 4.3657142857142855e-06,
      "loss": 0.093,
      "step": 41040
    },
    {
      "epoch": 11.728571428571428,
      "grad_norm": 610.4248046875,
      "learning_rate": 4.361904761904762e-06,
      "loss": 0.5042,
      "step": 41050
    },
    {
      "epoch": 11.731428571428571,
      "grad_norm": 0.6615870594978333,
      "learning_rate": 4.358095238095238e-06,
      "loss": 0.2966,
      "step": 41060
    },
    {
      "epoch": 11.734285714285715,
      "grad_norm": 0.4054712653160095,
      "learning_rate": 4.354285714285715e-06,
      "loss": 0.2188,
      "step": 41070
    },
    {
      "epoch": 11.737142857142857,
      "grad_norm": 135.17372131347656,
      "learning_rate": 4.350476190476191e-06,
      "loss": 0.4073,
      "step": 41080
    },
    {
      "epoch": 11.74,
      "grad_norm": 4.764344692230225,
      "learning_rate": 4.346666666666667e-06,
      "loss": 0.339,
      "step": 41090
    },
    {
      "epoch": 11.742857142857144,
      "grad_norm": 40.300559997558594,
      "learning_rate": 4.342857142857143e-06,
      "loss": 0.3523,
      "step": 41100
    },
    {
      "epoch": 11.745714285714286,
      "grad_norm": 0.1602202206850052,
      "learning_rate": 4.33904761904762e-06,
      "loss": 0.1041,
      "step": 41110
    },
    {
      "epoch": 11.748571428571429,
      "grad_norm": 0.15298932790756226,
      "learning_rate": 4.335238095238095e-06,
      "loss": 0.1159,
      "step": 41120
    },
    {
      "epoch": 11.751428571428571,
      "grad_norm": 0.7886979579925537,
      "learning_rate": 4.331428571428572e-06,
      "loss": 0.8092,
      "step": 41130
    },
    {
      "epoch": 11.754285714285714,
      "grad_norm": 15.477039337158203,
      "learning_rate": 4.327619047619048e-06,
      "loss": 0.1346,
      "step": 41140
    },
    {
      "epoch": 11.757142857142856,
      "grad_norm": 61.81188201904297,
      "learning_rate": 4.323809523809524e-06,
      "loss": 0.8015,
      "step": 41150
    },
    {
      "epoch": 11.76,
      "grad_norm": 0.10615354031324387,
      "learning_rate": 4.32e-06,
      "loss": 0.749,
      "step": 41160
    },
    {
      "epoch": 11.762857142857143,
      "grad_norm": 0.451953262090683,
      "learning_rate": 4.3161904761904766e-06,
      "loss": 0.9014,
      "step": 41170
    },
    {
      "epoch": 11.765714285714285,
      "grad_norm": 1.1230130195617676,
      "learning_rate": 4.312380952380953e-06,
      "loss": 0.5198,
      "step": 41180
    },
    {
      "epoch": 11.768571428571429,
      "grad_norm": 0.7388894557952881,
      "learning_rate": 4.3085714285714294e-06,
      "loss": 0.6403,
      "step": 41190
    },
    {
      "epoch": 11.771428571428572,
      "grad_norm": 0.20067322254180908,
      "learning_rate": 4.304761904761905e-06,
      "loss": 0.2307,
      "step": 41200
    },
    {
      "epoch": 11.774285714285714,
      "grad_norm": 0.4845329225063324,
      "learning_rate": 4.3009523809523814e-06,
      "loss": 0.2313,
      "step": 41210
    },
    {
      "epoch": 11.777142857142858,
      "grad_norm": 15.490757942199707,
      "learning_rate": 4.297142857142858e-06,
      "loss": 0.2255,
      "step": 41220
    },
    {
      "epoch": 11.78,
      "grad_norm": 0.1047227755188942,
      "learning_rate": 4.2933333333333334e-06,
      "loss": 0.4292,
      "step": 41230
    },
    {
      "epoch": 11.782857142857143,
      "grad_norm": 39.25006866455078,
      "learning_rate": 4.28952380952381e-06,
      "loss": 0.7302,
      "step": 41240
    },
    {
      "epoch": 11.785714285714286,
      "grad_norm": 16.88646125793457,
      "learning_rate": 4.2857142857142855e-06,
      "loss": 0.5758,
      "step": 41250
    },
    {
      "epoch": 11.788571428571428,
      "grad_norm": 32.22452163696289,
      "learning_rate": 4.281904761904762e-06,
      "loss": 0.7531,
      "step": 41260
    },
    {
      "epoch": 11.791428571428572,
      "grad_norm": 0.9694523811340332,
      "learning_rate": 4.278095238095238e-06,
      "loss": 0.3056,
      "step": 41270
    },
    {
      "epoch": 11.794285714285714,
      "grad_norm": 0.6173496246337891,
      "learning_rate": 4.274285714285715e-06,
      "loss": 0.1697,
      "step": 41280
    },
    {
      "epoch": 11.797142857142857,
      "grad_norm": 0.013379628770053387,
      "learning_rate": 4.270476190476191e-06,
      "loss": 0.5281,
      "step": 41290
    },
    {
      "epoch": 11.8,
      "grad_norm": 0.27377259731292725,
      "learning_rate": 4.266666666666668e-06,
      "loss": 0.5217,
      "step": 41300
    },
    {
      "epoch": 11.802857142857142,
      "grad_norm": 13.050528526306152,
      "learning_rate": 4.262857142857143e-06,
      "loss": 0.582,
      "step": 41310
    },
    {
      "epoch": 11.805714285714286,
      "grad_norm": 17.73834228515625,
      "learning_rate": 4.25904761904762e-06,
      "loss": 0.5435,
      "step": 41320
    },
    {
      "epoch": 11.808571428571428,
      "grad_norm": 31.323638916015625,
      "learning_rate": 4.255238095238095e-06,
      "loss": 0.6871,
      "step": 41330
    },
    {
      "epoch": 11.811428571428571,
      "grad_norm": 0.13800334930419922,
      "learning_rate": 4.251428571428572e-06,
      "loss": 0.1902,
      "step": 41340
    },
    {
      "epoch": 11.814285714285715,
      "grad_norm": 12.339713096618652,
      "learning_rate": 4.247619047619048e-06,
      "loss": 0.5084,
      "step": 41350
    },
    {
      "epoch": 11.817142857142857,
      "grad_norm": 0.3343784511089325,
      "learning_rate": 4.243809523809524e-06,
      "loss": 0.6482,
      "step": 41360
    },
    {
      "epoch": 11.82,
      "grad_norm": 20.60103988647461,
      "learning_rate": 4.24e-06,
      "loss": 0.1552,
      "step": 41370
    },
    {
      "epoch": 11.822857142857142,
      "grad_norm": 13.959946632385254,
      "learning_rate": 4.2361904761904765e-06,
      "loss": 0.5265,
      "step": 41380
    },
    {
      "epoch": 11.825714285714286,
      "grad_norm": 0.18015435338020325,
      "learning_rate": 4.232380952380953e-06,
      "loss": 0.3077,
      "step": 41390
    },
    {
      "epoch": 11.82857142857143,
      "grad_norm": 12.799849510192871,
      "learning_rate": 4.228571428571429e-06,
      "loss": 0.5667,
      "step": 41400
    },
    {
      "epoch": 11.831428571428571,
      "grad_norm": 0.6889582276344299,
      "learning_rate": 4.224761904761905e-06,
      "loss": 0.2636,
      "step": 41410
    },
    {
      "epoch": 11.834285714285715,
      "grad_norm": 14.770344734191895,
      "learning_rate": 4.220952380952381e-06,
      "loss": 0.3269,
      "step": 41420
    },
    {
      "epoch": 11.837142857142858,
      "grad_norm": 0.5068164467811584,
      "learning_rate": 4.217142857142858e-06,
      "loss": 0.2299,
      "step": 41430
    },
    {
      "epoch": 11.84,
      "grad_norm": 0.9129427671432495,
      "learning_rate": 4.213333333333333e-06,
      "loss": 0.4352,
      "step": 41440
    },
    {
      "epoch": 11.842857142857143,
      "grad_norm": 0.15119341015815735,
      "learning_rate": 4.20952380952381e-06,
      "loss": 0.2458,
      "step": 41450
    },
    {
      "epoch": 11.845714285714285,
      "grad_norm": 1.0454407930374146,
      "learning_rate": 4.205714285714286e-06,
      "loss": 0.4302,
      "step": 41460
    },
    {
      "epoch": 11.848571428571429,
      "grad_norm": 0.18460102379322052,
      "learning_rate": 4.201904761904762e-06,
      "loss": 0.3341,
      "step": 41470
    },
    {
      "epoch": 11.85142857142857,
      "grad_norm": 7.723759174346924,
      "learning_rate": 4.198095238095238e-06,
      "loss": 0.3574,
      "step": 41480
    },
    {
      "epoch": 11.854285714285714,
      "grad_norm": 0.1517987847328186,
      "learning_rate": 4.194285714285715e-06,
      "loss": 0.4072,
      "step": 41490
    },
    {
      "epoch": 11.857142857142858,
      "grad_norm": 0.40394535660743713,
      "learning_rate": 4.190476190476191e-06,
      "loss": 0.1717,
      "step": 41500
    },
    {
      "epoch": 11.86,
      "grad_norm": 0.5354711413383484,
      "learning_rate": 4.1866666666666675e-06,
      "loss": 0.4068,
      "step": 41510
    },
    {
      "epoch": 11.862857142857143,
      "grad_norm": 0.2707016170024872,
      "learning_rate": 4.182857142857143e-06,
      "loss": 0.2018,
      "step": 41520
    },
    {
      "epoch": 11.865714285714287,
      "grad_norm": 15.933332443237305,
      "learning_rate": 4.1790476190476195e-06,
      "loss": 0.5706,
      "step": 41530
    },
    {
      "epoch": 11.868571428571428,
      "grad_norm": 0.15077942609786987,
      "learning_rate": 4.175238095238095e-06,
      "loss": 0.1128,
      "step": 41540
    },
    {
      "epoch": 11.871428571428572,
      "grad_norm": 0.2496262490749359,
      "learning_rate": 4.1714285714285715e-06,
      "loss": 0.0047,
      "step": 41550
    },
    {
      "epoch": 11.874285714285714,
      "grad_norm": 0.11065435409545898,
      "learning_rate": 4.167619047619048e-06,
      "loss": 0.199,
      "step": 41560
    },
    {
      "epoch": 11.877142857142857,
      "grad_norm": 0.5418339967727661,
      "learning_rate": 4.163809523809524e-06,
      "loss": 0.3307,
      "step": 41570
    },
    {
      "epoch": 11.88,
      "grad_norm": 0.05300494655966759,
      "learning_rate": 4.16e-06,
      "loss": 0.5066,
      "step": 41580
    },
    {
      "epoch": 11.882857142857143,
      "grad_norm": 0.15420924127101898,
      "learning_rate": 4.156190476190476e-06,
      "loss": 0.0037,
      "step": 41590
    },
    {
      "epoch": 11.885714285714286,
      "grad_norm": 0.3323269784450531,
      "learning_rate": 4.152380952380953e-06,
      "loss": 0.2953,
      "step": 41600
    },
    {
      "epoch": 11.888571428571428,
      "grad_norm": 13.158236503601074,
      "learning_rate": 4.148571428571429e-06,
      "loss": 0.3785,
      "step": 41610
    },
    {
      "epoch": 11.891428571428571,
      "grad_norm": 0.24394522607326508,
      "learning_rate": 4.144761904761906e-06,
      "loss": 0.3263,
      "step": 41620
    },
    {
      "epoch": 11.894285714285715,
      "grad_norm": 0.2713843882083893,
      "learning_rate": 4.140952380952381e-06,
      "loss": 0.1271,
      "step": 41630
    },
    {
      "epoch": 11.897142857142857,
      "grad_norm": 29.631929397583008,
      "learning_rate": 4.137142857142858e-06,
      "loss": 0.5507,
      "step": 41640
    },
    {
      "epoch": 11.9,
      "grad_norm": 17.31025505065918,
      "learning_rate": 4.133333333333333e-06,
      "loss": 0.4784,
      "step": 41650
    },
    {
      "epoch": 11.902857142857142,
      "grad_norm": 13.070738792419434,
      "learning_rate": 4.12952380952381e-06,
      "loss": 0.5699,
      "step": 41660
    },
    {
      "epoch": 11.905714285714286,
      "grad_norm": 15.488500595092773,
      "learning_rate": 4.125714285714286e-06,
      "loss": 0.689,
      "step": 41670
    },
    {
      "epoch": 11.90857142857143,
      "grad_norm": 0.23375307023525238,
      "learning_rate": 4.1219047619047626e-06,
      "loss": 0.273,
      "step": 41680
    },
    {
      "epoch": 11.911428571428571,
      "grad_norm": 0.19890660047531128,
      "learning_rate": 4.118095238095238e-06,
      "loss": 0.2882,
      "step": 41690
    },
    {
      "epoch": 11.914285714285715,
      "grad_norm": 0.13241657614707947,
      "learning_rate": 4.114285714285715e-06,
      "loss": 0.3486,
      "step": 41700
    },
    {
      "epoch": 11.917142857142856,
      "grad_norm": 12.61276912689209,
      "learning_rate": 4.110476190476191e-06,
      "loss": 0.3941,
      "step": 41710
    },
    {
      "epoch": 11.92,
      "grad_norm": 15.010849952697754,
      "learning_rate": 4.1066666666666674e-06,
      "loss": 0.5432,
      "step": 41720
    },
    {
      "epoch": 11.922857142857143,
      "grad_norm": 10.216137886047363,
      "learning_rate": 4.102857142857143e-06,
      "loss": 0.6216,
      "step": 41730
    },
    {
      "epoch": 11.925714285714285,
      "grad_norm": 11.40255069732666,
      "learning_rate": 4.0990476190476195e-06,
      "loss": 0.442,
      "step": 41740
    },
    {
      "epoch": 11.928571428571429,
      "grad_norm": 0.2247340828180313,
      "learning_rate": 4.095238095238096e-06,
      "loss": 0.4681,
      "step": 41750
    },
    {
      "epoch": 11.93142857142857,
      "grad_norm": 154.48992919921875,
      "learning_rate": 4.0914285714285715e-06,
      "loss": 0.1912,
      "step": 41760
    },
    {
      "epoch": 11.934285714285714,
      "grad_norm": 2.1929101943969727,
      "learning_rate": 4.087619047619048e-06,
      "loss": 0.6564,
      "step": 41770
    },
    {
      "epoch": 11.937142857142858,
      "grad_norm": 13.444283485412598,
      "learning_rate": 4.083809523809524e-06,
      "loss": 0.272,
      "step": 41780
    },
    {
      "epoch": 11.94,
      "grad_norm": 0.566828191280365,
      "learning_rate": 4.08e-06,
      "loss": 0.266,
      "step": 41790
    },
    {
      "epoch": 11.942857142857143,
      "grad_norm": 12.820157051086426,
      "learning_rate": 4.076190476190476e-06,
      "loss": 0.4877,
      "step": 41800
    },
    {
      "epoch": 11.945714285714285,
      "grad_norm": 0.9040089845657349,
      "learning_rate": 4.072380952380953e-06,
      "loss": 0.1373,
      "step": 41810
    },
    {
      "epoch": 11.948571428571428,
      "grad_norm": 0.1679624766111374,
      "learning_rate": 4.068571428571429e-06,
      "loss": 0.17,
      "step": 41820
    },
    {
      "epoch": 11.951428571428572,
      "grad_norm": 0.15689164400100708,
      "learning_rate": 4.064761904761906e-06,
      "loss": 0.5988,
      "step": 41830
    },
    {
      "epoch": 11.954285714285714,
      "grad_norm": 16.25977897644043,
      "learning_rate": 4.060952380952381e-06,
      "loss": 0.1207,
      "step": 41840
    },
    {
      "epoch": 11.957142857142857,
      "grad_norm": 12.94935131072998,
      "learning_rate": 4.057142857142858e-06,
      "loss": 0.6881,
      "step": 41850
    },
    {
      "epoch": 11.96,
      "grad_norm": 13.876715660095215,
      "learning_rate": 4.053333333333333e-06,
      "loss": 0.561,
      "step": 41860
    },
    {
      "epoch": 11.962857142857143,
      "grad_norm": 0.3757514953613281,
      "learning_rate": 4.04952380952381e-06,
      "loss": 0.3429,
      "step": 41870
    },
    {
      "epoch": 11.965714285714286,
      "grad_norm": 35.20394515991211,
      "learning_rate": 4.045714285714286e-06,
      "loss": 0.6432,
      "step": 41880
    },
    {
      "epoch": 11.968571428571428,
      "grad_norm": 0.24592827260494232,
      "learning_rate": 4.0419047619047625e-06,
      "loss": 0.1109,
      "step": 41890
    },
    {
      "epoch": 11.971428571428572,
      "grad_norm": 51.382686614990234,
      "learning_rate": 4.038095238095238e-06,
      "loss": 0.5148,
      "step": 41900
    },
    {
      "epoch": 11.974285714285715,
      "grad_norm": 0.5983108282089233,
      "learning_rate": 4.0342857142857145e-06,
      "loss": 0.4843,
      "step": 41910
    },
    {
      "epoch": 11.977142857142857,
      "grad_norm": 0.40584442019462585,
      "learning_rate": 4.030476190476191e-06,
      "loss": 0.2486,
      "step": 41920
    },
    {
      "epoch": 11.98,
      "grad_norm": 0.4475821852684021,
      "learning_rate": 4.026666666666667e-06,
      "loss": 0.4405,
      "step": 41930
    },
    {
      "epoch": 11.982857142857142,
      "grad_norm": 0.2084665447473526,
      "learning_rate": 4.022857142857143e-06,
      "loss": 0.0054,
      "step": 41940
    },
    {
      "epoch": 11.985714285714286,
      "grad_norm": 32.52603530883789,
      "learning_rate": 4.019047619047619e-06,
      "loss": 0.4427,
      "step": 41950
    },
    {
      "epoch": 11.98857142857143,
      "grad_norm": 0.1192995011806488,
      "learning_rate": 4.015238095238096e-06,
      "loss": 0.4831,
      "step": 41960
    },
    {
      "epoch": 11.991428571428571,
      "grad_norm": 0.2880397439002991,
      "learning_rate": 4.011428571428571e-06,
      "loss": 0.0061,
      "step": 41970
    },
    {
      "epoch": 11.994285714285715,
      "grad_norm": 0.4167521893978119,
      "learning_rate": 4.007619047619048e-06,
      "loss": 0.6621,
      "step": 41980
    },
    {
      "epoch": 11.997142857142856,
      "grad_norm": 0.2935645580291748,
      "learning_rate": 4.003809523809524e-06,
      "loss": 0.3219,
      "step": 41990
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.20156167447566986,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.5516,
      "step": 42000
    },
    {
      "epoch": 12.0,
      "eval_accuracy": 0.8985215053763441,
      "eval_f1": 0.45878136200716846,
      "eval_loss": 0.510096549987793,
      "eval_precision": 0.735632183908046,
      "eval_recall": 0.3333333333333333,
      "eval_runtime": 47.9641,
      "eval_samples_per_second": 62.547,
      "eval_steps_per_second": 1.96,
      "step": 42000
    },
    {
      "epoch": 12.002857142857144,
      "grad_norm": 0.11704698204994202,
      "learning_rate": 3.996190476190476e-06,
      "loss": 0.5511,
      "step": 42010
    },
    {
      "epoch": 12.005714285714285,
      "grad_norm": 0.7548840045928955,
      "learning_rate": 3.992380952380953e-06,
      "loss": 0.4096,
      "step": 42020
    },
    {
      "epoch": 12.008571428571429,
      "grad_norm": 0.20417776703834534,
      "learning_rate": 3.988571428571429e-06,
      "loss": 0.2289,
      "step": 42030
    },
    {
      "epoch": 12.01142857142857,
      "grad_norm": 17.538761138916016,
      "learning_rate": 3.9847619047619055e-06,
      "loss": 0.5715,
      "step": 42040
    },
    {
      "epoch": 12.014285714285714,
      "grad_norm": 3.3889496326446533,
      "learning_rate": 3.980952380952381e-06,
      "loss": 0.509,
      "step": 42050
    },
    {
      "epoch": 12.017142857142858,
      "grad_norm": 0.4399711489677429,
      "learning_rate": 3.9771428571428575e-06,
      "loss": 0.2697,
      "step": 42060
    },
    {
      "epoch": 12.02,
      "grad_norm": 18.67270278930664,
      "learning_rate": 3.973333333333333e-06,
      "loss": 0.5644,
      "step": 42070
    },
    {
      "epoch": 12.022857142857143,
      "grad_norm": 1.0145304203033447,
      "learning_rate": 3.9695238095238096e-06,
      "loss": 0.3772,
      "step": 42080
    },
    {
      "epoch": 12.025714285714285,
      "grad_norm": 0.6233939528465271,
      "learning_rate": 3.965714285714286e-06,
      "loss": 0.2461,
      "step": 42090
    },
    {
      "epoch": 12.028571428571428,
      "grad_norm": 14.427820205688477,
      "learning_rate": 3.961904761904762e-06,
      "loss": 0.3218,
      "step": 42100
    },
    {
      "epoch": 12.031428571428572,
      "grad_norm": 0.3420810401439667,
      "learning_rate": 3.958095238095239e-06,
      "loss": 0.484,
      "step": 42110
    },
    {
      "epoch": 12.034285714285714,
      "grad_norm": 42.0069580078125,
      "learning_rate": 3.954285714285714e-06,
      "loss": 1.0207,
      "step": 42120
    },
    {
      "epoch": 12.037142857142857,
      "grad_norm": 0.0221423227339983,
      "learning_rate": 3.950476190476191e-06,
      "loss": 0.0052,
      "step": 42130
    },
    {
      "epoch": 12.04,
      "grad_norm": 0.5961572527885437,
      "learning_rate": 3.946666666666667e-06,
      "loss": 0.4958,
      "step": 42140
    },
    {
      "epoch": 12.042857142857143,
      "grad_norm": 0.2806037962436676,
      "learning_rate": 3.942857142857143e-06,
      "loss": 0.5846,
      "step": 42150
    },
    {
      "epoch": 12.045714285714286,
      "grad_norm": 0.2755005955696106,
      "learning_rate": 3.939047619047619e-06,
      "loss": 0.1482,
      "step": 42160
    },
    {
      "epoch": 12.048571428571428,
      "grad_norm": 0.1980709284543991,
      "learning_rate": 3.935238095238096e-06,
      "loss": 0.7972,
      "step": 42170
    },
    {
      "epoch": 12.051428571428572,
      "grad_norm": 12.039497375488281,
      "learning_rate": 3.931428571428571e-06,
      "loss": 0.4006,
      "step": 42180
    },
    {
      "epoch": 12.054285714285715,
      "grad_norm": 19.898574829101562,
      "learning_rate": 3.927619047619048e-06,
      "loss": 0.419,
      "step": 42190
    },
    {
      "epoch": 12.057142857142857,
      "grad_norm": 2.6714844703674316,
      "learning_rate": 3.923809523809524e-06,
      "loss": 0.3638,
      "step": 42200
    },
    {
      "epoch": 12.06,
      "grad_norm": 34.17988586425781,
      "learning_rate": 3.920000000000001e-06,
      "loss": 0.4091,
      "step": 42210
    },
    {
      "epoch": 12.062857142857142,
      "grad_norm": 0.20756053924560547,
      "learning_rate": 3.916190476190477e-06,
      "loss": 0.3107,
      "step": 42220
    },
    {
      "epoch": 12.065714285714286,
      "grad_norm": 0.3014075756072998,
      "learning_rate": 3.912380952380953e-06,
      "loss": 0.169,
      "step": 42230
    },
    {
      "epoch": 12.06857142857143,
      "grad_norm": 0.17227387428283691,
      "learning_rate": 3.908571428571429e-06,
      "loss": 0.0066,
      "step": 42240
    },
    {
      "epoch": 12.071428571428571,
      "grad_norm": 16.013343811035156,
      "learning_rate": 3.9047619047619055e-06,
      "loss": 0.2702,
      "step": 42250
    },
    {
      "epoch": 12.074285714285715,
      "grad_norm": 0.07549535483121872,
      "learning_rate": 3.900952380952381e-06,
      "loss": 0.3361,
      "step": 42260
    },
    {
      "epoch": 12.077142857142857,
      "grad_norm": 0.11862204223871231,
      "learning_rate": 3.8971428571428575e-06,
      "loss": 0.5977,
      "step": 42270
    },
    {
      "epoch": 12.08,
      "grad_norm": 27.67476463317871,
      "learning_rate": 3.893333333333333e-06,
      "loss": 0.5154,
      "step": 42280
    },
    {
      "epoch": 12.082857142857144,
      "grad_norm": 0.043867941945791245,
      "learning_rate": 3.8895238095238095e-06,
      "loss": 0.7924,
      "step": 42290
    },
    {
      "epoch": 12.085714285714285,
      "grad_norm": 0.04984625056385994,
      "learning_rate": 3.885714285714286e-06,
      "loss": 0.3998,
      "step": 42300
    },
    {
      "epoch": 12.088571428571429,
      "grad_norm": 0.23329424858093262,
      "learning_rate": 3.881904761904762e-06,
      "loss": 0.3479,
      "step": 42310
    },
    {
      "epoch": 12.09142857142857,
      "grad_norm": 0.38182172179222107,
      "learning_rate": 3.878095238095239e-06,
      "loss": 0.1376,
      "step": 42320
    },
    {
      "epoch": 12.094285714285714,
      "grad_norm": 0.005382690113037825,
      "learning_rate": 3.874285714285715e-06,
      "loss": 0.3066,
      "step": 42330
    },
    {
      "epoch": 12.097142857142858,
      "grad_norm": 1.3816254138946533,
      "learning_rate": 3.870476190476191e-06,
      "loss": 0.1022,
      "step": 42340
    },
    {
      "epoch": 12.1,
      "grad_norm": 0.2906356155872345,
      "learning_rate": 3.866666666666667e-06,
      "loss": 0.6118,
      "step": 42350
    },
    {
      "epoch": 12.102857142857143,
      "grad_norm": 39.97365951538086,
      "learning_rate": 3.862857142857143e-06,
      "loss": 0.3243,
      "step": 42360
    },
    {
      "epoch": 12.105714285714285,
      "grad_norm": 18.821367263793945,
      "learning_rate": 3.859047619047619e-06,
      "loss": 0.5782,
      "step": 42370
    },
    {
      "epoch": 12.108571428571429,
      "grad_norm": 14.846535682678223,
      "learning_rate": 3.855238095238096e-06,
      "loss": 0.179,
      "step": 42380
    },
    {
      "epoch": 12.111428571428572,
      "grad_norm": 0.36014384031295776,
      "learning_rate": 3.851428571428571e-06,
      "loss": 0.1141,
      "step": 42390
    },
    {
      "epoch": 12.114285714285714,
      "grad_norm": 0.02467803657054901,
      "learning_rate": 3.847619047619048e-06,
      "loss": 0.6076,
      "step": 42400
    },
    {
      "epoch": 12.117142857142857,
      "grad_norm": 23.85310935974121,
      "learning_rate": 3.843809523809524e-06,
      "loss": 0.206,
      "step": 42410
    },
    {
      "epoch": 12.12,
      "grad_norm": 0.7277748584747314,
      "learning_rate": 3.8400000000000005e-06,
      "loss": 0.2706,
      "step": 42420
    },
    {
      "epoch": 12.122857142857143,
      "grad_norm": 17.84757423400879,
      "learning_rate": 3.836190476190477e-06,
      "loss": 0.239,
      "step": 42430
    },
    {
      "epoch": 12.125714285714286,
      "grad_norm": 0.9074865579605103,
      "learning_rate": 3.8323809523809525e-06,
      "loss": 0.4663,
      "step": 42440
    },
    {
      "epoch": 12.128571428571428,
      "grad_norm": 47.98207092285156,
      "learning_rate": 3.828571428571429e-06,
      "loss": 0.7278,
      "step": 42450
    },
    {
      "epoch": 12.131428571428572,
      "grad_norm": 0.15726934373378754,
      "learning_rate": 3.824761904761905e-06,
      "loss": 0.4322,
      "step": 42460
    },
    {
      "epoch": 12.134285714285713,
      "grad_norm": 0.5649588704109192,
      "learning_rate": 3.820952380952381e-06,
      "loss": 0.1583,
      "step": 42470
    },
    {
      "epoch": 12.137142857142857,
      "grad_norm": 0.13900788128376007,
      "learning_rate": 3.817142857142857e-06,
      "loss": 0.1258,
      "step": 42480
    },
    {
      "epoch": 12.14,
      "grad_norm": 0.1043286994099617,
      "learning_rate": 3.813333333333334e-06,
      "loss": 0.0717,
      "step": 42490
    },
    {
      "epoch": 12.142857142857142,
      "grad_norm": 0.13966162502765656,
      "learning_rate": 3.80952380952381e-06,
      "loss": 0.8541,
      "step": 42500
    },
    {
      "epoch": 12.145714285714286,
      "grad_norm": 0.08977627009153366,
      "learning_rate": 3.805714285714286e-06,
      "loss": 0.2721,
      "step": 42510
    },
    {
      "epoch": 12.14857142857143,
      "grad_norm": 0.13295891880989075,
      "learning_rate": 3.8019047619047622e-06,
      "loss": 0.0071,
      "step": 42520
    },
    {
      "epoch": 12.151428571428571,
      "grad_norm": 0.1861453354358673,
      "learning_rate": 3.7980952380952387e-06,
      "loss": 0.2964,
      "step": 42530
    },
    {
      "epoch": 12.154285714285715,
      "grad_norm": 0.1920377016067505,
      "learning_rate": 3.7942857142857147e-06,
      "loss": 0.528,
      "step": 42540
    },
    {
      "epoch": 12.157142857142857,
      "grad_norm": 0.1991274058818817,
      "learning_rate": 3.7904761904761907e-06,
      "loss": 0.4068,
      "step": 42550
    },
    {
      "epoch": 12.16,
      "grad_norm": 33.92294692993164,
      "learning_rate": 3.7866666666666667e-06,
      "loss": 0.5255,
      "step": 42560
    },
    {
      "epoch": 12.162857142857144,
      "grad_norm": 0.15850110352039337,
      "learning_rate": 3.782857142857143e-06,
      "loss": 0.0819,
      "step": 42570
    },
    {
      "epoch": 12.165714285714285,
      "grad_norm": 0.37707170844078064,
      "learning_rate": 3.7790476190476196e-06,
      "loss": 0.1453,
      "step": 42580
    },
    {
      "epoch": 12.168571428571429,
      "grad_norm": 21.497085571289062,
      "learning_rate": 3.7752380952380956e-06,
      "loss": 0.2688,
      "step": 42590
    },
    {
      "epoch": 12.17142857142857,
      "grad_norm": 0.14612476527690887,
      "learning_rate": 3.771428571428572e-06,
      "loss": 0.151,
      "step": 42600
    },
    {
      "epoch": 12.174285714285714,
      "grad_norm": 0.1650892049074173,
      "learning_rate": 3.7676190476190476e-06,
      "loss": 0.4045,
      "step": 42610
    },
    {
      "epoch": 12.177142857142858,
      "grad_norm": 0.09136398881673813,
      "learning_rate": 3.763809523809524e-06,
      "loss": 0.4955,
      "step": 42620
    },
    {
      "epoch": 12.18,
      "grad_norm": 265.2677307128906,
      "learning_rate": 3.7600000000000004e-06,
      "loss": 0.3684,
      "step": 42630
    },
    {
      "epoch": 12.182857142857143,
      "grad_norm": 0.38757169246673584,
      "learning_rate": 3.7561904761904764e-06,
      "loss": 0.2179,
      "step": 42640
    },
    {
      "epoch": 12.185714285714285,
      "grad_norm": 0.09795590490102768,
      "learning_rate": 3.752380952380953e-06,
      "loss": 0.5104,
      "step": 42650
    },
    {
      "epoch": 12.188571428571429,
      "grad_norm": 0.6748751997947693,
      "learning_rate": 3.7485714285714284e-06,
      "loss": 0.0039,
      "step": 42660
    },
    {
      "epoch": 12.191428571428572,
      "grad_norm": 0.056367579847574234,
      "learning_rate": 3.744761904761905e-06,
      "loss": 0.0698,
      "step": 42670
    },
    {
      "epoch": 12.194285714285714,
      "grad_norm": 0.44658881425857544,
      "learning_rate": 3.7409523809523813e-06,
      "loss": 0.5248,
      "step": 42680
    },
    {
      "epoch": 12.197142857142858,
      "grad_norm": 19.26219367980957,
      "learning_rate": 3.7371428571428577e-06,
      "loss": 0.2684,
      "step": 42690
    },
    {
      "epoch": 12.2,
      "grad_norm": 44.176124572753906,
      "learning_rate": 3.7333333333333337e-06,
      "loss": 0.3445,
      "step": 42700
    },
    {
      "epoch": 12.202857142857143,
      "grad_norm": 12.3749361038208,
      "learning_rate": 3.72952380952381e-06,
      "loss": 0.235,
      "step": 42710
    },
    {
      "epoch": 12.205714285714286,
      "grad_norm": 0.08836884051561356,
      "learning_rate": 3.7257142857142857e-06,
      "loss": 0.3796,
      "step": 42720
    },
    {
      "epoch": 12.208571428571428,
      "grad_norm": 19.541412353515625,
      "learning_rate": 3.721904761904762e-06,
      "loss": 0.2454,
      "step": 42730
    },
    {
      "epoch": 12.211428571428572,
      "grad_norm": 0.007870241068303585,
      "learning_rate": 3.7180952380952386e-06,
      "loss": 0.1323,
      "step": 42740
    },
    {
      "epoch": 12.214285714285714,
      "grad_norm": 0.07391434907913208,
      "learning_rate": 3.7142857142857146e-06,
      "loss": 0.1533,
      "step": 42750
    },
    {
      "epoch": 12.217142857142857,
      "grad_norm": 0.024285294115543365,
      "learning_rate": 3.710476190476191e-06,
      "loss": 0.2578,
      "step": 42760
    },
    {
      "epoch": 12.22,
      "grad_norm": 0.048553187400102615,
      "learning_rate": 3.7066666666666666e-06,
      "loss": 0.3793,
      "step": 42770
    },
    {
      "epoch": 12.222857142857142,
      "grad_norm": 0.15065248310565948,
      "learning_rate": 3.702857142857143e-06,
      "loss": 0.5623,
      "step": 42780
    },
    {
      "epoch": 12.225714285714286,
      "grad_norm": 0.34515732526779175,
      "learning_rate": 3.6990476190476195e-06,
      "loss": 0.4587,
      "step": 42790
    },
    {
      "epoch": 12.228571428571428,
      "grad_norm": 0.1283632516860962,
      "learning_rate": 3.6952380952380955e-06,
      "loss": 0.6278,
      "step": 42800
    },
    {
      "epoch": 12.231428571428571,
      "grad_norm": 9.04577350616455,
      "learning_rate": 3.691428571428572e-06,
      "loss": 0.4351,
      "step": 42810
    },
    {
      "epoch": 12.234285714285715,
      "grad_norm": 9.22623348236084,
      "learning_rate": 3.6876190476190475e-06,
      "loss": 0.3135,
      "step": 42820
    },
    {
      "epoch": 12.237142857142857,
      "grad_norm": 1.1791495084762573,
      "learning_rate": 3.683809523809524e-06,
      "loss": 0.4819,
      "step": 42830
    },
    {
      "epoch": 12.24,
      "grad_norm": 0.18865354359149933,
      "learning_rate": 3.6800000000000003e-06,
      "loss": 0.2499,
      "step": 42840
    },
    {
      "epoch": 12.242857142857142,
      "grad_norm": 0.2677736282348633,
      "learning_rate": 3.6761904761904763e-06,
      "loss": 0.5092,
      "step": 42850
    },
    {
      "epoch": 12.245714285714286,
      "grad_norm": 19.617176055908203,
      "learning_rate": 3.6723809523809528e-06,
      "loss": 0.3803,
      "step": 42860
    },
    {
      "epoch": 12.248571428571429,
      "grad_norm": 18.867990493774414,
      "learning_rate": 3.668571428571429e-06,
      "loss": 0.1355,
      "step": 42870
    },
    {
      "epoch": 12.251428571428571,
      "grad_norm": 0.09635108709335327,
      "learning_rate": 3.6647619047619048e-06,
      "loss": 0.0036,
      "step": 42880
    },
    {
      "epoch": 12.254285714285714,
      "grad_norm": 0.9256359934806824,
      "learning_rate": 3.6609523809523812e-06,
      "loss": 0.5038,
      "step": 42890
    },
    {
      "epoch": 12.257142857142856,
      "grad_norm": 21.213003158569336,
      "learning_rate": 3.6571428571428576e-06,
      "loss": 0.2174,
      "step": 42900
    },
    {
      "epoch": 12.26,
      "grad_norm": 0.19332949817180634,
      "learning_rate": 3.6533333333333336e-06,
      "loss": 0.2111,
      "step": 42910
    },
    {
      "epoch": 12.262857142857143,
      "grad_norm": 22.14954948425293,
      "learning_rate": 3.64952380952381e-06,
      "loss": 0.122,
      "step": 42920
    },
    {
      "epoch": 12.265714285714285,
      "grad_norm": 15.483304023742676,
      "learning_rate": 3.6457142857142857e-06,
      "loss": 0.477,
      "step": 42930
    },
    {
      "epoch": 12.268571428571429,
      "grad_norm": 13.68285846710205,
      "learning_rate": 3.641904761904762e-06,
      "loss": 0.3786,
      "step": 42940
    },
    {
      "epoch": 12.271428571428572,
      "grad_norm": 25.79119110107422,
      "learning_rate": 3.6380952380952385e-06,
      "loss": 0.459,
      "step": 42950
    },
    {
      "epoch": 12.274285714285714,
      "grad_norm": 0.18718476593494415,
      "learning_rate": 3.6342857142857145e-06,
      "loss": 0.3344,
      "step": 42960
    },
    {
      "epoch": 12.277142857142858,
      "grad_norm": 1.2155157327651978,
      "learning_rate": 3.630476190476191e-06,
      "loss": 0.1511,
      "step": 42970
    },
    {
      "epoch": 12.28,
      "grad_norm": 23.337940216064453,
      "learning_rate": 3.6266666666666674e-06,
      "loss": 0.4391,
      "step": 42980
    },
    {
      "epoch": 12.282857142857143,
      "grad_norm": 0.08152065426111221,
      "learning_rate": 3.622857142857143e-06,
      "loss": 0.3817,
      "step": 42990
    },
    {
      "epoch": 12.285714285714286,
      "grad_norm": 2196.31689453125,
      "learning_rate": 3.6190476190476194e-06,
      "loss": 0.169,
      "step": 43000
    },
    {
      "epoch": 12.288571428571428,
      "grad_norm": 179.41925048828125,
      "learning_rate": 3.6152380952380954e-06,
      "loss": 0.2734,
      "step": 43010
    },
    {
      "epoch": 12.291428571428572,
      "grad_norm": 17.015605926513672,
      "learning_rate": 3.611428571428572e-06,
      "loss": 0.7054,
      "step": 43020
    },
    {
      "epoch": 12.294285714285714,
      "grad_norm": 66.44225311279297,
      "learning_rate": 3.6076190476190483e-06,
      "loss": 0.6129,
      "step": 43030
    },
    {
      "epoch": 12.297142857142857,
      "grad_norm": 0.3063419759273529,
      "learning_rate": 3.603809523809524e-06,
      "loss": 0.353,
      "step": 43040
    },
    {
      "epoch": 12.3,
      "grad_norm": 2.2643182277679443,
      "learning_rate": 3.6000000000000003e-06,
      "loss": 0.3596,
      "step": 43050
    },
    {
      "epoch": 12.302857142857142,
      "grad_norm": 40.682247161865234,
      "learning_rate": 3.5961904761904763e-06,
      "loss": 0.0823,
      "step": 43060
    },
    {
      "epoch": 12.305714285714286,
      "grad_norm": 0.16208788752555847,
      "learning_rate": 3.5923809523809527e-06,
      "loss": 0.556,
      "step": 43070
    },
    {
      "epoch": 12.308571428571428,
      "grad_norm": 0.16328483819961548,
      "learning_rate": 3.588571428571429e-06,
      "loss": 0.4237,
      "step": 43080
    },
    {
      "epoch": 12.311428571428571,
      "grad_norm": 0.4652596116065979,
      "learning_rate": 3.584761904761905e-06,
      "loss": 0.4417,
      "step": 43090
    },
    {
      "epoch": 12.314285714285715,
      "grad_norm": 0.1540592461824417,
      "learning_rate": 3.580952380952381e-06,
      "loss": 0.1243,
      "step": 43100
    },
    {
      "epoch": 12.317142857142857,
      "grad_norm": 16.86196517944336,
      "learning_rate": 3.5771428571428576e-06,
      "loss": 0.2334,
      "step": 43110
    },
    {
      "epoch": 12.32,
      "grad_norm": 0.0947941243648529,
      "learning_rate": 3.5733333333333336e-06,
      "loss": 0.5656,
      "step": 43120
    },
    {
      "epoch": 12.322857142857142,
      "grad_norm": 1.3626205921173096,
      "learning_rate": 3.56952380952381e-06,
      "loss": 0.5824,
      "step": 43130
    },
    {
      "epoch": 12.325714285714286,
      "grad_norm": 0.19016410410404205,
      "learning_rate": 3.5657142857142864e-06,
      "loss": 0.584,
      "step": 43140
    },
    {
      "epoch": 12.32857142857143,
      "grad_norm": 0.41821160912513733,
      "learning_rate": 3.561904761904762e-06,
      "loss": 0.0033,
      "step": 43150
    },
    {
      "epoch": 12.331428571428571,
      "grad_norm": 0.817209780216217,
      "learning_rate": 3.5580952380952384e-06,
      "loss": 0.6501,
      "step": 43160
    },
    {
      "epoch": 12.334285714285715,
      "grad_norm": 0.41242897510528564,
      "learning_rate": 3.5542857142857144e-06,
      "loss": 0.4268,
      "step": 43170
    },
    {
      "epoch": 12.337142857142856,
      "grad_norm": 13.763959884643555,
      "learning_rate": 3.550476190476191e-06,
      "loss": 0.3421,
      "step": 43180
    },
    {
      "epoch": 12.34,
      "grad_norm": 0.16044829785823822,
      "learning_rate": 3.5466666666666673e-06,
      "loss": 0.3902,
      "step": 43190
    },
    {
      "epoch": 12.342857142857143,
      "grad_norm": 0.0952632799744606,
      "learning_rate": 3.542857142857143e-06,
      "loss": 0.0039,
      "step": 43200
    },
    {
      "epoch": 12.345714285714285,
      "grad_norm": 0.07614998519420624,
      "learning_rate": 3.5390476190476193e-06,
      "loss": 0.2146,
      "step": 43210
    },
    {
      "epoch": 12.348571428571429,
      "grad_norm": 0.04849203675985336,
      "learning_rate": 3.5352380952380953e-06,
      "loss": 0.0033,
      "step": 43220
    },
    {
      "epoch": 12.35142857142857,
      "grad_norm": 0.0691809132695198,
      "learning_rate": 3.5314285714285717e-06,
      "loss": 0.2217,
      "step": 43230
    },
    {
      "epoch": 12.354285714285714,
      "grad_norm": 27.792898178100586,
      "learning_rate": 3.527619047619048e-06,
      "loss": 0.9165,
      "step": 43240
    },
    {
      "epoch": 12.357142857142858,
      "grad_norm": 0.6346774697303772,
      "learning_rate": 3.523809523809524e-06,
      "loss": 0.2217,
      "step": 43250
    },
    {
      "epoch": 12.36,
      "grad_norm": 22.959131240844727,
      "learning_rate": 3.52e-06,
      "loss": 0.3083,
      "step": 43260
    },
    {
      "epoch": 12.362857142857143,
      "grad_norm": 0.04975513368844986,
      "learning_rate": 3.516190476190476e-06,
      "loss": 0.0041,
      "step": 43270
    },
    {
      "epoch": 12.365714285714287,
      "grad_norm": 0.252949982881546,
      "learning_rate": 3.5123809523809526e-06,
      "loss": 0.3766,
      "step": 43280
    },
    {
      "epoch": 12.368571428571428,
      "grad_norm": 0.21306836605072021,
      "learning_rate": 3.508571428571429e-06,
      "loss": 0.0999,
      "step": 43290
    },
    {
      "epoch": 12.371428571428572,
      "grad_norm": 0.051281556487083435,
      "learning_rate": 3.504761904761905e-06,
      "loss": 0.5153,
      "step": 43300
    },
    {
      "epoch": 12.374285714285714,
      "grad_norm": 0.10865180194377899,
      "learning_rate": 3.500952380952381e-06,
      "loss": 0.2097,
      "step": 43310
    },
    {
      "epoch": 12.377142857142857,
      "grad_norm": 0.13823680579662323,
      "learning_rate": 3.4971428571428575e-06,
      "loss": 0.2272,
      "step": 43320
    },
    {
      "epoch": 12.38,
      "grad_norm": 0.0910554751753807,
      "learning_rate": 3.4933333333333335e-06,
      "loss": 0.2706,
      "step": 43330
    },
    {
      "epoch": 12.382857142857143,
      "grad_norm": 18.822973251342773,
      "learning_rate": 3.48952380952381e-06,
      "loss": 0.7364,
      "step": 43340
    },
    {
      "epoch": 12.385714285714286,
      "grad_norm": 0.2938993573188782,
      "learning_rate": 3.4857142857142863e-06,
      "loss": 0.567,
      "step": 43350
    },
    {
      "epoch": 12.388571428571428,
      "grad_norm": 0.47336506843566895,
      "learning_rate": 3.4819047619047623e-06,
      "loss": 0.4808,
      "step": 43360
    },
    {
      "epoch": 12.391428571428571,
      "grad_norm": 0.24573872983455658,
      "learning_rate": 3.4780952380952384e-06,
      "loss": 0.3218,
      "step": 43370
    },
    {
      "epoch": 12.394285714285715,
      "grad_norm": 0.4784300625324249,
      "learning_rate": 3.4742857142857144e-06,
      "loss": 0.3009,
      "step": 43380
    },
    {
      "epoch": 12.397142857142857,
      "grad_norm": 19.721590042114258,
      "learning_rate": 3.4704761904761908e-06,
      "loss": 0.2308,
      "step": 43390
    },
    {
      "epoch": 12.4,
      "grad_norm": 0.06447812914848328,
      "learning_rate": 3.4666666666666672e-06,
      "loss": 0.5308,
      "step": 43400
    },
    {
      "epoch": 12.402857142857142,
      "grad_norm": 0.6832358837127686,
      "learning_rate": 3.4628571428571432e-06,
      "loss": 0.2452,
      "step": 43410
    },
    {
      "epoch": 12.405714285714286,
      "grad_norm": 22.579803466796875,
      "learning_rate": 3.4590476190476192e-06,
      "loss": 0.2771,
      "step": 43420
    },
    {
      "epoch": 12.40857142857143,
      "grad_norm": 26.086048126220703,
      "learning_rate": 3.4552380952380952e-06,
      "loss": 0.5454,
      "step": 43430
    },
    {
      "epoch": 12.411428571428571,
      "grad_norm": 17.624235153198242,
      "learning_rate": 3.4514285714285717e-06,
      "loss": 0.3635,
      "step": 43440
    },
    {
      "epoch": 12.414285714285715,
      "grad_norm": 0.5533507466316223,
      "learning_rate": 3.447619047619048e-06,
      "loss": 0.4521,
      "step": 43450
    },
    {
      "epoch": 12.417142857142856,
      "grad_norm": 0.08379816263914108,
      "learning_rate": 3.443809523809524e-06,
      "loss": 0.158,
      "step": 43460
    },
    {
      "epoch": 12.42,
      "grad_norm": 0.0752691850066185,
      "learning_rate": 3.44e-06,
      "loss": 0.5417,
      "step": 43470
    },
    {
      "epoch": 12.422857142857143,
      "grad_norm": 15.203669548034668,
      "learning_rate": 3.436190476190476e-06,
      "loss": 0.6079,
      "step": 43480
    },
    {
      "epoch": 12.425714285714285,
      "grad_norm": 14.613394737243652,
      "learning_rate": 3.4323809523809525e-06,
      "loss": 0.3441,
      "step": 43490
    },
    {
      "epoch": 12.428571428571429,
      "grad_norm": 0.5696195960044861,
      "learning_rate": 3.428571428571429e-06,
      "loss": 0.2581,
      "step": 43500
    },
    {
      "epoch": 12.43142857142857,
      "grad_norm": 18.8100528717041,
      "learning_rate": 3.424761904761905e-06,
      "loss": 0.6503,
      "step": 43510
    },
    {
      "epoch": 12.434285714285714,
      "grad_norm": 0.1478581428527832,
      "learning_rate": 3.4209523809523814e-06,
      "loss": 0.4926,
      "step": 43520
    },
    {
      "epoch": 12.437142857142858,
      "grad_norm": 0.1974962204694748,
      "learning_rate": 3.4171428571428574e-06,
      "loss": 0.0264,
      "step": 43530
    },
    {
      "epoch": 12.44,
      "grad_norm": 13.875858306884766,
      "learning_rate": 3.4133333333333334e-06,
      "loss": 0.8604,
      "step": 43540
    },
    {
      "epoch": 12.442857142857143,
      "grad_norm": 0.24392744898796082,
      "learning_rate": 3.40952380952381e-06,
      "loss": 0.0954,
      "step": 43550
    },
    {
      "epoch": 12.445714285714285,
      "grad_norm": 0.42458900809288025,
      "learning_rate": 3.4057142857142863e-06,
      "loss": 0.4666,
      "step": 43560
    },
    {
      "epoch": 12.448571428571428,
      "grad_norm": 16.38531494140625,
      "learning_rate": 3.4019047619047623e-06,
      "loss": 0.2867,
      "step": 43570
    },
    {
      "epoch": 12.451428571428572,
      "grad_norm": 4.592634677886963,
      "learning_rate": 3.3980952380952383e-06,
      "loss": 0.3426,
      "step": 43580
    },
    {
      "epoch": 12.454285714285714,
      "grad_norm": 0.10191923379898071,
      "learning_rate": 3.3942857142857143e-06,
      "loss": 0.5119,
      "step": 43590
    },
    {
      "epoch": 12.457142857142857,
      "grad_norm": 51.815181732177734,
      "learning_rate": 3.3904761904761907e-06,
      "loss": 0.3486,
      "step": 43600
    },
    {
      "epoch": 12.46,
      "grad_norm": 13.548095703125,
      "learning_rate": 3.386666666666667e-06,
      "loss": 0.3748,
      "step": 43610
    },
    {
      "epoch": 12.462857142857143,
      "grad_norm": 0.10867732763290405,
      "learning_rate": 3.382857142857143e-06,
      "loss": 0.1231,
      "step": 43620
    },
    {
      "epoch": 12.465714285714286,
      "grad_norm": 0.35463181138038635,
      "learning_rate": 3.3790476190476196e-06,
      "loss": 0.1274,
      "step": 43630
    },
    {
      "epoch": 12.468571428571428,
      "grad_norm": 0.040840134024620056,
      "learning_rate": 3.375238095238095e-06,
      "loss": 0.0985,
      "step": 43640
    },
    {
      "epoch": 12.471428571428572,
      "grad_norm": 0.30328863859176636,
      "learning_rate": 3.3714285714285716e-06,
      "loss": 0.1237,
      "step": 43650
    },
    {
      "epoch": 12.474285714285715,
      "grad_norm": 29.819032669067383,
      "learning_rate": 3.367619047619048e-06,
      "loss": 0.3989,
      "step": 43660
    },
    {
      "epoch": 12.477142857142857,
      "grad_norm": 0.42716118693351746,
      "learning_rate": 3.363809523809524e-06,
      "loss": 0.3035,
      "step": 43670
    },
    {
      "epoch": 12.48,
      "grad_norm": 0.442216157913208,
      "learning_rate": 3.3600000000000004e-06,
      "loss": 0.2629,
      "step": 43680
    },
    {
      "epoch": 12.482857142857142,
      "grad_norm": 0.04743678867816925,
      "learning_rate": 3.356190476190476e-06,
      "loss": 0.4446,
      "step": 43690
    },
    {
      "epoch": 12.485714285714286,
      "grad_norm": 35.27580261230469,
      "learning_rate": 3.3523809523809525e-06,
      "loss": 0.3616,
      "step": 43700
    },
    {
      "epoch": 12.48857142857143,
      "grad_norm": 0.2960686683654785,
      "learning_rate": 3.348571428571429e-06,
      "loss": 0.4868,
      "step": 43710
    },
    {
      "epoch": 12.491428571428571,
      "grad_norm": 17.442716598510742,
      "learning_rate": 3.3447619047619053e-06,
      "loss": 0.588,
      "step": 43720
    },
    {
      "epoch": 12.494285714285715,
      "grad_norm": 53.603797912597656,
      "learning_rate": 3.3409523809523813e-06,
      "loss": 0.7631,
      "step": 43730
    },
    {
      "epoch": 12.497142857142856,
      "grad_norm": 30.27958869934082,
      "learning_rate": 3.3371428571428577e-06,
      "loss": 0.468,
      "step": 43740
    },
    {
      "epoch": 12.5,
      "grad_norm": 0.9601671695709229,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.0066,
      "step": 43750
    },
    {
      "epoch": 12.502857142857144,
      "grad_norm": 0.27092650532722473,
      "learning_rate": 3.3295238095238098e-06,
      "loss": 0.2335,
      "step": 43760
    },
    {
      "epoch": 12.505714285714285,
      "grad_norm": 0.20521433651447296,
      "learning_rate": 3.325714285714286e-06,
      "loss": 0.3487,
      "step": 43770
    },
    {
      "epoch": 12.508571428571429,
      "grad_norm": 0.05230024829506874,
      "learning_rate": 3.321904761904762e-06,
      "loss": 0.3849,
      "step": 43780
    },
    {
      "epoch": 12.51142857142857,
      "grad_norm": 0.11567533761262894,
      "learning_rate": 3.3180952380952386e-06,
      "loss": 0.2192,
      "step": 43790
    },
    {
      "epoch": 12.514285714285714,
      "grad_norm": 0.11429470777511597,
      "learning_rate": 3.314285714285714e-06,
      "loss": 0.0976,
      "step": 43800
    },
    {
      "epoch": 12.517142857142858,
      "grad_norm": 0.15801404416561127,
      "learning_rate": 3.3104761904761906e-06,
      "loss": 0.3515,
      "step": 43810
    },
    {
      "epoch": 12.52,
      "grad_norm": 0.3790454864501953,
      "learning_rate": 3.306666666666667e-06,
      "loss": 0.2544,
      "step": 43820
    },
    {
      "epoch": 12.522857142857143,
      "grad_norm": 0.217831552028656,
      "learning_rate": 3.302857142857143e-06,
      "loss": 0.223,
      "step": 43830
    },
    {
      "epoch": 12.525714285714285,
      "grad_norm": 0.12405775487422943,
      "learning_rate": 3.2990476190476195e-06,
      "loss": 0.5251,
      "step": 43840
    },
    {
      "epoch": 12.528571428571428,
      "grad_norm": 17.619102478027344,
      "learning_rate": 3.295238095238095e-06,
      "loss": 0.2582,
      "step": 43850
    },
    {
      "epoch": 12.531428571428572,
      "grad_norm": 0.07511727511882782,
      "learning_rate": 3.2914285714285715e-06,
      "loss": 0.4669,
      "step": 43860
    },
    {
      "epoch": 12.534285714285714,
      "grad_norm": 0.3480747938156128,
      "learning_rate": 3.287619047619048e-06,
      "loss": 0.4883,
      "step": 43870
    },
    {
      "epoch": 12.537142857142857,
      "grad_norm": 15.682963371276855,
      "learning_rate": 3.283809523809524e-06,
      "loss": 0.6159,
      "step": 43880
    },
    {
      "epoch": 12.54,
      "grad_norm": 20.28911018371582,
      "learning_rate": 3.2800000000000004e-06,
      "loss": 0.2005,
      "step": 43890
    },
    {
      "epoch": 12.542857142857143,
      "grad_norm": 0.2647901773452759,
      "learning_rate": 3.276190476190477e-06,
      "loss": 0.2897,
      "step": 43900
    },
    {
      "epoch": 12.545714285714286,
      "grad_norm": 0.4196893274784088,
      "learning_rate": 3.2723809523809524e-06,
      "loss": 0.3686,
      "step": 43910
    },
    {
      "epoch": 12.548571428571428,
      "grad_norm": 0.05661166459321976,
      "learning_rate": 3.268571428571429e-06,
      "loss": 0.21,
      "step": 43920
    },
    {
      "epoch": 12.551428571428572,
      "grad_norm": 0.2976113557815552,
      "learning_rate": 3.2647619047619052e-06,
      "loss": 0.665,
      "step": 43930
    },
    {
      "epoch": 12.554285714285715,
      "grad_norm": 0.0861499235033989,
      "learning_rate": 3.2609523809523812e-06,
      "loss": 0.1793,
      "step": 43940
    },
    {
      "epoch": 12.557142857142857,
      "grad_norm": 25.04102325439453,
      "learning_rate": 3.2571428571428577e-06,
      "loss": 0.3436,
      "step": 43950
    },
    {
      "epoch": 12.56,
      "grad_norm": 0.0880976989865303,
      "learning_rate": 3.2533333333333332e-06,
      "loss": 0.2089,
      "step": 43960
    },
    {
      "epoch": 12.562857142857142,
      "grad_norm": 17.030195236206055,
      "learning_rate": 3.2495238095238097e-06,
      "loss": 0.3544,
      "step": 43970
    },
    {
      "epoch": 12.565714285714286,
      "grad_norm": 0.17518679797649384,
      "learning_rate": 3.245714285714286e-06,
      "loss": 0.2526,
      "step": 43980
    },
    {
      "epoch": 12.56857142857143,
      "grad_norm": 38.01095199584961,
      "learning_rate": 3.241904761904762e-06,
      "loss": 0.6359,
      "step": 43990
    },
    {
      "epoch": 12.571428571428571,
      "grad_norm": 0.5355839729309082,
      "learning_rate": 3.2380952380952385e-06,
      "loss": 0.5725,
      "step": 44000
    },
    {
      "epoch": 12.574285714285715,
      "grad_norm": 15.55972957611084,
      "learning_rate": 3.234285714285715e-06,
      "loss": 0.212,
      "step": 44010
    },
    {
      "epoch": 12.577142857142857,
      "grad_norm": 28.731037139892578,
      "learning_rate": 3.2304761904761905e-06,
      "loss": 0.6746,
      "step": 44020
    },
    {
      "epoch": 12.58,
      "grad_norm": 0.07353007048368454,
      "learning_rate": 3.226666666666667e-06,
      "loss": 0.2841,
      "step": 44030
    },
    {
      "epoch": 12.582857142857144,
      "grad_norm": 13.312566757202148,
      "learning_rate": 3.222857142857143e-06,
      "loss": 0.6188,
      "step": 44040
    },
    {
      "epoch": 12.585714285714285,
      "grad_norm": 53.00337600708008,
      "learning_rate": 3.2190476190476194e-06,
      "loss": 0.4681,
      "step": 44050
    },
    {
      "epoch": 12.588571428571429,
      "grad_norm": 0.6053951382637024,
      "learning_rate": 3.215238095238096e-06,
      "loss": 0.4455,
      "step": 44060
    },
    {
      "epoch": 12.59142857142857,
      "grad_norm": 0.2037304937839508,
      "learning_rate": 3.2114285714285714e-06,
      "loss": 0.1197,
      "step": 44070
    },
    {
      "epoch": 12.594285714285714,
      "grad_norm": 0.2549644708633423,
      "learning_rate": 3.207619047619048e-06,
      "loss": 0.2594,
      "step": 44080
    },
    {
      "epoch": 12.597142857142858,
      "grad_norm": 0.20119911432266235,
      "learning_rate": 3.203809523809524e-06,
      "loss": 0.21,
      "step": 44090
    },
    {
      "epoch": 12.6,
      "grad_norm": 0.29607394337654114,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 0.3747,
      "step": 44100
    },
    {
      "epoch": 12.602857142857143,
      "grad_norm": 14.329205513000488,
      "learning_rate": 3.1961904761904767e-06,
      "loss": 0.3473,
      "step": 44110
    },
    {
      "epoch": 12.605714285714285,
      "grad_norm": 397.8423767089844,
      "learning_rate": 3.1923809523809527e-06,
      "loss": 0.3016,
      "step": 44120
    },
    {
      "epoch": 12.608571428571429,
      "grad_norm": 0.11465425789356232,
      "learning_rate": 3.1885714285714287e-06,
      "loss": 0.7564,
      "step": 44130
    },
    {
      "epoch": 12.611428571428572,
      "grad_norm": 23.416324615478516,
      "learning_rate": 3.184761904761905e-06,
      "loss": 0.3335,
      "step": 44140
    },
    {
      "epoch": 12.614285714285714,
      "grad_norm": 0.19241943955421448,
      "learning_rate": 3.180952380952381e-06,
      "loss": 0.2677,
      "step": 44150
    },
    {
      "epoch": 12.617142857142857,
      "grad_norm": 2428.846435546875,
      "learning_rate": 3.1771428571428576e-06,
      "loss": 0.3856,
      "step": 44160
    },
    {
      "epoch": 12.62,
      "grad_norm": 0.6466999650001526,
      "learning_rate": 3.173333333333334e-06,
      "loss": 0.6358,
      "step": 44170
    },
    {
      "epoch": 12.622857142857143,
      "grad_norm": 0.2915605902671814,
      "learning_rate": 3.1695238095238096e-06,
      "loss": 0.4368,
      "step": 44180
    },
    {
      "epoch": 12.625714285714286,
      "grad_norm": 0.6805558204650879,
      "learning_rate": 3.165714285714286e-06,
      "loss": 0.2501,
      "step": 44190
    },
    {
      "epoch": 12.628571428571428,
      "grad_norm": 0.25827503204345703,
      "learning_rate": 3.161904761904762e-06,
      "loss": 0.5618,
      "step": 44200
    },
    {
      "epoch": 12.631428571428572,
      "grad_norm": 0.5876274704933167,
      "learning_rate": 3.1580952380952385e-06,
      "loss": 0.4137,
      "step": 44210
    },
    {
      "epoch": 12.634285714285713,
      "grad_norm": 0.2099113166332245,
      "learning_rate": 3.154285714285715e-06,
      "loss": 0.5852,
      "step": 44220
    },
    {
      "epoch": 12.637142857142857,
      "grad_norm": 0.5104374885559082,
      "learning_rate": 3.1504761904761905e-06,
      "loss": 0.189,
      "step": 44230
    },
    {
      "epoch": 12.64,
      "grad_norm": 0.4099375307559967,
      "learning_rate": 3.146666666666667e-06,
      "loss": 0.0098,
      "step": 44240
    },
    {
      "epoch": 12.642857142857142,
      "grad_norm": 18.311437606811523,
      "learning_rate": 3.142857142857143e-06,
      "loss": 0.1626,
      "step": 44250
    },
    {
      "epoch": 12.645714285714286,
      "grad_norm": 21.955331802368164,
      "learning_rate": 3.1390476190476193e-06,
      "loss": 0.982,
      "step": 44260
    },
    {
      "epoch": 12.64857142857143,
      "grad_norm": 0.07804953306913376,
      "learning_rate": 3.1352380952380958e-06,
      "loss": 0.6907,
      "step": 44270
    },
    {
      "epoch": 12.651428571428571,
      "grad_norm": 0.16730718314647675,
      "learning_rate": 3.1314285714285718e-06,
      "loss": 0.0345,
      "step": 44280
    },
    {
      "epoch": 12.654285714285715,
      "grad_norm": 228.09967041015625,
      "learning_rate": 3.1276190476190478e-06,
      "loss": 0.3966,
      "step": 44290
    },
    {
      "epoch": 12.657142857142857,
      "grad_norm": 0.21431800723075867,
      "learning_rate": 3.1238095238095238e-06,
      "loss": 0.0976,
      "step": 44300
    },
    {
      "epoch": 12.66,
      "grad_norm": 101.76498413085938,
      "learning_rate": 3.12e-06,
      "loss": 0.7658,
      "step": 44310
    },
    {
      "epoch": 12.662857142857142,
      "grad_norm": 0.08383209258317947,
      "learning_rate": 3.1161904761904766e-06,
      "loss": 0.3998,
      "step": 44320
    },
    {
      "epoch": 12.665714285714285,
      "grad_norm": 0.07286699116230011,
      "learning_rate": 3.1123809523809526e-06,
      "loss": 0.1177,
      "step": 44330
    },
    {
      "epoch": 12.668571428571429,
      "grad_norm": 0.22210383415222168,
      "learning_rate": 3.1085714285714286e-06,
      "loss": 0.0808,
      "step": 44340
    },
    {
      "epoch": 12.67142857142857,
      "grad_norm": 1.7539293766021729,
      "learning_rate": 3.104761904761905e-06,
      "loss": 0.4276,
      "step": 44350
    },
    {
      "epoch": 12.674285714285714,
      "grad_norm": 0.34326475858688354,
      "learning_rate": 3.100952380952381e-06,
      "loss": 0.226,
      "step": 44360
    },
    {
      "epoch": 12.677142857142858,
      "grad_norm": 0.24093835055828094,
      "learning_rate": 3.0971428571428575e-06,
      "loss": 0.2834,
      "step": 44370
    },
    {
      "epoch": 12.68,
      "grad_norm": 31.22707748413086,
      "learning_rate": 3.093333333333334e-06,
      "loss": 0.3405,
      "step": 44380
    },
    {
      "epoch": 12.682857142857143,
      "grad_norm": 0.7949172854423523,
      "learning_rate": 3.08952380952381e-06,
      "loss": 0.004,
      "step": 44390
    },
    {
      "epoch": 12.685714285714285,
      "grad_norm": 0.22167973220348358,
      "learning_rate": 3.085714285714286e-06,
      "loss": 0.2877,
      "step": 44400
    },
    {
      "epoch": 12.688571428571429,
      "grad_norm": 34.30644989013672,
      "learning_rate": 3.081904761904762e-06,
      "loss": 0.272,
      "step": 44410
    },
    {
      "epoch": 12.691428571428572,
      "grad_norm": 0.11018940061330795,
      "learning_rate": 3.0780952380952384e-06,
      "loss": 0.3029,
      "step": 44420
    },
    {
      "epoch": 12.694285714285714,
      "grad_norm": 96.37653350830078,
      "learning_rate": 3.074285714285715e-06,
      "loss": 0.5806,
      "step": 44430
    },
    {
      "epoch": 12.697142857142858,
      "grad_norm": 0.5368788242340088,
      "learning_rate": 3.070476190476191e-06,
      "loss": 0.5596,
      "step": 44440
    },
    {
      "epoch": 12.7,
      "grad_norm": 0.14659631252288818,
      "learning_rate": 3.066666666666667e-06,
      "loss": 0.2583,
      "step": 44450
    },
    {
      "epoch": 12.702857142857143,
      "grad_norm": 0.19842126965522766,
      "learning_rate": 3.062857142857143e-06,
      "loss": 0.3677,
      "step": 44460
    },
    {
      "epoch": 12.705714285714286,
      "grad_norm": 0.592609703540802,
      "learning_rate": 3.0590476190476192e-06,
      "loss": 0.5389,
      "step": 44470
    },
    {
      "epoch": 12.708571428571428,
      "grad_norm": 0.48362940549850464,
      "learning_rate": 3.0552380952380957e-06,
      "loss": 0.3283,
      "step": 44480
    },
    {
      "epoch": 12.711428571428572,
      "grad_norm": 0.3354169428348541,
      "learning_rate": 3.0514285714285717e-06,
      "loss": 0.2481,
      "step": 44490
    },
    {
      "epoch": 12.714285714285714,
      "grad_norm": 0.14664195477962494,
      "learning_rate": 3.047619047619048e-06,
      "loss": 0.2963,
      "step": 44500
    },
    {
      "epoch": 12.717142857142857,
      "grad_norm": 0.10606605559587479,
      "learning_rate": 3.0438095238095237e-06,
      "loss": 0.3134,
      "step": 44510
    },
    {
      "epoch": 12.72,
      "grad_norm": 23.91996192932129,
      "learning_rate": 3.04e-06,
      "loss": 0.4661,
      "step": 44520
    },
    {
      "epoch": 12.722857142857142,
      "grad_norm": 0.2677331268787384,
      "learning_rate": 3.0361904761904765e-06,
      "loss": 0.2223,
      "step": 44530
    },
    {
      "epoch": 12.725714285714286,
      "grad_norm": 569.3622436523438,
      "learning_rate": 3.0323809523809526e-06,
      "loss": 0.4854,
      "step": 44540
    },
    {
      "epoch": 12.728571428571428,
      "grad_norm": 0.6449916958808899,
      "learning_rate": 3.028571428571429e-06,
      "loss": 0.221,
      "step": 44550
    },
    {
      "epoch": 12.731428571428571,
      "grad_norm": 0.10592907667160034,
      "learning_rate": 3.024761904761905e-06,
      "loss": 0.1324,
      "step": 44560
    },
    {
      "epoch": 12.734285714285715,
      "grad_norm": 0.1082112193107605,
      "learning_rate": 3.020952380952381e-06,
      "loss": 0.1049,
      "step": 44570
    },
    {
      "epoch": 12.737142857142857,
      "grad_norm": 50.424983978271484,
      "learning_rate": 3.0171428571428574e-06,
      "loss": 0.3824,
      "step": 44580
    },
    {
      "epoch": 12.74,
      "grad_norm": 27.509370803833008,
      "learning_rate": 3.013333333333334e-06,
      "loss": 0.1494,
      "step": 44590
    },
    {
      "epoch": 12.742857142857144,
      "grad_norm": 0.16214033961296082,
      "learning_rate": 3.00952380952381e-06,
      "loss": 0.343,
      "step": 44600
    },
    {
      "epoch": 12.745714285714286,
      "grad_norm": 14.072379112243652,
      "learning_rate": 3.005714285714286e-06,
      "loss": 0.2414,
      "step": 44610
    },
    {
      "epoch": 12.748571428571429,
      "grad_norm": 0.02729935757815838,
      "learning_rate": 3.001904761904762e-06,
      "loss": 0.12,
      "step": 44620
    },
    {
      "epoch": 12.751428571428571,
      "grad_norm": 0.38255545496940613,
      "learning_rate": 2.9980952380952383e-06,
      "loss": 0.1116,
      "step": 44630
    },
    {
      "epoch": 12.754285714285714,
      "grad_norm": 0.08390791714191437,
      "learning_rate": 2.9942857142857147e-06,
      "loss": 0.5122,
      "step": 44640
    },
    {
      "epoch": 12.757142857142856,
      "grad_norm": 0.7610434889793396,
      "learning_rate": 2.9904761904761907e-06,
      "loss": 0.2247,
      "step": 44650
    },
    {
      "epoch": 12.76,
      "grad_norm": 0.15516634285449982,
      "learning_rate": 2.986666666666667e-06,
      "loss": 0.5847,
      "step": 44660
    },
    {
      "epoch": 12.762857142857143,
      "grad_norm": 0.11987416446208954,
      "learning_rate": 2.9828571428571427e-06,
      "loss": 0.0028,
      "step": 44670
    },
    {
      "epoch": 12.765714285714285,
      "grad_norm": 0.2371954768896103,
      "learning_rate": 2.979047619047619e-06,
      "loss": 0.1128,
      "step": 44680
    },
    {
      "epoch": 12.768571428571429,
      "grad_norm": 21.37704086303711,
      "learning_rate": 2.9752380952380956e-06,
      "loss": 0.6784,
      "step": 44690
    },
    {
      "epoch": 12.771428571428572,
      "grad_norm": 0.015473412349820137,
      "learning_rate": 2.9714285714285716e-06,
      "loss": 0.3545,
      "step": 44700
    },
    {
      "epoch": 12.774285714285714,
      "grad_norm": 0.2479257434606552,
      "learning_rate": 2.967619047619048e-06,
      "loss": 0.3395,
      "step": 44710
    },
    {
      "epoch": 12.777142857142858,
      "grad_norm": 0.7504163980484009,
      "learning_rate": 2.9638095238095236e-06,
      "loss": 0.3377,
      "step": 44720
    },
    {
      "epoch": 12.78,
      "grad_norm": 0.009452743455767632,
      "learning_rate": 2.96e-06,
      "loss": 0.4188,
      "step": 44730
    },
    {
      "epoch": 12.782857142857143,
      "grad_norm": 133.8494415283203,
      "learning_rate": 2.9561904761904765e-06,
      "loss": 0.4667,
      "step": 44740
    },
    {
      "epoch": 12.785714285714286,
      "grad_norm": 61.818721771240234,
      "learning_rate": 2.9523809523809525e-06,
      "loss": 0.6992,
      "step": 44750
    },
    {
      "epoch": 12.788571428571428,
      "grad_norm": 0.09426524490118027,
      "learning_rate": 2.948571428571429e-06,
      "loss": 0.6919,
      "step": 44760
    },
    {
      "epoch": 12.791428571428572,
      "grad_norm": 0.4611511826515198,
      "learning_rate": 2.9447619047619053e-06,
      "loss": 0.332,
      "step": 44770
    },
    {
      "epoch": 12.794285714285714,
      "grad_norm": 16.761741638183594,
      "learning_rate": 2.940952380952381e-06,
      "loss": 0.2527,
      "step": 44780
    },
    {
      "epoch": 12.797142857142857,
      "grad_norm": 20.61597442626953,
      "learning_rate": 2.9371428571428573e-06,
      "loss": 0.3919,
      "step": 44790
    },
    {
      "epoch": 12.8,
      "grad_norm": 0.16295424103736877,
      "learning_rate": 2.9333333333333338e-06,
      "loss": 0.2955,
      "step": 44800
    },
    {
      "epoch": 12.802857142857142,
      "grad_norm": 14.353327751159668,
      "learning_rate": 2.9295238095238098e-06,
      "loss": 0.2661,
      "step": 44810
    },
    {
      "epoch": 12.805714285714286,
      "grad_norm": 0.3208806812763214,
      "learning_rate": 2.925714285714286e-06,
      "loss": 0.0059,
      "step": 44820
    },
    {
      "epoch": 12.808571428571428,
      "grad_norm": 0.0655268207192421,
      "learning_rate": 2.9219047619047618e-06,
      "loss": 0.4729,
      "step": 44830
    },
    {
      "epoch": 12.811428571428571,
      "grad_norm": 0.25689995288848877,
      "learning_rate": 2.918095238095238e-06,
      "loss": 0.1654,
      "step": 44840
    },
    {
      "epoch": 12.814285714285715,
      "grad_norm": 0.03871060535311699,
      "learning_rate": 2.9142857142857146e-06,
      "loss": 0.2384,
      "step": 44850
    },
    {
      "epoch": 12.817142857142857,
      "grad_norm": 21.744508743286133,
      "learning_rate": 2.9104761904761906e-06,
      "loss": 0.4517,
      "step": 44860
    },
    {
      "epoch": 12.82,
      "grad_norm": 147.39077758789062,
      "learning_rate": 2.906666666666667e-06,
      "loss": 0.3151,
      "step": 44870
    },
    {
      "epoch": 12.822857142857142,
      "grad_norm": 0.5721571445465088,
      "learning_rate": 2.9028571428571427e-06,
      "loss": 0.3342,
      "step": 44880
    },
    {
      "epoch": 12.825714285714286,
      "grad_norm": 0.23692233860492706,
      "learning_rate": 2.899047619047619e-06,
      "loss": 0.4374,
      "step": 44890
    },
    {
      "epoch": 12.82857142857143,
      "grad_norm": 0.400430828332901,
      "learning_rate": 2.8952380952380955e-06,
      "loss": 0.1425,
      "step": 44900
    },
    {
      "epoch": 12.831428571428571,
      "grad_norm": 0.13854067027568817,
      "learning_rate": 2.8914285714285715e-06,
      "loss": 0.2499,
      "step": 44910
    },
    {
      "epoch": 12.834285714285715,
      "grad_norm": 16.16657257080078,
      "learning_rate": 2.887619047619048e-06,
      "loss": 0.2914,
      "step": 44920
    },
    {
      "epoch": 12.837142857142858,
      "grad_norm": 0.1407865285873413,
      "learning_rate": 2.8838095238095244e-06,
      "loss": 0.0052,
      "step": 44930
    },
    {
      "epoch": 12.84,
      "grad_norm": 0.44585081934928894,
      "learning_rate": 2.88e-06,
      "loss": 0.7361,
      "step": 44940
    },
    {
      "epoch": 12.842857142857143,
      "grad_norm": 0.43578702211380005,
      "learning_rate": 2.8761904761904764e-06,
      "loss": 0.3512,
      "step": 44950
    },
    {
      "epoch": 12.845714285714285,
      "grad_norm": 14.90200138092041,
      "learning_rate": 2.872380952380953e-06,
      "loss": 0.4295,
      "step": 44960
    },
    {
      "epoch": 12.848571428571429,
      "grad_norm": 0.14483682811260223,
      "learning_rate": 2.868571428571429e-06,
      "loss": 0.3456,
      "step": 44970
    },
    {
      "epoch": 12.85142857142857,
      "grad_norm": 0.36700379848480225,
      "learning_rate": 2.8647619047619052e-06,
      "loss": 0.462,
      "step": 44980
    },
    {
      "epoch": 12.854285714285714,
      "grad_norm": 0.09917359054088593,
      "learning_rate": 2.860952380952381e-06,
      "loss": 0.3166,
      "step": 44990
    },
    {
      "epoch": 12.857142857142858,
      "grad_norm": 12.067126274108887,
      "learning_rate": 2.8571428571428573e-06,
      "loss": 0.181,
      "step": 45000
    },
    {
      "epoch": 12.86,
      "grad_norm": 0.04060029238462448,
      "learning_rate": 2.8533333333333337e-06,
      "loss": 0.3904,
      "step": 45010
    },
    {
      "epoch": 12.862857142857143,
      "grad_norm": 0.09901344776153564,
      "learning_rate": 2.8495238095238097e-06,
      "loss": 0.3159,
      "step": 45020
    },
    {
      "epoch": 12.865714285714287,
      "grad_norm": 0.426510214805603,
      "learning_rate": 2.845714285714286e-06,
      "loss": 0.5964,
      "step": 45030
    },
    {
      "epoch": 12.868571428571428,
      "grad_norm": 0.3280876576900482,
      "learning_rate": 2.8419047619047625e-06,
      "loss": 0.5242,
      "step": 45040
    },
    {
      "epoch": 12.871428571428572,
      "grad_norm": 13.737927436828613,
      "learning_rate": 2.838095238095238e-06,
      "loss": 0.2114,
      "step": 45050
    },
    {
      "epoch": 12.874285714285714,
      "grad_norm": 26.37360954284668,
      "learning_rate": 2.8342857142857146e-06,
      "loss": 0.5862,
      "step": 45060
    },
    {
      "epoch": 12.877142857142857,
      "grad_norm": 0.22689908742904663,
      "learning_rate": 2.8304761904761906e-06,
      "loss": 0.2225,
      "step": 45070
    },
    {
      "epoch": 12.88,
      "grad_norm": 0.9061212539672852,
      "learning_rate": 2.826666666666667e-06,
      "loss": 0.0134,
      "step": 45080
    },
    {
      "epoch": 12.882857142857143,
      "grad_norm": 0.1742541640996933,
      "learning_rate": 2.8228571428571434e-06,
      "loss": 0.4092,
      "step": 45090
    },
    {
      "epoch": 12.885714285714286,
      "grad_norm": 0.05505884438753128,
      "learning_rate": 2.819047619047619e-06,
      "loss": 0.4885,
      "step": 45100
    },
    {
      "epoch": 12.888571428571428,
      "grad_norm": 0.540529727935791,
      "learning_rate": 2.8152380952380954e-06,
      "loss": 0.6894,
      "step": 45110
    },
    {
      "epoch": 12.891428571428571,
      "grad_norm": 0.3110141158103943,
      "learning_rate": 2.8114285714285714e-06,
      "loss": 0.1546,
      "step": 45120
    },
    {
      "epoch": 12.894285714285715,
      "grad_norm": 0.4864654242992401,
      "learning_rate": 2.807619047619048e-06,
      "loss": 0.24,
      "step": 45130
    },
    {
      "epoch": 12.897142857142857,
      "grad_norm": 0.18280340731143951,
      "learning_rate": 2.8038095238095243e-06,
      "loss": 0.2301,
      "step": 45140
    },
    {
      "epoch": 12.9,
      "grad_norm": 0.3047335147857666,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 0.3247,
      "step": 45150
    },
    {
      "epoch": 12.902857142857142,
      "grad_norm": 0.40118420124053955,
      "learning_rate": 2.7961904761904763e-06,
      "loss": 0.4019,
      "step": 45160
    },
    {
      "epoch": 12.905714285714286,
      "grad_norm": 0.31245818734169006,
      "learning_rate": 2.7923809523809527e-06,
      "loss": 0.3483,
      "step": 45170
    },
    {
      "epoch": 12.90857142857143,
      "grad_norm": 0.5009006261825562,
      "learning_rate": 2.7885714285714287e-06,
      "loss": 0.1093,
      "step": 45180
    },
    {
      "epoch": 12.911428571428571,
      "grad_norm": 24.431184768676758,
      "learning_rate": 2.784761904761905e-06,
      "loss": 0.3433,
      "step": 45190
    },
    {
      "epoch": 12.914285714285715,
      "grad_norm": 32.54804611206055,
      "learning_rate": 2.7809523809523816e-06,
      "loss": 0.0243,
      "step": 45200
    },
    {
      "epoch": 12.917142857142856,
      "grad_norm": 0.3658406138420105,
      "learning_rate": 2.777142857142857e-06,
      "loss": 0.0032,
      "step": 45210
    },
    {
      "epoch": 12.92,
      "grad_norm": 0.606799304485321,
      "learning_rate": 2.7733333333333336e-06,
      "loss": 0.1934,
      "step": 45220
    },
    {
      "epoch": 12.922857142857143,
      "grad_norm": 33.53236770629883,
      "learning_rate": 2.7695238095238096e-06,
      "loss": 0.1232,
      "step": 45230
    },
    {
      "epoch": 12.925714285714285,
      "grad_norm": 0.09128259867429733,
      "learning_rate": 2.765714285714286e-06,
      "loss": 0.1415,
      "step": 45240
    },
    {
      "epoch": 12.928571428571429,
      "grad_norm": 0.6437480449676514,
      "learning_rate": 2.7619047619047625e-06,
      "loss": 0.0031,
      "step": 45250
    },
    {
      "epoch": 12.93142857142857,
      "grad_norm": 0.09630302339792252,
      "learning_rate": 2.758095238095238e-06,
      "loss": 0.1507,
      "step": 45260
    },
    {
      "epoch": 12.934285714285714,
      "grad_norm": 0.07580378651618958,
      "learning_rate": 2.7542857142857145e-06,
      "loss": 0.5303,
      "step": 45270
    },
    {
      "epoch": 12.937142857142858,
      "grad_norm": 0.32885977625846863,
      "learning_rate": 2.7504761904761905e-06,
      "loss": 0.4068,
      "step": 45280
    },
    {
      "epoch": 12.94,
      "grad_norm": 0.28899115324020386,
      "learning_rate": 2.746666666666667e-06,
      "loss": 0.1487,
      "step": 45290
    },
    {
      "epoch": 12.942857142857143,
      "grad_norm": 0.06733457744121552,
      "learning_rate": 2.7428571428571433e-06,
      "loss": 0.487,
      "step": 45300
    },
    {
      "epoch": 12.945714285714285,
      "grad_norm": 0.25723084807395935,
      "learning_rate": 2.7390476190476193e-06,
      "loss": 0.3648,
      "step": 45310
    },
    {
      "epoch": 12.948571428571428,
      "grad_norm": 0.18458367884159088,
      "learning_rate": 2.7352380952380953e-06,
      "loss": 0.3576,
      "step": 45320
    },
    {
      "epoch": 12.951428571428572,
      "grad_norm": 0.18153227865695953,
      "learning_rate": 2.7314285714285714e-06,
      "loss": 0.4277,
      "step": 45330
    },
    {
      "epoch": 12.954285714285714,
      "grad_norm": 0.13043950498104095,
      "learning_rate": 2.7276190476190478e-06,
      "loss": 0.3097,
      "step": 45340
    },
    {
      "epoch": 12.957142857142857,
      "grad_norm": 16.769350051879883,
      "learning_rate": 2.723809523809524e-06,
      "loss": 0.5038,
      "step": 45350
    },
    {
      "epoch": 12.96,
      "grad_norm": 19.496824264526367,
      "learning_rate": 2.7200000000000002e-06,
      "loss": 0.126,
      "step": 45360
    },
    {
      "epoch": 12.962857142857143,
      "grad_norm": 0.30349022150039673,
      "learning_rate": 2.7161904761904762e-06,
      "loss": 0.0586,
      "step": 45370
    },
    {
      "epoch": 12.965714285714286,
      "grad_norm": 0.16333043575286865,
      "learning_rate": 2.7123809523809526e-06,
      "loss": 0.3678,
      "step": 45380
    },
    {
      "epoch": 12.968571428571428,
      "grad_norm": 0.1707857847213745,
      "learning_rate": 2.7085714285714287e-06,
      "loss": 0.2288,
      "step": 45390
    },
    {
      "epoch": 12.971428571428572,
      "grad_norm": 0.2783605456352234,
      "learning_rate": 2.704761904761905e-06,
      "loss": 0.1408,
      "step": 45400
    },
    {
      "epoch": 12.974285714285715,
      "grad_norm": 17.21691131591797,
      "learning_rate": 2.7009523809523815e-06,
      "loss": 0.3205,
      "step": 45410
    },
    {
      "epoch": 12.977142857142857,
      "grad_norm": 0.07818128168582916,
      "learning_rate": 2.6971428571428575e-06,
      "loss": 0.3354,
      "step": 45420
    },
    {
      "epoch": 12.98,
      "grad_norm": 19.547025680541992,
      "learning_rate": 2.6933333333333335e-06,
      "loss": 0.3981,
      "step": 45430
    },
    {
      "epoch": 12.982857142857142,
      "grad_norm": 0.14041516184806824,
      "learning_rate": 2.6895238095238095e-06,
      "loss": 0.3562,
      "step": 45440
    },
    {
      "epoch": 12.985714285714286,
      "grad_norm": 0.4460963308811188,
      "learning_rate": 2.685714285714286e-06,
      "loss": 0.4649,
      "step": 45450
    },
    {
      "epoch": 12.98857142857143,
      "grad_norm": 0.39002639055252075,
      "learning_rate": 2.6819047619047624e-06,
      "loss": 0.3819,
      "step": 45460
    },
    {
      "epoch": 12.991428571428571,
      "grad_norm": 0.1765984147787094,
      "learning_rate": 2.6780952380952384e-06,
      "loss": 0.4696,
      "step": 45470
    },
    {
      "epoch": 12.994285714285715,
      "grad_norm": 38.437496185302734,
      "learning_rate": 2.6742857142857144e-06,
      "loss": 0.3743,
      "step": 45480
    },
    {
      "epoch": 12.997142857142856,
      "grad_norm": 0.25611159205436707,
      "learning_rate": 2.6704761904761904e-06,
      "loss": 0.2994,
      "step": 45490
    },
    {
      "epoch": 13.0,
      "grad_norm": 2.3654613494873047,
      "learning_rate": 2.666666666666667e-06,
      "loss": 0.0758,
      "step": 45500
    },
    {
      "epoch": 13.0,
      "eval_accuracy": 0.9240591397849462,
      "eval_f1": 0.6646884272997033,
      "eval_loss": 0.3473859131336212,
      "eval_precision": 0.7724137931034483,
      "eval_recall": 0.5833333333333334,
      "eval_runtime": 47.3958,
      "eval_samples_per_second": 63.297,
      "eval_steps_per_second": 1.983,
      "step": 45500
    },
    {
      "epoch": 13.002857142857144,
      "grad_norm": 0.20335128903388977,
      "learning_rate": 2.6628571428571433e-06,
      "loss": 0.1027,
      "step": 45510
    },
    {
      "epoch": 13.005714285714285,
      "grad_norm": 14.770472526550293,
      "learning_rate": 2.6590476190476193e-06,
      "loss": 0.3344,
      "step": 45520
    },
    {
      "epoch": 13.008571428571429,
      "grad_norm": 0.4287126064300537,
      "learning_rate": 2.6552380952380957e-06,
      "loss": 0.1237,
      "step": 45530
    },
    {
      "epoch": 13.01142857142857,
      "grad_norm": 48.83379364013672,
      "learning_rate": 2.6514285714285713e-06,
      "loss": 0.4234,
      "step": 45540
    },
    {
      "epoch": 13.014285714285714,
      "grad_norm": 25.581308364868164,
      "learning_rate": 2.6476190476190477e-06,
      "loss": 0.416,
      "step": 45550
    },
    {
      "epoch": 13.017142857142858,
      "grad_norm": 0.16467590630054474,
      "learning_rate": 2.643809523809524e-06,
      "loss": 0.2086,
      "step": 45560
    },
    {
      "epoch": 13.02,
      "grad_norm": 15.485198974609375,
      "learning_rate": 2.64e-06,
      "loss": 0.6656,
      "step": 45570
    },
    {
      "epoch": 13.022857142857143,
      "grad_norm": 0.30121171474456787,
      "learning_rate": 2.6361904761904766e-06,
      "loss": 0.0044,
      "step": 45580
    },
    {
      "epoch": 13.025714285714285,
      "grad_norm": 0.2967594265937805,
      "learning_rate": 2.6323809523809526e-06,
      "loss": 0.1965,
      "step": 45590
    },
    {
      "epoch": 13.028571428571428,
      "grad_norm": 0.3256430923938751,
      "learning_rate": 2.6285714285714286e-06,
      "loss": 0.3407,
      "step": 45600
    },
    {
      "epoch": 13.031428571428572,
      "grad_norm": 0.10445981472730637,
      "learning_rate": 2.624761904761905e-06,
      "loss": 0.7151,
      "step": 45610
    },
    {
      "epoch": 13.034285714285714,
      "grad_norm": 25.655122756958008,
      "learning_rate": 2.6209523809523814e-06,
      "loss": 0.3911,
      "step": 45620
    },
    {
      "epoch": 13.037142857142857,
      "grad_norm": 0.11904970556497574,
      "learning_rate": 2.6171428571428574e-06,
      "loss": 0.0081,
      "step": 45630
    },
    {
      "epoch": 13.04,
      "grad_norm": 0.16776834428310394,
      "learning_rate": 2.6133333333333334e-06,
      "loss": 0.3793,
      "step": 45640
    },
    {
      "epoch": 13.042857142857143,
      "grad_norm": 0.1911526322364807,
      "learning_rate": 2.6095238095238094e-06,
      "loss": 0.2789,
      "step": 45650
    },
    {
      "epoch": 13.045714285714286,
      "grad_norm": 0.07849138975143433,
      "learning_rate": 2.605714285714286e-06,
      "loss": 0.3829,
      "step": 45660
    },
    {
      "epoch": 13.048571428571428,
      "grad_norm": 0.3241812586784363,
      "learning_rate": 2.6019047619047623e-06,
      "loss": 0.1758,
      "step": 45670
    },
    {
      "epoch": 13.051428571428572,
      "grad_norm": 1.0649968385696411,
      "learning_rate": 2.5980952380952383e-06,
      "loss": 0.2848,
      "step": 45680
    },
    {
      "epoch": 13.054285714285715,
      "grad_norm": 0.08411289751529694,
      "learning_rate": 2.5942857142857147e-06,
      "loss": 0.2704,
      "step": 45690
    },
    {
      "epoch": 13.057142857142857,
      "grad_norm": 50.504878997802734,
      "learning_rate": 2.5904761904761903e-06,
      "loss": 0.7578,
      "step": 45700
    },
    {
      "epoch": 13.06,
      "grad_norm": 0.056510861963033676,
      "learning_rate": 2.5866666666666667e-06,
      "loss": 0.3533,
      "step": 45710
    },
    {
      "epoch": 13.062857142857142,
      "grad_norm": 0.1668844372034073,
      "learning_rate": 2.582857142857143e-06,
      "loss": 0.3403,
      "step": 45720
    },
    {
      "epoch": 13.065714285714286,
      "grad_norm": 0.14922404289245605,
      "learning_rate": 2.579047619047619e-06,
      "loss": 0.1659,
      "step": 45730
    },
    {
      "epoch": 13.06857142857143,
      "grad_norm": 0.032328300178050995,
      "learning_rate": 2.5752380952380956e-06,
      "loss": 0.4081,
      "step": 45740
    },
    {
      "epoch": 13.071428571428571,
      "grad_norm": 24.109329223632812,
      "learning_rate": 2.571428571428571e-06,
      "loss": 0.2676,
      "step": 45750
    },
    {
      "epoch": 13.074285714285715,
      "grad_norm": 99.66077423095703,
      "learning_rate": 2.5676190476190476e-06,
      "loss": 0.1758,
      "step": 45760
    },
    {
      "epoch": 13.077142857142857,
      "grad_norm": 0.106514111161232,
      "learning_rate": 2.563809523809524e-06,
      "loss": 0.2224,
      "step": 45770
    },
    {
      "epoch": 13.08,
      "grad_norm": 0.17056098580360413,
      "learning_rate": 2.56e-06,
      "loss": 0.1392,
      "step": 45780
    },
    {
      "epoch": 13.082857142857144,
      "grad_norm": 0.18414972722530365,
      "learning_rate": 2.5561904761904765e-06,
      "loss": 0.5844,
      "step": 45790
    },
    {
      "epoch": 13.085714285714285,
      "grad_norm": 0.29337960481643677,
      "learning_rate": 2.552380952380953e-06,
      "loss": 0.3168,
      "step": 45800
    },
    {
      "epoch": 13.088571428571429,
      "grad_norm": 0.21002092957496643,
      "learning_rate": 2.5485714285714285e-06,
      "loss": 0.1566,
      "step": 45810
    },
    {
      "epoch": 13.09142857142857,
      "grad_norm": 28.591821670532227,
      "learning_rate": 2.544761904761905e-06,
      "loss": 0.3054,
      "step": 45820
    },
    {
      "epoch": 13.094285714285714,
      "grad_norm": 0.11344537883996964,
      "learning_rate": 2.5409523809523813e-06,
      "loss": 0.2457,
      "step": 45830
    },
    {
      "epoch": 13.097142857142858,
      "grad_norm": 0.1161041259765625,
      "learning_rate": 2.5371428571428574e-06,
      "loss": 0.1887,
      "step": 45840
    },
    {
      "epoch": 13.1,
      "grad_norm": 0.43644312024116516,
      "learning_rate": 2.5333333333333338e-06,
      "loss": 0.4819,
      "step": 45850
    },
    {
      "epoch": 13.102857142857143,
      "grad_norm": 40.68330001831055,
      "learning_rate": 2.5295238095238094e-06,
      "loss": 0.2885,
      "step": 45860
    },
    {
      "epoch": 13.105714285714285,
      "grad_norm": 0.35691821575164795,
      "learning_rate": 2.525714285714286e-06,
      "loss": 0.4643,
      "step": 45870
    },
    {
      "epoch": 13.108571428571429,
      "grad_norm": 0.1488160341978073,
      "learning_rate": 2.5219047619047622e-06,
      "loss": 0.3221,
      "step": 45880
    },
    {
      "epoch": 13.111428571428572,
      "grad_norm": 0.32251065969467163,
      "learning_rate": 2.5180952380952382e-06,
      "loss": 0.2613,
      "step": 45890
    },
    {
      "epoch": 13.114285714285714,
      "grad_norm": 0.2327050119638443,
      "learning_rate": 2.5142857142857147e-06,
      "loss": 0.3677,
      "step": 45900
    },
    {
      "epoch": 13.117142857142857,
      "grad_norm": 0.04762672632932663,
      "learning_rate": 2.510476190476191e-06,
      "loss": 0.3451,
      "step": 45910
    },
    {
      "epoch": 13.12,
      "grad_norm": 0.6427881121635437,
      "learning_rate": 2.5066666666666667e-06,
      "loss": 0.4418,
      "step": 45920
    },
    {
      "epoch": 13.122857142857143,
      "grad_norm": 0.09537023305892944,
      "learning_rate": 2.502857142857143e-06,
      "loss": 0.3508,
      "step": 45930
    },
    {
      "epoch": 13.125714285714286,
      "grad_norm": 0.0059339129365980625,
      "learning_rate": 2.499047619047619e-06,
      "loss": 0.4401,
      "step": 45940
    },
    {
      "epoch": 13.128571428571428,
      "grad_norm": 0.056803781539201736,
      "learning_rate": 2.4952380952380955e-06,
      "loss": 0.5114,
      "step": 45950
    },
    {
      "epoch": 13.131428571428572,
      "grad_norm": 0.3980739116668701,
      "learning_rate": 2.4914285714285715e-06,
      "loss": 0.328,
      "step": 45960
    },
    {
      "epoch": 13.134285714285713,
      "grad_norm": 0.40108564496040344,
      "learning_rate": 2.487619047619048e-06,
      "loss": 0.2376,
      "step": 45970
    },
    {
      "epoch": 13.137142857142857,
      "grad_norm": 0.2881688177585602,
      "learning_rate": 2.483809523809524e-06,
      "loss": 0.2753,
      "step": 45980
    },
    {
      "epoch": 13.14,
      "grad_norm": 0.19554834067821503,
      "learning_rate": 2.4800000000000004e-06,
      "loss": 0.1091,
      "step": 45990
    },
    {
      "epoch": 13.142857142857142,
      "grad_norm": 0.3385273516178131,
      "learning_rate": 2.4761904761904764e-06,
      "loss": 0.1157,
      "step": 46000
    },
    {
      "epoch": 13.145714285714286,
      "grad_norm": 16.17573356628418,
      "learning_rate": 2.4723809523809524e-06,
      "loss": 0.6311,
      "step": 46010
    },
    {
      "epoch": 13.14857142857143,
      "grad_norm": 16.518115997314453,
      "learning_rate": 2.468571428571429e-06,
      "loss": 0.3923,
      "step": 46020
    },
    {
      "epoch": 13.151428571428571,
      "grad_norm": 15.093719482421875,
      "learning_rate": 2.4647619047619053e-06,
      "loss": 0.2702,
      "step": 46030
    },
    {
      "epoch": 13.154285714285715,
      "grad_norm": 0.7494841814041138,
      "learning_rate": 2.4609523809523813e-06,
      "loss": 0.1017,
      "step": 46040
    },
    {
      "epoch": 13.157142857142857,
      "grad_norm": 0.18246936798095703,
      "learning_rate": 2.4571428571428573e-06,
      "loss": 0.0076,
      "step": 46050
    },
    {
      "epoch": 13.16,
      "grad_norm": 39.23122024536133,
      "learning_rate": 2.4533333333333333e-06,
      "loss": 0.2288,
      "step": 46060
    },
    {
      "epoch": 13.162857142857144,
      "grad_norm": 0.531340479850769,
      "learning_rate": 2.4495238095238097e-06,
      "loss": 0.2662,
      "step": 46070
    },
    {
      "epoch": 13.165714285714285,
      "grad_norm": 0.05678462237119675,
      "learning_rate": 2.445714285714286e-06,
      "loss": 0.2988,
      "step": 46080
    },
    {
      "epoch": 13.168571428571429,
      "grad_norm": 0.1050289124250412,
      "learning_rate": 2.441904761904762e-06,
      "loss": 0.0776,
      "step": 46090
    },
    {
      "epoch": 13.17142857142857,
      "grad_norm": 0.0598042756319046,
      "learning_rate": 2.438095238095238e-06,
      "loss": 0.0027,
      "step": 46100
    },
    {
      "epoch": 13.174285714285714,
      "grad_norm": 0.5117185115814209,
      "learning_rate": 2.4342857142857146e-06,
      "loss": 0.0093,
      "step": 46110
    },
    {
      "epoch": 13.177142857142858,
      "grad_norm": 0.03671637177467346,
      "learning_rate": 2.4304761904761906e-06,
      "loss": 0.0982,
      "step": 46120
    },
    {
      "epoch": 13.18,
      "grad_norm": 0.2934418022632599,
      "learning_rate": 2.426666666666667e-06,
      "loss": 0.306,
      "step": 46130
    },
    {
      "epoch": 13.182857142857143,
      "grad_norm": 0.5902653932571411,
      "learning_rate": 2.422857142857143e-06,
      "loss": 0.0214,
      "step": 46140
    },
    {
      "epoch": 13.185714285714285,
      "grad_norm": 0.02384108304977417,
      "learning_rate": 2.419047619047619e-06,
      "loss": 0.0787,
      "step": 46150
    },
    {
      "epoch": 13.188571428571429,
      "grad_norm": 0.0541221909224987,
      "learning_rate": 2.4152380952380954e-06,
      "loss": 0.6079,
      "step": 46160
    },
    {
      "epoch": 13.191428571428572,
      "grad_norm": 0.41120821237564087,
      "learning_rate": 2.4114285714285715e-06,
      "loss": 0.1664,
      "step": 46170
    },
    {
      "epoch": 13.194285714285714,
      "grad_norm": 310.1622314453125,
      "learning_rate": 2.407619047619048e-06,
      "loss": 0.2626,
      "step": 46180
    },
    {
      "epoch": 13.197142857142858,
      "grad_norm": 0.20922128856182098,
      "learning_rate": 2.403809523809524e-06,
      "loss": 0.2912,
      "step": 46190
    },
    {
      "epoch": 13.2,
      "grad_norm": 0.5030141472816467,
      "learning_rate": 2.4000000000000003e-06,
      "loss": 0.2024,
      "step": 46200
    },
    {
      "epoch": 13.202857142857143,
      "grad_norm": 0.26781439781188965,
      "learning_rate": 2.3961904761904763e-06,
      "loss": 0.5138,
      "step": 46210
    },
    {
      "epoch": 13.205714285714286,
      "grad_norm": 0.6496488451957703,
      "learning_rate": 2.3923809523809527e-06,
      "loss": 0.3522,
      "step": 46220
    },
    {
      "epoch": 13.208571428571428,
      "grad_norm": 0.07936201244592667,
      "learning_rate": 2.3885714285714288e-06,
      "loss": 0.2769,
      "step": 46230
    },
    {
      "epoch": 13.211428571428572,
      "grad_norm": 0.06909286975860596,
      "learning_rate": 2.384761904761905e-06,
      "loss": 0.1599,
      "step": 46240
    },
    {
      "epoch": 13.214285714285714,
      "grad_norm": 0.07082011550664902,
      "learning_rate": 2.380952380952381e-06,
      "loss": 0.0021,
      "step": 46250
    },
    {
      "epoch": 13.217142857142857,
      "grad_norm": 0.015599831007421017,
      "learning_rate": 2.377142857142857e-06,
      "loss": 0.3051,
      "step": 46260
    },
    {
      "epoch": 13.22,
      "grad_norm": 0.1539023220539093,
      "learning_rate": 2.3733333333333336e-06,
      "loss": 0.0096,
      "step": 46270
    },
    {
      "epoch": 13.222857142857142,
      "grad_norm": 0.13586591184139252,
      "learning_rate": 2.3695238095238096e-06,
      "loss": 0.3569,
      "step": 46280
    },
    {
      "epoch": 13.225714285714286,
      "grad_norm": 0.22816798090934753,
      "learning_rate": 2.365714285714286e-06,
      "loss": 0.1434,
      "step": 46290
    },
    {
      "epoch": 13.228571428571428,
      "grad_norm": 0.09861104935407639,
      "learning_rate": 2.361904761904762e-06,
      "loss": 0.2463,
      "step": 46300
    },
    {
      "epoch": 13.231428571428571,
      "grad_norm": 67.55387115478516,
      "learning_rate": 2.358095238095238e-06,
      "loss": 0.4223,
      "step": 46310
    },
    {
      "epoch": 13.234285714285715,
      "grad_norm": 0.044649288058280945,
      "learning_rate": 2.3542857142857145e-06,
      "loss": 0.3878,
      "step": 46320
    },
    {
      "epoch": 13.237142857142857,
      "grad_norm": 12.519028663635254,
      "learning_rate": 2.350476190476191e-06,
      "loss": 0.2739,
      "step": 46330
    },
    {
      "epoch": 13.24,
      "grad_norm": 0.01997038722038269,
      "learning_rate": 2.346666666666667e-06,
      "loss": 0.0031,
      "step": 46340
    },
    {
      "epoch": 13.242857142857142,
      "grad_norm": 0.027657274156808853,
      "learning_rate": 2.342857142857143e-06,
      "loss": 0.0459,
      "step": 46350
    },
    {
      "epoch": 13.245714285714286,
      "grad_norm": 13.077661514282227,
      "learning_rate": 2.339047619047619e-06,
      "loss": 0.3854,
      "step": 46360
    },
    {
      "epoch": 13.248571428571429,
      "grad_norm": 0.0710868388414383,
      "learning_rate": 2.3352380952380954e-06,
      "loss": 0.2317,
      "step": 46370
    },
    {
      "epoch": 13.251428571428571,
      "grad_norm": 0.08904065936803818,
      "learning_rate": 2.331428571428572e-06,
      "loss": 0.0015,
      "step": 46380
    },
    {
      "epoch": 13.254285714285714,
      "grad_norm": 0.11776615679264069,
      "learning_rate": 2.327619047619048e-06,
      "loss": 0.0435,
      "step": 46390
    },
    {
      "epoch": 13.257142857142856,
      "grad_norm": 27.80153465270996,
      "learning_rate": 2.323809523809524e-06,
      "loss": 0.6178,
      "step": 46400
    },
    {
      "epoch": 13.26,
      "grad_norm": 0.12728506326675415,
      "learning_rate": 2.3200000000000002e-06,
      "loss": 0.0026,
      "step": 46410
    },
    {
      "epoch": 13.262857142857143,
      "grad_norm": 0.01918356865644455,
      "learning_rate": 2.3161904761904762e-06,
      "loss": 0.3673,
      "step": 46420
    },
    {
      "epoch": 13.265714285714285,
      "grad_norm": 0.12997901439666748,
      "learning_rate": 2.3123809523809527e-06,
      "loss": 0.7071,
      "step": 46430
    },
    {
      "epoch": 13.268571428571429,
      "grad_norm": 0.021763663738965988,
      "learning_rate": 2.3085714285714287e-06,
      "loss": 0.2165,
      "step": 46440
    },
    {
      "epoch": 13.271428571428572,
      "grad_norm": 0.11899780482053757,
      "learning_rate": 2.304761904761905e-06,
      "loss": 0.1765,
      "step": 46450
    },
    {
      "epoch": 13.274285714285714,
      "grad_norm": 21.114709854125977,
      "learning_rate": 2.300952380952381e-06,
      "loss": 0.585,
      "step": 46460
    },
    {
      "epoch": 13.277142857142858,
      "grad_norm": 0.08720658719539642,
      "learning_rate": 2.297142857142857e-06,
      "loss": 0.1871,
      "step": 46470
    },
    {
      "epoch": 13.28,
      "grad_norm": 0.20207397639751434,
      "learning_rate": 2.2933333333333335e-06,
      "loss": 0.0009,
      "step": 46480
    },
    {
      "epoch": 13.282857142857143,
      "grad_norm": 0.11091727763414383,
      "learning_rate": 2.28952380952381e-06,
      "loss": 0.1361,
      "step": 46490
    },
    {
      "epoch": 13.285714285714286,
      "grad_norm": 0.3744911849498749,
      "learning_rate": 2.285714285714286e-06,
      "loss": 0.209,
      "step": 46500
    },
    {
      "epoch": 13.288571428571428,
      "grad_norm": 0.06036365032196045,
      "learning_rate": 2.281904761904762e-06,
      "loss": 0.0011,
      "step": 46510
    },
    {
      "epoch": 13.291428571428572,
      "grad_norm": 0.08112978935241699,
      "learning_rate": 2.2780952380952384e-06,
      "loss": 0.162,
      "step": 46520
    },
    {
      "epoch": 13.294285714285714,
      "grad_norm": 0.3884560763835907,
      "learning_rate": 2.2742857142857144e-06,
      "loss": 0.2261,
      "step": 46530
    },
    {
      "epoch": 13.297142857142857,
      "grad_norm": 0.01591474935412407,
      "learning_rate": 2.270476190476191e-06,
      "loss": 0.2757,
      "step": 46540
    },
    {
      "epoch": 13.3,
      "grad_norm": 0.007673238404095173,
      "learning_rate": 2.266666666666667e-06,
      "loss": 0.2274,
      "step": 46550
    },
    {
      "epoch": 13.302857142857142,
      "grad_norm": 0.01908564567565918,
      "learning_rate": 2.262857142857143e-06,
      "loss": 0.2951,
      "step": 46560
    },
    {
      "epoch": 13.305714285714286,
      "grad_norm": 20.161149978637695,
      "learning_rate": 2.2590476190476193e-06,
      "loss": 0.3654,
      "step": 46570
    },
    {
      "epoch": 13.308571428571428,
      "grad_norm": 0.3724687695503235,
      "learning_rate": 2.2552380952380953e-06,
      "loss": 0.141,
      "step": 46580
    },
    {
      "epoch": 13.311428571428571,
      "grad_norm": 37.50647735595703,
      "learning_rate": 2.2514285714285717e-06,
      "loss": 0.4962,
      "step": 46590
    },
    {
      "epoch": 13.314285714285715,
      "grad_norm": 0.3845049738883972,
      "learning_rate": 2.2476190476190477e-06,
      "loss": 0.405,
      "step": 46600
    },
    {
      "epoch": 13.317142857142857,
      "grad_norm": 17.96099090576172,
      "learning_rate": 2.243809523809524e-06,
      "loss": 0.3491,
      "step": 46610
    },
    {
      "epoch": 13.32,
      "grad_norm": 0.01331198588013649,
      "learning_rate": 2.24e-06,
      "loss": 0.2928,
      "step": 46620
    },
    {
      "epoch": 13.322857142857142,
      "grad_norm": 0.08521701395511627,
      "learning_rate": 2.236190476190476e-06,
      "loss": 0.0988,
      "step": 46630
    },
    {
      "epoch": 13.325714285714286,
      "grad_norm": 0.23087652027606964,
      "learning_rate": 2.2323809523809526e-06,
      "loss": 0.4,
      "step": 46640
    },
    {
      "epoch": 13.32857142857143,
      "grad_norm": 0.016314957290887833,
      "learning_rate": 2.228571428571429e-06,
      "loss": 0.2992,
      "step": 46650
    },
    {
      "epoch": 13.331428571428571,
      "grad_norm": 0.04047925025224686,
      "learning_rate": 2.224761904761905e-06,
      "loss": 0.4303,
      "step": 46660
    },
    {
      "epoch": 13.334285714285715,
      "grad_norm": 38.415836334228516,
      "learning_rate": 2.220952380952381e-06,
      "loss": 0.4506,
      "step": 46670
    },
    {
      "epoch": 13.337142857142856,
      "grad_norm": 0.3302496671676636,
      "learning_rate": 2.2171428571428575e-06,
      "loss": 0.1041,
      "step": 46680
    },
    {
      "epoch": 13.34,
      "grad_norm": 0.14169760048389435,
      "learning_rate": 2.2133333333333335e-06,
      "loss": 0.1518,
      "step": 46690
    },
    {
      "epoch": 13.342857142857143,
      "grad_norm": 0.057069286704063416,
      "learning_rate": 2.20952380952381e-06,
      "loss": 0.384,
      "step": 46700
    },
    {
      "epoch": 13.345714285714285,
      "grad_norm": 0.11050910502672195,
      "learning_rate": 2.205714285714286e-06,
      "loss": 0.0494,
      "step": 46710
    },
    {
      "epoch": 13.348571428571429,
      "grad_norm": 0.03038208745419979,
      "learning_rate": 2.201904761904762e-06,
      "loss": 0.0092,
      "step": 46720
    },
    {
      "epoch": 13.35142857142857,
      "grad_norm": 38.47100830078125,
      "learning_rate": 2.1980952380952383e-06,
      "loss": 0.4659,
      "step": 46730
    },
    {
      "epoch": 13.354285714285714,
      "grad_norm": 0.03454859182238579,
      "learning_rate": 2.1942857142857143e-06,
      "loss": 0.4024,
      "step": 46740
    },
    {
      "epoch": 13.357142857142858,
      "grad_norm": 0.05535377934575081,
      "learning_rate": 2.1904761904761908e-06,
      "loss": 0.361,
      "step": 46750
    },
    {
      "epoch": 13.36,
      "grad_norm": 0.09263941645622253,
      "learning_rate": 2.1866666666666668e-06,
      "loss": 0.0147,
      "step": 46760
    },
    {
      "epoch": 13.362857142857143,
      "grad_norm": 0.0702851265668869,
      "learning_rate": 2.1828571428571428e-06,
      "loss": 0.0896,
      "step": 46770
    },
    {
      "epoch": 13.365714285714287,
      "grad_norm": 0.09197789430618286,
      "learning_rate": 2.179047619047619e-06,
      "loss": 0.2144,
      "step": 46780
    },
    {
      "epoch": 13.368571428571428,
      "grad_norm": 0.061618704348802567,
      "learning_rate": 2.1752380952380956e-06,
      "loss": 0.8202,
      "step": 46790
    },
    {
      "epoch": 13.371428571428572,
      "grad_norm": 0.2834652066230774,
      "learning_rate": 2.1714285714285716e-06,
      "loss": 0.0982,
      "step": 46800
    },
    {
      "epoch": 13.374285714285714,
      "grad_norm": 1.1802397966384888,
      "learning_rate": 2.1676190476190476e-06,
      "loss": 0.3471,
      "step": 46810
    },
    {
      "epoch": 13.377142857142857,
      "grad_norm": 22.81243133544922,
      "learning_rate": 2.163809523809524e-06,
      "loss": 0.596,
      "step": 46820
    },
    {
      "epoch": 13.38,
      "grad_norm": 0.03245134279131889,
      "learning_rate": 2.16e-06,
      "loss": 0.1501,
      "step": 46830
    },
    {
      "epoch": 13.382857142857143,
      "grad_norm": 0.03274116665124893,
      "learning_rate": 2.1561904761904765e-06,
      "loss": 0.1145,
      "step": 46840
    },
    {
      "epoch": 13.385714285714286,
      "grad_norm": 15.773589134216309,
      "learning_rate": 2.1523809523809525e-06,
      "loss": 0.1106,
      "step": 46850
    },
    {
      "epoch": 13.388571428571428,
      "grad_norm": 0.16052088141441345,
      "learning_rate": 2.148571428571429e-06,
      "loss": 0.0083,
      "step": 46860
    },
    {
      "epoch": 13.391428571428571,
      "grad_norm": 0.8843589425086975,
      "learning_rate": 2.144761904761905e-06,
      "loss": 0.3858,
      "step": 46870
    },
    {
      "epoch": 13.394285714285715,
      "grad_norm": 0.033978018909692764,
      "learning_rate": 2.140952380952381e-06,
      "loss": 0.4237,
      "step": 46880
    },
    {
      "epoch": 13.397142857142857,
      "grad_norm": 0.02193164825439453,
      "learning_rate": 2.1371428571428574e-06,
      "loss": 0.0037,
      "step": 46890
    },
    {
      "epoch": 13.4,
      "grad_norm": 0.04261736944317818,
      "learning_rate": 2.133333333333334e-06,
      "loss": 0.1161,
      "step": 46900
    },
    {
      "epoch": 13.402857142857142,
      "grad_norm": 0.21471230685710907,
      "learning_rate": 2.12952380952381e-06,
      "loss": 0.1584,
      "step": 46910
    },
    {
      "epoch": 13.405714285714286,
      "grad_norm": 0.3966287672519684,
      "learning_rate": 2.125714285714286e-06,
      "loss": 0.3424,
      "step": 46920
    },
    {
      "epoch": 13.40857142857143,
      "grad_norm": 0.18510611355304718,
      "learning_rate": 2.121904761904762e-06,
      "loss": 0.1648,
      "step": 46930
    },
    {
      "epoch": 13.411428571428571,
      "grad_norm": 0.5407851934432983,
      "learning_rate": 2.1180952380952382e-06,
      "loss": 0.1145,
      "step": 46940
    },
    {
      "epoch": 13.414285714285715,
      "grad_norm": 0.06632653623819351,
      "learning_rate": 2.1142857142857147e-06,
      "loss": 0.6271,
      "step": 46950
    },
    {
      "epoch": 13.417142857142856,
      "grad_norm": 0.01995719037950039,
      "learning_rate": 2.1104761904761907e-06,
      "loss": 0.0018,
      "step": 46960
    },
    {
      "epoch": 13.42,
      "grad_norm": 0.17990007996559143,
      "learning_rate": 2.1066666666666667e-06,
      "loss": 0.3051,
      "step": 46970
    },
    {
      "epoch": 13.422857142857143,
      "grad_norm": 0.03159859776496887,
      "learning_rate": 2.102857142857143e-06,
      "loss": 0.4684,
      "step": 46980
    },
    {
      "epoch": 13.425714285714285,
      "grad_norm": 0.04355575516819954,
      "learning_rate": 2.099047619047619e-06,
      "loss": 0.2381,
      "step": 46990
    },
    {
      "epoch": 13.428571428571429,
      "grad_norm": 0.0409669391810894,
      "learning_rate": 2.0952380952380955e-06,
      "loss": 0.2102,
      "step": 47000
    },
    {
      "epoch": 13.43142857142857,
      "grad_norm": 0.15568414330482483,
      "learning_rate": 2.0914285714285716e-06,
      "loss": 0.3207,
      "step": 47010
    },
    {
      "epoch": 13.434285714285714,
      "grad_norm": 0.020120635628700256,
      "learning_rate": 2.0876190476190476e-06,
      "loss": 0.0021,
      "step": 47020
    },
    {
      "epoch": 13.437142857142858,
      "grad_norm": 0.0527280792593956,
      "learning_rate": 2.083809523809524e-06,
      "loss": 0.207,
      "step": 47030
    },
    {
      "epoch": 13.44,
      "grad_norm": 0.13929890096187592,
      "learning_rate": 2.08e-06,
      "loss": 0.1278,
      "step": 47040
    },
    {
      "epoch": 13.442857142857143,
      "grad_norm": 0.012147855013608932,
      "learning_rate": 2.0761904761904764e-06,
      "loss": 0.2976,
      "step": 47050
    },
    {
      "epoch": 13.445714285714285,
      "grad_norm": 0.2361804097890854,
      "learning_rate": 2.072380952380953e-06,
      "loss": 0.3186,
      "step": 47060
    },
    {
      "epoch": 13.448571428571428,
      "grad_norm": 0.042846862226724625,
      "learning_rate": 2.068571428571429e-06,
      "loss": 0.0019,
      "step": 47070
    },
    {
      "epoch": 13.451428571428572,
      "grad_norm": 0.08755817264318466,
      "learning_rate": 2.064761904761905e-06,
      "loss": 0.6088,
      "step": 47080
    },
    {
      "epoch": 13.454285714285714,
      "grad_norm": 0.105312779545784,
      "learning_rate": 2.0609523809523813e-06,
      "loss": 0.0024,
      "step": 47090
    },
    {
      "epoch": 13.457142857142857,
      "grad_norm": 0.04431065544486046,
      "learning_rate": 2.0571428571428573e-06,
      "loss": 0.1801,
      "step": 47100
    },
    {
      "epoch": 13.46,
      "grad_norm": 3.735261917114258,
      "learning_rate": 2.0533333333333337e-06,
      "loss": 0.378,
      "step": 47110
    },
    {
      "epoch": 13.462857142857143,
      "grad_norm": 0.33719387650489807,
      "learning_rate": 2.0495238095238097e-06,
      "loss": 0.4531,
      "step": 47120
    },
    {
      "epoch": 13.465714285714286,
      "grad_norm": 0.17195603251457214,
      "learning_rate": 2.0457142857142857e-06,
      "loss": 0.2029,
      "step": 47130
    },
    {
      "epoch": 13.468571428571428,
      "grad_norm": 0.11722034960985184,
      "learning_rate": 2.041904761904762e-06,
      "loss": 0.1907,
      "step": 47140
    },
    {
      "epoch": 13.471428571428572,
      "grad_norm": 20.072635650634766,
      "learning_rate": 2.038095238095238e-06,
      "loss": 0.2565,
      "step": 47150
    },
    {
      "epoch": 13.474285714285715,
      "grad_norm": 0.015156557783484459,
      "learning_rate": 2.0342857142857146e-06,
      "loss": 0.1305,
      "step": 47160
    },
    {
      "epoch": 13.477142857142857,
      "grad_norm": 0.0803307294845581,
      "learning_rate": 2.0304761904761906e-06,
      "loss": 0.2265,
      "step": 47170
    },
    {
      "epoch": 13.48,
      "grad_norm": 0.08520731329917908,
      "learning_rate": 2.0266666666666666e-06,
      "loss": 0.0662,
      "step": 47180
    },
    {
      "epoch": 13.482857142857142,
      "grad_norm": 0.14450837671756744,
      "learning_rate": 2.022857142857143e-06,
      "loss": 0.2595,
      "step": 47190
    },
    {
      "epoch": 13.485714285714286,
      "grad_norm": 0.07218042761087418,
      "learning_rate": 2.019047619047619e-06,
      "loss": 0.3475,
      "step": 47200
    },
    {
      "epoch": 13.48857142857143,
      "grad_norm": 5.149697303771973,
      "learning_rate": 2.0152380952380955e-06,
      "loss": 0.1167,
      "step": 47210
    },
    {
      "epoch": 13.491428571428571,
      "grad_norm": 0.038214076310396194,
      "learning_rate": 2.0114285714285715e-06,
      "loss": 0.3278,
      "step": 47220
    },
    {
      "epoch": 13.494285714285715,
      "grad_norm": 0.1087978333234787,
      "learning_rate": 2.007619047619048e-06,
      "loss": 0.0017,
      "step": 47230
    },
    {
      "epoch": 13.497142857142856,
      "grad_norm": 0.01974228024482727,
      "learning_rate": 2.003809523809524e-06,
      "loss": 0.172,
      "step": 47240
    },
    {
      "epoch": 13.5,
      "grad_norm": 0.03328251093626022,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.002,
      "step": 47250
    },
    {
      "epoch": 13.502857142857144,
      "grad_norm": 16.42141342163086,
      "learning_rate": 1.9961904761904763e-06,
      "loss": 0.1623,
      "step": 47260
    },
    {
      "epoch": 13.505714285714285,
      "grad_norm": 0.04020147770643234,
      "learning_rate": 1.9923809523809528e-06,
      "loss": 0.0007,
      "step": 47270
    },
    {
      "epoch": 13.508571428571429,
      "grad_norm": 15.029295921325684,
      "learning_rate": 1.9885714285714288e-06,
      "loss": 0.3755,
      "step": 47280
    },
    {
      "epoch": 13.51142857142857,
      "grad_norm": 0.5769190192222595,
      "learning_rate": 1.9847619047619048e-06,
      "loss": 0.1209,
      "step": 47290
    },
    {
      "epoch": 13.514285714285714,
      "grad_norm": 117.860595703125,
      "learning_rate": 1.980952380952381e-06,
      "loss": 0.3245,
      "step": 47300
    },
    {
      "epoch": 13.517142857142858,
      "grad_norm": 0.07189932465553284,
      "learning_rate": 1.977142857142857e-06,
      "loss": 0.1918,
      "step": 47310
    },
    {
      "epoch": 13.52,
      "grad_norm": 0.18851646780967712,
      "learning_rate": 1.9733333333333336e-06,
      "loss": 0.0733,
      "step": 47320
    },
    {
      "epoch": 13.522857142857143,
      "grad_norm": 14.748387336730957,
      "learning_rate": 1.9695238095238096e-06,
      "loss": 0.7985,
      "step": 47330
    },
    {
      "epoch": 13.525714285714285,
      "grad_norm": 0.04192890226840973,
      "learning_rate": 1.9657142857142856e-06,
      "loss": 0.1188,
      "step": 47340
    },
    {
      "epoch": 13.528571428571428,
      "grad_norm": 4.118359088897705,
      "learning_rate": 1.961904761904762e-06,
      "loss": 0.3532,
      "step": 47350
    },
    {
      "epoch": 13.531428571428572,
      "grad_norm": 16.307315826416016,
      "learning_rate": 1.9580952380952385e-06,
      "loss": 0.4278,
      "step": 47360
    },
    {
      "epoch": 13.534285714285714,
      "grad_norm": 0.5162439346313477,
      "learning_rate": 1.9542857142857145e-06,
      "loss": 0.187,
      "step": 47370
    },
    {
      "epoch": 13.537142857142857,
      "grad_norm": 0.021139806136488914,
      "learning_rate": 1.9504761904761905e-06,
      "loss": 0.124,
      "step": 47380
    },
    {
      "epoch": 13.54,
      "grad_norm": 0.23365698754787445,
      "learning_rate": 1.9466666666666665e-06,
      "loss": 0.133,
      "step": 47390
    },
    {
      "epoch": 13.542857142857143,
      "grad_norm": 8.05354118347168,
      "learning_rate": 1.942857142857143e-06,
      "loss": 0.2764,
      "step": 47400
    },
    {
      "epoch": 13.545714285714286,
      "grad_norm": 15.441725730895996,
      "learning_rate": 1.9390476190476194e-06,
      "loss": 0.2779,
      "step": 47410
    },
    {
      "epoch": 13.548571428571428,
      "grad_norm": 0.04619184508919716,
      "learning_rate": 1.9352380952380954e-06,
      "loss": 0.5489,
      "step": 47420
    },
    {
      "epoch": 13.551428571428572,
      "grad_norm": 0.14308534562587738,
      "learning_rate": 1.9314285714285714e-06,
      "loss": 0.1547,
      "step": 47430
    },
    {
      "epoch": 13.554285714285715,
      "grad_norm": 16.715024948120117,
      "learning_rate": 1.927619047619048e-06,
      "loss": 0.3171,
      "step": 47440
    },
    {
      "epoch": 13.557142857142857,
      "grad_norm": 12.669123649597168,
      "learning_rate": 1.923809523809524e-06,
      "loss": 0.1789,
      "step": 47450
    },
    {
      "epoch": 13.56,
      "grad_norm": 0.20603132247924805,
      "learning_rate": 1.9200000000000003e-06,
      "loss": 0.3801,
      "step": 47460
    },
    {
      "epoch": 13.562857142857142,
      "grad_norm": 0.03524617478251457,
      "learning_rate": 1.9161904761904763e-06,
      "loss": 0.137,
      "step": 47470
    },
    {
      "epoch": 13.565714285714286,
      "grad_norm": 14.15065860748291,
      "learning_rate": 1.9123809523809527e-06,
      "loss": 0.267,
      "step": 47480
    },
    {
      "epoch": 13.56857142857143,
      "grad_norm": 0.09135687351226807,
      "learning_rate": 1.9085714285714287e-06,
      "loss": 0.1666,
      "step": 47490
    },
    {
      "epoch": 13.571428571428571,
      "grad_norm": 0.1037163957953453,
      "learning_rate": 1.904761904761905e-06,
      "loss": 0.3036,
      "step": 47500
    },
    {
      "epoch": 13.574285714285715,
      "grad_norm": 0.15286508202552795,
      "learning_rate": 1.9009523809523811e-06,
      "loss": 0.3868,
      "step": 47510
    },
    {
      "epoch": 13.577142857142857,
      "grad_norm": 27.936580657958984,
      "learning_rate": 1.8971428571428573e-06,
      "loss": 0.3106,
      "step": 47520
    },
    {
      "epoch": 13.58,
      "grad_norm": 32.50297927856445,
      "learning_rate": 1.8933333333333333e-06,
      "loss": 0.1674,
      "step": 47530
    },
    {
      "epoch": 13.582857142857144,
      "grad_norm": 0.06459101289510727,
      "learning_rate": 1.8895238095238098e-06,
      "loss": 0.2821,
      "step": 47540
    },
    {
      "epoch": 13.585714285714285,
      "grad_norm": 0.04076733440160751,
      "learning_rate": 1.885714285714286e-06,
      "loss": 0.1468,
      "step": 47550
    },
    {
      "epoch": 13.588571428571429,
      "grad_norm": 0.04105465114116669,
      "learning_rate": 1.881904761904762e-06,
      "loss": 0.0747,
      "step": 47560
    },
    {
      "epoch": 13.59142857142857,
      "grad_norm": 0.08187608420848846,
      "learning_rate": 1.8780952380952382e-06,
      "loss": 0.1249,
      "step": 47570
    },
    {
      "epoch": 13.594285714285714,
      "grad_norm": 0.03454134240746498,
      "learning_rate": 1.8742857142857142e-06,
      "loss": 0.3562,
      "step": 47580
    },
    {
      "epoch": 13.597142857142858,
      "grad_norm": 0.11177327483892441,
      "learning_rate": 1.8704761904761906e-06,
      "loss": 0.0022,
      "step": 47590
    },
    {
      "epoch": 13.6,
      "grad_norm": 52.54182052612305,
      "learning_rate": 1.8666666666666669e-06,
      "loss": 0.1802,
      "step": 47600
    },
    {
      "epoch": 13.602857142857143,
      "grad_norm": 25.146541595458984,
      "learning_rate": 1.8628571428571429e-06,
      "loss": 0.416,
      "step": 47610
    },
    {
      "epoch": 13.605714285714285,
      "grad_norm": 0.029684092849493027,
      "learning_rate": 1.8590476190476193e-06,
      "loss": 0.2588,
      "step": 47620
    },
    {
      "epoch": 13.608571428571429,
      "grad_norm": 0.0796007290482521,
      "learning_rate": 1.8552380952380955e-06,
      "loss": 0.0014,
      "step": 47630
    },
    {
      "epoch": 13.611428571428572,
      "grad_norm": 0.059517525136470795,
      "learning_rate": 1.8514285714285715e-06,
      "loss": 0.1619,
      "step": 47640
    },
    {
      "epoch": 13.614285714285714,
      "grad_norm": 0.15394635498523712,
      "learning_rate": 1.8476190476190477e-06,
      "loss": 0.2665,
      "step": 47650
    },
    {
      "epoch": 13.617142857142857,
      "grad_norm": 23.468921661376953,
      "learning_rate": 1.8438095238095237e-06,
      "loss": 0.579,
      "step": 47660
    },
    {
      "epoch": 13.62,
      "grad_norm": 0.09844359755516052,
      "learning_rate": 1.8400000000000002e-06,
      "loss": 0.2671,
      "step": 47670
    },
    {
      "epoch": 13.622857142857143,
      "grad_norm": 20.923084259033203,
      "learning_rate": 1.8361904761904764e-06,
      "loss": 0.8099,
      "step": 47680
    },
    {
      "epoch": 13.625714285714286,
      "grad_norm": 23.257715225219727,
      "learning_rate": 1.8323809523809524e-06,
      "loss": 0.147,
      "step": 47690
    },
    {
      "epoch": 13.628571428571428,
      "grad_norm": 14.959022521972656,
      "learning_rate": 1.8285714285714288e-06,
      "loss": 0.1532,
      "step": 47700
    },
    {
      "epoch": 13.631428571428572,
      "grad_norm": 651.6953125,
      "learning_rate": 1.824761904761905e-06,
      "loss": 0.2457,
      "step": 47710
    },
    {
      "epoch": 13.634285714285713,
      "grad_norm": 0.0868568941950798,
      "learning_rate": 1.820952380952381e-06,
      "loss": 0.1182,
      "step": 47720
    },
    {
      "epoch": 13.637142857142857,
      "grad_norm": 0.07302000373601913,
      "learning_rate": 1.8171428571428573e-06,
      "loss": 0.4121,
      "step": 47730
    },
    {
      "epoch": 13.64,
      "grad_norm": 0.11809363961219788,
      "learning_rate": 1.8133333333333337e-06,
      "loss": 0.1338,
      "step": 47740
    },
    {
      "epoch": 13.642857142857142,
      "grad_norm": 35.66688537597656,
      "learning_rate": 1.8095238095238097e-06,
      "loss": 0.5383,
      "step": 47750
    },
    {
      "epoch": 13.645714285714286,
      "grad_norm": 0.3392573595046997,
      "learning_rate": 1.805714285714286e-06,
      "loss": 0.741,
      "step": 47760
    },
    {
      "epoch": 13.64857142857143,
      "grad_norm": 0.14758692681789398,
      "learning_rate": 1.801904761904762e-06,
      "loss": 0.4011,
      "step": 47770
    },
    {
      "epoch": 13.651428571428571,
      "grad_norm": 14.731741905212402,
      "learning_rate": 1.7980952380952381e-06,
      "loss": 0.3095,
      "step": 47780
    },
    {
      "epoch": 13.654285714285715,
      "grad_norm": 15.75442886352539,
      "learning_rate": 1.7942857142857146e-06,
      "loss": 0.1002,
      "step": 47790
    },
    {
      "epoch": 13.657142857142857,
      "grad_norm": 15.73099422454834,
      "learning_rate": 1.7904761904761906e-06,
      "loss": 0.6288,
      "step": 47800
    },
    {
      "epoch": 13.66,
      "grad_norm": 13.145395278930664,
      "learning_rate": 1.7866666666666668e-06,
      "loss": 0.4234,
      "step": 47810
    },
    {
      "epoch": 13.662857142857142,
      "grad_norm": 0.2869630455970764,
      "learning_rate": 1.7828571428571432e-06,
      "loss": 0.2454,
      "step": 47820
    },
    {
      "epoch": 13.665714285714285,
      "grad_norm": 0.03923416882753372,
      "learning_rate": 1.7790476190476192e-06,
      "loss": 0.312,
      "step": 47830
    },
    {
      "epoch": 13.668571428571429,
      "grad_norm": 0.22948987782001495,
      "learning_rate": 1.7752380952380954e-06,
      "loss": 0.4135,
      "step": 47840
    },
    {
      "epoch": 13.67142857142857,
      "grad_norm": 2.7751519680023193,
      "learning_rate": 1.7714285714285714e-06,
      "loss": 0.1389,
      "step": 47850
    },
    {
      "epoch": 13.674285714285714,
      "grad_norm": 0.11021255701780319,
      "learning_rate": 1.7676190476190477e-06,
      "loss": 0.2307,
      "step": 47860
    },
    {
      "epoch": 13.677142857142858,
      "grad_norm": 0.05993044748902321,
      "learning_rate": 1.763809523809524e-06,
      "loss": 0.0032,
      "step": 47870
    },
    {
      "epoch": 13.68,
      "grad_norm": 0.12211205810308456,
      "learning_rate": 1.76e-06,
      "loss": 0.0826,
      "step": 47880
    },
    {
      "epoch": 13.682857142857143,
      "grad_norm": 29.86172103881836,
      "learning_rate": 1.7561904761904763e-06,
      "loss": 0.6957,
      "step": 47890
    },
    {
      "epoch": 13.685714285714285,
      "grad_norm": 0.036170441657304764,
      "learning_rate": 1.7523809523809525e-06,
      "loss": 0.2838,
      "step": 47900
    },
    {
      "epoch": 13.688571428571429,
      "grad_norm": 0.1121567040681839,
      "learning_rate": 1.7485714285714287e-06,
      "loss": 0.1488,
      "step": 47910
    },
    {
      "epoch": 13.691428571428572,
      "grad_norm": 0.0997244343161583,
      "learning_rate": 1.744761904761905e-06,
      "loss": 0.2138,
      "step": 47920
    },
    {
      "epoch": 13.694285714285714,
      "grad_norm": 3.1237382888793945,
      "learning_rate": 1.7409523809523812e-06,
      "loss": 0.123,
      "step": 47930
    },
    {
      "epoch": 13.697142857142858,
      "grad_norm": 14.421210289001465,
      "learning_rate": 1.7371428571428572e-06,
      "loss": 0.7143,
      "step": 47940
    },
    {
      "epoch": 13.7,
      "grad_norm": 0.28519755601882935,
      "learning_rate": 1.7333333333333336e-06,
      "loss": 0.0023,
      "step": 47950
    },
    {
      "epoch": 13.702857142857143,
      "grad_norm": 0.08781873434782028,
      "learning_rate": 1.7295238095238096e-06,
      "loss": 0.6841,
      "step": 47960
    },
    {
      "epoch": 13.705714285714286,
      "grad_norm": 6.343773365020752,
      "learning_rate": 1.7257142857142858e-06,
      "loss": 0.2916,
      "step": 47970
    },
    {
      "epoch": 13.708571428571428,
      "grad_norm": 13.564789772033691,
      "learning_rate": 1.721904761904762e-06,
      "loss": 0.7897,
      "step": 47980
    },
    {
      "epoch": 13.711428571428572,
      "grad_norm": 0.1487492322921753,
      "learning_rate": 1.718095238095238e-06,
      "loss": 0.0166,
      "step": 47990
    },
    {
      "epoch": 13.714285714285714,
      "grad_norm": 0.15333785116672516,
      "learning_rate": 1.7142857142857145e-06,
      "loss": 0.316,
      "step": 48000
    },
    {
      "epoch": 13.717142857142857,
      "grad_norm": 28.146270751953125,
      "learning_rate": 1.7104761904761907e-06,
      "loss": 0.0821,
      "step": 48010
    },
    {
      "epoch": 13.72,
      "grad_norm": 0.28718677163124084,
      "learning_rate": 1.7066666666666667e-06,
      "loss": 0.475,
      "step": 48020
    },
    {
      "epoch": 13.722857142857142,
      "grad_norm": 0.18215838074684143,
      "learning_rate": 1.7028571428571431e-06,
      "loss": 0.0035,
      "step": 48030
    },
    {
      "epoch": 13.725714285714286,
      "grad_norm": 0.10633637756109238,
      "learning_rate": 1.6990476190476191e-06,
      "loss": 0.0729,
      "step": 48040
    },
    {
      "epoch": 13.728571428571428,
      "grad_norm": 0.03160155937075615,
      "learning_rate": 1.6952380952380954e-06,
      "loss": 0.3353,
      "step": 48050
    },
    {
      "epoch": 13.731428571428571,
      "grad_norm": 0.0745348259806633,
      "learning_rate": 1.6914285714285716e-06,
      "loss": 0.0962,
      "step": 48060
    },
    {
      "epoch": 13.734285714285715,
      "grad_norm": 172.51394653320312,
      "learning_rate": 1.6876190476190476e-06,
      "loss": 0.2953,
      "step": 48070
    },
    {
      "epoch": 13.737142857142857,
      "grad_norm": 0.3545401394367218,
      "learning_rate": 1.683809523809524e-06,
      "loss": 0.3211,
      "step": 48080
    },
    {
      "epoch": 13.74,
      "grad_norm": 0.05095776170492172,
      "learning_rate": 1.6800000000000002e-06,
      "loss": 0.0024,
      "step": 48090
    },
    {
      "epoch": 13.742857142857144,
      "grad_norm": 0.09167908877134323,
      "learning_rate": 1.6761904761904762e-06,
      "loss": 0.4318,
      "step": 48100
    },
    {
      "epoch": 13.745714285714286,
      "grad_norm": 13.98309326171875,
      "learning_rate": 1.6723809523809527e-06,
      "loss": 0.1204,
      "step": 48110
    },
    {
      "epoch": 13.748571428571429,
      "grad_norm": 0.1499256044626236,
      "learning_rate": 1.6685714285714289e-06,
      "loss": 0.0035,
      "step": 48120
    },
    {
      "epoch": 13.751428571428571,
      "grad_norm": 0.15351548790931702,
      "learning_rate": 1.6647619047619049e-06,
      "loss": 0.4606,
      "step": 48130
    },
    {
      "epoch": 13.754285714285714,
      "grad_norm": 0.3413710296154022,
      "learning_rate": 1.660952380952381e-06,
      "loss": 0.1142,
      "step": 48140
    },
    {
      "epoch": 13.757142857142856,
      "grad_norm": 0.6060353517532349,
      "learning_rate": 1.657142857142857e-06,
      "loss": 0.4267,
      "step": 48150
    },
    {
      "epoch": 13.76,
      "grad_norm": 0.1473872810602188,
      "learning_rate": 1.6533333333333335e-06,
      "loss": 0.5284,
      "step": 48160
    },
    {
      "epoch": 13.762857142857143,
      "grad_norm": 0.04210086539387703,
      "learning_rate": 1.6495238095238097e-06,
      "loss": 0.1598,
      "step": 48170
    },
    {
      "epoch": 13.765714285714285,
      "grad_norm": 0.1150469034910202,
      "learning_rate": 1.6457142857142857e-06,
      "loss": 0.7238,
      "step": 48180
    },
    {
      "epoch": 13.768571428571429,
      "grad_norm": 0.07553970813751221,
      "learning_rate": 1.641904761904762e-06,
      "loss": 0.1027,
      "step": 48190
    },
    {
      "epoch": 13.771428571428572,
      "grad_norm": 213.76429748535156,
      "learning_rate": 1.6380952380952384e-06,
      "loss": 0.1444,
      "step": 48200
    },
    {
      "epoch": 13.774285714285714,
      "grad_norm": 0.08662375807762146,
      "learning_rate": 1.6342857142857144e-06,
      "loss": 0.0153,
      "step": 48210
    },
    {
      "epoch": 13.777142857142858,
      "grad_norm": 0.021453462541103363,
      "learning_rate": 1.6304761904761906e-06,
      "loss": 0.0024,
      "step": 48220
    },
    {
      "epoch": 13.78,
      "grad_norm": 17.942319869995117,
      "learning_rate": 1.6266666666666666e-06,
      "loss": 0.304,
      "step": 48230
    },
    {
      "epoch": 13.782857142857143,
      "grad_norm": 0.17700648307800293,
      "learning_rate": 1.622857142857143e-06,
      "loss": 0.0032,
      "step": 48240
    },
    {
      "epoch": 13.785714285714286,
      "grad_norm": 361.9914855957031,
      "learning_rate": 1.6190476190476193e-06,
      "loss": 0.577,
      "step": 48250
    },
    {
      "epoch": 13.788571428571428,
      "grad_norm": 20.276199340820312,
      "learning_rate": 1.6152380952380953e-06,
      "loss": 0.3991,
      "step": 48260
    },
    {
      "epoch": 13.791428571428572,
      "grad_norm": 226.36279296875,
      "learning_rate": 1.6114285714285715e-06,
      "loss": 0.4419,
      "step": 48270
    },
    {
      "epoch": 13.794285714285714,
      "grad_norm": 0.15863923728466034,
      "learning_rate": 1.607619047619048e-06,
      "loss": 0.075,
      "step": 48280
    },
    {
      "epoch": 13.797142857142857,
      "grad_norm": 0.3346579372882843,
      "learning_rate": 1.603809523809524e-06,
      "loss": 0.2215,
      "step": 48290
    },
    {
      "epoch": 13.8,
      "grad_norm": 0.02197141759097576,
      "learning_rate": 1.6000000000000001e-06,
      "loss": 0.1394,
      "step": 48300
    },
    {
      "epoch": 13.802857142857142,
      "grad_norm": 2.2344067096710205,
      "learning_rate": 1.5961904761904764e-06,
      "loss": 0.5957,
      "step": 48310
    },
    {
      "epoch": 13.805714285714286,
      "grad_norm": 356.5555419921875,
      "learning_rate": 1.5923809523809526e-06,
      "loss": 0.7248,
      "step": 48320
    },
    {
      "epoch": 13.808571428571428,
      "grad_norm": 0.03112710453569889,
      "learning_rate": 1.5885714285714288e-06,
      "loss": 0.0024,
      "step": 48330
    },
    {
      "epoch": 13.811428571428571,
      "grad_norm": 0.3004418611526489,
      "learning_rate": 1.5847619047619048e-06,
      "loss": 0.0528,
      "step": 48340
    },
    {
      "epoch": 13.814285714285715,
      "grad_norm": 0.06101781502366066,
      "learning_rate": 1.580952380952381e-06,
      "loss": 0.2688,
      "step": 48350
    },
    {
      "epoch": 13.817142857142857,
      "grad_norm": 0.2884359657764435,
      "learning_rate": 1.5771428571428574e-06,
      "loss": 0.1909,
      "step": 48360
    },
    {
      "epoch": 13.82,
      "grad_norm": 0.1783076673746109,
      "learning_rate": 1.5733333333333334e-06,
      "loss": 0.658,
      "step": 48370
    },
    {
      "epoch": 13.822857142857142,
      "grad_norm": 0.06648951768875122,
      "learning_rate": 1.5695238095238097e-06,
      "loss": 0.1254,
      "step": 48380
    },
    {
      "epoch": 13.825714285714286,
      "grad_norm": 0.13879327476024628,
      "learning_rate": 1.5657142857142859e-06,
      "loss": 0.0038,
      "step": 48390
    },
    {
      "epoch": 13.82857142857143,
      "grad_norm": 0.08068142831325531,
      "learning_rate": 1.5619047619047619e-06,
      "loss": 0.2639,
      "step": 48400
    },
    {
      "epoch": 13.831428571428571,
      "grad_norm": 0.04039579629898071,
      "learning_rate": 1.5580952380952383e-06,
      "loss": 0.1819,
      "step": 48410
    },
    {
      "epoch": 13.834285714285715,
      "grad_norm": 0.07680529356002808,
      "learning_rate": 1.5542857142857143e-06,
      "loss": 0.2636,
      "step": 48420
    },
    {
      "epoch": 13.837142857142858,
      "grad_norm": 0.06895073503255844,
      "learning_rate": 1.5504761904761905e-06,
      "loss": 0.001,
      "step": 48430
    },
    {
      "epoch": 13.84,
      "grad_norm": 11.614717483520508,
      "learning_rate": 1.546666666666667e-06,
      "loss": 0.1537,
      "step": 48440
    },
    {
      "epoch": 13.842857142857143,
      "grad_norm": 0.1654829978942871,
      "learning_rate": 1.542857142857143e-06,
      "loss": 0.189,
      "step": 48450
    },
    {
      "epoch": 13.845714285714285,
      "grad_norm": 0.03875747323036194,
      "learning_rate": 1.5390476190476192e-06,
      "loss": 0.016,
      "step": 48460
    },
    {
      "epoch": 13.848571428571429,
      "grad_norm": 0.03187165409326553,
      "learning_rate": 1.5352380952380954e-06,
      "loss": 0.1849,
      "step": 48470
    },
    {
      "epoch": 13.85142857142857,
      "grad_norm": 0.19960980117321014,
      "learning_rate": 1.5314285714285714e-06,
      "loss": 0.0032,
      "step": 48480
    },
    {
      "epoch": 13.854285714285714,
      "grad_norm": 0.026290131732821465,
      "learning_rate": 1.5276190476190478e-06,
      "loss": 0.023,
      "step": 48490
    },
    {
      "epoch": 13.857142857142858,
      "grad_norm": 0.0748661532998085,
      "learning_rate": 1.523809523809524e-06,
      "loss": 0.0086,
      "step": 48500
    },
    {
      "epoch": 13.86,
      "grad_norm": 0.10246998816728592,
      "learning_rate": 1.52e-06,
      "loss": 0.4316,
      "step": 48510
    },
    {
      "epoch": 13.862857142857143,
      "grad_norm": 0.040155645459890366,
      "learning_rate": 1.5161904761904763e-06,
      "loss": 0.5594,
      "step": 48520
    },
    {
      "epoch": 13.865714285714287,
      "grad_norm": 0.19866609573364258,
      "learning_rate": 1.5123809523809525e-06,
      "loss": 0.0019,
      "step": 48530
    },
    {
      "epoch": 13.868571428571428,
      "grad_norm": 0.06780388206243515,
      "learning_rate": 1.5085714285714287e-06,
      "loss": 0.118,
      "step": 48540
    },
    {
      "epoch": 13.871428571428572,
      "grad_norm": 0.1748766303062439,
      "learning_rate": 1.504761904761905e-06,
      "loss": 0.4565,
      "step": 48550
    },
    {
      "epoch": 13.874285714285714,
      "grad_norm": 0.02577880769968033,
      "learning_rate": 1.500952380952381e-06,
      "loss": 0.1293,
      "step": 48560
    },
    {
      "epoch": 13.877142857142857,
      "grad_norm": 0.19353702664375305,
      "learning_rate": 1.4971428571428574e-06,
      "loss": 0.0064,
      "step": 48570
    },
    {
      "epoch": 13.88,
      "grad_norm": 0.02398303896188736,
      "learning_rate": 1.4933333333333336e-06,
      "loss": 0.0016,
      "step": 48580
    },
    {
      "epoch": 13.882857142857143,
      "grad_norm": 28.00525665283203,
      "learning_rate": 1.4895238095238096e-06,
      "loss": 0.402,
      "step": 48590
    },
    {
      "epoch": 13.885714285714286,
      "grad_norm": 0.29207128286361694,
      "learning_rate": 1.4857142857142858e-06,
      "loss": 0.2234,
      "step": 48600
    },
    {
      "epoch": 13.888571428571428,
      "grad_norm": 0.16371269524097443,
      "learning_rate": 1.4819047619047618e-06,
      "loss": 0.0854,
      "step": 48610
    },
    {
      "epoch": 13.891428571428571,
      "grad_norm": 15.729501724243164,
      "learning_rate": 1.4780952380952382e-06,
      "loss": 0.2528,
      "step": 48620
    },
    {
      "epoch": 13.894285714285715,
      "grad_norm": 0.05527295172214508,
      "learning_rate": 1.4742857142857144e-06,
      "loss": 0.1239,
      "step": 48630
    },
    {
      "epoch": 13.897142857142857,
      "grad_norm": 38.62812423706055,
      "learning_rate": 1.4704761904761905e-06,
      "loss": 0.3998,
      "step": 48640
    },
    {
      "epoch": 13.9,
      "grad_norm": 21.107555389404297,
      "learning_rate": 1.4666666666666669e-06,
      "loss": 0.3454,
      "step": 48650
    },
    {
      "epoch": 13.902857142857142,
      "grad_norm": 4.736466407775879,
      "learning_rate": 1.462857142857143e-06,
      "loss": 0.0039,
      "step": 48660
    },
    {
      "epoch": 13.905714285714286,
      "grad_norm": 0.2973150610923767,
      "learning_rate": 1.459047619047619e-06,
      "loss": 0.0019,
      "step": 48670
    },
    {
      "epoch": 13.90857142857143,
      "grad_norm": 1.1475942134857178,
      "learning_rate": 1.4552380952380953e-06,
      "loss": 0.3821,
      "step": 48680
    },
    {
      "epoch": 13.911428571428571,
      "grad_norm": 0.16100043058395386,
      "learning_rate": 1.4514285714285713e-06,
      "loss": 0.0027,
      "step": 48690
    },
    {
      "epoch": 13.914285714285715,
      "grad_norm": 64.3623275756836,
      "learning_rate": 1.4476190476190478e-06,
      "loss": 0.0706,
      "step": 48700
    },
    {
      "epoch": 13.917142857142856,
      "grad_norm": 0.5574236512184143,
      "learning_rate": 1.443809523809524e-06,
      "loss": 0.5193,
      "step": 48710
    },
    {
      "epoch": 13.92,
      "grad_norm": 0.13775533437728882,
      "learning_rate": 1.44e-06,
      "loss": 0.1238,
      "step": 48720
    },
    {
      "epoch": 13.922857142857143,
      "grad_norm": 25.954294204711914,
      "learning_rate": 1.4361904761904764e-06,
      "loss": 0.0579,
      "step": 48730
    },
    {
      "epoch": 13.925714285714285,
      "grad_norm": 0.4593219757080078,
      "learning_rate": 1.4323809523809526e-06,
      "loss": 0.1234,
      "step": 48740
    },
    {
      "epoch": 13.928571428571429,
      "grad_norm": 0.5183882117271423,
      "learning_rate": 1.4285714285714286e-06,
      "loss": 0.0015,
      "step": 48750
    },
    {
      "epoch": 13.93142857142857,
      "grad_norm": 0.11657769978046417,
      "learning_rate": 1.4247619047619048e-06,
      "loss": 0.0031,
      "step": 48760
    },
    {
      "epoch": 13.934285714285714,
      "grad_norm": 0.13834992051124573,
      "learning_rate": 1.4209523809523813e-06,
      "loss": 0.0617,
      "step": 48770
    },
    {
      "epoch": 13.937142857142858,
      "grad_norm": 0.12048212438821793,
      "learning_rate": 1.4171428571428573e-06,
      "loss": 0.4488,
      "step": 48780
    },
    {
      "epoch": 13.94,
      "grad_norm": 0.0709988996386528,
      "learning_rate": 1.4133333333333335e-06,
      "loss": 0.0014,
      "step": 48790
    },
    {
      "epoch": 13.942857142857143,
      "grad_norm": 18.03086280822754,
      "learning_rate": 1.4095238095238095e-06,
      "loss": 0.4692,
      "step": 48800
    },
    {
      "epoch": 13.945714285714285,
      "grad_norm": 0.26313602924346924,
      "learning_rate": 1.4057142857142857e-06,
      "loss": 0.4104,
      "step": 48810
    },
    {
      "epoch": 13.948571428571428,
      "grad_norm": 0.06055092066526413,
      "learning_rate": 1.4019047619047621e-06,
      "loss": 0.3004,
      "step": 48820
    },
    {
      "epoch": 13.951428571428572,
      "grad_norm": 0.03785523772239685,
      "learning_rate": 1.3980952380952382e-06,
      "loss": 0.3095,
      "step": 48830
    },
    {
      "epoch": 13.954285714285714,
      "grad_norm": 0.013308580964803696,
      "learning_rate": 1.3942857142857144e-06,
      "loss": 0.0894,
      "step": 48840
    },
    {
      "epoch": 13.957142857142857,
      "grad_norm": 0.05533156171441078,
      "learning_rate": 1.3904761904761908e-06,
      "loss": 0.0515,
      "step": 48850
    },
    {
      "epoch": 13.96,
      "grad_norm": 0.21110457181930542,
      "learning_rate": 1.3866666666666668e-06,
      "loss": 0.1269,
      "step": 48860
    },
    {
      "epoch": 13.962857142857143,
      "grad_norm": 0.023929962888360023,
      "learning_rate": 1.382857142857143e-06,
      "loss": 0.2589,
      "step": 48870
    },
    {
      "epoch": 13.965714285714286,
      "grad_norm": 0.049482766538858414,
      "learning_rate": 1.379047619047619e-06,
      "loss": 0.173,
      "step": 48880
    },
    {
      "epoch": 13.968571428571428,
      "grad_norm": 13.617321968078613,
      "learning_rate": 1.3752380952380952e-06,
      "loss": 0.1208,
      "step": 48890
    },
    {
      "epoch": 13.971428571428572,
      "grad_norm": 23.23000144958496,
      "learning_rate": 1.3714285714285717e-06,
      "loss": 0.3042,
      "step": 48900
    },
    {
      "epoch": 13.974285714285715,
      "grad_norm": 0.028846733272075653,
      "learning_rate": 1.3676190476190477e-06,
      "loss": 0.5754,
      "step": 48910
    },
    {
      "epoch": 13.977142857142857,
      "grad_norm": 0.07386988401412964,
      "learning_rate": 1.3638095238095239e-06,
      "loss": 0.0015,
      "step": 48920
    },
    {
      "epoch": 13.98,
      "grad_norm": 14.402233123779297,
      "learning_rate": 1.3600000000000001e-06,
      "loss": 0.5987,
      "step": 48930
    },
    {
      "epoch": 13.982857142857142,
      "grad_norm": 0.013564464636147022,
      "learning_rate": 1.3561904761904763e-06,
      "loss": 0.0338,
      "step": 48940
    },
    {
      "epoch": 13.985714285714286,
      "grad_norm": 13.94658088684082,
      "learning_rate": 1.3523809523809525e-06,
      "loss": 0.1017,
      "step": 48950
    },
    {
      "epoch": 13.98857142857143,
      "grad_norm": 0.08013548702001572,
      "learning_rate": 1.3485714285714288e-06,
      "loss": 0.0028,
      "step": 48960
    },
    {
      "epoch": 13.991428571428571,
      "grad_norm": 0.05142829194664955,
      "learning_rate": 1.3447619047619048e-06,
      "loss": 0.3581,
      "step": 48970
    },
    {
      "epoch": 13.994285714285715,
      "grad_norm": 0.058947429060935974,
      "learning_rate": 1.3409523809523812e-06,
      "loss": 0.617,
      "step": 48980
    },
    {
      "epoch": 13.997142857142856,
      "grad_norm": 0.051121003925800323,
      "learning_rate": 1.3371428571428572e-06,
      "loss": 0.1721,
      "step": 48990
    },
    {
      "epoch": 14.0,
      "grad_norm": 0.021555136889219284,
      "learning_rate": 1.3333333333333334e-06,
      "loss": 0.5156,
      "step": 49000
    },
    {
      "epoch": 14.0,
      "eval_accuracy": 0.9371639784946236,
      "eval_f1": 0.723781388478582,
      "eval_loss": 0.35351139307022095,
      "eval_precision": 0.8361774744027304,
      "eval_recall": 0.6380208333333334,
      "eval_runtime": 48.3587,
      "eval_samples_per_second": 62.036,
      "eval_steps_per_second": 1.944,
      "step": 49000
    },
    {
      "epoch": 14.002857142857144,
      "grad_norm": 13.297307968139648,
      "learning_rate": 1.3295238095238096e-06,
      "loss": 0.7091,
      "step": 49010
    },
    {
      "epoch": 14.005714285714285,
      "grad_norm": 0.1562615931034088,
      "learning_rate": 1.3257142857142856e-06,
      "loss": 0.1381,
      "step": 49020
    },
    {
      "epoch": 14.008571428571429,
      "grad_norm": 0.03637893125414848,
      "learning_rate": 1.321904761904762e-06,
      "loss": 0.0018,
      "step": 49030
    },
    {
      "epoch": 14.01142857142857,
      "grad_norm": 20.114442825317383,
      "learning_rate": 1.3180952380952383e-06,
      "loss": 0.1091,
      "step": 49040
    },
    {
      "epoch": 14.014285714285714,
      "grad_norm": 2.3833065032958984,
      "learning_rate": 1.3142857142857143e-06,
      "loss": 0.4075,
      "step": 49050
    },
    {
      "epoch": 14.017142857142858,
      "grad_norm": 0.050218962132930756,
      "learning_rate": 1.3104761904761907e-06,
      "loss": 0.5758,
      "step": 49060
    },
    {
      "epoch": 14.02,
      "grad_norm": 0.1381322741508484,
      "learning_rate": 1.3066666666666667e-06,
      "loss": 0.1204,
      "step": 49070
    },
    {
      "epoch": 14.022857142857143,
      "grad_norm": 0.015636857599020004,
      "learning_rate": 1.302857142857143e-06,
      "loss": 0.009,
      "step": 49080
    },
    {
      "epoch": 14.025714285714285,
      "grad_norm": 0.21203190088272095,
      "learning_rate": 1.2990476190476192e-06,
      "loss": 0.1158,
      "step": 49090
    },
    {
      "epoch": 14.028571428571428,
      "grad_norm": 0.025341244414448738,
      "learning_rate": 1.2952380952380952e-06,
      "loss": 0.2769,
      "step": 49100
    },
    {
      "epoch": 14.031428571428572,
      "grad_norm": 0.04163084924221039,
      "learning_rate": 1.2914285714285716e-06,
      "loss": 0.1628,
      "step": 49110
    },
    {
      "epoch": 14.034285714285714,
      "grad_norm": 0.03426679968833923,
      "learning_rate": 1.2876190476190478e-06,
      "loss": 0.0144,
      "step": 49120
    },
    {
      "epoch": 14.037142857142857,
      "grad_norm": 0.0478186197578907,
      "learning_rate": 1.2838095238095238e-06,
      "loss": 0.3388,
      "step": 49130
    },
    {
      "epoch": 14.04,
      "grad_norm": 34.17843246459961,
      "learning_rate": 1.28e-06,
      "loss": 0.2149,
      "step": 49140
    },
    {
      "epoch": 14.042857142857143,
      "grad_norm": 0.23893414437770844,
      "learning_rate": 1.2761904761904765e-06,
      "loss": 0.3621,
      "step": 49150
    },
    {
      "epoch": 14.045714285714286,
      "grad_norm": 0.11311385035514832,
      "learning_rate": 1.2723809523809525e-06,
      "loss": 0.002,
      "step": 49160
    },
    {
      "epoch": 14.048571428571428,
      "grad_norm": 0.09881769865751266,
      "learning_rate": 1.2685714285714287e-06,
      "loss": 0.0113,
      "step": 49170
    },
    {
      "epoch": 14.051428571428572,
      "grad_norm": 0.01694002002477646,
      "learning_rate": 1.2647619047619047e-06,
      "loss": 0.1758,
      "step": 49180
    },
    {
      "epoch": 14.054285714285715,
      "grad_norm": 0.8505202531814575,
      "learning_rate": 1.2609523809523811e-06,
      "loss": 0.0979,
      "step": 49190
    },
    {
      "epoch": 14.057142857142857,
      "grad_norm": 0.01960780657827854,
      "learning_rate": 1.2571428571428573e-06,
      "loss": 0.3595,
      "step": 49200
    },
    {
      "epoch": 14.06,
      "grad_norm": 0.09022308140993118,
      "learning_rate": 1.2533333333333333e-06,
      "loss": 0.117,
      "step": 49210
    },
    {
      "epoch": 14.062857142857142,
      "grad_norm": 0.07316480576992035,
      "learning_rate": 1.2495238095238095e-06,
      "loss": 0.1222,
      "step": 49220
    },
    {
      "epoch": 14.065714285714286,
      "grad_norm": 0.02817603014409542,
      "learning_rate": 1.2457142857142858e-06,
      "loss": 0.2705,
      "step": 49230
    },
    {
      "epoch": 14.06857142857143,
      "grad_norm": 0.04036170244216919,
      "learning_rate": 1.241904761904762e-06,
      "loss": 0.4574,
      "step": 49240
    },
    {
      "epoch": 14.071428571428571,
      "grad_norm": 0.05798066034913063,
      "learning_rate": 1.2380952380952382e-06,
      "loss": 0.0019,
      "step": 49250
    },
    {
      "epoch": 14.074285714285715,
      "grad_norm": 14.13890552520752,
      "learning_rate": 1.2342857142857144e-06,
      "loss": 0.1348,
      "step": 49260
    },
    {
      "epoch": 14.077142857142857,
      "grad_norm": 0.042727433145046234,
      "learning_rate": 1.2304761904761906e-06,
      "loss": 0.1303,
      "step": 49270
    },
    {
      "epoch": 14.08,
      "grad_norm": 0.021195633336901665,
      "learning_rate": 1.2266666666666666e-06,
      "loss": 0.0945,
      "step": 49280
    },
    {
      "epoch": 14.082857142857144,
      "grad_norm": 0.14753340184688568,
      "learning_rate": 1.222857142857143e-06,
      "loss": 0.0023,
      "step": 49290
    },
    {
      "epoch": 14.085714285714285,
      "grad_norm": 28.81751823425293,
      "learning_rate": 1.219047619047619e-06,
      "loss": 0.1759,
      "step": 49300
    },
    {
      "epoch": 14.088571428571429,
      "grad_norm": 0.11264428496360779,
      "learning_rate": 1.2152380952380953e-06,
      "loss": 0.896,
      "step": 49310
    },
    {
      "epoch": 14.09142857142857,
      "grad_norm": 0.05830489099025726,
      "learning_rate": 1.2114285714285715e-06,
      "loss": 0.1024,
      "step": 49320
    },
    {
      "epoch": 14.094285714285714,
      "grad_norm": 0.08695604652166367,
      "learning_rate": 1.2076190476190477e-06,
      "loss": 0.121,
      "step": 49330
    },
    {
      "epoch": 14.097142857142858,
      "grad_norm": 0.1135333925485611,
      "learning_rate": 1.203809523809524e-06,
      "loss": 0.0029,
      "step": 49340
    },
    {
      "epoch": 14.1,
      "grad_norm": 0.07972471415996552,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 0.0018,
      "step": 49350
    },
    {
      "epoch": 14.102857142857143,
      "grad_norm": 0.07289119064807892,
      "learning_rate": 1.1961904761904764e-06,
      "loss": 0.0014,
      "step": 49360
    },
    {
      "epoch": 14.105714285714285,
      "grad_norm": 0.01816069893538952,
      "learning_rate": 1.1923809523809526e-06,
      "loss": 0.3972,
      "step": 49370
    },
    {
      "epoch": 14.108571428571429,
      "grad_norm": 218.32383728027344,
      "learning_rate": 1.1885714285714286e-06,
      "loss": 0.1826,
      "step": 49380
    },
    {
      "epoch": 14.111428571428572,
      "grad_norm": 0.023954058066010475,
      "learning_rate": 1.1847619047619048e-06,
      "loss": 0.4033,
      "step": 49390
    },
    {
      "epoch": 14.114285714285714,
      "grad_norm": 0.04913188889622688,
      "learning_rate": 1.180952380952381e-06,
      "loss": 0.0013,
      "step": 49400
    },
    {
      "epoch": 14.117142857142857,
      "grad_norm": 0.031664177775382996,
      "learning_rate": 1.1771428571428572e-06,
      "loss": 0.0013,
      "step": 49410
    },
    {
      "epoch": 14.12,
      "grad_norm": 0.1597200632095337,
      "learning_rate": 1.1733333333333335e-06,
      "loss": 0.0031,
      "step": 49420
    },
    {
      "epoch": 14.122857142857143,
      "grad_norm": 0.9344968199729919,
      "learning_rate": 1.1695238095238095e-06,
      "loss": 0.1134,
      "step": 49430
    },
    {
      "epoch": 14.125714285714286,
      "grad_norm": 0.08998406678438187,
      "learning_rate": 1.165714285714286e-06,
      "loss": 0.2758,
      "step": 49440
    },
    {
      "epoch": 14.128571428571428,
      "grad_norm": 0.0536581426858902,
      "learning_rate": 1.161904761904762e-06,
      "loss": 0.001,
      "step": 49450
    },
    {
      "epoch": 14.131428571428572,
      "grad_norm": 13.66235065460205,
      "learning_rate": 1.1580952380952381e-06,
      "loss": 0.2511,
      "step": 49460
    },
    {
      "epoch": 14.134285714285713,
      "grad_norm": 0.0442843958735466,
      "learning_rate": 1.1542857142857143e-06,
      "loss": 0.1645,
      "step": 49470
    },
    {
      "epoch": 14.137142857142857,
      "grad_norm": 0.011411471292376518,
      "learning_rate": 1.1504761904761906e-06,
      "loss": 0.2586,
      "step": 49480
    },
    {
      "epoch": 14.14,
      "grad_norm": 0.14227484166622162,
      "learning_rate": 1.1466666666666668e-06,
      "loss": 0.1035,
      "step": 49490
    },
    {
      "epoch": 14.142857142857142,
      "grad_norm": 0.01690003275871277,
      "learning_rate": 1.142857142857143e-06,
      "loss": 0.1082,
      "step": 49500
    },
    {
      "epoch": 14.145714285714286,
      "grad_norm": 115.56046295166016,
      "learning_rate": 1.1390476190476192e-06,
      "loss": 0.4871,
      "step": 49510
    },
    {
      "epoch": 14.14857142857143,
      "grad_norm": 0.03293662890791893,
      "learning_rate": 1.1352380952380954e-06,
      "loss": 0.001,
      "step": 49520
    },
    {
      "epoch": 14.151428571428571,
      "grad_norm": 0.06098249927163124,
      "learning_rate": 1.1314285714285714e-06,
      "loss": 0.2099,
      "step": 49530
    },
    {
      "epoch": 14.154285714285715,
      "grad_norm": 0.04867266118526459,
      "learning_rate": 1.1276190476190476e-06,
      "loss": 0.0007,
      "step": 49540
    },
    {
      "epoch": 14.157142857142857,
      "grad_norm": 0.038880255073308945,
      "learning_rate": 1.1238095238095239e-06,
      "loss": 0.1244,
      "step": 49550
    },
    {
      "epoch": 14.16,
      "grad_norm": 0.029992368072271347,
      "learning_rate": 1.12e-06,
      "loss": 0.171,
      "step": 49560
    },
    {
      "epoch": 14.162857142857144,
      "grad_norm": 0.027857398614287376,
      "learning_rate": 1.1161904761904763e-06,
      "loss": 0.1138,
      "step": 49570
    },
    {
      "epoch": 14.165714285714285,
      "grad_norm": 0.12499143183231354,
      "learning_rate": 1.1123809523809525e-06,
      "loss": 0.1099,
      "step": 49580
    },
    {
      "epoch": 14.168571428571429,
      "grad_norm": 0.06079862639307976,
      "learning_rate": 1.1085714285714287e-06,
      "loss": 0.1541,
      "step": 49590
    },
    {
      "epoch": 14.17142857142857,
      "grad_norm": 0.4233596920967102,
      "learning_rate": 1.104761904761905e-06,
      "loss": 0.1206,
      "step": 49600
    },
    {
      "epoch": 14.174285714285714,
      "grad_norm": 0.2861507833003998,
      "learning_rate": 1.100952380952381e-06,
      "loss": 0.1159,
      "step": 49610
    },
    {
      "epoch": 14.177142857142858,
      "grad_norm": 0.0684800073504448,
      "learning_rate": 1.0971428571428572e-06,
      "loss": 0.3122,
      "step": 49620
    },
    {
      "epoch": 14.18,
      "grad_norm": 0.579116702079773,
      "learning_rate": 1.0933333333333334e-06,
      "loss": 0.0018,
      "step": 49630
    },
    {
      "epoch": 14.182857142857143,
      "grad_norm": 0.03293198347091675,
      "learning_rate": 1.0895238095238096e-06,
      "loss": 0.2084,
      "step": 49640
    },
    {
      "epoch": 14.185714285714285,
      "grad_norm": 0.04003779590129852,
      "learning_rate": 1.0857142857142858e-06,
      "loss": 0.1685,
      "step": 49650
    },
    {
      "epoch": 14.188571428571429,
      "grad_norm": 0.05373559892177582,
      "learning_rate": 1.081904761904762e-06,
      "loss": 0.9191,
      "step": 49660
    },
    {
      "epoch": 14.191428571428572,
      "grad_norm": 18.671127319335938,
      "learning_rate": 1.0780952380952383e-06,
      "loss": 0.4416,
      "step": 49670
    },
    {
      "epoch": 14.194285714285714,
      "grad_norm": 0.0332961268723011,
      "learning_rate": 1.0742857142857145e-06,
      "loss": 0.001,
      "step": 49680
    },
    {
      "epoch": 14.197142857142858,
      "grad_norm": 0.07074923813343048,
      "learning_rate": 1.0704761904761905e-06,
      "loss": 0.2828,
      "step": 49690
    },
    {
      "epoch": 14.2,
      "grad_norm": 0.05547165498137474,
      "learning_rate": 1.066666666666667e-06,
      "loss": 0.001,
      "step": 49700
    },
    {
      "epoch": 14.202857142857143,
      "grad_norm": 0.06430773437023163,
      "learning_rate": 1.062857142857143e-06,
      "loss": 0.3643,
      "step": 49710
    },
    {
      "epoch": 14.205714285714286,
      "grad_norm": 1.7897791862487793,
      "learning_rate": 1.0590476190476191e-06,
      "loss": 0.6391,
      "step": 49720
    },
    {
      "epoch": 14.208571428571428,
      "grad_norm": 0.06575002521276474,
      "learning_rate": 1.0552380952380953e-06,
      "loss": 0.1076,
      "step": 49730
    },
    {
      "epoch": 14.211428571428572,
      "grad_norm": 0.028401313349604607,
      "learning_rate": 1.0514285714285716e-06,
      "loss": 0.0015,
      "step": 49740
    },
    {
      "epoch": 14.214285714285714,
      "grad_norm": 115.77107238769531,
      "learning_rate": 1.0476190476190478e-06,
      "loss": 0.0211,
      "step": 49750
    },
    {
      "epoch": 14.217142857142857,
      "grad_norm": 0.255243182182312,
      "learning_rate": 1.0438095238095238e-06,
      "loss": 0.0021,
      "step": 49760
    },
    {
      "epoch": 14.22,
      "grad_norm": 0.255586177110672,
      "learning_rate": 1.04e-06,
      "loss": 0.1419,
      "step": 49770
    },
    {
      "epoch": 14.222857142857142,
      "grad_norm": 0.07291743904352188,
      "learning_rate": 1.0361904761904764e-06,
      "loss": 0.3096,
      "step": 49780
    },
    {
      "epoch": 14.225714285714286,
      "grad_norm": 0.023984095081686974,
      "learning_rate": 1.0323809523809524e-06,
      "loss": 0.2151,
      "step": 49790
    },
    {
      "epoch": 14.228571428571428,
      "grad_norm": 0.15820913016796112,
      "learning_rate": 1.0285714285714286e-06,
      "loss": 0.1319,
      "step": 49800
    },
    {
      "epoch": 14.231428571428571,
      "grad_norm": 0.09808933734893799,
      "learning_rate": 1.0247619047619049e-06,
      "loss": 0.4538,
      "step": 49810
    },
    {
      "epoch": 14.234285714285715,
      "grad_norm": 0.11347252875566483,
      "learning_rate": 1.020952380952381e-06,
      "loss": 0.8421,
      "step": 49820
    },
    {
      "epoch": 14.237142857142857,
      "grad_norm": 0.03512057289481163,
      "learning_rate": 1.0171428571428573e-06,
      "loss": 0.0983,
      "step": 49830
    },
    {
      "epoch": 14.24,
      "grad_norm": 0.014221493154764175,
      "learning_rate": 1.0133333333333333e-06,
      "loss": 0.1407,
      "step": 49840
    },
    {
      "epoch": 14.242857142857142,
      "grad_norm": 0.02891658991575241,
      "learning_rate": 1.0095238095238095e-06,
      "loss": 0.2303,
      "step": 49850
    },
    {
      "epoch": 14.245714285714286,
      "grad_norm": 0.053160201758146286,
      "learning_rate": 1.0057142857142857e-06,
      "loss": 0.0965,
      "step": 49860
    },
    {
      "epoch": 14.248571428571429,
      "grad_norm": 0.08521954715251923,
      "learning_rate": 1.001904761904762e-06,
      "loss": 0.2788,
      "step": 49870
    },
    {
      "epoch": 14.251428571428571,
      "grad_norm": 0.07984423637390137,
      "learning_rate": 9.980952380952382e-07,
      "loss": 0.2198,
      "step": 49880
    },
    {
      "epoch": 14.254285714285714,
      "grad_norm": 15.645127296447754,
      "learning_rate": 9.942857142857144e-07,
      "loss": 0.1793,
      "step": 49890
    },
    {
      "epoch": 14.257142857142856,
      "grad_norm": 0.5614358186721802,
      "learning_rate": 9.904761904761906e-07,
      "loss": 0.1898,
      "step": 49900
    },
    {
      "epoch": 14.26,
      "grad_norm": 0.02798020839691162,
      "learning_rate": 9.866666666666668e-07,
      "loss": 0.3693,
      "step": 49910
    },
    {
      "epoch": 14.262857142857143,
      "grad_norm": 0.10127250850200653,
      "learning_rate": 9.828571428571428e-07,
      "loss": 0.1406,
      "step": 49920
    },
    {
      "epoch": 14.265714285714285,
      "grad_norm": 0.03152953088283539,
      "learning_rate": 9.790476190476193e-07,
      "loss": 0.0018,
      "step": 49930
    },
    {
      "epoch": 14.268571428571429,
      "grad_norm": 0.0837823897600174,
      "learning_rate": 9.752380952380953e-07,
      "loss": 0.002,
      "step": 49940
    },
    {
      "epoch": 14.271428571428572,
      "grad_norm": 0.026826748624444008,
      "learning_rate": 9.714285714285715e-07,
      "loss": 0.1599,
      "step": 49950
    },
    {
      "epoch": 14.274285714285714,
      "grad_norm": 0.28741469979286194,
      "learning_rate": 9.676190476190477e-07,
      "loss": 0.3223,
      "step": 49960
    },
    {
      "epoch": 14.277142857142858,
      "grad_norm": 21.690885543823242,
      "learning_rate": 9.63809523809524e-07,
      "loss": 0.1511,
      "step": 49970
    },
    {
      "epoch": 14.28,
      "grad_norm": 20.29979705810547,
      "learning_rate": 9.600000000000001e-07,
      "loss": 0.7415,
      "step": 49980
    },
    {
      "epoch": 14.282857142857143,
      "grad_norm": 219.11756896972656,
      "learning_rate": 9.561904761904763e-07,
      "loss": 0.2181,
      "step": 49990
    },
    {
      "epoch": 14.285714285714286,
      "grad_norm": 0.22298851609230042,
      "learning_rate": 9.523809523809525e-07,
      "loss": 0.3516,
      "step": 50000
    },
    {
      "epoch": 14.288571428571428,
      "grad_norm": 0.08160600811243057,
      "learning_rate": 9.485714285714287e-07,
      "loss": 0.1629,
      "step": 50010
    },
    {
      "epoch": 14.291428571428572,
      "grad_norm": 0.9033966064453125,
      "learning_rate": 9.447619047619049e-07,
      "loss": 0.0928,
      "step": 50020
    },
    {
      "epoch": 14.294285714285714,
      "grad_norm": 0.17339253425598145,
      "learning_rate": 9.40952380952381e-07,
      "loss": 0.1957,
      "step": 50030
    },
    {
      "epoch": 14.297142857142857,
      "grad_norm": 0.017046628519892693,
      "learning_rate": 9.371428571428571e-07,
      "loss": 0.2624,
      "step": 50040
    },
    {
      "epoch": 14.3,
      "grad_norm": 0.15326854586601257,
      "learning_rate": 9.333333333333334e-07,
      "loss": 0.0905,
      "step": 50050
    },
    {
      "epoch": 14.302857142857142,
      "grad_norm": 0.036314960569143295,
      "learning_rate": 9.295238095238096e-07,
      "loss": 0.3372,
      "step": 50060
    },
    {
      "epoch": 14.305714285714286,
      "grad_norm": 1.1894023418426514,
      "learning_rate": 9.257142857142858e-07,
      "loss": 0.189,
      "step": 50070
    },
    {
      "epoch": 14.308571428571428,
      "grad_norm": 0.09843260794878006,
      "learning_rate": 9.219047619047619e-07,
      "loss": 0.2165,
      "step": 50080
    },
    {
      "epoch": 14.311428571428571,
      "grad_norm": 0.04815942794084549,
      "learning_rate": 9.180952380952382e-07,
      "loss": 0.1665,
      "step": 50090
    },
    {
      "epoch": 14.314285714285715,
      "grad_norm": 13.845032691955566,
      "learning_rate": 9.142857142857144e-07,
      "loss": 0.2537,
      "step": 50100
    },
    {
      "epoch": 14.317142857142857,
      "grad_norm": 0.07313351333141327,
      "learning_rate": 9.104761904761905e-07,
      "loss": 0.107,
      "step": 50110
    },
    {
      "epoch": 14.32,
      "grad_norm": 0.09611839056015015,
      "learning_rate": 9.066666666666668e-07,
      "loss": 0.1072,
      "step": 50120
    },
    {
      "epoch": 14.322857142857142,
      "grad_norm": 17.99967384338379,
      "learning_rate": 9.02857142857143e-07,
      "loss": 0.4808,
      "step": 50130
    },
    {
      "epoch": 14.325714285714286,
      "grad_norm": 54.84739685058594,
      "learning_rate": 8.990476190476191e-07,
      "loss": 0.1139,
      "step": 50140
    },
    {
      "epoch": 14.32857142857143,
      "grad_norm": 26.823740005493164,
      "learning_rate": 8.952380952380953e-07,
      "loss": 0.3892,
      "step": 50150
    },
    {
      "epoch": 14.331428571428571,
      "grad_norm": 0.1515132486820221,
      "learning_rate": 8.914285714285716e-07,
      "loss": 0.0016,
      "step": 50160
    },
    {
      "epoch": 14.334285714285715,
      "grad_norm": 0.0503549687564373,
      "learning_rate": 8.876190476190477e-07,
      "loss": 0.0253,
      "step": 50170
    },
    {
      "epoch": 14.337142857142856,
      "grad_norm": 0.06424341350793839,
      "learning_rate": 8.838095238095238e-07,
      "loss": 0.0022,
      "step": 50180
    },
    {
      "epoch": 14.34,
      "grad_norm": 0.041987307369709015,
      "learning_rate": 8.8e-07,
      "loss": 0.0018,
      "step": 50190
    },
    {
      "epoch": 14.342857142857143,
      "grad_norm": 15.72675895690918,
      "learning_rate": 8.761904761904763e-07,
      "loss": 0.3515,
      "step": 50200
    },
    {
      "epoch": 14.345714285714285,
      "grad_norm": 0.031917326152324677,
      "learning_rate": 8.723809523809525e-07,
      "loss": 0.3968,
      "step": 50210
    },
    {
      "epoch": 14.348571428571429,
      "grad_norm": 0.1868659406900406,
      "learning_rate": 8.685714285714286e-07,
      "loss": 0.1693,
      "step": 50220
    },
    {
      "epoch": 14.35142857142857,
      "grad_norm": 0.029276954010128975,
      "learning_rate": 8.647619047619048e-07,
      "loss": 0.0559,
      "step": 50230
    },
    {
      "epoch": 14.354285714285714,
      "grad_norm": 0.06910750269889832,
      "learning_rate": 8.60952380952381e-07,
      "loss": 0.0015,
      "step": 50240
    },
    {
      "epoch": 14.357142857142858,
      "grad_norm": 0.034908972680568695,
      "learning_rate": 8.571428571428572e-07,
      "loss": 0.4868,
      "step": 50250
    },
    {
      "epoch": 14.36,
      "grad_norm": 0.07172755151987076,
      "learning_rate": 8.533333333333334e-07,
      "loss": 0.3814,
      "step": 50260
    },
    {
      "epoch": 14.362857142857143,
      "grad_norm": 0.03705829009413719,
      "learning_rate": 8.495238095238096e-07,
      "loss": 0.1768,
      "step": 50270
    },
    {
      "epoch": 14.365714285714287,
      "grad_norm": 1.298194169998169,
      "learning_rate": 8.457142857142858e-07,
      "loss": 0.2717,
      "step": 50280
    },
    {
      "epoch": 14.368571428571428,
      "grad_norm": 29.538585662841797,
      "learning_rate": 8.41904761904762e-07,
      "loss": 0.1944,
      "step": 50290
    },
    {
      "epoch": 14.371428571428572,
      "grad_norm": 0.048719700425863266,
      "learning_rate": 8.380952380952381e-07,
      "loss": 0.3503,
      "step": 50300
    },
    {
      "epoch": 14.374285714285714,
      "grad_norm": 0.03769765421748161,
      "learning_rate": 8.342857142857144e-07,
      "loss": 0.1295,
      "step": 50310
    },
    {
      "epoch": 14.377142857142857,
      "grad_norm": 0.12906119227409363,
      "learning_rate": 8.304761904761905e-07,
      "loss": 0.1103,
      "step": 50320
    },
    {
      "epoch": 14.38,
      "grad_norm": 0.0770554319024086,
      "learning_rate": 8.266666666666668e-07,
      "loss": 0.535,
      "step": 50330
    },
    {
      "epoch": 14.382857142857143,
      "grad_norm": 0.05583536624908447,
      "learning_rate": 8.228571428571429e-07,
      "loss": 0.2665,
      "step": 50340
    },
    {
      "epoch": 14.385714285714286,
      "grad_norm": 0.08617512881755829,
      "learning_rate": 8.190476190476192e-07,
      "loss": 0.1629,
      "step": 50350
    },
    {
      "epoch": 14.388571428571428,
      "grad_norm": 0.13608376681804657,
      "learning_rate": 8.152380952380953e-07,
      "loss": 0.16,
      "step": 50360
    },
    {
      "epoch": 14.391428571428571,
      "grad_norm": 0.02554395981132984,
      "learning_rate": 8.114285714285715e-07,
      "loss": 0.2653,
      "step": 50370
    },
    {
      "epoch": 14.394285714285715,
      "grad_norm": 0.04048750549554825,
      "learning_rate": 8.076190476190476e-07,
      "loss": 0.2887,
      "step": 50380
    },
    {
      "epoch": 14.397142857142857,
      "grad_norm": 0.04020288959145546,
      "learning_rate": 8.03809523809524e-07,
      "loss": 0.314,
      "step": 50390
    },
    {
      "epoch": 14.4,
      "grad_norm": 15.712169647216797,
      "learning_rate": 8.000000000000001e-07,
      "loss": 0.6328,
      "step": 50400
    },
    {
      "epoch": 14.402857142857142,
      "grad_norm": 15.870436668395996,
      "learning_rate": 7.961904761904763e-07,
      "loss": 0.2905,
      "step": 50410
    },
    {
      "epoch": 14.405714285714286,
      "grad_norm": 0.0837700143456459,
      "learning_rate": 7.923809523809524e-07,
      "loss": 0.1228,
      "step": 50420
    },
    {
      "epoch": 14.40857142857143,
      "grad_norm": 0.13127702474594116,
      "learning_rate": 7.885714285714287e-07,
      "loss": 0.5709,
      "step": 50430
    },
    {
      "epoch": 14.411428571428571,
      "grad_norm": 0.04936210811138153,
      "learning_rate": 7.847619047619048e-07,
      "loss": 0.2153,
      "step": 50440
    },
    {
      "epoch": 14.414285714285715,
      "grad_norm": 0.061443161219358444,
      "learning_rate": 7.809523809523809e-07,
      "loss": 0.0035,
      "step": 50450
    },
    {
      "epoch": 14.417142857142856,
      "grad_norm": 0.11815318465232849,
      "learning_rate": 7.771428571428572e-07,
      "loss": 0.1421,
      "step": 50460
    },
    {
      "epoch": 14.42,
      "grad_norm": 0.11760439723730087,
      "learning_rate": 7.733333333333335e-07,
      "loss": 0.2249,
      "step": 50470
    },
    {
      "epoch": 14.422857142857143,
      "grad_norm": 0.039730846881866455,
      "learning_rate": 7.695238095238096e-07,
      "loss": 0.1694,
      "step": 50480
    },
    {
      "epoch": 14.425714285714285,
      "grad_norm": 0.8880558013916016,
      "learning_rate": 7.657142857142857e-07,
      "loss": 0.15,
      "step": 50490
    },
    {
      "epoch": 14.428571428571429,
      "grad_norm": 0.07775847613811493,
      "learning_rate": 7.61904761904762e-07,
      "loss": 0.3144,
      "step": 50500
    },
    {
      "epoch": 14.43142857142857,
      "grad_norm": 0.07782458513975143,
      "learning_rate": 7.580952380952381e-07,
      "loss": 0.0028,
      "step": 50510
    },
    {
      "epoch": 14.434285714285714,
      "grad_norm": 14.042630195617676,
      "learning_rate": 7.542857142857144e-07,
      "loss": 0.1894,
      "step": 50520
    },
    {
      "epoch": 14.437142857142858,
      "grad_norm": 17.699237823486328,
      "learning_rate": 7.504761904761905e-07,
      "loss": 0.2849,
      "step": 50530
    },
    {
      "epoch": 14.44,
      "grad_norm": 63.987850189208984,
      "learning_rate": 7.466666666666668e-07,
      "loss": 0.0651,
      "step": 50540
    },
    {
      "epoch": 14.442857142857143,
      "grad_norm": 0.3883034288883209,
      "learning_rate": 7.428571428571429e-07,
      "loss": 0.2982,
      "step": 50550
    },
    {
      "epoch": 14.445714285714285,
      "grad_norm": 0.024330923333764076,
      "learning_rate": 7.390476190476191e-07,
      "loss": 0.1223,
      "step": 50560
    },
    {
      "epoch": 14.448571428571428,
      "grad_norm": 0.11634872108697891,
      "learning_rate": 7.352380952380952e-07,
      "loss": 0.247,
      "step": 50570
    },
    {
      "epoch": 14.451428571428572,
      "grad_norm": 0.05720416456460953,
      "learning_rate": 7.314285714285715e-07,
      "loss": 0.0015,
      "step": 50580
    },
    {
      "epoch": 14.454285714285714,
      "grad_norm": 0.031666118651628494,
      "learning_rate": 7.276190476190477e-07,
      "loss": 0.2419,
      "step": 50590
    },
    {
      "epoch": 14.457142857142857,
      "grad_norm": 0.08231382071971893,
      "learning_rate": 7.238095238095239e-07,
      "loss": 0.3395,
      "step": 50600
    },
    {
      "epoch": 14.46,
      "grad_norm": 15.159719467163086,
      "learning_rate": 7.2e-07,
      "loss": 0.3774,
      "step": 50610
    },
    {
      "epoch": 14.462857142857143,
      "grad_norm": 0.036501798778772354,
      "learning_rate": 7.161904761904763e-07,
      "loss": 0.1642,
      "step": 50620
    },
    {
      "epoch": 14.465714285714286,
      "grad_norm": 0.07996445149183273,
      "learning_rate": 7.123809523809524e-07,
      "loss": 0.0811,
      "step": 50630
    },
    {
      "epoch": 14.468571428571428,
      "grad_norm": 0.09291011095046997,
      "learning_rate": 7.085714285714286e-07,
      "loss": 0.3961,
      "step": 50640
    },
    {
      "epoch": 14.471428571428572,
      "grad_norm": 0.05279289558529854,
      "learning_rate": 7.047619047619048e-07,
      "loss": 0.1845,
      "step": 50650
    },
    {
      "epoch": 14.474285714285715,
      "grad_norm": 0.11158720403909683,
      "learning_rate": 7.009523809523811e-07,
      "loss": 0.3215,
      "step": 50660
    },
    {
      "epoch": 14.477142857142857,
      "grad_norm": 167.03591918945312,
      "learning_rate": 6.971428571428572e-07,
      "loss": 0.0232,
      "step": 50670
    },
    {
      "epoch": 14.48,
      "grad_norm": 0.1414756029844284,
      "learning_rate": 6.933333333333334e-07,
      "loss": 0.5079,
      "step": 50680
    },
    {
      "epoch": 14.482857142857142,
      "grad_norm": 2.065530776977539,
      "learning_rate": 6.895238095238095e-07,
      "loss": 0.3601,
      "step": 50690
    },
    {
      "epoch": 14.485714285714286,
      "grad_norm": 0.12167578935623169,
      "learning_rate": 6.857142857142858e-07,
      "loss": 0.2502,
      "step": 50700
    },
    {
      "epoch": 14.48857142857143,
      "grad_norm": 0.04499763250350952,
      "learning_rate": 6.819047619047619e-07,
      "loss": 0.1377,
      "step": 50710
    },
    {
      "epoch": 14.491428571428571,
      "grad_norm": 0.06296416372060776,
      "learning_rate": 6.780952380952382e-07,
      "loss": 0.0337,
      "step": 50720
    },
    {
      "epoch": 14.494285714285715,
      "grad_norm": 0.05122329667210579,
      "learning_rate": 6.742857142857144e-07,
      "loss": 0.1024,
      "step": 50730
    },
    {
      "epoch": 14.497142857142856,
      "grad_norm": 0.04639066755771637,
      "learning_rate": 6.704761904761906e-07,
      "loss": 0.4506,
      "step": 50740
    },
    {
      "epoch": 14.5,
      "grad_norm": 64.34127044677734,
      "learning_rate": 6.666666666666667e-07,
      "loss": 0.1239,
      "step": 50750
    },
    {
      "epoch": 14.502857142857144,
      "grad_norm": 0.059915293008089066,
      "learning_rate": 6.628571428571428e-07,
      "loss": 0.1462,
      "step": 50760
    },
    {
      "epoch": 14.505714285714285,
      "grad_norm": 14.095992088317871,
      "learning_rate": 6.590476190476191e-07,
      "loss": 0.5894,
      "step": 50770
    },
    {
      "epoch": 14.508571428571429,
      "grad_norm": 0.10488657653331757,
      "learning_rate": 6.552380952380954e-07,
      "loss": 0.3079,
      "step": 50780
    },
    {
      "epoch": 14.51142857142857,
      "grad_norm": 0.19237355887889862,
      "learning_rate": 6.514285714285715e-07,
      "loss": 0.2352,
      "step": 50790
    },
    {
      "epoch": 14.514285714285714,
      "grad_norm": 0.09657365083694458,
      "learning_rate": 6.476190476190476e-07,
      "loss": 0.1267,
      "step": 50800
    },
    {
      "epoch": 14.517142857142858,
      "grad_norm": 0.0704948827624321,
      "learning_rate": 6.438095238095239e-07,
      "loss": 0.2541,
      "step": 50810
    },
    {
      "epoch": 14.52,
      "grad_norm": 0.18473957479000092,
      "learning_rate": 6.4e-07,
      "loss": 0.1669,
      "step": 50820
    },
    {
      "epoch": 14.522857142857143,
      "grad_norm": 0.12856359779834747,
      "learning_rate": 6.361904761904762e-07,
      "loss": 0.2713,
      "step": 50830
    },
    {
      "epoch": 14.525714285714285,
      "grad_norm": 28.282962799072266,
      "learning_rate": 6.323809523809523e-07,
      "loss": 0.3202,
      "step": 50840
    },
    {
      "epoch": 14.528571428571428,
      "grad_norm": 0.04890861362218857,
      "learning_rate": 6.285714285714287e-07,
      "loss": 0.1202,
      "step": 50850
    },
    {
      "epoch": 14.531428571428572,
      "grad_norm": 0.13978242874145508,
      "learning_rate": 6.247619047619048e-07,
      "loss": 0.2544,
      "step": 50860
    },
    {
      "epoch": 14.534285714285714,
      "grad_norm": 0.10442132502794266,
      "learning_rate": 6.20952380952381e-07,
      "loss": 0.1353,
      "step": 50870
    },
    {
      "epoch": 14.537142857142857,
      "grad_norm": 0.10409122705459595,
      "learning_rate": 6.171428571428572e-07,
      "loss": 0.0061,
      "step": 50880
    },
    {
      "epoch": 14.54,
      "grad_norm": 0.1586434245109558,
      "learning_rate": 6.133333333333333e-07,
      "loss": 0.2376,
      "step": 50890
    },
    {
      "epoch": 14.542857142857143,
      "grad_norm": 0.05505076423287392,
      "learning_rate": 6.095238095238095e-07,
      "loss": 0.0848,
      "step": 50900
    },
    {
      "epoch": 14.545714285714286,
      "grad_norm": 0.04901287332177162,
      "learning_rate": 6.057142857142858e-07,
      "loss": 0.0013,
      "step": 50910
    },
    {
      "epoch": 14.548571428571428,
      "grad_norm": 0.24567997455596924,
      "learning_rate": 6.01904761904762e-07,
      "loss": 0.1532,
      "step": 50920
    },
    {
      "epoch": 14.551428571428572,
      "grad_norm": 0.026978757232427597,
      "learning_rate": 5.980952380952382e-07,
      "loss": 0.0099,
      "step": 50930
    },
    {
      "epoch": 14.554285714285715,
      "grad_norm": 0.05681910365819931,
      "learning_rate": 5.942857142857143e-07,
      "loss": 0.1281,
      "step": 50940
    },
    {
      "epoch": 14.557142857142857,
      "grad_norm": 0.03608759492635727,
      "learning_rate": 5.904761904761905e-07,
      "loss": 0.3328,
      "step": 50950
    },
    {
      "epoch": 14.56,
      "grad_norm": 0.477363258600235,
      "learning_rate": 5.866666666666667e-07,
      "loss": 0.0024,
      "step": 50960
    },
    {
      "epoch": 14.562857142857142,
      "grad_norm": 0.04788244515657425,
      "learning_rate": 5.82857142857143e-07,
      "loss": 0.306,
      "step": 50970
    },
    {
      "epoch": 14.565714285714286,
      "grad_norm": 0.02302316203713417,
      "learning_rate": 5.790476190476191e-07,
      "loss": 0.459,
      "step": 50980
    },
    {
      "epoch": 14.56857142857143,
      "grad_norm": 31.78554344177246,
      "learning_rate": 5.752380952380953e-07,
      "loss": 0.1378,
      "step": 50990
    },
    {
      "epoch": 14.571428571428571,
      "grad_norm": 0.071841761469841,
      "learning_rate": 5.714285714285715e-07,
      "loss": 0.255,
      "step": 51000
    },
    {
      "epoch": 14.574285714285715,
      "grad_norm": 0.09155654162168503,
      "learning_rate": 5.676190476190477e-07,
      "loss": 0.2518,
      "step": 51010
    },
    {
      "epoch": 14.577142857142857,
      "grad_norm": 0.6106501221656799,
      "learning_rate": 5.638095238095238e-07,
      "loss": 0.0021,
      "step": 51020
    },
    {
      "epoch": 14.58,
      "grad_norm": 0.03529076278209686,
      "learning_rate": 5.6e-07,
      "loss": 0.0021,
      "step": 51030
    },
    {
      "epoch": 14.582857142857144,
      "grad_norm": 0.2798011600971222,
      "learning_rate": 5.561904761904763e-07,
      "loss": 0.0875,
      "step": 51040
    },
    {
      "epoch": 14.585714285714285,
      "grad_norm": 17.394533157348633,
      "learning_rate": 5.523809523809525e-07,
      "loss": 0.3495,
      "step": 51050
    },
    {
      "epoch": 14.588571428571429,
      "grad_norm": 0.06573503464460373,
      "learning_rate": 5.485714285714286e-07,
      "loss": 0.1565,
      "step": 51060
    },
    {
      "epoch": 14.59142857142857,
      "grad_norm": 0.465678870677948,
      "learning_rate": 5.447619047619048e-07,
      "loss": 0.3294,
      "step": 51070
    },
    {
      "epoch": 14.594285714285714,
      "grad_norm": 35.898494720458984,
      "learning_rate": 5.40952380952381e-07,
      "loss": 0.1297,
      "step": 51080
    },
    {
      "epoch": 14.597142857142858,
      "grad_norm": 12.67872428894043,
      "learning_rate": 5.371428571428572e-07,
      "loss": 0.3104,
      "step": 51090
    },
    {
      "epoch": 14.6,
      "grad_norm": 0.08272524923086166,
      "learning_rate": 5.333333333333335e-07,
      "loss": 0.2982,
      "step": 51100
    },
    {
      "epoch": 14.602857142857143,
      "grad_norm": 24.190622329711914,
      "learning_rate": 5.295238095238096e-07,
      "loss": 0.4924,
      "step": 51110
    },
    {
      "epoch": 14.605714285714285,
      "grad_norm": 0.09617716073989868,
      "learning_rate": 5.257142857142858e-07,
      "loss": 0.0019,
      "step": 51120
    },
    {
      "epoch": 14.608571428571429,
      "grad_norm": 0.014282256364822388,
      "learning_rate": 5.219047619047619e-07,
      "loss": 0.2956,
      "step": 51130
    },
    {
      "epoch": 14.611428571428572,
      "grad_norm": 0.03650284931063652,
      "learning_rate": 5.180952380952382e-07,
      "loss": 0.0044,
      "step": 51140
    },
    {
      "epoch": 14.614285714285714,
      "grad_norm": 0.0384262390434742,
      "learning_rate": 5.142857142857143e-07,
      "loss": 0.112,
      "step": 51150
    },
    {
      "epoch": 14.617142857142857,
      "grad_norm": 0.13288185000419617,
      "learning_rate": 5.104761904761905e-07,
      "loss": 0.3689,
      "step": 51160
    },
    {
      "epoch": 14.62,
      "grad_norm": 0.055739302188158035,
      "learning_rate": 5.066666666666667e-07,
      "loss": 0.1183,
      "step": 51170
    },
    {
      "epoch": 14.622857142857143,
      "grad_norm": 0.06671100854873657,
      "learning_rate": 5.028571428571429e-07,
      "loss": 0.1193,
      "step": 51180
    },
    {
      "epoch": 14.625714285714286,
      "grad_norm": 0.15729261934757233,
      "learning_rate": 4.990476190476191e-07,
      "loss": 0.3626,
      "step": 51190
    },
    {
      "epoch": 14.628571428571428,
      "grad_norm": 0.03312452882528305,
      "learning_rate": 4.952380952380953e-07,
      "loss": 0.135,
      "step": 51200
    },
    {
      "epoch": 14.631428571428572,
      "grad_norm": 3.1685750484466553,
      "learning_rate": 4.914285714285714e-07,
      "loss": 0.1125,
      "step": 51210
    },
    {
      "epoch": 14.634285714285713,
      "grad_norm": 0.009812982752919197,
      "learning_rate": 4.876190476190476e-07,
      "loss": 0.0293,
      "step": 51220
    },
    {
      "epoch": 14.637142857142857,
      "grad_norm": 0.06456637382507324,
      "learning_rate": 4.838095238095238e-07,
      "loss": 0.2798,
      "step": 51230
    },
    {
      "epoch": 14.64,
      "grad_norm": 0.03512771800160408,
      "learning_rate": 4.800000000000001e-07,
      "loss": 0.5038,
      "step": 51240
    },
    {
      "epoch": 14.642857142857142,
      "grad_norm": 14.61936092376709,
      "learning_rate": 4.7619047619047623e-07,
      "loss": 0.2826,
      "step": 51250
    },
    {
      "epoch": 14.645714285714286,
      "grad_norm": 14.999134063720703,
      "learning_rate": 4.7238095238095244e-07,
      "loss": 0.1428,
      "step": 51260
    },
    {
      "epoch": 14.64857142857143,
      "grad_norm": 1041.5372314453125,
      "learning_rate": 4.6857142857142855e-07,
      "loss": 0.2872,
      "step": 51270
    },
    {
      "epoch": 14.651428571428571,
      "grad_norm": 0.053430166095495224,
      "learning_rate": 4.647619047619048e-07,
      "loss": 0.1318,
      "step": 51280
    },
    {
      "epoch": 14.654285714285715,
      "grad_norm": 0.029375191777944565,
      "learning_rate": 4.6095238095238094e-07,
      "loss": 0.1096,
      "step": 51290
    },
    {
      "epoch": 14.657142857142857,
      "grad_norm": 0.08318764716386795,
      "learning_rate": 4.571428571428572e-07,
      "loss": 0.4147,
      "step": 51300
    },
    {
      "epoch": 14.66,
      "grad_norm": 0.08615584671497345,
      "learning_rate": 4.533333333333334e-07,
      "loss": 0.1528,
      "step": 51310
    },
    {
      "epoch": 14.662857142857142,
      "grad_norm": 0.10620000213384628,
      "learning_rate": 4.4952380952380953e-07,
      "loss": 0.0819,
      "step": 51320
    },
    {
      "epoch": 14.665714285714285,
      "grad_norm": 0.02911265753209591,
      "learning_rate": 4.457142857142858e-07,
      "loss": 0.0026,
      "step": 51330
    },
    {
      "epoch": 14.668571428571429,
      "grad_norm": 0.15154960751533508,
      "learning_rate": 4.419047619047619e-07,
      "loss": 0.1191,
      "step": 51340
    },
    {
      "epoch": 14.67142857142857,
      "grad_norm": 0.08047085255384445,
      "learning_rate": 4.3809523809523813e-07,
      "loss": 0.2257,
      "step": 51350
    },
    {
      "epoch": 14.674285714285714,
      "grad_norm": 0.06748660653829575,
      "learning_rate": 4.342857142857143e-07,
      "loss": 0.0017,
      "step": 51360
    },
    {
      "epoch": 14.677142857142858,
      "grad_norm": 13.030233383178711,
      "learning_rate": 4.304761904761905e-07,
      "loss": 0.3362,
      "step": 51370
    },
    {
      "epoch": 14.68,
      "grad_norm": 25.488412857055664,
      "learning_rate": 4.266666666666667e-07,
      "loss": 0.278,
      "step": 51380
    },
    {
      "epoch": 14.682857142857143,
      "grad_norm": 0.051582079380750656,
      "learning_rate": 4.228571428571429e-07,
      "loss": 0.038,
      "step": 51390
    },
    {
      "epoch": 14.685714285714285,
      "grad_norm": 0.007189581636339426,
      "learning_rate": 4.1904761904761906e-07,
      "loss": 0.1371,
      "step": 51400
    },
    {
      "epoch": 14.688571428571429,
      "grad_norm": 0.07612280547618866,
      "learning_rate": 4.1523809523809527e-07,
      "loss": 0.4246,
      "step": 51410
    },
    {
      "epoch": 14.691428571428572,
      "grad_norm": 0.02525598742067814,
      "learning_rate": 4.1142857142857144e-07,
      "loss": 0.3554,
      "step": 51420
    },
    {
      "epoch": 14.694285714285714,
      "grad_norm": 21.110626220703125,
      "learning_rate": 4.0761904761904765e-07,
      "loss": 0.2824,
      "step": 51430
    },
    {
      "epoch": 14.697142857142858,
      "grad_norm": 0.05872022360563278,
      "learning_rate": 4.038095238095238e-07,
      "loss": 0.182,
      "step": 51440
    },
    {
      "epoch": 14.7,
      "grad_norm": 0.053853586316108704,
      "learning_rate": 4.0000000000000003e-07,
      "loss": 0.2501,
      "step": 51450
    },
    {
      "epoch": 14.702857142857143,
      "grad_norm": 0.1935126930475235,
      "learning_rate": 3.961904761904762e-07,
      "loss": 0.301,
      "step": 51460
    },
    {
      "epoch": 14.705714285714286,
      "grad_norm": 0.09483500570058823,
      "learning_rate": 3.923809523809524e-07,
      "loss": 0.2512,
      "step": 51470
    },
    {
      "epoch": 14.708571428571428,
      "grad_norm": 65.7983169555664,
      "learning_rate": 3.885714285714286e-07,
      "loss": 0.1772,
      "step": 51480
    },
    {
      "epoch": 14.711428571428572,
      "grad_norm": 0.06493847817182541,
      "learning_rate": 3.847619047619048e-07,
      "loss": 0.126,
      "step": 51490
    },
    {
      "epoch": 14.714285714285714,
      "grad_norm": 17.345308303833008,
      "learning_rate": 3.80952380952381e-07,
      "loss": 0.2984,
      "step": 51500
    },
    {
      "epoch": 14.717142857142857,
      "grad_norm": 0.07988863438367844,
      "learning_rate": 3.771428571428572e-07,
      "loss": 0.0017,
      "step": 51510
    },
    {
      "epoch": 14.72,
      "grad_norm": 0.1670856475830078,
      "learning_rate": 3.733333333333334e-07,
      "loss": 0.2347,
      "step": 51520
    },
    {
      "epoch": 14.722857142857142,
      "grad_norm": 0.10290948301553726,
      "learning_rate": 3.6952380952380956e-07,
      "loss": 0.1466,
      "step": 51530
    },
    {
      "epoch": 14.725714285714286,
      "grad_norm": 0.09540745615959167,
      "learning_rate": 3.657142857142858e-07,
      "loss": 0.2157,
      "step": 51540
    },
    {
      "epoch": 14.728571428571428,
      "grad_norm": 0.32392942905426025,
      "learning_rate": 3.6190476190476194e-07,
      "loss": 0.0023,
      "step": 51550
    },
    {
      "epoch": 14.731428571428571,
      "grad_norm": 0.05242707580327988,
      "learning_rate": 3.5809523809523816e-07,
      "loss": 0.0996,
      "step": 51560
    },
    {
      "epoch": 14.734285714285715,
      "grad_norm": 0.09889578074216843,
      "learning_rate": 3.542857142857143e-07,
      "loss": 0.0071,
      "step": 51570
    },
    {
      "epoch": 14.737142857142857,
      "grad_norm": 0.0465642511844635,
      "learning_rate": 3.5047619047619054e-07,
      "loss": 0.3352,
      "step": 51580
    },
    {
      "epoch": 14.74,
      "grad_norm": 0.03403487056493759,
      "learning_rate": 3.466666666666667e-07,
      "loss": 0.2154,
      "step": 51590
    },
    {
      "epoch": 14.742857142857144,
      "grad_norm": 0.04844890907406807,
      "learning_rate": 3.428571428571429e-07,
      "loss": 0.1334,
      "step": 51600
    },
    {
      "epoch": 14.745714285714286,
      "grad_norm": 0.041541118174791336,
      "learning_rate": 3.390476190476191e-07,
      "loss": 0.1235,
      "step": 51610
    },
    {
      "epoch": 14.748571428571429,
      "grad_norm": 0.04820866882801056,
      "learning_rate": 3.352380952380953e-07,
      "loss": 0.4211,
      "step": 51620
    },
    {
      "epoch": 14.751428571428571,
      "grad_norm": 3515.1845703125,
      "learning_rate": 3.314285714285714e-07,
      "loss": 0.3122,
      "step": 51630
    },
    {
      "epoch": 14.754285714285714,
      "grad_norm": 0.29273131489753723,
      "learning_rate": 3.276190476190477e-07,
      "loss": 0.0021,
      "step": 51640
    },
    {
      "epoch": 14.757142857142856,
      "grad_norm": 15.42359733581543,
      "learning_rate": 3.238095238095238e-07,
      "loss": 0.4371,
      "step": 51650
    },
    {
      "epoch": 14.76,
      "grad_norm": 0.09288845211267471,
      "learning_rate": 3.2e-07,
      "loss": 0.1769,
      "step": 51660
    },
    {
      "epoch": 14.762857142857143,
      "grad_norm": 24.526199340820312,
      "learning_rate": 3.1619047619047617e-07,
      "loss": 0.2299,
      "step": 51670
    },
    {
      "epoch": 14.765714285714285,
      "grad_norm": 17.05101203918457,
      "learning_rate": 3.123809523809524e-07,
      "loss": 0.2705,
      "step": 51680
    },
    {
      "epoch": 14.768571428571429,
      "grad_norm": 0.0810663178563118,
      "learning_rate": 3.085714285714286e-07,
      "loss": 0.1514,
      "step": 51690
    },
    {
      "epoch": 14.771428571428572,
      "grad_norm": 0.2289842963218689,
      "learning_rate": 3.0476190476190477e-07,
      "loss": 0.0025,
      "step": 51700
    },
    {
      "epoch": 14.774285714285714,
      "grad_norm": 0.02920755185186863,
      "learning_rate": 3.00952380952381e-07,
      "loss": 0.0724,
      "step": 51710
    },
    {
      "epoch": 14.777142857142858,
      "grad_norm": 0.2297394722700119,
      "learning_rate": 2.9714285714285715e-07,
      "loss": 0.0012,
      "step": 51720
    },
    {
      "epoch": 14.78,
      "grad_norm": 0.18863363564014435,
      "learning_rate": 2.9333333333333337e-07,
      "loss": 0.3494,
      "step": 51730
    },
    {
      "epoch": 14.782857142857143,
      "grad_norm": 0.2905265986919403,
      "learning_rate": 2.8952380952380953e-07,
      "loss": 0.2134,
      "step": 51740
    },
    {
      "epoch": 14.785714285714286,
      "grad_norm": 0.04297002777457237,
      "learning_rate": 2.8571428571428575e-07,
      "loss": 0.4637,
      "step": 51750
    },
    {
      "epoch": 14.788571428571428,
      "grad_norm": 0.026570620015263557,
      "learning_rate": 2.819047619047619e-07,
      "loss": 0.0384,
      "step": 51760
    },
    {
      "epoch": 14.791428571428572,
      "grad_norm": 0.05405164137482643,
      "learning_rate": 2.7809523809523813e-07,
      "loss": 0.0015,
      "step": 51770
    },
    {
      "epoch": 14.794285714285714,
      "grad_norm": 0.19949683547019958,
      "learning_rate": 2.742857142857143e-07,
      "loss": 0.3061,
      "step": 51780
    },
    {
      "epoch": 14.797142857142857,
      "grad_norm": 0.07850586622953415,
      "learning_rate": 2.704761904761905e-07,
      "loss": 0.3271,
      "step": 51790
    },
    {
      "epoch": 14.8,
      "grad_norm": 0.3010477125644684,
      "learning_rate": 2.666666666666667e-07,
      "loss": 0.312,
      "step": 51800
    },
    {
      "epoch": 14.802857142857142,
      "grad_norm": 0.0772361233830452,
      "learning_rate": 2.628571428571429e-07,
      "loss": 0.3135,
      "step": 51810
    },
    {
      "epoch": 14.805714285714286,
      "grad_norm": 151.46170043945312,
      "learning_rate": 2.590476190476191e-07,
      "loss": 0.382,
      "step": 51820
    },
    {
      "epoch": 14.808571428571428,
      "grad_norm": 0.08168341964483261,
      "learning_rate": 2.5523809523809527e-07,
      "loss": 0.6112,
      "step": 51830
    },
    {
      "epoch": 14.811428571428571,
      "grad_norm": 39.51991653442383,
      "learning_rate": 2.5142857142857143e-07,
      "loss": 0.1093,
      "step": 51840
    },
    {
      "epoch": 14.814285714285715,
      "grad_norm": 0.050268661230802536,
      "learning_rate": 2.4761904761904765e-07,
      "loss": 0.5214,
      "step": 51850
    },
    {
      "epoch": 14.817142857142857,
      "grad_norm": 0.0881015732884407,
      "learning_rate": 2.438095238095238e-07,
      "loss": 0.2588,
      "step": 51860
    },
    {
      "epoch": 14.82,
      "grad_norm": 98.62513732910156,
      "learning_rate": 2.4000000000000003e-07,
      "loss": 0.2698,
      "step": 51870
    },
    {
      "epoch": 14.822857142857142,
      "grad_norm": 0.07832694053649902,
      "learning_rate": 2.3619047619047622e-07,
      "loss": 0.0012,
      "step": 51880
    },
    {
      "epoch": 14.825714285714286,
      "grad_norm": 0.045287664979696274,
      "learning_rate": 2.323809523809524e-07,
      "loss": 0.0031,
      "step": 51890
    },
    {
      "epoch": 14.82857142857143,
      "grad_norm": 0.0430779755115509,
      "learning_rate": 2.285714285714286e-07,
      "loss": 0.3057,
      "step": 51900
    },
    {
      "epoch": 14.831428571428571,
      "grad_norm": 0.035482294857501984,
      "learning_rate": 2.2476190476190477e-07,
      "loss": 0.2938,
      "step": 51910
    },
    {
      "epoch": 14.834285714285715,
      "grad_norm": 0.07101727277040482,
      "learning_rate": 2.2095238095238096e-07,
      "loss": 0.0025,
      "step": 51920
    },
    {
      "epoch": 14.837142857142858,
      "grad_norm": 0.03543557971715927,
      "learning_rate": 2.1714285714285715e-07,
      "loss": 0.3766,
      "step": 51930
    },
    {
      "epoch": 14.84,
      "grad_norm": 0.1664668619632721,
      "learning_rate": 2.1333333333333334e-07,
      "loss": 0.3845,
      "step": 51940
    },
    {
      "epoch": 14.842857142857143,
      "grad_norm": 0.10223017632961273,
      "learning_rate": 2.0952380952380953e-07,
      "loss": 0.1817,
      "step": 51950
    },
    {
      "epoch": 14.845714285714285,
      "grad_norm": 0.1444154679775238,
      "learning_rate": 2.0571428571428572e-07,
      "loss": 0.1503,
      "step": 51960
    },
    {
      "epoch": 14.848571428571429,
      "grad_norm": 0.02887907437980175,
      "learning_rate": 2.019047619047619e-07,
      "loss": 0.0016,
      "step": 51970
    },
    {
      "epoch": 14.85142857142857,
      "grad_norm": 0.04855700954794884,
      "learning_rate": 1.980952380952381e-07,
      "loss": 0.5005,
      "step": 51980
    },
    {
      "epoch": 14.854285714285714,
      "grad_norm": 0.1820809692144394,
      "learning_rate": 1.942857142857143e-07,
      "loss": 0.0017,
      "step": 51990
    },
    {
      "epoch": 14.857142857142858,
      "grad_norm": 0.14134523272514343,
      "learning_rate": 1.904761904761905e-07,
      "loss": 0.2891,
      "step": 52000
    },
    {
      "epoch": 14.86,
      "grad_norm": 83.58905029296875,
      "learning_rate": 1.866666666666667e-07,
      "loss": 0.3775,
      "step": 52010
    },
    {
      "epoch": 14.862857142857143,
      "grad_norm": 0.7652655839920044,
      "learning_rate": 1.828571428571429e-07,
      "loss": 0.0018,
      "step": 52020
    },
    {
      "epoch": 14.865714285714287,
      "grad_norm": 0.2405189871788025,
      "learning_rate": 1.7904761904761908e-07,
      "loss": 0.0012,
      "step": 52030
    },
    {
      "epoch": 14.868571428571428,
      "grad_norm": 0.04523453488945961,
      "learning_rate": 1.7523809523809527e-07,
      "loss": 0.0015,
      "step": 52040
    },
    {
      "epoch": 14.871428571428572,
      "grad_norm": 442.564697265625,
      "learning_rate": 1.7142857142857146e-07,
      "loss": 0.1478,
      "step": 52050
    },
    {
      "epoch": 14.874285714285714,
      "grad_norm": 0.014143514446914196,
      "learning_rate": 1.6761904761904765e-07,
      "loss": 0.4042,
      "step": 52060
    },
    {
      "epoch": 14.877142857142857,
      "grad_norm": 0.07361641526222229,
      "learning_rate": 1.6380952380952384e-07,
      "loss": 0.634,
      "step": 52070
    },
    {
      "epoch": 14.88,
      "grad_norm": 0.0754939541220665,
      "learning_rate": 1.6e-07,
      "loss": 0.1916,
      "step": 52080
    },
    {
      "epoch": 14.882857142857143,
      "grad_norm": 0.19233658909797668,
      "learning_rate": 1.561904761904762e-07,
      "loss": 0.2822,
      "step": 52090
    },
    {
      "epoch": 14.885714285714286,
      "grad_norm": 97.49813842773438,
      "learning_rate": 1.5238095238095238e-07,
      "loss": 0.0944,
      "step": 52100
    },
    {
      "epoch": 14.888571428571428,
      "grad_norm": 14.804689407348633,
      "learning_rate": 1.4857142857142857e-07,
      "loss": 0.112,
      "step": 52110
    },
    {
      "epoch": 14.891428571428571,
      "grad_norm": 0.044885046780109406,
      "learning_rate": 1.4476190476190476e-07,
      "loss": 0.0197,
      "step": 52120
    },
    {
      "epoch": 14.894285714285715,
      "grad_norm": 7.5826826095581055,
      "learning_rate": 1.4095238095238096e-07,
      "loss": 0.097,
      "step": 52130
    },
    {
      "epoch": 14.897142857142857,
      "grad_norm": 0.046697355806827545,
      "learning_rate": 1.3714285714285715e-07,
      "loss": 0.1117,
      "step": 52140
    },
    {
      "epoch": 14.9,
      "grad_norm": 12.435383796691895,
      "learning_rate": 1.3333333333333336e-07,
      "loss": 0.1442,
      "step": 52150
    },
    {
      "epoch": 14.902857142857142,
      "grad_norm": 0.06565240025520325,
      "learning_rate": 1.2952380952380955e-07,
      "loss": 0.0684,
      "step": 52160
    },
    {
      "epoch": 14.905714285714286,
      "grad_norm": 0.13973991572856903,
      "learning_rate": 1.2571428571428572e-07,
      "loss": 0.0038,
      "step": 52170
    },
    {
      "epoch": 14.90857142857143,
      "grad_norm": 0.1466553658246994,
      "learning_rate": 1.219047619047619e-07,
      "loss": 0.174,
      "step": 52180
    },
    {
      "epoch": 14.911428571428571,
      "grad_norm": 0.10091786086559296,
      "learning_rate": 1.1809523809523811e-07,
      "loss": 0.1354,
      "step": 52190
    },
    {
      "epoch": 14.914285714285715,
      "grad_norm": 0.0723676010966301,
      "learning_rate": 1.142857142857143e-07,
      "loss": 0.0032,
      "step": 52200
    },
    {
      "epoch": 14.917142857142856,
      "grad_norm": 0.10706876963376999,
      "learning_rate": 1.1047619047619048e-07,
      "loss": 0.3054,
      "step": 52210
    },
    {
      "epoch": 14.92,
      "grad_norm": 0.19997821748256683,
      "learning_rate": 1.0666666666666667e-07,
      "loss": 0.3032,
      "step": 52220
    },
    {
      "epoch": 14.922857142857143,
      "grad_norm": 23.110795974731445,
      "learning_rate": 1.0285714285714286e-07,
      "loss": 0.3412,
      "step": 52230
    },
    {
      "epoch": 14.925714285714285,
      "grad_norm": 0.062380626797676086,
      "learning_rate": 9.904761904761905e-08,
      "loss": 0.0013,
      "step": 52240
    },
    {
      "epoch": 14.928571428571429,
      "grad_norm": 19.585735321044922,
      "learning_rate": 9.523809523809525e-08,
      "loss": 0.365,
      "step": 52250
    },
    {
      "epoch": 14.93142857142857,
      "grad_norm": 19.548782348632812,
      "learning_rate": 9.142857142857144e-08,
      "loss": 0.1263,
      "step": 52260
    },
    {
      "epoch": 14.934285714285714,
      "grad_norm": 0.481283962726593,
      "learning_rate": 8.761904761904763e-08,
      "loss": 0.1884,
      "step": 52270
    },
    {
      "epoch": 14.937142857142858,
      "grad_norm": 0.29279980063438416,
      "learning_rate": 8.380952380952382e-08,
      "loss": 0.4546,
      "step": 52280
    },
    {
      "epoch": 14.94,
      "grad_norm": 0.2669292986392975,
      "learning_rate": 8e-08,
      "loss": 0.3538,
      "step": 52290
    },
    {
      "epoch": 14.942857142857143,
      "grad_norm": 0.2209763526916504,
      "learning_rate": 7.619047619047619e-08,
      "loss": 0.2497,
      "step": 52300
    },
    {
      "epoch": 14.945714285714285,
      "grad_norm": 0.17329707741737366,
      "learning_rate": 7.238095238095238e-08,
      "loss": 0.2465,
      "step": 52310
    },
    {
      "epoch": 14.948571428571428,
      "grad_norm": 0.2852948009967804,
      "learning_rate": 6.857142857142857e-08,
      "loss": 0.6451,
      "step": 52320
    },
    {
      "epoch": 14.951428571428572,
      "grad_norm": 0.29524219036102295,
      "learning_rate": 6.476190476190478e-08,
      "loss": 0.0019,
      "step": 52330
    },
    {
      "epoch": 14.954285714285714,
      "grad_norm": 0.10870379209518433,
      "learning_rate": 6.095238095238095e-08,
      "loss": 0.2887,
      "step": 52340
    },
    {
      "epoch": 14.957142857142857,
      "grad_norm": 0.08638546615839005,
      "learning_rate": 5.714285714285715e-08,
      "loss": 0.13,
      "step": 52350
    },
    {
      "epoch": 14.96,
      "grad_norm": 15.858102798461914,
      "learning_rate": 5.3333333333333334e-08,
      "loss": 0.6198,
      "step": 52360
    },
    {
      "epoch": 14.962857142857143,
      "grad_norm": 0.16822019219398499,
      "learning_rate": 4.9523809523809525e-08,
      "loss": 0.1213,
      "step": 52370
    },
    {
      "epoch": 14.965714285714286,
      "grad_norm": 0.05259522795677185,
      "learning_rate": 4.571428571428572e-08,
      "loss": 0.1551,
      "step": 52380
    },
    {
      "epoch": 14.968571428571428,
      "grad_norm": 0.02476252242922783,
      "learning_rate": 4.190476190476191e-08,
      "loss": 0.0037,
      "step": 52390
    },
    {
      "epoch": 14.971428571428572,
      "grad_norm": 0.024708697572350502,
      "learning_rate": 3.8095238095238096e-08,
      "loss": 0.2488,
      "step": 52400
    },
    {
      "epoch": 14.974285714285715,
      "grad_norm": 0.2358374148607254,
      "learning_rate": 3.4285714285714286e-08,
      "loss": 0.4423,
      "step": 52410
    },
    {
      "epoch": 14.977142857142857,
      "grad_norm": 0.043278370052576065,
      "learning_rate": 3.047619047619048e-08,
      "loss": 0.5508,
      "step": 52420
    },
    {
      "epoch": 14.98,
      "grad_norm": 0.03530744090676308,
      "learning_rate": 2.6666666666666667e-08,
      "loss": 0.1077,
      "step": 52430
    },
    {
      "epoch": 14.982857142857142,
      "grad_norm": 0.024656910449266434,
      "learning_rate": 2.285714285714286e-08,
      "loss": 0.0053,
      "step": 52440
    },
    {
      "epoch": 14.985714285714286,
      "grad_norm": 12.22358226776123,
      "learning_rate": 1.9047619047619048e-08,
      "loss": 0.2004,
      "step": 52450
    },
    {
      "epoch": 14.98857142857143,
      "grad_norm": 233.15701293945312,
      "learning_rate": 1.523809523809524e-08,
      "loss": 0.3659,
      "step": 52460
    },
    {
      "epoch": 14.991428571428571,
      "grad_norm": 0.12899580597877502,
      "learning_rate": 1.142857142857143e-08,
      "loss": 0.1316,
      "step": 52470
    },
    {
      "epoch": 14.994285714285715,
      "grad_norm": 0.07358110696077347,
      "learning_rate": 7.61904761904762e-09,
      "loss": 0.4652,
      "step": 52480
    },
    {
      "epoch": 14.997142857142856,
      "grad_norm": 0.04185178503394127,
      "learning_rate": 3.80952380952381e-09,
      "loss": 0.2556,
      "step": 52490
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.05028800293803215,
      "learning_rate": 0.0,
      "loss": 0.1362,
      "step": 52500
    },
    {
      "epoch": 15.0,
      "eval_accuracy": 0.9401881720430108,
      "eval_f1": 0.7442528735632185,
      "eval_loss": 0.3414746820926666,
      "eval_precision": 0.8301282051282052,
      "eval_recall": 0.6744791666666666,
      "eval_runtime": 47.3285,
      "eval_samples_per_second": 63.387,
      "eval_steps_per_second": 1.986,
      "step": 52500
    }
  ],
  "logging_steps": 10,
  "max_steps": 52500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6592638711733808e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
