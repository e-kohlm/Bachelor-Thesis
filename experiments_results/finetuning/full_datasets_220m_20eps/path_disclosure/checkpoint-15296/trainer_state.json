{
  "best_metric": 0.98,
  "best_model_checkpoint": "../saved_models/path_disclosure/checkpoint-15296",
  "epoch": 16.0,
  "eval_steps": 500,
  "global_step": 15296,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0010460251046025104,
      "grad_norm": 76.89061737060547,
      "learning_rate": 1.9998953974895397e-05,
      "loss": 0.8738,
      "step": 1
    },
    {
      "epoch": 0.010460251046025104,
      "grad_norm": 12.438011169433594,
      "learning_rate": 1.9989539748953978e-05,
      "loss": 0.4802,
      "step": 10
    },
    {
      "epoch": 0.02092050209205021,
      "grad_norm": 13.518056869506836,
      "learning_rate": 1.997907949790795e-05,
      "loss": 0.4172,
      "step": 20
    },
    {
      "epoch": 0.03138075313807531,
      "grad_norm": 10.840113639831543,
      "learning_rate": 1.9968619246861927e-05,
      "loss": 0.3959,
      "step": 30
    },
    {
      "epoch": 0.04184100418410042,
      "grad_norm": 69.20661926269531,
      "learning_rate": 1.99581589958159e-05,
      "loss": 0.4087,
      "step": 40
    },
    {
      "epoch": 0.05230125523012552,
      "grad_norm": 9.090202331542969,
      "learning_rate": 1.9947698744769877e-05,
      "loss": 0.3983,
      "step": 50
    },
    {
      "epoch": 0.06276150627615062,
      "grad_norm": 7.9960455894470215,
      "learning_rate": 1.9937238493723853e-05,
      "loss": 0.3854,
      "step": 60
    },
    {
      "epoch": 0.07322175732217573,
      "grad_norm": 6.717163562774658,
      "learning_rate": 1.9926778242677826e-05,
      "loss": 0.3463,
      "step": 70
    },
    {
      "epoch": 0.08368200836820083,
      "grad_norm": 12.618672370910645,
      "learning_rate": 1.9916317991631803e-05,
      "loss": 0.3917,
      "step": 80
    },
    {
      "epoch": 0.09414225941422594,
      "grad_norm": 7.31168270111084,
      "learning_rate": 1.9905857740585776e-05,
      "loss": 0.3499,
      "step": 90
    },
    {
      "epoch": 0.10460251046025104,
      "grad_norm": 7.790650367736816,
      "learning_rate": 1.989539748953975e-05,
      "loss": 0.3403,
      "step": 100
    },
    {
      "epoch": 0.11506276150627615,
      "grad_norm": 7.55903434753418,
      "learning_rate": 1.9884937238493725e-05,
      "loss": 0.3645,
      "step": 110
    },
    {
      "epoch": 0.12552301255230125,
      "grad_norm": 6.120738506317139,
      "learning_rate": 1.9874476987447698e-05,
      "loss": 0.3304,
      "step": 120
    },
    {
      "epoch": 0.13598326359832635,
      "grad_norm": 7.743635177612305,
      "learning_rate": 1.9864016736401674e-05,
      "loss": 0.3479,
      "step": 130
    },
    {
      "epoch": 0.14644351464435146,
      "grad_norm": 10.990821838378906,
      "learning_rate": 1.985355648535565e-05,
      "loss": 0.2947,
      "step": 140
    },
    {
      "epoch": 0.15690376569037656,
      "grad_norm": 5.841431617736816,
      "learning_rate": 1.9843096234309624e-05,
      "loss": 0.3164,
      "step": 150
    },
    {
      "epoch": 0.16736401673640167,
      "grad_norm": 5.095950603485107,
      "learning_rate": 1.98326359832636e-05,
      "loss": 0.3191,
      "step": 160
    },
    {
      "epoch": 0.17782426778242677,
      "grad_norm": 5.268848419189453,
      "learning_rate": 1.9822175732217573e-05,
      "loss": 0.2872,
      "step": 170
    },
    {
      "epoch": 0.18828451882845187,
      "grad_norm": 5.03251838684082,
      "learning_rate": 1.981171548117155e-05,
      "loss": 0.3228,
      "step": 180
    },
    {
      "epoch": 0.19874476987447698,
      "grad_norm": 4.187830924987793,
      "learning_rate": 1.9801255230125526e-05,
      "loss": 0.3148,
      "step": 190
    },
    {
      "epoch": 0.20920502092050208,
      "grad_norm": 4.4851975440979,
      "learning_rate": 1.97907949790795e-05,
      "loss": 0.2738,
      "step": 200
    },
    {
      "epoch": 0.2196652719665272,
      "grad_norm": 3.7340312004089355,
      "learning_rate": 1.9780334728033476e-05,
      "loss": 0.2436,
      "step": 210
    },
    {
      "epoch": 0.2301255230125523,
      "grad_norm": 5.62823486328125,
      "learning_rate": 1.976987447698745e-05,
      "loss": 0.2739,
      "step": 220
    },
    {
      "epoch": 0.2405857740585774,
      "grad_norm": 6.398805141448975,
      "learning_rate": 1.9759414225941425e-05,
      "loss": 0.2879,
      "step": 230
    },
    {
      "epoch": 0.2510460251046025,
      "grad_norm": 5.707062721252441,
      "learning_rate": 1.9748953974895398e-05,
      "loss": 0.2556,
      "step": 240
    },
    {
      "epoch": 0.2615062761506276,
      "grad_norm": 5.114287376403809,
      "learning_rate": 1.9738493723849374e-05,
      "loss": 0.2669,
      "step": 250
    },
    {
      "epoch": 0.2719665271966527,
      "grad_norm": 5.6166090965271,
      "learning_rate": 1.972803347280335e-05,
      "loss": 0.2926,
      "step": 260
    },
    {
      "epoch": 0.2824267782426778,
      "grad_norm": 5.5443644523620605,
      "learning_rate": 1.9717573221757324e-05,
      "loss": 0.3445,
      "step": 270
    },
    {
      "epoch": 0.2928870292887029,
      "grad_norm": 5.3020782470703125,
      "learning_rate": 1.97071129707113e-05,
      "loss": 0.2428,
      "step": 280
    },
    {
      "epoch": 0.303347280334728,
      "grad_norm": 3.5475831031799316,
      "learning_rate": 1.9696652719665273e-05,
      "loss": 0.2425,
      "step": 290
    },
    {
      "epoch": 0.3138075313807531,
      "grad_norm": 7.7649126052856445,
      "learning_rate": 1.9686192468619246e-05,
      "loss": 0.3331,
      "step": 300
    },
    {
      "epoch": 0.32426778242677823,
      "grad_norm": 8.333574295043945,
      "learning_rate": 1.9675732217573223e-05,
      "loss": 0.2646,
      "step": 310
    },
    {
      "epoch": 0.33472803347280333,
      "grad_norm": 6.77968692779541,
      "learning_rate": 1.96652719665272e-05,
      "loss": 0.2649,
      "step": 320
    },
    {
      "epoch": 0.34518828451882844,
      "grad_norm": 5.083942890167236,
      "learning_rate": 1.9654811715481172e-05,
      "loss": 0.2681,
      "step": 330
    },
    {
      "epoch": 0.35564853556485354,
      "grad_norm": 3.963552713394165,
      "learning_rate": 1.964435146443515e-05,
      "loss": 0.2169,
      "step": 340
    },
    {
      "epoch": 0.36610878661087864,
      "grad_norm": 5.4982380867004395,
      "learning_rate": 1.963389121338912e-05,
      "loss": 0.2234,
      "step": 350
    },
    {
      "epoch": 0.37656903765690375,
      "grad_norm": 5.702127456665039,
      "learning_rate": 1.9623430962343098e-05,
      "loss": 0.2245,
      "step": 360
    },
    {
      "epoch": 0.38702928870292885,
      "grad_norm": 5.8754754066467285,
      "learning_rate": 1.961297071129707e-05,
      "loss": 0.2092,
      "step": 370
    },
    {
      "epoch": 0.39748953974895396,
      "grad_norm": 3.4542341232299805,
      "learning_rate": 1.9602510460251047e-05,
      "loss": 0.2169,
      "step": 380
    },
    {
      "epoch": 0.40794979079497906,
      "grad_norm": 4.669724464416504,
      "learning_rate": 1.9592050209205024e-05,
      "loss": 0.2125,
      "step": 390
    },
    {
      "epoch": 0.41841004184100417,
      "grad_norm": 4.7386674880981445,
      "learning_rate": 1.9581589958158997e-05,
      "loss": 0.1865,
      "step": 400
    },
    {
      "epoch": 0.42887029288702927,
      "grad_norm": 5.594017028808594,
      "learning_rate": 1.9571129707112973e-05,
      "loss": 0.2383,
      "step": 410
    },
    {
      "epoch": 0.4393305439330544,
      "grad_norm": 3.7560579776763916,
      "learning_rate": 1.9560669456066946e-05,
      "loss": 0.2197,
      "step": 420
    },
    {
      "epoch": 0.4497907949790795,
      "grad_norm": 3.595973014831543,
      "learning_rate": 1.9550209205020922e-05,
      "loss": 0.1862,
      "step": 430
    },
    {
      "epoch": 0.4602510460251046,
      "grad_norm": 4.783253192901611,
      "learning_rate": 1.95397489539749e-05,
      "loss": 0.1927,
      "step": 440
    },
    {
      "epoch": 0.4707112970711297,
      "grad_norm": 6.696100234985352,
      "learning_rate": 1.9529288702928872e-05,
      "loss": 0.2095,
      "step": 450
    },
    {
      "epoch": 0.4811715481171548,
      "grad_norm": 4.946411609649658,
      "learning_rate": 1.9518828451882848e-05,
      "loss": 0.1782,
      "step": 460
    },
    {
      "epoch": 0.4916317991631799,
      "grad_norm": 3.783377170562744,
      "learning_rate": 1.950836820083682e-05,
      "loss": 0.1458,
      "step": 470
    },
    {
      "epoch": 0.502092050209205,
      "grad_norm": 5.182208061218262,
      "learning_rate": 1.9497907949790798e-05,
      "loss": 0.1662,
      "step": 480
    },
    {
      "epoch": 0.5125523012552301,
      "grad_norm": 4.505505084991455,
      "learning_rate": 1.9487447698744774e-05,
      "loss": 0.1733,
      "step": 490
    },
    {
      "epoch": 0.5230125523012552,
      "grad_norm": 6.710949897766113,
      "learning_rate": 1.9476987447698744e-05,
      "loss": 0.1981,
      "step": 500
    },
    {
      "epoch": 0.5334728033472803,
      "grad_norm": 3.7223522663116455,
      "learning_rate": 1.946652719665272e-05,
      "loss": 0.1472,
      "step": 510
    },
    {
      "epoch": 0.5439330543933054,
      "grad_norm": 4.521567344665527,
      "learning_rate": 1.9456066945606696e-05,
      "loss": 0.166,
      "step": 520
    },
    {
      "epoch": 0.5543933054393305,
      "grad_norm": 4.901477336883545,
      "learning_rate": 1.944560669456067e-05,
      "loss": 0.1654,
      "step": 530
    },
    {
      "epoch": 0.5648535564853556,
      "grad_norm": 5.700386047363281,
      "learning_rate": 1.9435146443514646e-05,
      "loss": 0.176,
      "step": 540
    },
    {
      "epoch": 0.5753138075313807,
      "grad_norm": 2.5836572647094727,
      "learning_rate": 1.942468619246862e-05,
      "loss": 0.1748,
      "step": 550
    },
    {
      "epoch": 0.5857740585774058,
      "grad_norm": 5.330174446105957,
      "learning_rate": 1.9414225941422595e-05,
      "loss": 0.1431,
      "step": 560
    },
    {
      "epoch": 0.5962343096234309,
      "grad_norm": 6.071039199829102,
      "learning_rate": 1.940376569037657e-05,
      "loss": 0.1776,
      "step": 570
    },
    {
      "epoch": 0.606694560669456,
      "grad_norm": 4.005063056945801,
      "learning_rate": 1.9393305439330545e-05,
      "loss": 0.1583,
      "step": 580
    },
    {
      "epoch": 0.6171548117154811,
      "grad_norm": 5.501367092132568,
      "learning_rate": 1.938284518828452e-05,
      "loss": 0.138,
      "step": 590
    },
    {
      "epoch": 0.6276150627615062,
      "grad_norm": 3.1384551525115967,
      "learning_rate": 1.9372384937238494e-05,
      "loss": 0.1715,
      "step": 600
    },
    {
      "epoch": 0.6380753138075314,
      "grad_norm": 4.188352584838867,
      "learning_rate": 1.936192468619247e-05,
      "loss": 0.1277,
      "step": 610
    },
    {
      "epoch": 0.6485355648535565,
      "grad_norm": 6.307796955108643,
      "learning_rate": 1.9351464435146447e-05,
      "loss": 0.139,
      "step": 620
    },
    {
      "epoch": 0.6589958158995816,
      "grad_norm": 4.159860610961914,
      "learning_rate": 1.934100418410042e-05,
      "loss": 0.1147,
      "step": 630
    },
    {
      "epoch": 0.6694560669456067,
      "grad_norm": 4.926093101501465,
      "learning_rate": 1.9330543933054396e-05,
      "loss": 0.1361,
      "step": 640
    },
    {
      "epoch": 0.6799163179916318,
      "grad_norm": 4.493704319000244,
      "learning_rate": 1.932008368200837e-05,
      "loss": 0.1478,
      "step": 650
    },
    {
      "epoch": 0.6903765690376569,
      "grad_norm": 4.632372856140137,
      "learning_rate": 1.9309623430962346e-05,
      "loss": 0.1436,
      "step": 660
    },
    {
      "epoch": 0.700836820083682,
      "grad_norm": 4.507720470428467,
      "learning_rate": 1.929916317991632e-05,
      "loss": 0.1197,
      "step": 670
    },
    {
      "epoch": 0.7112970711297071,
      "grad_norm": 3.172194004058838,
      "learning_rate": 1.9288702928870295e-05,
      "loss": 0.1376,
      "step": 680
    },
    {
      "epoch": 0.7217573221757322,
      "grad_norm": 4.91483211517334,
      "learning_rate": 1.927824267782427e-05,
      "loss": 0.1095,
      "step": 690
    },
    {
      "epoch": 0.7322175732217573,
      "grad_norm": 4.504169940948486,
      "learning_rate": 1.9267782426778245e-05,
      "loss": 0.1337,
      "step": 700
    },
    {
      "epoch": 0.7426778242677824,
      "grad_norm": 3.0317366123199463,
      "learning_rate": 1.9257322175732218e-05,
      "loss": 0.1157,
      "step": 710
    },
    {
      "epoch": 0.7531380753138075,
      "grad_norm": 6.3451948165893555,
      "learning_rate": 1.9246861924686194e-05,
      "loss": 0.1015,
      "step": 720
    },
    {
      "epoch": 0.7635983263598326,
      "grad_norm": 3.2946584224700928,
      "learning_rate": 1.9236401673640167e-05,
      "loss": 0.1156,
      "step": 730
    },
    {
      "epoch": 0.7740585774058577,
      "grad_norm": 6.600484848022461,
      "learning_rate": 1.9225941422594143e-05,
      "loss": 0.14,
      "step": 740
    },
    {
      "epoch": 0.7845188284518828,
      "grad_norm": 3.7719614505767822,
      "learning_rate": 1.921548117154812e-05,
      "loss": 0.0938,
      "step": 750
    },
    {
      "epoch": 0.7949790794979079,
      "grad_norm": 4.657994270324707,
      "learning_rate": 1.9205020920502093e-05,
      "loss": 0.1016,
      "step": 760
    },
    {
      "epoch": 0.805439330543933,
      "grad_norm": 12.631793022155762,
      "learning_rate": 1.919456066945607e-05,
      "loss": 0.1164,
      "step": 770
    },
    {
      "epoch": 0.8158995815899581,
      "grad_norm": 5.518421173095703,
      "learning_rate": 1.9184100418410042e-05,
      "loss": 0.1122,
      "step": 780
    },
    {
      "epoch": 0.8263598326359832,
      "grad_norm": 3.9673912525177,
      "learning_rate": 1.917364016736402e-05,
      "loss": 0.1049,
      "step": 790
    },
    {
      "epoch": 0.8368200836820083,
      "grad_norm": 5.547253131866455,
      "learning_rate": 1.916317991631799e-05,
      "loss": 0.1201,
      "step": 800
    },
    {
      "epoch": 0.8472803347280334,
      "grad_norm": 3.2947933673858643,
      "learning_rate": 1.9152719665271968e-05,
      "loss": 0.0951,
      "step": 810
    },
    {
      "epoch": 0.8577405857740585,
      "grad_norm": 5.408397197723389,
      "learning_rate": 1.9142259414225944e-05,
      "loss": 0.1202,
      "step": 820
    },
    {
      "epoch": 0.8682008368200836,
      "grad_norm": 4.5075764656066895,
      "learning_rate": 1.9131799163179917e-05,
      "loss": 0.0754,
      "step": 830
    },
    {
      "epoch": 0.8786610878661087,
      "grad_norm": 5.560410499572754,
      "learning_rate": 1.9121338912133894e-05,
      "loss": 0.0845,
      "step": 840
    },
    {
      "epoch": 0.8891213389121339,
      "grad_norm": 7.5776591300964355,
      "learning_rate": 1.9110878661087867e-05,
      "loss": 0.0796,
      "step": 850
    },
    {
      "epoch": 0.899581589958159,
      "grad_norm": 5.397644519805908,
      "learning_rate": 1.9100418410041843e-05,
      "loss": 0.143,
      "step": 860
    },
    {
      "epoch": 0.9100418410041841,
      "grad_norm": 4.083990097045898,
      "learning_rate": 1.908995815899582e-05,
      "loss": 0.1173,
      "step": 870
    },
    {
      "epoch": 0.9205020920502092,
      "grad_norm": 16.54612159729004,
      "learning_rate": 1.9079497907949793e-05,
      "loss": 0.0758,
      "step": 880
    },
    {
      "epoch": 0.9309623430962343,
      "grad_norm": 3.5800740718841553,
      "learning_rate": 1.906903765690377e-05,
      "loss": 0.0917,
      "step": 890
    },
    {
      "epoch": 0.9414225941422594,
      "grad_norm": 4.3252787590026855,
      "learning_rate": 1.9058577405857742e-05,
      "loss": 0.0994,
      "step": 900
    },
    {
      "epoch": 0.9518828451882845,
      "grad_norm": 3.4889602661132812,
      "learning_rate": 1.9048117154811715e-05,
      "loss": 0.0785,
      "step": 910
    },
    {
      "epoch": 0.9623430962343096,
      "grad_norm": 7.665345668792725,
      "learning_rate": 1.903765690376569e-05,
      "loss": 0.1073,
      "step": 920
    },
    {
      "epoch": 0.9728033472803347,
      "grad_norm": 5.793982982635498,
      "learning_rate": 1.9027196652719664e-05,
      "loss": 0.0925,
      "step": 930
    },
    {
      "epoch": 0.9832635983263598,
      "grad_norm": 9.048720359802246,
      "learning_rate": 1.901673640167364e-05,
      "loss": 0.0874,
      "step": 940
    },
    {
      "epoch": 0.9937238493723849,
      "grad_norm": 3.539977550506592,
      "learning_rate": 1.9006276150627617e-05,
      "loss": 0.0737,
      "step": 950
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9777439024390244,
      "eval_f1": 0.8995873452544704,
      "eval_loss": 0.06086462363600731,
      "eval_precision": 0.9285376242309512,
      "eval_recall": 0.8723877278790574,
      "eval_runtime": 323.9306,
      "eval_samples_per_second": 60.757,
      "eval_steps_per_second": 2.534,
      "step": 956
    },
    {
      "epoch": 1.00418410041841,
      "grad_norm": 2.8359766006469727,
      "learning_rate": 1.899581589958159e-05,
      "loss": 0.0611,
      "step": 960
    },
    {
      "epoch": 1.0146443514644352,
      "grad_norm": 2.264669179916382,
      "learning_rate": 1.8985355648535567e-05,
      "loss": 0.0652,
      "step": 970
    },
    {
      "epoch": 1.0251046025104602,
      "grad_norm": 3.7298126220703125,
      "learning_rate": 1.897489539748954e-05,
      "loss": 0.0673,
      "step": 980
    },
    {
      "epoch": 1.0355648535564854,
      "grad_norm": 6.8629584312438965,
      "learning_rate": 1.8964435146443516e-05,
      "loss": 0.0658,
      "step": 990
    },
    {
      "epoch": 1.0460251046025104,
      "grad_norm": 4.135858535766602,
      "learning_rate": 1.8953974895397492e-05,
      "loss": 0.0868,
      "step": 1000
    },
    {
      "epoch": 1.0564853556485356,
      "grad_norm": 2.4617629051208496,
      "learning_rate": 1.8943514644351465e-05,
      "loss": 0.0671,
      "step": 1010
    },
    {
      "epoch": 1.0669456066945606,
      "grad_norm": 6.315227031707764,
      "learning_rate": 1.8933054393305442e-05,
      "loss": 0.0787,
      "step": 1020
    },
    {
      "epoch": 1.0774058577405858,
      "grad_norm": 4.271310806274414,
      "learning_rate": 1.8922594142259415e-05,
      "loss": 0.0936,
      "step": 1030
    },
    {
      "epoch": 1.0878661087866108,
      "grad_norm": 3.037954092025757,
      "learning_rate": 1.891213389121339e-05,
      "loss": 0.0564,
      "step": 1040
    },
    {
      "epoch": 1.098326359832636,
      "grad_norm": 5.546891689300537,
      "learning_rate": 1.8901673640167368e-05,
      "loss": 0.0891,
      "step": 1050
    },
    {
      "epoch": 1.108786610878661,
      "grad_norm": 4.560745716094971,
      "learning_rate": 1.889121338912134e-05,
      "loss": 0.0714,
      "step": 1060
    },
    {
      "epoch": 1.1192468619246863,
      "grad_norm": 4.528234004974365,
      "learning_rate": 1.8880753138075317e-05,
      "loss": 0.0966,
      "step": 1070
    },
    {
      "epoch": 1.1297071129707112,
      "grad_norm": 2.2858495712280273,
      "learning_rate": 1.887029288702929e-05,
      "loss": 0.0722,
      "step": 1080
    },
    {
      "epoch": 1.1401673640167365,
      "grad_norm": 1.5279911756515503,
      "learning_rate": 1.8859832635983266e-05,
      "loss": 0.0407,
      "step": 1090
    },
    {
      "epoch": 1.1506276150627615,
      "grad_norm": 2.59973406791687,
      "learning_rate": 1.884937238493724e-05,
      "loss": 0.0506,
      "step": 1100
    },
    {
      "epoch": 1.1610878661087867,
      "grad_norm": 6.03679895401001,
      "learning_rate": 1.8838912133891213e-05,
      "loss": 0.0644,
      "step": 1110
    },
    {
      "epoch": 1.1715481171548117,
      "grad_norm": 3.9836931228637695,
      "learning_rate": 1.882845188284519e-05,
      "loss": 0.0618,
      "step": 1120
    },
    {
      "epoch": 1.1820083682008369,
      "grad_norm": 2.1669321060180664,
      "learning_rate": 1.8817991631799165e-05,
      "loss": 0.0619,
      "step": 1130
    },
    {
      "epoch": 1.1924686192468619,
      "grad_norm": 4.000410556793213,
      "learning_rate": 1.880753138075314e-05,
      "loss": 0.0738,
      "step": 1140
    },
    {
      "epoch": 1.202928870292887,
      "grad_norm": 2.5954811573028564,
      "learning_rate": 1.8797071129707115e-05,
      "loss": 0.0579,
      "step": 1150
    },
    {
      "epoch": 1.213389121338912,
      "grad_norm": 2.4451422691345215,
      "learning_rate": 1.8786610878661088e-05,
      "loss": 0.0504,
      "step": 1160
    },
    {
      "epoch": 1.2238493723849373,
      "grad_norm": 2.6711478233337402,
      "learning_rate": 1.8776150627615064e-05,
      "loss": 0.0626,
      "step": 1170
    },
    {
      "epoch": 1.2343096234309623,
      "grad_norm": 6.443387985229492,
      "learning_rate": 1.876569037656904e-05,
      "loss": 0.0725,
      "step": 1180
    },
    {
      "epoch": 1.2447698744769875,
      "grad_norm": 4.090080261230469,
      "learning_rate": 1.8755230125523014e-05,
      "loss": 0.0294,
      "step": 1190
    },
    {
      "epoch": 1.2552301255230125,
      "grad_norm": 2.9574637413024902,
      "learning_rate": 1.874476987447699e-05,
      "loss": 0.0596,
      "step": 1200
    },
    {
      "epoch": 1.2656903765690377,
      "grad_norm": 2.0854010581970215,
      "learning_rate": 1.8734309623430963e-05,
      "loss": 0.0363,
      "step": 1210
    },
    {
      "epoch": 1.2761506276150627,
      "grad_norm": 6.4539570808410645,
      "learning_rate": 1.872384937238494e-05,
      "loss": 0.0596,
      "step": 1220
    },
    {
      "epoch": 1.286610878661088,
      "grad_norm": 6.181853294372559,
      "learning_rate": 1.8713389121338912e-05,
      "loss": 0.0946,
      "step": 1230
    },
    {
      "epoch": 1.297071129707113,
      "grad_norm": 6.571756839752197,
      "learning_rate": 1.870292887029289e-05,
      "loss": 0.0874,
      "step": 1240
    },
    {
      "epoch": 1.3075313807531381,
      "grad_norm": 3.0451953411102295,
      "learning_rate": 1.8692468619246865e-05,
      "loss": 0.0529,
      "step": 1250
    },
    {
      "epoch": 1.3179916317991631,
      "grad_norm": 1.8191800117492676,
      "learning_rate": 1.8682008368200838e-05,
      "loss": 0.0477,
      "step": 1260
    },
    {
      "epoch": 1.3284518828451883,
      "grad_norm": 4.010562896728516,
      "learning_rate": 1.8671548117154815e-05,
      "loss": 0.0774,
      "step": 1270
    },
    {
      "epoch": 1.3389121338912133,
      "grad_norm": 3.588857650756836,
      "learning_rate": 1.8661087866108788e-05,
      "loss": 0.0733,
      "step": 1280
    },
    {
      "epoch": 1.3493723849372385,
      "grad_norm": 1.381174087524414,
      "learning_rate": 1.8650627615062764e-05,
      "loss": 0.0379,
      "step": 1290
    },
    {
      "epoch": 1.3598326359832635,
      "grad_norm": 3.6804378032684326,
      "learning_rate": 1.8640167364016737e-05,
      "loss": 0.0587,
      "step": 1300
    },
    {
      "epoch": 1.3702928870292888,
      "grad_norm": 1.4798146486282349,
      "learning_rate": 1.8629707112970713e-05,
      "loss": 0.0446,
      "step": 1310
    },
    {
      "epoch": 1.3807531380753137,
      "grad_norm": 2.4774863719940186,
      "learning_rate": 1.8619246861924686e-05,
      "loss": 0.0472,
      "step": 1320
    },
    {
      "epoch": 1.391213389121339,
      "grad_norm": 1.7484071254730225,
      "learning_rate": 1.8608786610878663e-05,
      "loss": 0.0703,
      "step": 1330
    },
    {
      "epoch": 1.401673640167364,
      "grad_norm": 4.882641315460205,
      "learning_rate": 1.8598326359832636e-05,
      "loss": 0.0693,
      "step": 1340
    },
    {
      "epoch": 1.4121338912133892,
      "grad_norm": 2.6022212505340576,
      "learning_rate": 1.8587866108786612e-05,
      "loss": 0.0538,
      "step": 1350
    },
    {
      "epoch": 1.4225941422594142,
      "grad_norm": 2.1142261028289795,
      "learning_rate": 1.8577405857740585e-05,
      "loss": 0.0555,
      "step": 1360
    },
    {
      "epoch": 1.4330543933054394,
      "grad_norm": 2.1282799243927,
      "learning_rate": 1.856694560669456e-05,
      "loss": 0.0497,
      "step": 1370
    },
    {
      "epoch": 1.4435146443514644,
      "grad_norm": 4.611587047576904,
      "learning_rate": 1.8556485355648538e-05,
      "loss": 0.0536,
      "step": 1380
    },
    {
      "epoch": 1.4539748953974896,
      "grad_norm": 3.718817710876465,
      "learning_rate": 1.854602510460251e-05,
      "loss": 0.0451,
      "step": 1390
    },
    {
      "epoch": 1.4644351464435146,
      "grad_norm": 1.5255101919174194,
      "learning_rate": 1.8535564853556487e-05,
      "loss": 0.0517,
      "step": 1400
    },
    {
      "epoch": 1.4748953974895398,
      "grad_norm": 16.798847198486328,
      "learning_rate": 1.852510460251046e-05,
      "loss": 0.048,
      "step": 1410
    },
    {
      "epoch": 1.4853556485355648,
      "grad_norm": 1.070058822631836,
      "learning_rate": 1.8514644351464437e-05,
      "loss": 0.0654,
      "step": 1420
    },
    {
      "epoch": 1.49581589958159,
      "grad_norm": 0.48588258028030396,
      "learning_rate": 1.8504184100418413e-05,
      "loss": 0.0595,
      "step": 1430
    },
    {
      "epoch": 1.506276150627615,
      "grad_norm": 3.2824840545654297,
      "learning_rate": 1.8493723849372386e-05,
      "loss": 0.0372,
      "step": 1440
    },
    {
      "epoch": 1.5167364016736402,
      "grad_norm": 3.5696442127227783,
      "learning_rate": 1.8483263598326363e-05,
      "loss": 0.0555,
      "step": 1450
    },
    {
      "epoch": 1.5271966527196654,
      "grad_norm": 0.9533070921897888,
      "learning_rate": 1.8472803347280336e-05,
      "loss": 0.0578,
      "step": 1460
    },
    {
      "epoch": 1.5376569037656904,
      "grad_norm": 1.061668038368225,
      "learning_rate": 1.8462343096234312e-05,
      "loss": 0.0402,
      "step": 1470
    },
    {
      "epoch": 1.5481171548117154,
      "grad_norm": 2.742687463760376,
      "learning_rate": 1.845188284518829e-05,
      "loss": 0.0386,
      "step": 1480
    },
    {
      "epoch": 1.5585774058577406,
      "grad_norm": 4.374299049377441,
      "learning_rate": 1.844142259414226e-05,
      "loss": 0.0672,
      "step": 1490
    },
    {
      "epoch": 1.5690376569037658,
      "grad_norm": 2.8906054496765137,
      "learning_rate": 1.8430962343096234e-05,
      "loss": 0.0586,
      "step": 1500
    },
    {
      "epoch": 1.5794979079497908,
      "grad_norm": 4.005278587341309,
      "learning_rate": 1.842050209205021e-05,
      "loss": 0.0421,
      "step": 1510
    },
    {
      "epoch": 1.5899581589958158,
      "grad_norm": 4.838023662567139,
      "learning_rate": 1.8410041841004184e-05,
      "loss": 0.0619,
      "step": 1520
    },
    {
      "epoch": 1.600418410041841,
      "grad_norm": 2.5632026195526123,
      "learning_rate": 1.839958158995816e-05,
      "loss": 0.046,
      "step": 1530
    },
    {
      "epoch": 1.6108786610878663,
      "grad_norm": 3.7848684787750244,
      "learning_rate": 1.8389121338912133e-05,
      "loss": 0.0373,
      "step": 1540
    },
    {
      "epoch": 1.6213389121338913,
      "grad_norm": 1.6795481443405151,
      "learning_rate": 1.837866108786611e-05,
      "loss": 0.0432,
      "step": 1550
    },
    {
      "epoch": 1.6317991631799162,
      "grad_norm": 2.4946532249450684,
      "learning_rate": 1.8368200836820086e-05,
      "loss": 0.0565,
      "step": 1560
    },
    {
      "epoch": 1.6422594142259415,
      "grad_norm": 3.0107297897338867,
      "learning_rate": 1.835774058577406e-05,
      "loss": 0.0235,
      "step": 1570
    },
    {
      "epoch": 1.6527196652719667,
      "grad_norm": 1.7937043905258179,
      "learning_rate": 1.8347280334728036e-05,
      "loss": 0.0429,
      "step": 1580
    },
    {
      "epoch": 1.6631799163179917,
      "grad_norm": 1.5529779195785522,
      "learning_rate": 1.833682008368201e-05,
      "loss": 0.043,
      "step": 1590
    },
    {
      "epoch": 1.6736401673640167,
      "grad_norm": 2.041825532913208,
      "learning_rate": 1.8326359832635985e-05,
      "loss": 0.0417,
      "step": 1600
    },
    {
      "epoch": 1.6841004184100419,
      "grad_norm": 2.9514405727386475,
      "learning_rate": 1.831589958158996e-05,
      "loss": 0.0492,
      "step": 1610
    },
    {
      "epoch": 1.694560669456067,
      "grad_norm": 0.6419033408164978,
      "learning_rate": 1.8305439330543934e-05,
      "loss": 0.0313,
      "step": 1620
    },
    {
      "epoch": 1.705020920502092,
      "grad_norm": 5.304439067840576,
      "learning_rate": 1.829497907949791e-05,
      "loss": 0.0572,
      "step": 1630
    },
    {
      "epoch": 1.715481171548117,
      "grad_norm": 3.8230204582214355,
      "learning_rate": 1.8284518828451884e-05,
      "loss": 0.0327,
      "step": 1640
    },
    {
      "epoch": 1.7259414225941423,
      "grad_norm": 4.965395927429199,
      "learning_rate": 1.827405857740586e-05,
      "loss": 0.0481,
      "step": 1650
    },
    {
      "epoch": 1.7364016736401675,
      "grad_norm": 2.0998406410217285,
      "learning_rate": 1.8263598326359837e-05,
      "loss": 0.0306,
      "step": 1660
    },
    {
      "epoch": 1.7468619246861925,
      "grad_norm": 3.6487364768981934,
      "learning_rate": 1.825313807531381e-05,
      "loss": 0.0458,
      "step": 1670
    },
    {
      "epoch": 1.7573221757322175,
      "grad_norm": 4.531862258911133,
      "learning_rate": 1.8242677824267786e-05,
      "loss": 0.0221,
      "step": 1680
    },
    {
      "epoch": 1.7677824267782427,
      "grad_norm": 0.8992353081703186,
      "learning_rate": 1.823221757322176e-05,
      "loss": 0.0353,
      "step": 1690
    },
    {
      "epoch": 1.778242677824268,
      "grad_norm": 0.21804414689540863,
      "learning_rate": 1.8221757322175732e-05,
      "loss": 0.0357,
      "step": 1700
    },
    {
      "epoch": 1.788702928870293,
      "grad_norm": 3.5746381282806396,
      "learning_rate": 1.821129707112971e-05,
      "loss": 0.0287,
      "step": 1710
    },
    {
      "epoch": 1.799163179916318,
      "grad_norm": 3.334658622741699,
      "learning_rate": 1.820083682008368e-05,
      "loss": 0.0478,
      "step": 1720
    },
    {
      "epoch": 1.8096234309623431,
      "grad_norm": 3.0232086181640625,
      "learning_rate": 1.8190376569037658e-05,
      "loss": 0.0364,
      "step": 1730
    },
    {
      "epoch": 1.8200836820083683,
      "grad_norm": 4.5866618156433105,
      "learning_rate": 1.8179916317991634e-05,
      "loss": 0.0351,
      "step": 1740
    },
    {
      "epoch": 1.8305439330543933,
      "grad_norm": 3.4079537391662598,
      "learning_rate": 1.8169456066945607e-05,
      "loss": 0.0345,
      "step": 1750
    },
    {
      "epoch": 1.8410041841004183,
      "grad_norm": 3.124258279800415,
      "learning_rate": 1.8158995815899584e-05,
      "loss": 0.0482,
      "step": 1760
    },
    {
      "epoch": 1.8514644351464435,
      "grad_norm": 0.6461979746818542,
      "learning_rate": 1.8148535564853557e-05,
      "loss": 0.0299,
      "step": 1770
    },
    {
      "epoch": 1.8619246861924688,
      "grad_norm": 3.6773874759674072,
      "learning_rate": 1.8138075313807533e-05,
      "loss": 0.032,
      "step": 1780
    },
    {
      "epoch": 1.8723849372384938,
      "grad_norm": 3.7209386825561523,
      "learning_rate": 1.812761506276151e-05,
      "loss": 0.0395,
      "step": 1790
    },
    {
      "epoch": 1.8828451882845187,
      "grad_norm": 1.217613935470581,
      "learning_rate": 1.8117154811715482e-05,
      "loss": 0.0337,
      "step": 1800
    },
    {
      "epoch": 1.893305439330544,
      "grad_norm": 3.1766624450683594,
      "learning_rate": 1.810669456066946e-05,
      "loss": 0.0371,
      "step": 1810
    },
    {
      "epoch": 1.9037656903765692,
      "grad_norm": 4.417557239532471,
      "learning_rate": 1.8096234309623432e-05,
      "loss": 0.0389,
      "step": 1820
    },
    {
      "epoch": 1.9142259414225942,
      "grad_norm": 1.7213631868362427,
      "learning_rate": 1.8085774058577408e-05,
      "loss": 0.0732,
      "step": 1830
    },
    {
      "epoch": 1.9246861924686192,
      "grad_norm": 0.8656977415084839,
      "learning_rate": 1.807531380753138e-05,
      "loss": 0.0261,
      "step": 1840
    },
    {
      "epoch": 1.9351464435146444,
      "grad_norm": 3.693336248397827,
      "learning_rate": 1.8064853556485358e-05,
      "loss": 0.0424,
      "step": 1850
    },
    {
      "epoch": 1.9456066945606696,
      "grad_norm": 3.830660820007324,
      "learning_rate": 1.8054393305439334e-05,
      "loss": 0.0396,
      "step": 1860
    },
    {
      "epoch": 1.9560669456066946,
      "grad_norm": 1.774104356765747,
      "learning_rate": 1.8043933054393307e-05,
      "loss": 0.0265,
      "step": 1870
    },
    {
      "epoch": 1.9665271966527196,
      "grad_norm": 2.480105400085449,
      "learning_rate": 1.8033472803347283e-05,
      "loss": 0.0415,
      "step": 1880
    },
    {
      "epoch": 1.9769874476987448,
      "grad_norm": 3.5593388080596924,
      "learning_rate": 1.8023012552301256e-05,
      "loss": 0.0471,
      "step": 1890
    },
    {
      "epoch": 1.98744769874477,
      "grad_norm": 3.9639015197753906,
      "learning_rate": 1.801255230125523e-05,
      "loss": 0.0536,
      "step": 1900
    },
    {
      "epoch": 1.997907949790795,
      "grad_norm": 4.942607402801514,
      "learning_rate": 1.8002092050209206e-05,
      "loss": 0.0537,
      "step": 1910
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9898882113821138,
      "eval_f1": 0.9553711594527922,
      "eval_loss": 0.031164659187197685,
      "eval_precision": 0.9638009049773756,
      "eval_recall": 0.9470875944864384,
      "eval_runtime": 324.5342,
      "eval_samples_per_second": 60.644,
      "eval_steps_per_second": 2.53,
      "step": 1912
    },
    {
      "epoch": 2.00836820083682,
      "grad_norm": 1.7955524921417236,
      "learning_rate": 1.7991631799163182e-05,
      "loss": 0.0451,
      "step": 1920
    },
    {
      "epoch": 2.018828451882845,
      "grad_norm": 1.3303532600402832,
      "learning_rate": 1.7981171548117155e-05,
      "loss": 0.0235,
      "step": 1930
    },
    {
      "epoch": 2.0292887029288704,
      "grad_norm": 0.12639673054218292,
      "learning_rate": 1.797071129707113e-05,
      "loss": 0.0295,
      "step": 1940
    },
    {
      "epoch": 2.0397489539748954,
      "grad_norm": 0.8691103458404541,
      "learning_rate": 1.7960251046025105e-05,
      "loss": 0.0576,
      "step": 1950
    },
    {
      "epoch": 2.0502092050209204,
      "grad_norm": 1.2318377494812012,
      "learning_rate": 1.794979079497908e-05,
      "loss": 0.0203,
      "step": 1960
    },
    {
      "epoch": 2.0606694560669454,
      "grad_norm": 3.641785144805908,
      "learning_rate": 1.7939330543933054e-05,
      "loss": 0.0385,
      "step": 1970
    },
    {
      "epoch": 2.071129707112971,
      "grad_norm": 0.807912290096283,
      "learning_rate": 1.792887029288703e-05,
      "loss": 0.0221,
      "step": 1980
    },
    {
      "epoch": 2.081589958158996,
      "grad_norm": 1.4528104066848755,
      "learning_rate": 1.7918410041841007e-05,
      "loss": 0.0231,
      "step": 1990
    },
    {
      "epoch": 2.092050209205021,
      "grad_norm": 3.753145694732666,
      "learning_rate": 1.790794979079498e-05,
      "loss": 0.0534,
      "step": 2000
    },
    {
      "epoch": 2.102510460251046,
      "grad_norm": 1.9070883989334106,
      "learning_rate": 1.7897489539748956e-05,
      "loss": 0.0247,
      "step": 2010
    },
    {
      "epoch": 2.1129707112970713,
      "grad_norm": 1.6953750848770142,
      "learning_rate": 1.788702928870293e-05,
      "loss": 0.0243,
      "step": 2020
    },
    {
      "epoch": 2.1234309623430963,
      "grad_norm": 1.9449267387390137,
      "learning_rate": 1.7876569037656906e-05,
      "loss": 0.0307,
      "step": 2030
    },
    {
      "epoch": 2.1338912133891212,
      "grad_norm": 2.4459972381591797,
      "learning_rate": 1.7866108786610882e-05,
      "loss": 0.039,
      "step": 2040
    },
    {
      "epoch": 2.1443514644351462,
      "grad_norm": 3.2180447578430176,
      "learning_rate": 1.7855648535564855e-05,
      "loss": 0.0207,
      "step": 2050
    },
    {
      "epoch": 2.1548117154811717,
      "grad_norm": 0.08906514197587967,
      "learning_rate": 1.784518828451883e-05,
      "loss": 0.0194,
      "step": 2060
    },
    {
      "epoch": 2.1652719665271967,
      "grad_norm": 4.003251075744629,
      "learning_rate": 1.7834728033472805e-05,
      "loss": 0.0167,
      "step": 2070
    },
    {
      "epoch": 2.1757322175732217,
      "grad_norm": 2.9258475303649902,
      "learning_rate": 1.782426778242678e-05,
      "loss": 0.0307,
      "step": 2080
    },
    {
      "epoch": 2.1861924686192467,
      "grad_norm": 2.6761608123779297,
      "learning_rate": 1.7813807531380757e-05,
      "loss": 0.0265,
      "step": 2090
    },
    {
      "epoch": 2.196652719665272,
      "grad_norm": 3.3722574710845947,
      "learning_rate": 1.7803347280334727e-05,
      "loss": 0.0478,
      "step": 2100
    },
    {
      "epoch": 2.207112970711297,
      "grad_norm": 0.06532339751720428,
      "learning_rate": 1.7792887029288703e-05,
      "loss": 0.027,
      "step": 2110
    },
    {
      "epoch": 2.217573221757322,
      "grad_norm": 2.2501392364501953,
      "learning_rate": 1.778242677824268e-05,
      "loss": 0.0149,
      "step": 2120
    },
    {
      "epoch": 2.2280334728033475,
      "grad_norm": 5.171396732330322,
      "learning_rate": 1.7771966527196653e-05,
      "loss": 0.0765,
      "step": 2130
    },
    {
      "epoch": 2.2384937238493725,
      "grad_norm": 3.801947593688965,
      "learning_rate": 1.776150627615063e-05,
      "loss": 0.0341,
      "step": 2140
    },
    {
      "epoch": 2.2489539748953975,
      "grad_norm": 1.6435478925704956,
      "learning_rate": 1.7751046025104602e-05,
      "loss": 0.0454,
      "step": 2150
    },
    {
      "epoch": 2.2594142259414225,
      "grad_norm": 4.507822513580322,
      "learning_rate": 1.774058577405858e-05,
      "loss": 0.0417,
      "step": 2160
    },
    {
      "epoch": 2.2698744769874475,
      "grad_norm": 2.8507375717163086,
      "learning_rate": 1.7730125523012555e-05,
      "loss": 0.0359,
      "step": 2170
    },
    {
      "epoch": 2.280334728033473,
      "grad_norm": 3.1739492416381836,
      "learning_rate": 1.7719665271966528e-05,
      "loss": 0.0166,
      "step": 2180
    },
    {
      "epoch": 2.290794979079498,
      "grad_norm": 3.4581923484802246,
      "learning_rate": 1.7709205020920504e-05,
      "loss": 0.0261,
      "step": 2190
    },
    {
      "epoch": 2.301255230125523,
      "grad_norm": 0.993465781211853,
      "learning_rate": 1.7698744769874477e-05,
      "loss": 0.0182,
      "step": 2200
    },
    {
      "epoch": 2.3117154811715483,
      "grad_norm": 3.4708056449890137,
      "learning_rate": 1.7688284518828454e-05,
      "loss": 0.0233,
      "step": 2210
    },
    {
      "epoch": 2.3221757322175733,
      "grad_norm": 2.0008151531219482,
      "learning_rate": 1.767782426778243e-05,
      "loss": 0.0274,
      "step": 2220
    },
    {
      "epoch": 2.3326359832635983,
      "grad_norm": 6.841259002685547,
      "learning_rate": 1.7667364016736403e-05,
      "loss": 0.0482,
      "step": 2230
    },
    {
      "epoch": 2.3430962343096233,
      "grad_norm": 0.28066596388816833,
      "learning_rate": 1.765690376569038e-05,
      "loss": 0.0252,
      "step": 2240
    },
    {
      "epoch": 2.3535564853556483,
      "grad_norm": 1.797333002090454,
      "learning_rate": 1.7646443514644353e-05,
      "loss": 0.0272,
      "step": 2250
    },
    {
      "epoch": 2.3640167364016738,
      "grad_norm": 0.38733959197998047,
      "learning_rate": 1.763598326359833e-05,
      "loss": 0.0452,
      "step": 2260
    },
    {
      "epoch": 2.3744769874476988,
      "grad_norm": 1.2622672319412231,
      "learning_rate": 1.7625523012552302e-05,
      "loss": 0.021,
      "step": 2270
    },
    {
      "epoch": 2.3849372384937237,
      "grad_norm": 3.8274953365325928,
      "learning_rate": 1.761506276150628e-05,
      "loss": 0.0327,
      "step": 2280
    },
    {
      "epoch": 2.395397489539749,
      "grad_norm": 0.2882668972015381,
      "learning_rate": 1.7604602510460255e-05,
      "loss": 0.0298,
      "step": 2290
    },
    {
      "epoch": 2.405857740585774,
      "grad_norm": 0.4759063720703125,
      "learning_rate": 1.7594142259414228e-05,
      "loss": 0.0149,
      "step": 2300
    },
    {
      "epoch": 2.416317991631799,
      "grad_norm": 4.941986083984375,
      "learning_rate": 1.75836820083682e-05,
      "loss": 0.0193,
      "step": 2310
    },
    {
      "epoch": 2.426778242677824,
      "grad_norm": 4.190165042877197,
      "learning_rate": 1.7573221757322177e-05,
      "loss": 0.0271,
      "step": 2320
    },
    {
      "epoch": 2.437238493723849,
      "grad_norm": 3.4528160095214844,
      "learning_rate": 1.756276150627615e-05,
      "loss": 0.0434,
      "step": 2330
    },
    {
      "epoch": 2.4476987447698746,
      "grad_norm": 3.133659839630127,
      "learning_rate": 1.7552301255230127e-05,
      "loss": 0.0294,
      "step": 2340
    },
    {
      "epoch": 2.4581589958158996,
      "grad_norm": 1.6587878465652466,
      "learning_rate": 1.7541841004184103e-05,
      "loss": 0.0415,
      "step": 2350
    },
    {
      "epoch": 2.4686192468619246,
      "grad_norm": 4.726372718811035,
      "learning_rate": 1.7531380753138076e-05,
      "loss": 0.0348,
      "step": 2360
    },
    {
      "epoch": 2.47907949790795,
      "grad_norm": 2.5909640789031982,
      "learning_rate": 1.7520920502092052e-05,
      "loss": 0.0302,
      "step": 2370
    },
    {
      "epoch": 2.489539748953975,
      "grad_norm": 1.9666930437088013,
      "learning_rate": 1.7510460251046025e-05,
      "loss": 0.0286,
      "step": 2380
    },
    {
      "epoch": 2.5,
      "grad_norm": 5.776470184326172,
      "learning_rate": 1.7500000000000002e-05,
      "loss": 0.0129,
      "step": 2390
    },
    {
      "epoch": 2.510460251046025,
      "grad_norm": 0.318473219871521,
      "learning_rate": 1.7489539748953975e-05,
      "loss": 0.0231,
      "step": 2400
    },
    {
      "epoch": 2.52092050209205,
      "grad_norm": 0.9475606083869934,
      "learning_rate": 1.747907949790795e-05,
      "loss": 0.0227,
      "step": 2410
    },
    {
      "epoch": 2.5313807531380754,
      "grad_norm": 3.135499954223633,
      "learning_rate": 1.7468619246861928e-05,
      "loss": 0.0228,
      "step": 2420
    },
    {
      "epoch": 2.5418410041841004,
      "grad_norm": 4.2688188552856445,
      "learning_rate": 1.74581589958159e-05,
      "loss": 0.0223,
      "step": 2430
    },
    {
      "epoch": 2.5523012552301254,
      "grad_norm": 2.780498743057251,
      "learning_rate": 1.7447698744769877e-05,
      "loss": 0.0255,
      "step": 2440
    },
    {
      "epoch": 2.562761506276151,
      "grad_norm": 4.366386890411377,
      "learning_rate": 1.743723849372385e-05,
      "loss": 0.0304,
      "step": 2450
    },
    {
      "epoch": 2.573221757322176,
      "grad_norm": 3.4373250007629395,
      "learning_rate": 1.7426778242677826e-05,
      "loss": 0.0206,
      "step": 2460
    },
    {
      "epoch": 2.583682008368201,
      "grad_norm": 3.248351573944092,
      "learning_rate": 1.7416317991631803e-05,
      "loss": 0.0247,
      "step": 2470
    },
    {
      "epoch": 2.594142259414226,
      "grad_norm": 3.699845790863037,
      "learning_rate": 1.7405857740585776e-05,
      "loss": 0.0222,
      "step": 2480
    },
    {
      "epoch": 2.604602510460251,
      "grad_norm": 5.349005222320557,
      "learning_rate": 1.739539748953975e-05,
      "loss": 0.028,
      "step": 2490
    },
    {
      "epoch": 2.6150627615062763,
      "grad_norm": 1.6605565547943115,
      "learning_rate": 1.7384937238493725e-05,
      "loss": 0.0572,
      "step": 2500
    },
    {
      "epoch": 2.6255230125523012,
      "grad_norm": 0.3934902250766754,
      "learning_rate": 1.73744769874477e-05,
      "loss": 0.0226,
      "step": 2510
    },
    {
      "epoch": 2.6359832635983262,
      "grad_norm": 1.2252792119979858,
      "learning_rate": 1.7364016736401675e-05,
      "loss": 0.0249,
      "step": 2520
    },
    {
      "epoch": 2.6464435146443517,
      "grad_norm": 3.055999279022217,
      "learning_rate": 1.7353556485355648e-05,
      "loss": 0.0203,
      "step": 2530
    },
    {
      "epoch": 2.6569037656903767,
      "grad_norm": 2.2037620544433594,
      "learning_rate": 1.7343096234309624e-05,
      "loss": 0.0228,
      "step": 2540
    },
    {
      "epoch": 2.6673640167364017,
      "grad_norm": 2.7833845615386963,
      "learning_rate": 1.73326359832636e-05,
      "loss": 0.0183,
      "step": 2550
    },
    {
      "epoch": 2.6778242677824267,
      "grad_norm": 0.3431767225265503,
      "learning_rate": 1.7322175732217574e-05,
      "loss": 0.0303,
      "step": 2560
    },
    {
      "epoch": 2.6882845188284517,
      "grad_norm": 2.0098791122436523,
      "learning_rate": 1.731171548117155e-05,
      "loss": 0.0306,
      "step": 2570
    },
    {
      "epoch": 2.698744769874477,
      "grad_norm": 1.2938282489776611,
      "learning_rate": 1.7301255230125523e-05,
      "loss": 0.0163,
      "step": 2580
    },
    {
      "epoch": 2.709205020920502,
      "grad_norm": 2.001146078109741,
      "learning_rate": 1.72907949790795e-05,
      "loss": 0.013,
      "step": 2590
    },
    {
      "epoch": 2.719665271966527,
      "grad_norm": 5.078675746917725,
      "learning_rate": 1.7280334728033476e-05,
      "loss": 0.0271,
      "step": 2600
    },
    {
      "epoch": 2.7301255230125525,
      "grad_norm": 0.8808063268661499,
      "learning_rate": 1.726987447698745e-05,
      "loss": 0.0275,
      "step": 2610
    },
    {
      "epoch": 2.7405857740585775,
      "grad_norm": 3.764402389526367,
      "learning_rate": 1.7259414225941425e-05,
      "loss": 0.0184,
      "step": 2620
    },
    {
      "epoch": 2.7510460251046025,
      "grad_norm": 4.7417378425598145,
      "learning_rate": 1.7248953974895398e-05,
      "loss": 0.0204,
      "step": 2630
    },
    {
      "epoch": 2.7615062761506275,
      "grad_norm": 0.5411732792854309,
      "learning_rate": 1.7238493723849375e-05,
      "loss": 0.0127,
      "step": 2640
    },
    {
      "epoch": 2.7719665271966525,
      "grad_norm": 4.437287330627441,
      "learning_rate": 1.722803347280335e-05,
      "loss": 0.0339,
      "step": 2650
    },
    {
      "epoch": 2.782426778242678,
      "grad_norm": 0.056892283260822296,
      "learning_rate": 1.7217573221757324e-05,
      "loss": 0.0097,
      "step": 2660
    },
    {
      "epoch": 2.792887029288703,
      "grad_norm": 4.518437385559082,
      "learning_rate": 1.72071129707113e-05,
      "loss": 0.0604,
      "step": 2670
    },
    {
      "epoch": 2.803347280334728,
      "grad_norm": 0.9171425104141235,
      "learning_rate": 1.7196652719665273e-05,
      "loss": 0.0138,
      "step": 2680
    },
    {
      "epoch": 2.8138075313807533,
      "grad_norm": 1.4286179542541504,
      "learning_rate": 1.7186192468619246e-05,
      "loss": 0.0174,
      "step": 2690
    },
    {
      "epoch": 2.8242677824267783,
      "grad_norm": 0.8179612755775452,
      "learning_rate": 1.7175732217573223e-05,
      "loss": 0.0291,
      "step": 2700
    },
    {
      "epoch": 2.8347280334728033,
      "grad_norm": 0.1060134693980217,
      "learning_rate": 1.7165271966527196e-05,
      "loss": 0.0119,
      "step": 2710
    },
    {
      "epoch": 2.8451882845188283,
      "grad_norm": 0.2727988660335541,
      "learning_rate": 1.7154811715481172e-05,
      "loss": 0.0244,
      "step": 2720
    },
    {
      "epoch": 2.8556485355648533,
      "grad_norm": 2.639845609664917,
      "learning_rate": 1.714435146443515e-05,
      "loss": 0.0246,
      "step": 2730
    },
    {
      "epoch": 2.8661087866108788,
      "grad_norm": 0.34243762493133545,
      "learning_rate": 1.713389121338912e-05,
      "loss": 0.0174,
      "step": 2740
    },
    {
      "epoch": 2.8765690376569037,
      "grad_norm": 1.3434642553329468,
      "learning_rate": 1.7123430962343098e-05,
      "loss": 0.0363,
      "step": 2750
    },
    {
      "epoch": 2.8870292887029287,
      "grad_norm": 4.9278154373168945,
      "learning_rate": 1.711297071129707e-05,
      "loss": 0.043,
      "step": 2760
    },
    {
      "epoch": 2.897489539748954,
      "grad_norm": 0.1229037418961525,
      "learning_rate": 1.7102510460251047e-05,
      "loss": 0.0309,
      "step": 2770
    },
    {
      "epoch": 2.907949790794979,
      "grad_norm": 3.486819267272949,
      "learning_rate": 1.7092050209205024e-05,
      "loss": 0.0242,
      "step": 2780
    },
    {
      "epoch": 2.918410041841004,
      "grad_norm": 2.019089937210083,
      "learning_rate": 1.7081589958158997e-05,
      "loss": 0.033,
      "step": 2790
    },
    {
      "epoch": 2.928870292887029,
      "grad_norm": 2.4591267108917236,
      "learning_rate": 1.7071129707112973e-05,
      "loss": 0.01,
      "step": 2800
    },
    {
      "epoch": 2.939330543933054,
      "grad_norm": 1.5398876667022705,
      "learning_rate": 1.7060669456066946e-05,
      "loss": 0.031,
      "step": 2810
    },
    {
      "epoch": 2.9497907949790796,
      "grad_norm": 3.93877911567688,
      "learning_rate": 1.7050209205020923e-05,
      "loss": 0.0262,
      "step": 2820
    },
    {
      "epoch": 2.9602510460251046,
      "grad_norm": 2.3765294551849365,
      "learning_rate": 1.7039748953974896e-05,
      "loss": 0.024,
      "step": 2830
    },
    {
      "epoch": 2.9707112970711296,
      "grad_norm": 0.830758273601532,
      "learning_rate": 1.7029288702928872e-05,
      "loss": 0.0169,
      "step": 2840
    },
    {
      "epoch": 2.981171548117155,
      "grad_norm": 0.3873818516731262,
      "learning_rate": 1.701882845188285e-05,
      "loss": 0.0286,
      "step": 2850
    },
    {
      "epoch": 2.99163179916318,
      "grad_norm": 0.10083334147930145,
      "learning_rate": 1.700836820083682e-05,
      "loss": 0.0181,
      "step": 2860
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.9897357723577236,
      "eval_f1": 0.9557793345008757,
      "eval_loss": 0.03397877886891365,
      "eval_precision": 0.9413540319103062,
      "eval_recall": 0.9706536238328146,
      "eval_runtime": 323.9097,
      "eval_samples_per_second": 60.761,
      "eval_steps_per_second": 2.535,
      "step": 2868
    },
    {
      "epoch": 3.002092050209205,
      "grad_norm": 2.382007598876953,
      "learning_rate": 1.6997907949790798e-05,
      "loss": 0.0336,
      "step": 2870
    },
    {
      "epoch": 3.01255230125523,
      "grad_norm": 0.09704796969890594,
      "learning_rate": 1.698744769874477e-05,
      "loss": 0.0122,
      "step": 2880
    },
    {
      "epoch": 3.0230125523012554,
      "grad_norm": 1.3986403942108154,
      "learning_rate": 1.6976987447698744e-05,
      "loss": 0.0071,
      "step": 2890
    },
    {
      "epoch": 3.0334728033472804,
      "grad_norm": 8.875977516174316,
      "learning_rate": 1.696652719665272e-05,
      "loss": 0.0263,
      "step": 2900
    },
    {
      "epoch": 3.0439330543933054,
      "grad_norm": 2.2698254585266113,
      "learning_rate": 1.6956066945606697e-05,
      "loss": 0.0195,
      "step": 2910
    },
    {
      "epoch": 3.0543933054393304,
      "grad_norm": 0.5613242983818054,
      "learning_rate": 1.694560669456067e-05,
      "loss": 0.0062,
      "step": 2920
    },
    {
      "epoch": 3.064853556485356,
      "grad_norm": 0.7989571690559387,
      "learning_rate": 1.6935146443514646e-05,
      "loss": 0.0205,
      "step": 2930
    },
    {
      "epoch": 3.075313807531381,
      "grad_norm": 1.052080750465393,
      "learning_rate": 1.692468619246862e-05,
      "loss": 0.0242,
      "step": 2940
    },
    {
      "epoch": 3.085774058577406,
      "grad_norm": 0.6526485085487366,
      "learning_rate": 1.6914225941422595e-05,
      "loss": 0.0169,
      "step": 2950
    },
    {
      "epoch": 3.096234309623431,
      "grad_norm": 0.05121845006942749,
      "learning_rate": 1.690376569037657e-05,
      "loss": 0.0064,
      "step": 2960
    },
    {
      "epoch": 3.1066945606694563,
      "grad_norm": 0.008468964137136936,
      "learning_rate": 1.6893305439330545e-05,
      "loss": 0.0034,
      "step": 2970
    },
    {
      "epoch": 3.1171548117154813,
      "grad_norm": 3.0349302291870117,
      "learning_rate": 1.688284518828452e-05,
      "loss": 0.0084,
      "step": 2980
    },
    {
      "epoch": 3.1276150627615062,
      "grad_norm": 2.2412683963775635,
      "learning_rate": 1.6872384937238494e-05,
      "loss": 0.0131,
      "step": 2990
    },
    {
      "epoch": 3.1380753138075312,
      "grad_norm": 1.8573012351989746,
      "learning_rate": 1.686192468619247e-05,
      "loss": 0.0434,
      "step": 3000
    },
    {
      "epoch": 3.1485355648535567,
      "grad_norm": 0.05336947739124298,
      "learning_rate": 1.6851464435146444e-05,
      "loss": 0.0181,
      "step": 3010
    },
    {
      "epoch": 3.1589958158995817,
      "grad_norm": 0.060206878930330276,
      "learning_rate": 1.684100418410042e-05,
      "loss": 0.0205,
      "step": 3020
    },
    {
      "epoch": 3.1694560669456067,
      "grad_norm": 0.1694466918706894,
      "learning_rate": 1.6830543933054397e-05,
      "loss": 0.0123,
      "step": 3030
    },
    {
      "epoch": 3.1799163179916317,
      "grad_norm": 0.09893162548542023,
      "learning_rate": 1.682008368200837e-05,
      "loss": 0.0297,
      "step": 3040
    },
    {
      "epoch": 3.190376569037657,
      "grad_norm": 1.862335443496704,
      "learning_rate": 1.6809623430962346e-05,
      "loss": 0.02,
      "step": 3050
    },
    {
      "epoch": 3.200836820083682,
      "grad_norm": 5.317776679992676,
      "learning_rate": 1.679916317991632e-05,
      "loss": 0.0177,
      "step": 3060
    },
    {
      "epoch": 3.211297071129707,
      "grad_norm": 2.248112201690674,
      "learning_rate": 1.6788702928870295e-05,
      "loss": 0.0201,
      "step": 3070
    },
    {
      "epoch": 3.221757322175732,
      "grad_norm": 1.8350822925567627,
      "learning_rate": 1.6778242677824272e-05,
      "loss": 0.0205,
      "step": 3080
    },
    {
      "epoch": 3.2322175732217575,
      "grad_norm": 1.4186019897460938,
      "learning_rate": 1.676778242677824e-05,
      "loss": 0.0198,
      "step": 3090
    },
    {
      "epoch": 3.2426778242677825,
      "grad_norm": 3.1151814460754395,
      "learning_rate": 1.6757322175732218e-05,
      "loss": 0.0295,
      "step": 3100
    },
    {
      "epoch": 3.2531380753138075,
      "grad_norm": 0.33928945660591125,
      "learning_rate": 1.6746861924686194e-05,
      "loss": 0.0141,
      "step": 3110
    },
    {
      "epoch": 3.2635983263598325,
      "grad_norm": 0.6591081619262695,
      "learning_rate": 1.6736401673640167e-05,
      "loss": 0.0351,
      "step": 3120
    },
    {
      "epoch": 3.274058577405858,
      "grad_norm": 0.02712715044617653,
      "learning_rate": 1.6725941422594144e-05,
      "loss": 0.0204,
      "step": 3130
    },
    {
      "epoch": 3.284518828451883,
      "grad_norm": 0.16313162446022034,
      "learning_rate": 1.6715481171548117e-05,
      "loss": 0.0256,
      "step": 3140
    },
    {
      "epoch": 3.294979079497908,
      "grad_norm": 2.865786075592041,
      "learning_rate": 1.6705020920502093e-05,
      "loss": 0.0083,
      "step": 3150
    },
    {
      "epoch": 3.305439330543933,
      "grad_norm": 2.6397249698638916,
      "learning_rate": 1.669456066945607e-05,
      "loss": 0.0262,
      "step": 3160
    },
    {
      "epoch": 3.3158995815899583,
      "grad_norm": 0.6747831106185913,
      "learning_rate": 1.6684100418410042e-05,
      "loss": 0.0323,
      "step": 3170
    },
    {
      "epoch": 3.3263598326359833,
      "grad_norm": 0.05913003161549568,
      "learning_rate": 1.667364016736402e-05,
      "loss": 0.0365,
      "step": 3180
    },
    {
      "epoch": 3.3368200836820083,
      "grad_norm": 0.04764441400766373,
      "learning_rate": 1.6663179916317992e-05,
      "loss": 0.0206,
      "step": 3190
    },
    {
      "epoch": 3.3472803347280333,
      "grad_norm": 2.282992124557495,
      "learning_rate": 1.6652719665271968e-05,
      "loss": 0.025,
      "step": 3200
    },
    {
      "epoch": 3.3577405857740588,
      "grad_norm": 0.09597359597682953,
      "learning_rate": 1.6642259414225945e-05,
      "loss": 0.0353,
      "step": 3210
    },
    {
      "epoch": 3.3682008368200838,
      "grad_norm": 0.851118803024292,
      "learning_rate": 1.6631799163179918e-05,
      "loss": 0.0168,
      "step": 3220
    },
    {
      "epoch": 3.3786610878661087,
      "grad_norm": 0.04712601378560066,
      "learning_rate": 1.6621338912133894e-05,
      "loss": 0.0192,
      "step": 3230
    },
    {
      "epoch": 3.3891213389121337,
      "grad_norm": 0.5427168011665344,
      "learning_rate": 1.6610878661087867e-05,
      "loss": 0.0153,
      "step": 3240
    },
    {
      "epoch": 3.399581589958159,
      "grad_norm": 1.1601513624191284,
      "learning_rate": 1.6600418410041843e-05,
      "loss": 0.0232,
      "step": 3250
    },
    {
      "epoch": 3.410041841004184,
      "grad_norm": 0.07077427953481674,
      "learning_rate": 1.6589958158995816e-05,
      "loss": 0.0085,
      "step": 3260
    },
    {
      "epoch": 3.420502092050209,
      "grad_norm": 4.244482517242432,
      "learning_rate": 1.6579497907949793e-05,
      "loss": 0.0273,
      "step": 3270
    },
    {
      "epoch": 3.430962343096234,
      "grad_norm": 0.053521499037742615,
      "learning_rate": 1.656903765690377e-05,
      "loss": 0.0174,
      "step": 3280
    },
    {
      "epoch": 3.4414225941422596,
      "grad_norm": 0.23464462161064148,
      "learning_rate": 1.6558577405857742e-05,
      "loss": 0.0123,
      "step": 3290
    },
    {
      "epoch": 3.4518828451882846,
      "grad_norm": 1.8674160242080688,
      "learning_rate": 1.6548117154811715e-05,
      "loss": 0.033,
      "step": 3300
    },
    {
      "epoch": 3.4623430962343096,
      "grad_norm": 0.24051374197006226,
      "learning_rate": 1.653765690376569e-05,
      "loss": 0.0063,
      "step": 3310
    },
    {
      "epoch": 3.4728033472803346,
      "grad_norm": 0.044580910354852676,
      "learning_rate": 1.6527196652719665e-05,
      "loss": 0.0124,
      "step": 3320
    },
    {
      "epoch": 3.48326359832636,
      "grad_norm": 2.587144136428833,
      "learning_rate": 1.651673640167364e-05,
      "loss": 0.0076,
      "step": 3330
    },
    {
      "epoch": 3.493723849372385,
      "grad_norm": 1.4907445907592773,
      "learning_rate": 1.6506276150627617e-05,
      "loss": 0.0043,
      "step": 3340
    },
    {
      "epoch": 3.50418410041841,
      "grad_norm": 0.848504364490509,
      "learning_rate": 1.649581589958159e-05,
      "loss": 0.0238,
      "step": 3350
    },
    {
      "epoch": 3.514644351464435,
      "grad_norm": 0.39592403173446655,
      "learning_rate": 1.6485355648535567e-05,
      "loss": 0.0271,
      "step": 3360
    },
    {
      "epoch": 3.52510460251046,
      "grad_norm": 0.14280788600444794,
      "learning_rate": 1.647489539748954e-05,
      "loss": 0.0357,
      "step": 3370
    },
    {
      "epoch": 3.5355648535564854,
      "grad_norm": 1.3967416286468506,
      "learning_rate": 1.6464435146443516e-05,
      "loss": 0.0204,
      "step": 3380
    },
    {
      "epoch": 3.5460251046025104,
      "grad_norm": 0.15697211027145386,
      "learning_rate": 1.645397489539749e-05,
      "loss": 0.0265,
      "step": 3390
    },
    {
      "epoch": 3.5564853556485354,
      "grad_norm": 3.1517958641052246,
      "learning_rate": 1.6443514644351466e-05,
      "loss": 0.0136,
      "step": 3400
    },
    {
      "epoch": 3.566945606694561,
      "grad_norm": 1.059918761253357,
      "learning_rate": 1.6433054393305442e-05,
      "loss": 0.0134,
      "step": 3410
    },
    {
      "epoch": 3.577405857740586,
      "grad_norm": 2.4683303833007812,
      "learning_rate": 1.6422594142259415e-05,
      "loss": 0.0261,
      "step": 3420
    },
    {
      "epoch": 3.587866108786611,
      "grad_norm": 0.398120641708374,
      "learning_rate": 1.641213389121339e-05,
      "loss": 0.0252,
      "step": 3430
    },
    {
      "epoch": 3.598326359832636,
      "grad_norm": 2.563237190246582,
      "learning_rate": 1.6401673640167365e-05,
      "loss": 0.0156,
      "step": 3440
    },
    {
      "epoch": 3.608786610878661,
      "grad_norm": 2.469341278076172,
      "learning_rate": 1.639121338912134e-05,
      "loss": 0.0339,
      "step": 3450
    },
    {
      "epoch": 3.6192468619246863,
      "grad_norm": 5.83262825012207,
      "learning_rate": 1.6380753138075317e-05,
      "loss": 0.031,
      "step": 3460
    },
    {
      "epoch": 3.6297071129707112,
      "grad_norm": 1.8957594633102417,
      "learning_rate": 1.637029288702929e-05,
      "loss": 0.0251,
      "step": 3470
    },
    {
      "epoch": 3.6401673640167362,
      "grad_norm": 0.4541754126548767,
      "learning_rate": 1.6359832635983267e-05,
      "loss": 0.03,
      "step": 3480
    },
    {
      "epoch": 3.6506276150627617,
      "grad_norm": 1.6006953716278076,
      "learning_rate": 1.634937238493724e-05,
      "loss": 0.0307,
      "step": 3490
    },
    {
      "epoch": 3.6610878661087867,
      "grad_norm": 1.1091992855072021,
      "learning_rate": 1.6338912133891213e-05,
      "loss": 0.0294,
      "step": 3500
    },
    {
      "epoch": 3.6715481171548117,
      "grad_norm": 0.3186965882778168,
      "learning_rate": 1.632845188284519e-05,
      "loss": 0.0078,
      "step": 3510
    },
    {
      "epoch": 3.6820083682008367,
      "grad_norm": 1.3858366012573242,
      "learning_rate": 1.6317991631799162e-05,
      "loss": 0.0229,
      "step": 3520
    },
    {
      "epoch": 3.6924686192468616,
      "grad_norm": 0.16291667520999908,
      "learning_rate": 1.630753138075314e-05,
      "loss": 0.0168,
      "step": 3530
    },
    {
      "epoch": 3.702928870292887,
      "grad_norm": 1.1783419847488403,
      "learning_rate": 1.6297071129707115e-05,
      "loss": 0.0136,
      "step": 3540
    },
    {
      "epoch": 3.713389121338912,
      "grad_norm": 0.9986046552658081,
      "learning_rate": 1.6286610878661088e-05,
      "loss": 0.005,
      "step": 3550
    },
    {
      "epoch": 3.723849372384937,
      "grad_norm": 0.8452383279800415,
      "learning_rate": 1.6276150627615064e-05,
      "loss": 0.0271,
      "step": 3560
    },
    {
      "epoch": 3.7343096234309625,
      "grad_norm": 0.8150323033332825,
      "learning_rate": 1.6265690376569037e-05,
      "loss": 0.0139,
      "step": 3570
    },
    {
      "epoch": 3.7447698744769875,
      "grad_norm": 1.7886786460876465,
      "learning_rate": 1.6255230125523014e-05,
      "loss": 0.0128,
      "step": 3580
    },
    {
      "epoch": 3.7552301255230125,
      "grad_norm": 4.244496822357178,
      "learning_rate": 1.624476987447699e-05,
      "loss": 0.0366,
      "step": 3590
    },
    {
      "epoch": 3.7656903765690375,
      "grad_norm": 0.18293896317481995,
      "learning_rate": 1.6234309623430963e-05,
      "loss": 0.0147,
      "step": 3600
    },
    {
      "epoch": 3.776150627615063,
      "grad_norm": 2.6952779293060303,
      "learning_rate": 1.622384937238494e-05,
      "loss": 0.0142,
      "step": 3610
    },
    {
      "epoch": 3.786610878661088,
      "grad_norm": 1.8689581155776978,
      "learning_rate": 1.6213389121338913e-05,
      "loss": 0.0288,
      "step": 3620
    },
    {
      "epoch": 3.797071129707113,
      "grad_norm": 3.3946094512939453,
      "learning_rate": 1.620292887029289e-05,
      "loss": 0.0106,
      "step": 3630
    },
    {
      "epoch": 3.8075313807531384,
      "grad_norm": 0.7875844240188599,
      "learning_rate": 1.6192468619246865e-05,
      "loss": 0.0128,
      "step": 3640
    },
    {
      "epoch": 3.8179916317991633,
      "grad_norm": 0.3036261796951294,
      "learning_rate": 1.618200836820084e-05,
      "loss": 0.018,
      "step": 3650
    },
    {
      "epoch": 3.8284518828451883,
      "grad_norm": 3.81622314453125,
      "learning_rate": 1.6171548117154815e-05,
      "loss": 0.0181,
      "step": 3660
    },
    {
      "epoch": 3.8389121338912133,
      "grad_norm": 0.018394559621810913,
      "learning_rate": 1.6161087866108788e-05,
      "loss": 0.0232,
      "step": 3670
    },
    {
      "epoch": 3.8493723849372383,
      "grad_norm": 0.021736813709139824,
      "learning_rate": 1.6150627615062764e-05,
      "loss": 0.0102,
      "step": 3680
    },
    {
      "epoch": 3.8598326359832638,
      "grad_norm": 0.1602923423051834,
      "learning_rate": 1.6140167364016737e-05,
      "loss": 0.0222,
      "step": 3690
    },
    {
      "epoch": 3.8702928870292888,
      "grad_norm": 0.08635986596345901,
      "learning_rate": 1.612970711297071e-05,
      "loss": 0.0105,
      "step": 3700
    },
    {
      "epoch": 3.8807531380753137,
      "grad_norm": 2.1118462085723877,
      "learning_rate": 1.6119246861924687e-05,
      "loss": 0.0337,
      "step": 3710
    },
    {
      "epoch": 3.891213389121339,
      "grad_norm": 1.02269446849823,
      "learning_rate": 1.6108786610878663e-05,
      "loss": 0.0129,
      "step": 3720
    },
    {
      "epoch": 3.901673640167364,
      "grad_norm": 2.5329229831695557,
      "learning_rate": 1.6098326359832636e-05,
      "loss": 0.0234,
      "step": 3730
    },
    {
      "epoch": 3.912133891213389,
      "grad_norm": 0.10341464728116989,
      "learning_rate": 1.6087866108786612e-05,
      "loss": 0.0234,
      "step": 3740
    },
    {
      "epoch": 3.922594142259414,
      "grad_norm": 0.6225960850715637,
      "learning_rate": 1.6077405857740585e-05,
      "loss": 0.0216,
      "step": 3750
    },
    {
      "epoch": 3.933054393305439,
      "grad_norm": 2.2380166053771973,
      "learning_rate": 1.6066945606694562e-05,
      "loss": 0.0135,
      "step": 3760
    },
    {
      "epoch": 3.9435146443514646,
      "grad_norm": 1.9931213855743408,
      "learning_rate": 1.6056485355648538e-05,
      "loss": 0.0224,
      "step": 3770
    },
    {
      "epoch": 3.9539748953974896,
      "grad_norm": 0.08828020095825195,
      "learning_rate": 1.604602510460251e-05,
      "loss": 0.0186,
      "step": 3780
    },
    {
      "epoch": 3.9644351464435146,
      "grad_norm": 2.755626916885376,
      "learning_rate": 1.6035564853556488e-05,
      "loss": 0.0279,
      "step": 3790
    },
    {
      "epoch": 3.97489539748954,
      "grad_norm": 0.6418889164924622,
      "learning_rate": 1.602510460251046e-05,
      "loss": 0.0181,
      "step": 3800
    },
    {
      "epoch": 3.985355648535565,
      "grad_norm": 0.5183693766593933,
      "learning_rate": 1.6014644351464437e-05,
      "loss": 0.0122,
      "step": 3810
    },
    {
      "epoch": 3.99581589958159,
      "grad_norm": 2.3480026721954346,
      "learning_rate": 1.600418410041841e-05,
      "loss": 0.0232,
      "step": 3820
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.993140243902439,
      "eval_f1": 0.9700864170175049,
      "eval_loss": 0.02261405996978283,
      "eval_precision": 0.9668727915194346,
      "eval_recall": 0.9733214762116497,
      "eval_runtime": 322.5307,
      "eval_samples_per_second": 61.021,
      "eval_steps_per_second": 2.545,
      "step": 3824
    },
    {
      "epoch": 4.006276150627615,
      "grad_norm": 2.811028480529785,
      "learning_rate": 1.5993723849372386e-05,
      "loss": 0.017,
      "step": 3830
    },
    {
      "epoch": 4.01673640167364,
      "grad_norm": 0.024505967274308205,
      "learning_rate": 1.5983263598326363e-05,
      "loss": 0.0035,
      "step": 3840
    },
    {
      "epoch": 4.027196652719665,
      "grad_norm": 0.04839643836021423,
      "learning_rate": 1.5972803347280336e-05,
      "loss": 0.0124,
      "step": 3850
    },
    {
      "epoch": 4.03765690376569,
      "grad_norm": 0.014212054200470448,
      "learning_rate": 1.5962343096234312e-05,
      "loss": 0.0202,
      "step": 3860
    },
    {
      "epoch": 4.048117154811716,
      "grad_norm": 2.5228400230407715,
      "learning_rate": 1.5951882845188285e-05,
      "loss": 0.0151,
      "step": 3870
    },
    {
      "epoch": 4.058577405857741,
      "grad_norm": 0.746497392654419,
      "learning_rate": 1.594142259414226e-05,
      "loss": 0.0046,
      "step": 3880
    },
    {
      "epoch": 4.069037656903766,
      "grad_norm": 3.1082804203033447,
      "learning_rate": 1.5930962343096235e-05,
      "loss": 0.0138,
      "step": 3890
    },
    {
      "epoch": 4.079497907949791,
      "grad_norm": 0.21815693378448486,
      "learning_rate": 1.592050209205021e-05,
      "loss": 0.0094,
      "step": 3900
    },
    {
      "epoch": 4.089958158995816,
      "grad_norm": 0.645566463470459,
      "learning_rate": 1.5910041841004184e-05,
      "loss": 0.0115,
      "step": 3910
    },
    {
      "epoch": 4.100418410041841,
      "grad_norm": 3.036496877670288,
      "learning_rate": 1.589958158995816e-05,
      "loss": 0.0125,
      "step": 3920
    },
    {
      "epoch": 4.110878661087866,
      "grad_norm": 2.908108711242676,
      "learning_rate": 1.5889121338912134e-05,
      "loss": 0.0056,
      "step": 3930
    },
    {
      "epoch": 4.121338912133891,
      "grad_norm": 2.9931654930114746,
      "learning_rate": 1.587866108786611e-05,
      "loss": 0.0096,
      "step": 3940
    },
    {
      "epoch": 4.131799163179917,
      "grad_norm": 2.6536412239074707,
      "learning_rate": 1.5868200836820083e-05,
      "loss": 0.0174,
      "step": 3950
    },
    {
      "epoch": 4.142259414225942,
      "grad_norm": 0.058400120586156845,
      "learning_rate": 1.585774058577406e-05,
      "loss": 0.0047,
      "step": 3960
    },
    {
      "epoch": 4.152719665271967,
      "grad_norm": 0.09271316975355148,
      "learning_rate": 1.5847280334728036e-05,
      "loss": 0.0104,
      "step": 3970
    },
    {
      "epoch": 4.163179916317992,
      "grad_norm": 2.422036647796631,
      "learning_rate": 1.583682008368201e-05,
      "loss": 0.0227,
      "step": 3980
    },
    {
      "epoch": 4.173640167364017,
      "grad_norm": 0.49667486548423767,
      "learning_rate": 1.5826359832635985e-05,
      "loss": 0.0149,
      "step": 3990
    },
    {
      "epoch": 4.184100418410042,
      "grad_norm": 0.08284073323011398,
      "learning_rate": 1.5815899581589958e-05,
      "loss": 0.0186,
      "step": 4000
    },
    {
      "epoch": 4.194560669456067,
      "grad_norm": 0.07369940727949142,
      "learning_rate": 1.5805439330543935e-05,
      "loss": 0.0041,
      "step": 4010
    },
    {
      "epoch": 4.205020920502092,
      "grad_norm": 5.7692670822143555,
      "learning_rate": 1.579497907949791e-05,
      "loss": 0.0066,
      "step": 4020
    },
    {
      "epoch": 4.2154811715481175,
      "grad_norm": 0.14581872522830963,
      "learning_rate": 1.5784518828451884e-05,
      "loss": 0.0108,
      "step": 4030
    },
    {
      "epoch": 4.2259414225941425,
      "grad_norm": 0.049274835735559464,
      "learning_rate": 1.577405857740586e-05,
      "loss": 0.0048,
      "step": 4040
    },
    {
      "epoch": 4.2364016736401675,
      "grad_norm": 0.2258484810590744,
      "learning_rate": 1.5763598326359833e-05,
      "loss": 0.0012,
      "step": 4050
    },
    {
      "epoch": 4.2468619246861925,
      "grad_norm": 6.778924942016602,
      "learning_rate": 1.575313807531381e-05,
      "loss": 0.0142,
      "step": 4060
    },
    {
      "epoch": 4.2573221757322175,
      "grad_norm": 0.1600816398859024,
      "learning_rate": 1.5742677824267786e-05,
      "loss": 0.008,
      "step": 4070
    },
    {
      "epoch": 4.2677824267782425,
      "grad_norm": 4.657066822052002,
      "learning_rate": 1.573221757322176e-05,
      "loss": 0.0087,
      "step": 4080
    },
    {
      "epoch": 4.2782426778242675,
      "grad_norm": 2.606173276901245,
      "learning_rate": 1.5721757322175732e-05,
      "loss": 0.0139,
      "step": 4090
    },
    {
      "epoch": 4.2887029288702925,
      "grad_norm": 0.03535038232803345,
      "learning_rate": 1.571129707112971e-05,
      "loss": 0.0206,
      "step": 4100
    },
    {
      "epoch": 4.299163179916318,
      "grad_norm": 2.0264923572540283,
      "learning_rate": 1.570083682008368e-05,
      "loss": 0.0041,
      "step": 4110
    },
    {
      "epoch": 4.309623430962343,
      "grad_norm": 2.0208427906036377,
      "learning_rate": 1.5690376569037658e-05,
      "loss": 0.0089,
      "step": 4120
    },
    {
      "epoch": 4.320083682008368,
      "grad_norm": 0.3967248201370239,
      "learning_rate": 1.567991631799163e-05,
      "loss": 0.013,
      "step": 4130
    },
    {
      "epoch": 4.330543933054393,
      "grad_norm": 0.015406160615384579,
      "learning_rate": 1.5669456066945607e-05,
      "loss": 0.0154,
      "step": 4140
    },
    {
      "epoch": 4.341004184100418,
      "grad_norm": 6.955269813537598,
      "learning_rate": 1.5658995815899584e-05,
      "loss": 0.0286,
      "step": 4150
    },
    {
      "epoch": 4.351464435146443,
      "grad_norm": 0.022911502048373222,
      "learning_rate": 1.5648535564853557e-05,
      "loss": 0.0084,
      "step": 4160
    },
    {
      "epoch": 4.361924686192468,
      "grad_norm": 0.6563306450843811,
      "learning_rate": 1.5638075313807533e-05,
      "loss": 0.0108,
      "step": 4170
    },
    {
      "epoch": 4.372384937238493,
      "grad_norm": 2.0548999309539795,
      "learning_rate": 1.5627615062761506e-05,
      "loss": 0.0376,
      "step": 4180
    },
    {
      "epoch": 4.382845188284519,
      "grad_norm": 0.17054365575313568,
      "learning_rate": 1.5617154811715483e-05,
      "loss": 0.0155,
      "step": 4190
    },
    {
      "epoch": 4.393305439330544,
      "grad_norm": 0.14388145506381989,
      "learning_rate": 1.560669456066946e-05,
      "loss": 0.009,
      "step": 4200
    },
    {
      "epoch": 4.403765690376569,
      "grad_norm": 0.26202160120010376,
      "learning_rate": 1.5596234309623432e-05,
      "loss": 0.0241,
      "step": 4210
    },
    {
      "epoch": 4.414225941422594,
      "grad_norm": 2.3774666786193848,
      "learning_rate": 1.558577405857741e-05,
      "loss": 0.0208,
      "step": 4220
    },
    {
      "epoch": 4.424686192468619,
      "grad_norm": 0.5131089687347412,
      "learning_rate": 1.557531380753138e-05,
      "loss": 0.0067,
      "step": 4230
    },
    {
      "epoch": 4.435146443514644,
      "grad_norm": 0.3347182869911194,
      "learning_rate": 1.5564853556485358e-05,
      "loss": 0.017,
      "step": 4240
    },
    {
      "epoch": 4.445606694560669,
      "grad_norm": 0.06409993022680283,
      "learning_rate": 1.5554393305439334e-05,
      "loss": 0.0137,
      "step": 4250
    },
    {
      "epoch": 4.456066945606695,
      "grad_norm": 0.08996511250734329,
      "learning_rate": 1.5543933054393307e-05,
      "loss": 0.0093,
      "step": 4260
    },
    {
      "epoch": 4.46652719665272,
      "grad_norm": 2.0088257789611816,
      "learning_rate": 1.5533472803347284e-05,
      "loss": 0.0106,
      "step": 4270
    },
    {
      "epoch": 4.476987447698745,
      "grad_norm": 2.399167776107788,
      "learning_rate": 1.5523012552301257e-05,
      "loss": 0.0232,
      "step": 4280
    },
    {
      "epoch": 4.48744769874477,
      "grad_norm": 0.16868433356285095,
      "learning_rate": 1.551255230125523e-05,
      "loss": 0.0151,
      "step": 4290
    },
    {
      "epoch": 4.497907949790795,
      "grad_norm": 0.7308289408683777,
      "learning_rate": 1.5502092050209206e-05,
      "loss": 0.0097,
      "step": 4300
    },
    {
      "epoch": 4.50836820083682,
      "grad_norm": 3.7682883739471436,
      "learning_rate": 1.549163179916318e-05,
      "loss": 0.0144,
      "step": 4310
    },
    {
      "epoch": 4.518828451882845,
      "grad_norm": 0.04434977099299431,
      "learning_rate": 1.5481171548117155e-05,
      "loss": 0.0072,
      "step": 4320
    },
    {
      "epoch": 4.52928870292887,
      "grad_norm": 5.865050792694092,
      "learning_rate": 1.5470711297071132e-05,
      "loss": 0.0077,
      "step": 4330
    },
    {
      "epoch": 4.539748953974895,
      "grad_norm": 2.61897349357605,
      "learning_rate": 1.5460251046025105e-05,
      "loss": 0.0281,
      "step": 4340
    },
    {
      "epoch": 4.550209205020921,
      "grad_norm": 5.759036064147949,
      "learning_rate": 1.544979079497908e-05,
      "loss": 0.021,
      "step": 4350
    },
    {
      "epoch": 4.560669456066946,
      "grad_norm": 1.4431594610214233,
      "learning_rate": 1.5439330543933054e-05,
      "loss": 0.0116,
      "step": 4360
    },
    {
      "epoch": 4.571129707112971,
      "grad_norm": 3.109525203704834,
      "learning_rate": 1.542887029288703e-05,
      "loss": 0.0206,
      "step": 4370
    },
    {
      "epoch": 4.581589958158996,
      "grad_norm": 5.215949058532715,
      "learning_rate": 1.5418410041841007e-05,
      "loss": 0.0127,
      "step": 4380
    },
    {
      "epoch": 4.592050209205021,
      "grad_norm": 1.5164999961853027,
      "learning_rate": 1.540794979079498e-05,
      "loss": 0.013,
      "step": 4390
    },
    {
      "epoch": 4.602510460251046,
      "grad_norm": 0.34142184257507324,
      "learning_rate": 1.5397489539748957e-05,
      "loss": 0.0108,
      "step": 4400
    },
    {
      "epoch": 4.612970711297071,
      "grad_norm": 0.3108784556388855,
      "learning_rate": 1.538702928870293e-05,
      "loss": 0.0064,
      "step": 4410
    },
    {
      "epoch": 4.623430962343097,
      "grad_norm": 0.8973674774169922,
      "learning_rate": 1.5376569037656906e-05,
      "loss": 0.0124,
      "step": 4420
    },
    {
      "epoch": 4.633891213389122,
      "grad_norm": 0.3118196427822113,
      "learning_rate": 1.536610878661088e-05,
      "loss": 0.0165,
      "step": 4430
    },
    {
      "epoch": 4.644351464435147,
      "grad_norm": 1.8810967206954956,
      "learning_rate": 1.5355648535564855e-05,
      "loss": 0.0109,
      "step": 4440
    },
    {
      "epoch": 4.654811715481172,
      "grad_norm": 0.5860673785209656,
      "learning_rate": 1.5345188284518832e-05,
      "loss": 0.0235,
      "step": 4450
    },
    {
      "epoch": 4.665271966527197,
      "grad_norm": 2.880113363265991,
      "learning_rate": 1.5334728033472805e-05,
      "loss": 0.021,
      "step": 4460
    },
    {
      "epoch": 4.675732217573222,
      "grad_norm": 0.09019128233194351,
      "learning_rate": 1.532426778242678e-05,
      "loss": 0.0231,
      "step": 4470
    },
    {
      "epoch": 4.686192468619247,
      "grad_norm": 10.092875480651855,
      "learning_rate": 1.5313807531380754e-05,
      "loss": 0.0352,
      "step": 4480
    },
    {
      "epoch": 4.696652719665272,
      "grad_norm": 0.04693062603473663,
      "learning_rate": 1.5303347280334727e-05,
      "loss": 0.0032,
      "step": 4490
    },
    {
      "epoch": 4.707112970711297,
      "grad_norm": 3.6088201999664307,
      "learning_rate": 1.5292887029288704e-05,
      "loss": 0.0227,
      "step": 4500
    },
    {
      "epoch": 4.7175732217573225,
      "grad_norm": 0.022671079263091087,
      "learning_rate": 1.528242677824268e-05,
      "loss": 0.0142,
      "step": 4510
    },
    {
      "epoch": 4.7280334728033475,
      "grad_norm": 0.9544209241867065,
      "learning_rate": 1.5271966527196653e-05,
      "loss": 0.0059,
      "step": 4520
    },
    {
      "epoch": 4.7384937238493725,
      "grad_norm": 0.07538926601409912,
      "learning_rate": 1.526150627615063e-05,
      "loss": 0.0107,
      "step": 4530
    },
    {
      "epoch": 4.7489539748953975,
      "grad_norm": 0.6608030796051025,
      "learning_rate": 1.5251046025104604e-05,
      "loss": 0.0053,
      "step": 4540
    },
    {
      "epoch": 4.7594142259414225,
      "grad_norm": 0.03068314678966999,
      "learning_rate": 1.5240585774058579e-05,
      "loss": 0.008,
      "step": 4550
    },
    {
      "epoch": 4.7698744769874475,
      "grad_norm": 0.7068446278572083,
      "learning_rate": 1.5230125523012553e-05,
      "loss": 0.0158,
      "step": 4560
    },
    {
      "epoch": 4.7803347280334725,
      "grad_norm": 0.24813616275787354,
      "learning_rate": 1.5219665271966528e-05,
      "loss": 0.0133,
      "step": 4570
    },
    {
      "epoch": 4.790794979079498,
      "grad_norm": 0.5213486552238464,
      "learning_rate": 1.5209205020920503e-05,
      "loss": 0.0072,
      "step": 4580
    },
    {
      "epoch": 4.801255230125523,
      "grad_norm": 4.149127960205078,
      "learning_rate": 1.5198744769874478e-05,
      "loss": 0.0264,
      "step": 4590
    },
    {
      "epoch": 4.811715481171548,
      "grad_norm": 6.292382717132568,
      "learning_rate": 1.5188284518828454e-05,
      "loss": 0.0329,
      "step": 4600
    },
    {
      "epoch": 4.822175732217573,
      "grad_norm": 1.9554743766784668,
      "learning_rate": 1.5177824267782429e-05,
      "loss": 0.0129,
      "step": 4610
    },
    {
      "epoch": 4.832635983263598,
      "grad_norm": 0.3979036211967468,
      "learning_rate": 1.5167364016736403e-05,
      "loss": 0.016,
      "step": 4620
    },
    {
      "epoch": 4.843096234309623,
      "grad_norm": 2.7752413749694824,
      "learning_rate": 1.5156903765690378e-05,
      "loss": 0.018,
      "step": 4630
    },
    {
      "epoch": 4.853556485355648,
      "grad_norm": 0.08856832981109619,
      "learning_rate": 1.5146443514644353e-05,
      "loss": 0.0078,
      "step": 4640
    },
    {
      "epoch": 4.864016736401673,
      "grad_norm": 0.290585994720459,
      "learning_rate": 1.513598326359833e-05,
      "loss": 0.0187,
      "step": 4650
    },
    {
      "epoch": 4.874476987447698,
      "grad_norm": 1.1490538120269775,
      "learning_rate": 1.5125523012552304e-05,
      "loss": 0.0104,
      "step": 4660
    },
    {
      "epoch": 4.884937238493724,
      "grad_norm": 0.06920792907476425,
      "learning_rate": 1.5115062761506279e-05,
      "loss": 0.0123,
      "step": 4670
    },
    {
      "epoch": 4.895397489539749,
      "grad_norm": 2.2847869396209717,
      "learning_rate": 1.5104602510460253e-05,
      "loss": 0.004,
      "step": 4680
    },
    {
      "epoch": 4.905857740585774,
      "grad_norm": 0.15053580701351166,
      "learning_rate": 1.5094142259414226e-05,
      "loss": 0.0084,
      "step": 4690
    },
    {
      "epoch": 4.916317991631799,
      "grad_norm": 0.03937577083706856,
      "learning_rate": 1.5083682008368201e-05,
      "loss": 0.0129,
      "step": 4700
    },
    {
      "epoch": 4.926778242677824,
      "grad_norm": 1.1101092100143433,
      "learning_rate": 1.5073221757322176e-05,
      "loss": 0.0205,
      "step": 4710
    },
    {
      "epoch": 4.937238493723849,
      "grad_norm": 2.937619686126709,
      "learning_rate": 1.506276150627615e-05,
      "loss": 0.0105,
      "step": 4720
    },
    {
      "epoch": 4.947698744769874,
      "grad_norm": 0.664857029914856,
      "learning_rate": 1.5052301255230127e-05,
      "loss": 0.0145,
      "step": 4730
    },
    {
      "epoch": 4.9581589958159,
      "grad_norm": 0.0693618431687355,
      "learning_rate": 1.5041841004184102e-05,
      "loss": 0.0175,
      "step": 4740
    },
    {
      "epoch": 4.968619246861925,
      "grad_norm": 2.5605721473693848,
      "learning_rate": 1.5031380753138076e-05,
      "loss": 0.0185,
      "step": 4750
    },
    {
      "epoch": 4.97907949790795,
      "grad_norm": 0.08032147586345673,
      "learning_rate": 1.5020920502092051e-05,
      "loss": 0.012,
      "step": 4760
    },
    {
      "epoch": 4.989539748953975,
      "grad_norm": 0.986384391784668,
      "learning_rate": 1.5010460251046026e-05,
      "loss": 0.0104,
      "step": 4770
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.15083441138267517,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 0.0072,
      "step": 4780
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.994359756097561,
      "eval_f1": 0.9752949031827287,
      "eval_loss": 0.019649509340524673,
      "eval_precision": 0.9763814616755794,
      "eval_recall": 0.974210760337928,
      "eval_runtime": 322.4363,
      "eval_samples_per_second": 61.038,
      "eval_steps_per_second": 2.546,
      "step": 4780
    },
    {
      "epoch": 5.010460251046025,
      "grad_norm": 0.10785748809576035,
      "learning_rate": 1.4989539748953977e-05,
      "loss": 0.0078,
      "step": 4790
    },
    {
      "epoch": 5.02092050209205,
      "grad_norm": 2.0455658435821533,
      "learning_rate": 1.4979079497907951e-05,
      "loss": 0.0088,
      "step": 4800
    },
    {
      "epoch": 5.031380753138075,
      "grad_norm": 3.722834587097168,
      "learning_rate": 1.4968619246861926e-05,
      "loss": 0.0131,
      "step": 4810
    },
    {
      "epoch": 5.0418410041841,
      "grad_norm": 0.730668306350708,
      "learning_rate": 1.4958158995815901e-05,
      "loss": 0.0122,
      "step": 4820
    },
    {
      "epoch": 5.052301255230126,
      "grad_norm": 0.7439424991607666,
      "learning_rate": 1.4947698744769876e-05,
      "loss": 0.0063,
      "step": 4830
    },
    {
      "epoch": 5.062761506276151,
      "grad_norm": 0.025493862107396126,
      "learning_rate": 1.4937238493723852e-05,
      "loss": 0.0079,
      "step": 4840
    },
    {
      "epoch": 5.073221757322176,
      "grad_norm": 12.201004028320312,
      "learning_rate": 1.4926778242677827e-05,
      "loss": 0.0097,
      "step": 4850
    },
    {
      "epoch": 5.083682008368201,
      "grad_norm": 0.17358119785785675,
      "learning_rate": 1.4916317991631801e-05,
      "loss": 0.0061,
      "step": 4860
    },
    {
      "epoch": 5.094142259414226,
      "grad_norm": 1.461607813835144,
      "learning_rate": 1.4905857740585776e-05,
      "loss": 0.0099,
      "step": 4870
    },
    {
      "epoch": 5.104602510460251,
      "grad_norm": 3.349497079849243,
      "learning_rate": 1.4895397489539749e-05,
      "loss": 0.0146,
      "step": 4880
    },
    {
      "epoch": 5.115062761506276,
      "grad_norm": 0.6681133508682251,
      "learning_rate": 1.4884937238493724e-05,
      "loss": 0.01,
      "step": 4890
    },
    {
      "epoch": 5.125523012552302,
      "grad_norm": 0.6759454011917114,
      "learning_rate": 1.4874476987447699e-05,
      "loss": 0.0038,
      "step": 4900
    },
    {
      "epoch": 5.135983263598327,
      "grad_norm": 1.6980043649673462,
      "learning_rate": 1.4864016736401673e-05,
      "loss": 0.0074,
      "step": 4910
    },
    {
      "epoch": 5.146443514644352,
      "grad_norm": 1.2245373725891113,
      "learning_rate": 1.485355648535565e-05,
      "loss": 0.0094,
      "step": 4920
    },
    {
      "epoch": 5.156903765690377,
      "grad_norm": 0.005128068383783102,
      "learning_rate": 1.4843096234309624e-05,
      "loss": 0.026,
      "step": 4930
    },
    {
      "epoch": 5.167364016736402,
      "grad_norm": 0.0055213975720107555,
      "learning_rate": 1.4832635983263599e-05,
      "loss": 0.0039,
      "step": 4940
    },
    {
      "epoch": 5.177824267782427,
      "grad_norm": 0.3392409384250641,
      "learning_rate": 1.4822175732217574e-05,
      "loss": 0.0103,
      "step": 4950
    },
    {
      "epoch": 5.188284518828452,
      "grad_norm": 0.38511258363723755,
      "learning_rate": 1.4811715481171548e-05,
      "loss": 0.009,
      "step": 4960
    },
    {
      "epoch": 5.198744769874477,
      "grad_norm": 0.015897255390882492,
      "learning_rate": 1.4801255230125525e-05,
      "loss": 0.0009,
      "step": 4970
    },
    {
      "epoch": 5.209205020920502,
      "grad_norm": 0.7773270606994629,
      "learning_rate": 1.47907949790795e-05,
      "loss": 0.0178,
      "step": 4980
    },
    {
      "epoch": 5.2196652719665275,
      "grad_norm": 0.04815239459276199,
      "learning_rate": 1.4780334728033474e-05,
      "loss": 0.0084,
      "step": 4990
    },
    {
      "epoch": 5.2301255230125525,
      "grad_norm": 0.02402057871222496,
      "learning_rate": 1.4769874476987449e-05,
      "loss": 0.019,
      "step": 5000
    },
    {
      "epoch": 5.2405857740585775,
      "grad_norm": 2.1314051151275635,
      "learning_rate": 1.4759414225941424e-05,
      "loss": 0.0086,
      "step": 5010
    },
    {
      "epoch": 5.2510460251046025,
      "grad_norm": 1.7371563911437988,
      "learning_rate": 1.4748953974895398e-05,
      "loss": 0.0085,
      "step": 5020
    },
    {
      "epoch": 5.2615062761506275,
      "grad_norm": 0.7622381448745728,
      "learning_rate": 1.4738493723849375e-05,
      "loss": 0.0045,
      "step": 5030
    },
    {
      "epoch": 5.2719665271966525,
      "grad_norm": 0.015897799283266068,
      "learning_rate": 1.472803347280335e-05,
      "loss": 0.0073,
      "step": 5040
    },
    {
      "epoch": 5.2824267782426775,
      "grad_norm": 1.1606626510620117,
      "learning_rate": 1.4717573221757324e-05,
      "loss": 0.0029,
      "step": 5050
    },
    {
      "epoch": 5.292887029288703,
      "grad_norm": 0.025160454213619232,
      "learning_rate": 1.4707112970711299e-05,
      "loss": 0.0187,
      "step": 5060
    },
    {
      "epoch": 5.303347280334728,
      "grad_norm": 0.11695663630962372,
      "learning_rate": 1.4696652719665274e-05,
      "loss": 0.0013,
      "step": 5070
    },
    {
      "epoch": 5.313807531380753,
      "grad_norm": 0.06379745155572891,
      "learning_rate": 1.4686192468619247e-05,
      "loss": 0.0012,
      "step": 5080
    },
    {
      "epoch": 5.324267782426778,
      "grad_norm": 0.45383912324905396,
      "learning_rate": 1.4675732217573221e-05,
      "loss": 0.0197,
      "step": 5090
    },
    {
      "epoch": 5.334728033472803,
      "grad_norm": 0.015091732144355774,
      "learning_rate": 1.4665271966527198e-05,
      "loss": 0.0062,
      "step": 5100
    },
    {
      "epoch": 5.345188284518828,
      "grad_norm": 0.04201649874448776,
      "learning_rate": 1.4654811715481172e-05,
      "loss": 0.0047,
      "step": 5110
    },
    {
      "epoch": 5.355648535564853,
      "grad_norm": 0.0840614065527916,
      "learning_rate": 1.4644351464435147e-05,
      "loss": 0.0025,
      "step": 5120
    },
    {
      "epoch": 5.366108786610878,
      "grad_norm": 0.6619724631309509,
      "learning_rate": 1.4633891213389122e-05,
      "loss": 0.0222,
      "step": 5130
    },
    {
      "epoch": 5.376569037656903,
      "grad_norm": 3.1376471519470215,
      "learning_rate": 1.4623430962343097e-05,
      "loss": 0.0151,
      "step": 5140
    },
    {
      "epoch": 5.387029288702929,
      "grad_norm": 4.329312801361084,
      "learning_rate": 1.4612970711297071e-05,
      "loss": 0.006,
      "step": 5150
    },
    {
      "epoch": 5.397489539748954,
      "grad_norm": 3.9745988845825195,
      "learning_rate": 1.4602510460251048e-05,
      "loss": 0.0209,
      "step": 5160
    },
    {
      "epoch": 5.407949790794979,
      "grad_norm": 0.47773972153663635,
      "learning_rate": 1.4592050209205022e-05,
      "loss": 0.0097,
      "step": 5170
    },
    {
      "epoch": 5.418410041841004,
      "grad_norm": 0.6825791001319885,
      "learning_rate": 1.4581589958158997e-05,
      "loss": 0.0056,
      "step": 5180
    },
    {
      "epoch": 5.428870292887029,
      "grad_norm": 1.812488079071045,
      "learning_rate": 1.4571129707112972e-05,
      "loss": 0.0093,
      "step": 5190
    },
    {
      "epoch": 5.439330543933054,
      "grad_norm": 0.045173514634370804,
      "learning_rate": 1.4560669456066946e-05,
      "loss": 0.0142,
      "step": 5200
    },
    {
      "epoch": 5.449790794979079,
      "grad_norm": 0.7888805270195007,
      "learning_rate": 1.4550209205020923e-05,
      "loss": 0.0097,
      "step": 5210
    },
    {
      "epoch": 5.460251046025105,
      "grad_norm": 0.018911952152848244,
      "learning_rate": 1.4539748953974898e-05,
      "loss": 0.0177,
      "step": 5220
    },
    {
      "epoch": 5.47071129707113,
      "grad_norm": 0.3450901210308075,
      "learning_rate": 1.4529288702928872e-05,
      "loss": 0.0049,
      "step": 5230
    },
    {
      "epoch": 5.481171548117155,
      "grad_norm": 0.03241714835166931,
      "learning_rate": 1.4518828451882847e-05,
      "loss": 0.012,
      "step": 5240
    },
    {
      "epoch": 5.49163179916318,
      "grad_norm": 0.23361961543560028,
      "learning_rate": 1.4508368200836822e-05,
      "loss": 0.0105,
      "step": 5250
    },
    {
      "epoch": 5.502092050209205,
      "grad_norm": 0.6587584018707275,
      "learning_rate": 1.4497907949790796e-05,
      "loss": 0.0022,
      "step": 5260
    },
    {
      "epoch": 5.51255230125523,
      "grad_norm": 0.26080837845802307,
      "learning_rate": 1.4487447698744773e-05,
      "loss": 0.0007,
      "step": 5270
    },
    {
      "epoch": 5.523012552301255,
      "grad_norm": 0.08476976305246353,
      "learning_rate": 1.4476987447698744e-05,
      "loss": 0.011,
      "step": 5280
    },
    {
      "epoch": 5.53347280334728,
      "grad_norm": 1.7106753587722778,
      "learning_rate": 1.446652719665272e-05,
      "loss": 0.0095,
      "step": 5290
    },
    {
      "epoch": 5.543933054393305,
      "grad_norm": 0.024897227063775063,
      "learning_rate": 1.4456066945606695e-05,
      "loss": 0.0098,
      "step": 5300
    },
    {
      "epoch": 5.554393305439331,
      "grad_norm": 0.2457994520664215,
      "learning_rate": 1.444560669456067e-05,
      "loss": 0.0167,
      "step": 5310
    },
    {
      "epoch": 5.564853556485356,
      "grad_norm": 8.863692283630371,
      "learning_rate": 1.4435146443514645e-05,
      "loss": 0.0279,
      "step": 5320
    },
    {
      "epoch": 5.575313807531381,
      "grad_norm": 0.01450564619153738,
      "learning_rate": 1.442468619246862e-05,
      "loss": 0.0154,
      "step": 5330
    },
    {
      "epoch": 5.585774058577406,
      "grad_norm": 0.2363092601299286,
      "learning_rate": 1.4414225941422596e-05,
      "loss": 0.0119,
      "step": 5340
    },
    {
      "epoch": 5.596234309623431,
      "grad_norm": 0.7582390904426575,
      "learning_rate": 1.440376569037657e-05,
      "loss": 0.0051,
      "step": 5350
    },
    {
      "epoch": 5.606694560669456,
      "grad_norm": 0.07818667590618134,
      "learning_rate": 1.4393305439330545e-05,
      "loss": 0.0023,
      "step": 5360
    },
    {
      "epoch": 5.617154811715481,
      "grad_norm": 0.012373858131468296,
      "learning_rate": 1.438284518828452e-05,
      "loss": 0.0052,
      "step": 5370
    },
    {
      "epoch": 5.627615062761507,
      "grad_norm": 2.1398653984069824,
      "learning_rate": 1.4372384937238495e-05,
      "loss": 0.0186,
      "step": 5380
    },
    {
      "epoch": 5.638075313807532,
      "grad_norm": 0.020302463322877884,
      "learning_rate": 1.436192468619247e-05,
      "loss": 0.0023,
      "step": 5390
    },
    {
      "epoch": 5.648535564853557,
      "grad_norm": 0.027286982163786888,
      "learning_rate": 1.4351464435146446e-05,
      "loss": 0.0085,
      "step": 5400
    },
    {
      "epoch": 5.658995815899582,
      "grad_norm": 2.5557737350463867,
      "learning_rate": 1.434100418410042e-05,
      "loss": 0.0231,
      "step": 5410
    },
    {
      "epoch": 5.669456066945607,
      "grad_norm": 0.0034622165840119123,
      "learning_rate": 1.4330543933054395e-05,
      "loss": 0.011,
      "step": 5420
    },
    {
      "epoch": 5.679916317991632,
      "grad_norm": 2.66437029838562,
      "learning_rate": 1.432008368200837e-05,
      "loss": 0.0161,
      "step": 5430
    },
    {
      "epoch": 5.690376569037657,
      "grad_norm": 0.5504191517829895,
      "learning_rate": 1.4309623430962344e-05,
      "loss": 0.0108,
      "step": 5440
    },
    {
      "epoch": 5.700836820083682,
      "grad_norm": 1.2801567316055298,
      "learning_rate": 1.429916317991632e-05,
      "loss": 0.0053,
      "step": 5450
    },
    {
      "epoch": 5.711297071129707,
      "grad_norm": 0.38102608919143677,
      "learning_rate": 1.4288702928870296e-05,
      "loss": 0.0036,
      "step": 5460
    },
    {
      "epoch": 5.7217573221757325,
      "grad_norm": 0.03501882031559944,
      "learning_rate": 1.427824267782427e-05,
      "loss": 0.0297,
      "step": 5470
    },
    {
      "epoch": 5.7322175732217575,
      "grad_norm": 3.653024911880493,
      "learning_rate": 1.4267782426778243e-05,
      "loss": 0.0046,
      "step": 5480
    },
    {
      "epoch": 5.7426778242677825,
      "grad_norm": 2.567321300506592,
      "learning_rate": 1.4257322175732218e-05,
      "loss": 0.0129,
      "step": 5490
    },
    {
      "epoch": 5.7531380753138075,
      "grad_norm": 2.4844493865966797,
      "learning_rate": 1.4246861924686193e-05,
      "loss": 0.0186,
      "step": 5500
    },
    {
      "epoch": 5.7635983263598325,
      "grad_norm": 0.010114344768226147,
      "learning_rate": 1.4236401673640167e-05,
      "loss": 0.0051,
      "step": 5510
    },
    {
      "epoch": 5.7740585774058575,
      "grad_norm": 0.07947172969579697,
      "learning_rate": 1.4225941422594142e-05,
      "loss": 0.0064,
      "step": 5520
    },
    {
      "epoch": 5.7845188284518825,
      "grad_norm": 0.16315984725952148,
      "learning_rate": 1.4215481171548118e-05,
      "loss": 0.0136,
      "step": 5530
    },
    {
      "epoch": 5.794979079497908,
      "grad_norm": 2.5512514114379883,
      "learning_rate": 1.4205020920502093e-05,
      "loss": 0.014,
      "step": 5540
    },
    {
      "epoch": 5.805439330543933,
      "grad_norm": 2.5845751762390137,
      "learning_rate": 1.4194560669456068e-05,
      "loss": 0.0144,
      "step": 5550
    },
    {
      "epoch": 5.815899581589958,
      "grad_norm": 1.241189956665039,
      "learning_rate": 1.4184100418410043e-05,
      "loss": 0.0153,
      "step": 5560
    },
    {
      "epoch": 5.826359832635983,
      "grad_norm": 0.06316805630922318,
      "learning_rate": 1.4173640167364017e-05,
      "loss": 0.0137,
      "step": 5570
    },
    {
      "epoch": 5.836820083682008,
      "grad_norm": 0.15750086307525635,
      "learning_rate": 1.4163179916317992e-05,
      "loss": 0.0105,
      "step": 5580
    },
    {
      "epoch": 5.847280334728033,
      "grad_norm": 0.03962251543998718,
      "learning_rate": 1.4152719665271968e-05,
      "loss": 0.0118,
      "step": 5590
    },
    {
      "epoch": 5.857740585774058,
      "grad_norm": 1.08798348903656,
      "learning_rate": 1.4142259414225943e-05,
      "loss": 0.0072,
      "step": 5600
    },
    {
      "epoch": 5.868200836820083,
      "grad_norm": 0.10867585241794586,
      "learning_rate": 1.4131799163179918e-05,
      "loss": 0.0055,
      "step": 5610
    },
    {
      "epoch": 5.878661087866108,
      "grad_norm": 0.018097124993801117,
      "learning_rate": 1.4121338912133893e-05,
      "loss": 0.0096,
      "step": 5620
    },
    {
      "epoch": 5.889121338912134,
      "grad_norm": 2.5714354515075684,
      "learning_rate": 1.4110878661087867e-05,
      "loss": 0.0129,
      "step": 5630
    },
    {
      "epoch": 5.899581589958159,
      "grad_norm": 1.3128244876861572,
      "learning_rate": 1.4100418410041844e-05,
      "loss": 0.0062,
      "step": 5640
    },
    {
      "epoch": 5.910041841004184,
      "grad_norm": 2.804964542388916,
      "learning_rate": 1.4089958158995818e-05,
      "loss": 0.0256,
      "step": 5650
    },
    {
      "epoch": 5.920502092050209,
      "grad_norm": 5.116806507110596,
      "learning_rate": 1.4079497907949793e-05,
      "loss": 0.0087,
      "step": 5660
    },
    {
      "epoch": 5.930962343096234,
      "grad_norm": 0.06711515784263611,
      "learning_rate": 1.4069037656903768e-05,
      "loss": 0.0179,
      "step": 5670
    },
    {
      "epoch": 5.941422594142259,
      "grad_norm": 0.753262996673584,
      "learning_rate": 1.405857740585774e-05,
      "loss": 0.0111,
      "step": 5680
    },
    {
      "epoch": 5.951882845188284,
      "grad_norm": 9.629838943481445,
      "learning_rate": 1.4048117154811715e-05,
      "loss": 0.0088,
      "step": 5690
    },
    {
      "epoch": 5.96234309623431,
      "grad_norm": 0.023874957114458084,
      "learning_rate": 1.403765690376569e-05,
      "loss": 0.0047,
      "step": 5700
    },
    {
      "epoch": 5.972803347280335,
      "grad_norm": 0.2148483395576477,
      "learning_rate": 1.4027196652719665e-05,
      "loss": 0.0318,
      "step": 5710
    },
    {
      "epoch": 5.98326359832636,
      "grad_norm": 8.775250434875488,
      "learning_rate": 1.4016736401673641e-05,
      "loss": 0.0226,
      "step": 5720
    },
    {
      "epoch": 5.993723849372385,
      "grad_norm": 0.006346354726701975,
      "learning_rate": 1.4006276150627616e-05,
      "loss": 0.0079,
      "step": 5730
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.9940040650406504,
      "eval_f1": 0.9738938053097346,
      "eval_loss": 0.020281700417399406,
      "eval_precision": 0.9691765741963893,
      "eval_recall": 0.9786571809693198,
      "eval_runtime": 322.5164,
      "eval_samples_per_second": 61.023,
      "eval_steps_per_second": 2.546,
      "step": 5736
    },
    {
      "epoch": 6.00418410041841,
      "grad_norm": 0.03470314294099808,
      "learning_rate": 1.399581589958159e-05,
      "loss": 0.0218,
      "step": 5740
    },
    {
      "epoch": 6.014644351464435,
      "grad_norm": 1.9596412181854248,
      "learning_rate": 1.3985355648535565e-05,
      "loss": 0.0107,
      "step": 5750
    },
    {
      "epoch": 6.02510460251046,
      "grad_norm": 0.4574691355228424,
      "learning_rate": 1.397489539748954e-05,
      "loss": 0.0137,
      "step": 5760
    },
    {
      "epoch": 6.035564853556485,
      "grad_norm": 0.05069959908723831,
      "learning_rate": 1.3964435146443516e-05,
      "loss": 0.0144,
      "step": 5770
    },
    {
      "epoch": 6.046025104602511,
      "grad_norm": 0.053039778023958206,
      "learning_rate": 1.3953974895397491e-05,
      "loss": 0.0078,
      "step": 5780
    },
    {
      "epoch": 6.056485355648536,
      "grad_norm": 0.6965444087982178,
      "learning_rate": 1.3943514644351466e-05,
      "loss": 0.0118,
      "step": 5790
    },
    {
      "epoch": 6.066945606694561,
      "grad_norm": 1.951744794845581,
      "learning_rate": 1.393305439330544e-05,
      "loss": 0.0136,
      "step": 5800
    },
    {
      "epoch": 6.077405857740586,
      "grad_norm": 0.04447558522224426,
      "learning_rate": 1.3922594142259415e-05,
      "loss": 0.0031,
      "step": 5810
    },
    {
      "epoch": 6.087866108786611,
      "grad_norm": 0.8067129850387573,
      "learning_rate": 1.391213389121339e-05,
      "loss": 0.0075,
      "step": 5820
    },
    {
      "epoch": 6.098326359832636,
      "grad_norm": 1.30755615234375,
      "learning_rate": 1.3901673640167366e-05,
      "loss": 0.0075,
      "step": 5830
    },
    {
      "epoch": 6.108786610878661,
      "grad_norm": 0.03470581769943237,
      "learning_rate": 1.3891213389121341e-05,
      "loss": 0.0079,
      "step": 5840
    },
    {
      "epoch": 6.119246861924686,
      "grad_norm": 0.0387805812060833,
      "learning_rate": 1.3880753138075316e-05,
      "loss": 0.0049,
      "step": 5850
    },
    {
      "epoch": 6.129707112970712,
      "grad_norm": 0.032151881605386734,
      "learning_rate": 1.387029288702929e-05,
      "loss": 0.0009,
      "step": 5860
    },
    {
      "epoch": 6.140167364016737,
      "grad_norm": 4.140054702758789,
      "learning_rate": 1.3859832635983265e-05,
      "loss": 0.0056,
      "step": 5870
    },
    {
      "epoch": 6.150627615062762,
      "grad_norm": 0.03341955691576004,
      "learning_rate": 1.3849372384937238e-05,
      "loss": 0.0059,
      "step": 5880
    },
    {
      "epoch": 6.161087866108787,
      "grad_norm": 0.11763967573642731,
      "learning_rate": 1.3838912133891213e-05,
      "loss": 0.0084,
      "step": 5890
    },
    {
      "epoch": 6.171548117154812,
      "grad_norm": 2.8985679149627686,
      "learning_rate": 1.382845188284519e-05,
      "loss": 0.0067,
      "step": 5900
    },
    {
      "epoch": 6.182008368200837,
      "grad_norm": 0.10073977708816528,
      "learning_rate": 1.3817991631799164e-05,
      "loss": 0.0096,
      "step": 5910
    },
    {
      "epoch": 6.192468619246862,
      "grad_norm": 0.4701564610004425,
      "learning_rate": 1.3807531380753139e-05,
      "loss": 0.0215,
      "step": 5920
    },
    {
      "epoch": 6.202928870292887,
      "grad_norm": 1.732441782951355,
      "learning_rate": 1.3797071129707113e-05,
      "loss": 0.0093,
      "step": 5930
    },
    {
      "epoch": 6.2133891213389125,
      "grad_norm": 0.041051797568798065,
      "learning_rate": 1.3786610878661088e-05,
      "loss": 0.0026,
      "step": 5940
    },
    {
      "epoch": 6.2238493723849375,
      "grad_norm": 3.2860145568847656,
      "learning_rate": 1.3776150627615063e-05,
      "loss": 0.0111,
      "step": 5950
    },
    {
      "epoch": 6.2343096234309625,
      "grad_norm": 0.08106252551078796,
      "learning_rate": 1.376569037656904e-05,
      "loss": 0.0057,
      "step": 5960
    },
    {
      "epoch": 6.2447698744769875,
      "grad_norm": 0.5792752504348755,
      "learning_rate": 1.3755230125523014e-05,
      "loss": 0.0104,
      "step": 5970
    },
    {
      "epoch": 6.2552301255230125,
      "grad_norm": 0.02972513437271118,
      "learning_rate": 1.3744769874476989e-05,
      "loss": 0.0045,
      "step": 5980
    },
    {
      "epoch": 6.2656903765690375,
      "grad_norm": 0.6994945406913757,
      "learning_rate": 1.3734309623430963e-05,
      "loss": 0.0044,
      "step": 5990
    },
    {
      "epoch": 6.2761506276150625,
      "grad_norm": 0.6154918074607849,
      "learning_rate": 1.3723849372384938e-05,
      "loss": 0.0037,
      "step": 6000
    },
    {
      "epoch": 6.2866108786610875,
      "grad_norm": 0.1747797429561615,
      "learning_rate": 1.3713389121338913e-05,
      "loss": 0.0107,
      "step": 6010
    },
    {
      "epoch": 6.297071129707113,
      "grad_norm": 0.26993000507354736,
      "learning_rate": 1.370292887029289e-05,
      "loss": 0.0069,
      "step": 6020
    },
    {
      "epoch": 6.307531380753138,
      "grad_norm": 2.9327120780944824,
      "learning_rate": 1.3692468619246864e-05,
      "loss": 0.0037,
      "step": 6030
    },
    {
      "epoch": 6.317991631799163,
      "grad_norm": 0.6747775077819824,
      "learning_rate": 1.3682008368200839e-05,
      "loss": 0.0093,
      "step": 6040
    },
    {
      "epoch": 6.328451882845188,
      "grad_norm": 0.30069372057914734,
      "learning_rate": 1.3671548117154813e-05,
      "loss": 0.0018,
      "step": 6050
    },
    {
      "epoch": 6.338912133891213,
      "grad_norm": 0.9264745116233826,
      "learning_rate": 1.3661087866108788e-05,
      "loss": 0.01,
      "step": 6060
    },
    {
      "epoch": 6.349372384937238,
      "grad_norm": 2.318363666534424,
      "learning_rate": 1.3650627615062764e-05,
      "loss": 0.0105,
      "step": 6070
    },
    {
      "epoch": 6.359832635983263,
      "grad_norm": 0.0024205930531024933,
      "learning_rate": 1.3640167364016736e-05,
      "loss": 0.0075,
      "step": 6080
    },
    {
      "epoch": 6.370292887029288,
      "grad_norm": 0.009160670451819897,
      "learning_rate": 1.3629707112970712e-05,
      "loss": 0.0041,
      "step": 6090
    },
    {
      "epoch": 6.380753138075314,
      "grad_norm": 2.343193531036377,
      "learning_rate": 1.3619246861924687e-05,
      "loss": 0.0055,
      "step": 6100
    },
    {
      "epoch": 6.391213389121339,
      "grad_norm": 1.0710532665252686,
      "learning_rate": 1.3608786610878662e-05,
      "loss": 0.0096,
      "step": 6110
    },
    {
      "epoch": 6.401673640167364,
      "grad_norm": 0.014948555268347263,
      "learning_rate": 1.3598326359832636e-05,
      "loss": 0.0078,
      "step": 6120
    },
    {
      "epoch": 6.412133891213389,
      "grad_norm": 2.8184025287628174,
      "learning_rate": 1.3587866108786611e-05,
      "loss": 0.0112,
      "step": 6130
    },
    {
      "epoch": 6.422594142259414,
      "grad_norm": 0.0694928914308548,
      "learning_rate": 1.3577405857740586e-05,
      "loss": 0.0086,
      "step": 6140
    },
    {
      "epoch": 6.433054393305439,
      "grad_norm": 1.638946294784546,
      "learning_rate": 1.3566945606694562e-05,
      "loss": 0.0039,
      "step": 6150
    },
    {
      "epoch": 6.443514644351464,
      "grad_norm": 1.2031242847442627,
      "learning_rate": 1.3556485355648537e-05,
      "loss": 0.0072,
      "step": 6160
    },
    {
      "epoch": 6.453974895397489,
      "grad_norm": 0.0018954817205667496,
      "learning_rate": 1.3546025104602511e-05,
      "loss": 0.0028,
      "step": 6170
    },
    {
      "epoch": 6.464435146443515,
      "grad_norm": 0.023758359253406525,
      "learning_rate": 1.3535564853556486e-05,
      "loss": 0.0081,
      "step": 6180
    },
    {
      "epoch": 6.47489539748954,
      "grad_norm": 1.7081048488616943,
      "learning_rate": 1.3525104602510461e-05,
      "loss": 0.0022,
      "step": 6190
    },
    {
      "epoch": 6.485355648535565,
      "grad_norm": 0.033176809549331665,
      "learning_rate": 1.3514644351464437e-05,
      "loss": 0.0013,
      "step": 6200
    },
    {
      "epoch": 6.49581589958159,
      "grad_norm": 1.7863545417785645,
      "learning_rate": 1.3504184100418412e-05,
      "loss": 0.0155,
      "step": 6210
    },
    {
      "epoch": 6.506276150627615,
      "grad_norm": 0.002577838720753789,
      "learning_rate": 1.3493723849372387e-05,
      "loss": 0.0029,
      "step": 6220
    },
    {
      "epoch": 6.51673640167364,
      "grad_norm": 0.07130119204521179,
      "learning_rate": 1.3483263598326361e-05,
      "loss": 0.0073,
      "step": 6230
    },
    {
      "epoch": 6.527196652719665,
      "grad_norm": 0.17542944848537445,
      "learning_rate": 1.3472803347280336e-05,
      "loss": 0.0012,
      "step": 6240
    },
    {
      "epoch": 6.53765690376569,
      "grad_norm": 0.023169245570898056,
      "learning_rate": 1.346234309623431e-05,
      "loss": 0.0154,
      "step": 6250
    },
    {
      "epoch": 6.548117154811716,
      "grad_norm": 0.17198693752288818,
      "learning_rate": 1.3451882845188287e-05,
      "loss": 0.01,
      "step": 6260
    },
    {
      "epoch": 6.558577405857741,
      "grad_norm": 0.37553319334983826,
      "learning_rate": 1.3441422594142262e-05,
      "loss": 0.0078,
      "step": 6270
    },
    {
      "epoch": 6.569037656903766,
      "grad_norm": 0.5516108274459839,
      "learning_rate": 1.3430962343096235e-05,
      "loss": 0.0068,
      "step": 6280
    },
    {
      "epoch": 6.579497907949791,
      "grad_norm": 0.003402935341000557,
      "learning_rate": 1.342050209205021e-05,
      "loss": 0.011,
      "step": 6290
    },
    {
      "epoch": 6.589958158995816,
      "grad_norm": 0.01029148604720831,
      "learning_rate": 1.3410041841004184e-05,
      "loss": 0.002,
      "step": 6300
    },
    {
      "epoch": 6.600418410041841,
      "grad_norm": 1.420532464981079,
      "learning_rate": 1.3399581589958159e-05,
      "loss": 0.0188,
      "step": 6310
    },
    {
      "epoch": 6.610878661087866,
      "grad_norm": 0.19850893318653107,
      "learning_rate": 1.3389121338912134e-05,
      "loss": 0.005,
      "step": 6320
    },
    {
      "epoch": 6.621338912133892,
      "grad_norm": 0.02868890017271042,
      "learning_rate": 1.337866108786611e-05,
      "loss": 0.0097,
      "step": 6330
    },
    {
      "epoch": 6.631799163179917,
      "grad_norm": 0.6556462645530701,
      "learning_rate": 1.3368200836820085e-05,
      "loss": 0.0317,
      "step": 6340
    },
    {
      "epoch": 6.642259414225942,
      "grad_norm": 3.419379472732544,
      "learning_rate": 1.335774058577406e-05,
      "loss": 0.0265,
      "step": 6350
    },
    {
      "epoch": 6.652719665271967,
      "grad_norm": 0.5187654495239258,
      "learning_rate": 1.3347280334728034e-05,
      "loss": 0.0108,
      "step": 6360
    },
    {
      "epoch": 6.663179916317992,
      "grad_norm": 3.9292521476745605,
      "learning_rate": 1.3336820083682009e-05,
      "loss": 0.0112,
      "step": 6370
    },
    {
      "epoch": 6.673640167364017,
      "grad_norm": 0.019734252244234085,
      "learning_rate": 1.3326359832635984e-05,
      "loss": 0.0039,
      "step": 6380
    },
    {
      "epoch": 6.684100418410042,
      "grad_norm": 0.0060175321996212006,
      "learning_rate": 1.331589958158996e-05,
      "loss": 0.006,
      "step": 6390
    },
    {
      "epoch": 6.694560669456067,
      "grad_norm": 3.5893993377685547,
      "learning_rate": 1.3305439330543935e-05,
      "loss": 0.01,
      "step": 6400
    },
    {
      "epoch": 6.705020920502092,
      "grad_norm": 2.2052204608917236,
      "learning_rate": 1.329497907949791e-05,
      "loss": 0.0217,
      "step": 6410
    },
    {
      "epoch": 6.7154811715481175,
      "grad_norm": 0.07140044122934341,
      "learning_rate": 1.3284518828451884e-05,
      "loss": 0.0047,
      "step": 6420
    },
    {
      "epoch": 6.7259414225941425,
      "grad_norm": 2.1851842403411865,
      "learning_rate": 1.3274058577405859e-05,
      "loss": 0.0053,
      "step": 6430
    },
    {
      "epoch": 6.7364016736401675,
      "grad_norm": 1.144737958908081,
      "learning_rate": 1.3263598326359835e-05,
      "loss": 0.0198,
      "step": 6440
    },
    {
      "epoch": 6.7468619246861925,
      "grad_norm": 1.4519035816192627,
      "learning_rate": 1.325313807531381e-05,
      "loss": 0.0084,
      "step": 6450
    },
    {
      "epoch": 6.7573221757322175,
      "grad_norm": 0.015471264719963074,
      "learning_rate": 1.3242677824267785e-05,
      "loss": 0.0055,
      "step": 6460
    },
    {
      "epoch": 6.7677824267782425,
      "grad_norm": 0.0520220510661602,
      "learning_rate": 1.323221757322176e-05,
      "loss": 0.0038,
      "step": 6470
    },
    {
      "epoch": 6.7782426778242675,
      "grad_norm": 1.3282760381698608,
      "learning_rate": 1.3221757322175732e-05,
      "loss": 0.0109,
      "step": 6480
    },
    {
      "epoch": 6.788702928870293,
      "grad_norm": 0.017263641580939293,
      "learning_rate": 1.3211297071129707e-05,
      "loss": 0.0057,
      "step": 6490
    },
    {
      "epoch": 6.799163179916318,
      "grad_norm": 2.224031448364258,
      "learning_rate": 1.3200836820083682e-05,
      "loss": 0.0138,
      "step": 6500
    },
    {
      "epoch": 6.809623430962343,
      "grad_norm": 1.063555121421814,
      "learning_rate": 1.3190376569037657e-05,
      "loss": 0.0207,
      "step": 6510
    },
    {
      "epoch": 6.820083682008368,
      "grad_norm": 0.020601093769073486,
      "learning_rate": 1.3179916317991633e-05,
      "loss": 0.0041,
      "step": 6520
    },
    {
      "epoch": 6.830543933054393,
      "grad_norm": 0.02395152673125267,
      "learning_rate": 1.3169456066945608e-05,
      "loss": 0.0058,
      "step": 6530
    },
    {
      "epoch": 6.841004184100418,
      "grad_norm": 0.12854553759098053,
      "learning_rate": 1.3158995815899582e-05,
      "loss": 0.0048,
      "step": 6540
    },
    {
      "epoch": 6.851464435146443,
      "grad_norm": 0.35950466990470886,
      "learning_rate": 1.3148535564853557e-05,
      "loss": 0.0071,
      "step": 6550
    },
    {
      "epoch": 6.861924686192468,
      "grad_norm": 0.7964576482772827,
      "learning_rate": 1.3138075313807532e-05,
      "loss": 0.0132,
      "step": 6560
    },
    {
      "epoch": 6.872384937238493,
      "grad_norm": 0.003402992384508252,
      "learning_rate": 1.3127615062761508e-05,
      "loss": 0.0078,
      "step": 6570
    },
    {
      "epoch": 6.882845188284519,
      "grad_norm": 0.44936397671699524,
      "learning_rate": 1.3117154811715483e-05,
      "loss": 0.0077,
      "step": 6580
    },
    {
      "epoch": 6.893305439330544,
      "grad_norm": 0.018951930105686188,
      "learning_rate": 1.3106694560669458e-05,
      "loss": 0.0043,
      "step": 6590
    },
    {
      "epoch": 6.903765690376569,
      "grad_norm": 0.005048884078860283,
      "learning_rate": 1.3096234309623432e-05,
      "loss": 0.0174,
      "step": 6600
    },
    {
      "epoch": 6.914225941422594,
      "grad_norm": 0.01695535145699978,
      "learning_rate": 1.3085774058577407e-05,
      "loss": 0.0036,
      "step": 6610
    },
    {
      "epoch": 6.924686192468619,
      "grad_norm": 0.11795207858085632,
      "learning_rate": 1.3075313807531382e-05,
      "loss": 0.0041,
      "step": 6620
    },
    {
      "epoch": 6.935146443514644,
      "grad_norm": 0.05003035068511963,
      "learning_rate": 1.3064853556485358e-05,
      "loss": 0.0012,
      "step": 6630
    },
    {
      "epoch": 6.945606694560669,
      "grad_norm": 0.061291903257369995,
      "learning_rate": 1.3054393305439333e-05,
      "loss": 0.0113,
      "step": 6640
    },
    {
      "epoch": 6.956066945606695,
      "grad_norm": 0.4345490634441376,
      "learning_rate": 1.3043933054393307e-05,
      "loss": 0.0017,
      "step": 6650
    },
    {
      "epoch": 6.96652719665272,
      "grad_norm": 0.06886526942253113,
      "learning_rate": 1.3033472803347282e-05,
      "loss": 0.0055,
      "step": 6660
    },
    {
      "epoch": 6.976987447698745,
      "grad_norm": 2.7218713760375977,
      "learning_rate": 1.3023012552301257e-05,
      "loss": 0.0067,
      "step": 6670
    },
    {
      "epoch": 6.98744769874477,
      "grad_norm": 0.01299465261399746,
      "learning_rate": 1.301255230125523e-05,
      "loss": 0.0064,
      "step": 6680
    },
    {
      "epoch": 6.997907949790795,
      "grad_norm": 3.388698101043701,
      "learning_rate": 1.3002092050209205e-05,
      "loss": 0.0043,
      "step": 6690
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.9944613821138212,
      "eval_f1": 0.9758582502768549,
      "eval_loss": 0.024251243099570274,
      "eval_precision": 0.972197705207414,
      "eval_recall": 0.9795464650955981,
      "eval_runtime": 323.5956,
      "eval_samples_per_second": 60.82,
      "eval_steps_per_second": 2.537,
      "step": 6692
    },
    {
      "epoch": 7.00836820083682,
      "grad_norm": 0.005297305528074503,
      "learning_rate": 1.2991631799163181e-05,
      "loss": 0.0045,
      "step": 6700
    },
    {
      "epoch": 7.018828451882845,
      "grad_norm": 0.00305171892978251,
      "learning_rate": 1.2981171548117156e-05,
      "loss": 0.0002,
      "step": 6710
    },
    {
      "epoch": 7.02928870292887,
      "grad_norm": 1.0001767873764038,
      "learning_rate": 1.297071129707113e-05,
      "loss": 0.0098,
      "step": 6720
    },
    {
      "epoch": 7.039748953974895,
      "grad_norm": 1.7384082078933716,
      "learning_rate": 1.2960251046025105e-05,
      "loss": 0.0041,
      "step": 6730
    },
    {
      "epoch": 7.050209205020921,
      "grad_norm": 2.168041229248047,
      "learning_rate": 1.294979079497908e-05,
      "loss": 0.0064,
      "step": 6740
    },
    {
      "epoch": 7.060669456066946,
      "grad_norm": 0.055389512330293655,
      "learning_rate": 1.2939330543933055e-05,
      "loss": 0.0023,
      "step": 6750
    },
    {
      "epoch": 7.071129707112971,
      "grad_norm": 0.007706652395427227,
      "learning_rate": 1.2928870292887031e-05,
      "loss": 0.0031,
      "step": 6760
    },
    {
      "epoch": 7.081589958158996,
      "grad_norm": 0.1748928725719452,
      "learning_rate": 1.2918410041841006e-05,
      "loss": 0.0048,
      "step": 6770
    },
    {
      "epoch": 7.092050209205021,
      "grad_norm": 0.002431372180581093,
      "learning_rate": 1.290794979079498e-05,
      "loss": 0.0141,
      "step": 6780
    },
    {
      "epoch": 7.102510460251046,
      "grad_norm": 0.008102810941636562,
      "learning_rate": 1.2897489539748955e-05,
      "loss": 0.0062,
      "step": 6790
    },
    {
      "epoch": 7.112970711297071,
      "grad_norm": 0.2598331570625305,
      "learning_rate": 1.288702928870293e-05,
      "loss": 0.0043,
      "step": 6800
    },
    {
      "epoch": 7.123430962343096,
      "grad_norm": 0.02908705174922943,
      "learning_rate": 1.2876569037656904e-05,
      "loss": 0.0039,
      "step": 6810
    },
    {
      "epoch": 7.133891213389122,
      "grad_norm": 2.229039430618286,
      "learning_rate": 1.286610878661088e-05,
      "loss": 0.0067,
      "step": 6820
    },
    {
      "epoch": 7.144351464435147,
      "grad_norm": 0.06153210252523422,
      "learning_rate": 1.2855648535564856e-05,
      "loss": 0.011,
      "step": 6830
    },
    {
      "epoch": 7.154811715481172,
      "grad_norm": 0.18192772567272186,
      "learning_rate": 1.284518828451883e-05,
      "loss": 0.0006,
      "step": 6840
    },
    {
      "epoch": 7.165271966527197,
      "grad_norm": 0.5837148427963257,
      "learning_rate": 1.2834728033472805e-05,
      "loss": 0.0062,
      "step": 6850
    },
    {
      "epoch": 7.175732217573222,
      "grad_norm": 1.3681130409240723,
      "learning_rate": 1.282426778242678e-05,
      "loss": 0.0265,
      "step": 6860
    },
    {
      "epoch": 7.186192468619247,
      "grad_norm": 0.8667654991149902,
      "learning_rate": 1.2813807531380756e-05,
      "loss": 0.0075,
      "step": 6870
    },
    {
      "epoch": 7.196652719665272,
      "grad_norm": 0.6373124122619629,
      "learning_rate": 1.2803347280334727e-05,
      "loss": 0.0078,
      "step": 6880
    },
    {
      "epoch": 7.207112970711297,
      "grad_norm": 0.004132704343646765,
      "learning_rate": 1.2792887029288704e-05,
      "loss": 0.0037,
      "step": 6890
    },
    {
      "epoch": 7.2175732217573225,
      "grad_norm": 0.07984436303377151,
      "learning_rate": 1.2782426778242678e-05,
      "loss": 0.0033,
      "step": 6900
    },
    {
      "epoch": 7.2280334728033475,
      "grad_norm": 4.975765228271484,
      "learning_rate": 1.2771966527196653e-05,
      "loss": 0.0041,
      "step": 6910
    },
    {
      "epoch": 7.2384937238493725,
      "grad_norm": 0.39709120988845825,
      "learning_rate": 1.2761506276150628e-05,
      "loss": 0.0015,
      "step": 6920
    },
    {
      "epoch": 7.2489539748953975,
      "grad_norm": 0.013483456335961819,
      "learning_rate": 1.2751046025104603e-05,
      "loss": 0.0012,
      "step": 6930
    },
    {
      "epoch": 7.2594142259414225,
      "grad_norm": 0.045789510011672974,
      "learning_rate": 1.2740585774058577e-05,
      "loss": 0.0041,
      "step": 6940
    },
    {
      "epoch": 7.2698744769874475,
      "grad_norm": 0.015376154333353043,
      "learning_rate": 1.2730125523012554e-05,
      "loss": 0.0024,
      "step": 6950
    },
    {
      "epoch": 7.2803347280334725,
      "grad_norm": 0.009176469407975674,
      "learning_rate": 1.2719665271966528e-05,
      "loss": 0.0023,
      "step": 6960
    },
    {
      "epoch": 7.290794979079498,
      "grad_norm": 0.0031271614134311676,
      "learning_rate": 1.2709205020920503e-05,
      "loss": 0.0034,
      "step": 6970
    },
    {
      "epoch": 7.301255230125523,
      "grad_norm": 0.8380419611930847,
      "learning_rate": 1.2698744769874478e-05,
      "loss": 0.0169,
      "step": 6980
    },
    {
      "epoch": 7.311715481171548,
      "grad_norm": 0.06570329517126083,
      "learning_rate": 1.2688284518828453e-05,
      "loss": 0.0051,
      "step": 6990
    },
    {
      "epoch": 7.322175732217573,
      "grad_norm": 8.239527702331543,
      "learning_rate": 1.2677824267782429e-05,
      "loss": 0.01,
      "step": 7000
    },
    {
      "epoch": 7.332635983263598,
      "grad_norm": 0.05175669491291046,
      "learning_rate": 1.2667364016736404e-05,
      "loss": 0.0045,
      "step": 7010
    },
    {
      "epoch": 7.343096234309623,
      "grad_norm": 0.4446312189102173,
      "learning_rate": 1.2656903765690378e-05,
      "loss": 0.0144,
      "step": 7020
    },
    {
      "epoch": 7.353556485355648,
      "grad_norm": 0.5816986560821533,
      "learning_rate": 1.2646443514644353e-05,
      "loss": 0.0043,
      "step": 7030
    },
    {
      "epoch": 7.364016736401673,
      "grad_norm": 3.5213093757629395,
      "learning_rate": 1.2635983263598328e-05,
      "loss": 0.0058,
      "step": 7040
    },
    {
      "epoch": 7.374476987447698,
      "grad_norm": 0.007366403006017208,
      "learning_rate": 1.2625523012552302e-05,
      "loss": 0.0054,
      "step": 7050
    },
    {
      "epoch": 7.384937238493724,
      "grad_norm": 0.05021132156252861,
      "learning_rate": 1.2615062761506279e-05,
      "loss": 0.0195,
      "step": 7060
    },
    {
      "epoch": 7.395397489539749,
      "grad_norm": 0.2546978294849396,
      "learning_rate": 1.2604602510460254e-05,
      "loss": 0.0181,
      "step": 7070
    },
    {
      "epoch": 7.405857740585774,
      "grad_norm": 0.01042390801012516,
      "learning_rate": 1.2594142259414227e-05,
      "loss": 0.0075,
      "step": 7080
    },
    {
      "epoch": 7.416317991631799,
      "grad_norm": 0.44996362924575806,
      "learning_rate": 1.2583682008368201e-05,
      "loss": 0.0158,
      "step": 7090
    },
    {
      "epoch": 7.426778242677824,
      "grad_norm": 4.026692867279053,
      "learning_rate": 1.2573221757322176e-05,
      "loss": 0.0112,
      "step": 7100
    },
    {
      "epoch": 7.437238493723849,
      "grad_norm": 0.0068673137575387955,
      "learning_rate": 1.256276150627615e-05,
      "loss": 0.0141,
      "step": 7110
    },
    {
      "epoch": 7.447698744769874,
      "grad_norm": 1.2335366010665894,
      "learning_rate": 1.2552301255230125e-05,
      "loss": 0.0128,
      "step": 7120
    },
    {
      "epoch": 7.4581589958159,
      "grad_norm": 0.6573739051818848,
      "learning_rate": 1.2541841004184102e-05,
      "loss": 0.0175,
      "step": 7130
    },
    {
      "epoch": 7.468619246861925,
      "grad_norm": 4.200039386749268,
      "learning_rate": 1.2531380753138076e-05,
      "loss": 0.0103,
      "step": 7140
    },
    {
      "epoch": 7.47907949790795,
      "grad_norm": 7.557608604431152,
      "learning_rate": 1.2520920502092051e-05,
      "loss": 0.0053,
      "step": 7150
    },
    {
      "epoch": 7.489539748953975,
      "grad_norm": 1.2671303749084473,
      "learning_rate": 1.2510460251046026e-05,
      "loss": 0.0032,
      "step": 7160
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.08663883805274963,
      "learning_rate": 1.25e-05,
      "loss": 0.0061,
      "step": 7170
    },
    {
      "epoch": 7.510460251046025,
      "grad_norm": 1.7872257232666016,
      "learning_rate": 1.2489539748953975e-05,
      "loss": 0.0024,
      "step": 7180
    },
    {
      "epoch": 7.52092050209205,
      "grad_norm": 0.013939235359430313,
      "learning_rate": 1.2479079497907952e-05,
      "loss": 0.0116,
      "step": 7190
    },
    {
      "epoch": 7.531380753138075,
      "grad_norm": 0.011502432636916637,
      "learning_rate": 1.2468619246861926e-05,
      "loss": 0.0087,
      "step": 7200
    },
    {
      "epoch": 7.5418410041841,
      "grad_norm": 0.16509215533733368,
      "learning_rate": 1.2458158995815901e-05,
      "loss": 0.0056,
      "step": 7210
    },
    {
      "epoch": 7.552301255230126,
      "grad_norm": 0.04075455293059349,
      "learning_rate": 1.2447698744769876e-05,
      "loss": 0.0037,
      "step": 7220
    },
    {
      "epoch": 7.562761506276151,
      "grad_norm": 0.008568372577428818,
      "learning_rate": 1.243723849372385e-05,
      "loss": 0.0082,
      "step": 7230
    },
    {
      "epoch": 7.573221757322176,
      "grad_norm": 4.450885772705078,
      "learning_rate": 1.2426778242677825e-05,
      "loss": 0.0067,
      "step": 7240
    },
    {
      "epoch": 7.583682008368201,
      "grad_norm": 0.609218955039978,
      "learning_rate": 1.2416317991631802e-05,
      "loss": 0.0051,
      "step": 7250
    },
    {
      "epoch": 7.594142259414226,
      "grad_norm": 0.024522442370653152,
      "learning_rate": 1.2405857740585776e-05,
      "loss": 0.0136,
      "step": 7260
    },
    {
      "epoch": 7.604602510460251,
      "grad_norm": 0.00496947905048728,
      "learning_rate": 1.239539748953975e-05,
      "loss": 0.0018,
      "step": 7270
    },
    {
      "epoch": 7.615062761506276,
      "grad_norm": 2.9368064403533936,
      "learning_rate": 1.2384937238493724e-05,
      "loss": 0.0101,
      "step": 7280
    },
    {
      "epoch": 7.625523012552302,
      "grad_norm": 1.454293966293335,
      "learning_rate": 1.2374476987447699e-05,
      "loss": 0.0102,
      "step": 7290
    },
    {
      "epoch": 7.635983263598327,
      "grad_norm": 0.27647534012794495,
      "learning_rate": 1.2364016736401673e-05,
      "loss": 0.0006,
      "step": 7300
    },
    {
      "epoch": 7.646443514644352,
      "grad_norm": 0.12785863876342773,
      "learning_rate": 1.2353556485355648e-05,
      "loss": 0.0071,
      "step": 7310
    },
    {
      "epoch": 7.656903765690377,
      "grad_norm": 0.04486243799328804,
      "learning_rate": 1.2343096234309625e-05,
      "loss": 0.0042,
      "step": 7320
    },
    {
      "epoch": 7.667364016736402,
      "grad_norm": 0.34095194935798645,
      "learning_rate": 1.23326359832636e-05,
      "loss": 0.012,
      "step": 7330
    },
    {
      "epoch": 7.677824267782427,
      "grad_norm": 0.016385307535529137,
      "learning_rate": 1.2322175732217574e-05,
      "loss": 0.0089,
      "step": 7340
    },
    {
      "epoch": 7.688284518828452,
      "grad_norm": 0.01012856513261795,
      "learning_rate": 1.2311715481171549e-05,
      "loss": 0.0069,
      "step": 7350
    },
    {
      "epoch": 7.698744769874477,
      "grad_norm": 0.018584325909614563,
      "learning_rate": 1.2301255230125523e-05,
      "loss": 0.0043,
      "step": 7360
    },
    {
      "epoch": 7.709205020920502,
      "grad_norm": 0.6228091716766357,
      "learning_rate": 1.2290794979079498e-05,
      "loss": 0.0103,
      "step": 7370
    },
    {
      "epoch": 7.7196652719665275,
      "grad_norm": 0.006362119223922491,
      "learning_rate": 1.2280334728033474e-05,
      "loss": 0.0228,
      "step": 7380
    },
    {
      "epoch": 7.7301255230125525,
      "grad_norm": 0.1768590360879898,
      "learning_rate": 1.226987447698745e-05,
      "loss": 0.0104,
      "step": 7390
    },
    {
      "epoch": 7.7405857740585775,
      "grad_norm": 0.18664050102233887,
      "learning_rate": 1.2259414225941424e-05,
      "loss": 0.0057,
      "step": 7400
    },
    {
      "epoch": 7.7510460251046025,
      "grad_norm": 0.008269376121461391,
      "learning_rate": 1.2248953974895399e-05,
      "loss": 0.0019,
      "step": 7410
    },
    {
      "epoch": 7.7615062761506275,
      "grad_norm": 0.1517273634672165,
      "learning_rate": 1.2238493723849373e-05,
      "loss": 0.0006,
      "step": 7420
    },
    {
      "epoch": 7.7719665271966525,
      "grad_norm": 0.15151958167552948,
      "learning_rate": 1.222803347280335e-05,
      "loss": 0.0065,
      "step": 7430
    },
    {
      "epoch": 7.7824267782426775,
      "grad_norm": 0.2576197683811188,
      "learning_rate": 1.2217573221757324e-05,
      "loss": 0.0009,
      "step": 7440
    },
    {
      "epoch": 7.792887029288703,
      "grad_norm": 0.6635388135910034,
      "learning_rate": 1.2207112970711299e-05,
      "loss": 0.0022,
      "step": 7450
    },
    {
      "epoch": 7.803347280334728,
      "grad_norm": 0.08954248577356339,
      "learning_rate": 1.2196652719665274e-05,
      "loss": 0.0125,
      "step": 7460
    },
    {
      "epoch": 7.813807531380753,
      "grad_norm": 0.004163089673966169,
      "learning_rate": 1.2186192468619247e-05,
      "loss": 0.0062,
      "step": 7470
    },
    {
      "epoch": 7.824267782426778,
      "grad_norm": 0.010828296653926373,
      "learning_rate": 1.2175732217573222e-05,
      "loss": 0.0018,
      "step": 7480
    },
    {
      "epoch": 7.834728033472803,
      "grad_norm": 0.5145821571350098,
      "learning_rate": 1.2165271966527196e-05,
      "loss": 0.0053,
      "step": 7490
    },
    {
      "epoch": 7.845188284518828,
      "grad_norm": 0.3842771351337433,
      "learning_rate": 1.2154811715481171e-05,
      "loss": 0.0189,
      "step": 7500
    },
    {
      "epoch": 7.855648535564853,
      "grad_norm": 2.767310857772827,
      "learning_rate": 1.2144351464435147e-05,
      "loss": 0.0131,
      "step": 7510
    },
    {
      "epoch": 7.866108786610878,
      "grad_norm": 3.3323397636413574,
      "learning_rate": 1.2133891213389122e-05,
      "loss": 0.0096,
      "step": 7520
    },
    {
      "epoch": 7.876569037656903,
      "grad_norm": 0.09402449429035187,
      "learning_rate": 1.2123430962343097e-05,
      "loss": 0.0025,
      "step": 7530
    },
    {
      "epoch": 7.887029288702929,
      "grad_norm": 0.5665847659111023,
      "learning_rate": 1.2112970711297071e-05,
      "loss": 0.0037,
      "step": 7540
    },
    {
      "epoch": 7.897489539748954,
      "grad_norm": 0.004776005633175373,
      "learning_rate": 1.2102510460251046e-05,
      "loss": 0.0029,
      "step": 7550
    },
    {
      "epoch": 7.907949790794979,
      "grad_norm": 0.6047013401985168,
      "learning_rate": 1.2092050209205023e-05,
      "loss": 0.004,
      "step": 7560
    },
    {
      "epoch": 7.918410041841004,
      "grad_norm": 0.21122488379478455,
      "learning_rate": 1.2081589958158997e-05,
      "loss": 0.0035,
      "step": 7570
    },
    {
      "epoch": 7.928870292887029,
      "grad_norm": 2.398033380508423,
      "learning_rate": 1.2071129707112972e-05,
      "loss": 0.0045,
      "step": 7580
    },
    {
      "epoch": 7.939330543933054,
      "grad_norm": 0.009188818745315075,
      "learning_rate": 1.2060669456066947e-05,
      "loss": 0.0116,
      "step": 7590
    },
    {
      "epoch": 7.949790794979079,
      "grad_norm": 0.025868522003293037,
      "learning_rate": 1.2050209205020921e-05,
      "loss": 0.0035,
      "step": 7600
    },
    {
      "epoch": 7.960251046025105,
      "grad_norm": 3.6009953022003174,
      "learning_rate": 1.2039748953974896e-05,
      "loss": 0.0087,
      "step": 7610
    },
    {
      "epoch": 7.97071129707113,
      "grad_norm": 5.7674384117126465,
      "learning_rate": 1.2029288702928872e-05,
      "loss": 0.0259,
      "step": 7620
    },
    {
      "epoch": 7.981171548117155,
      "grad_norm": 0.3375048041343689,
      "learning_rate": 1.2018828451882847e-05,
      "loss": 0.0021,
      "step": 7630
    },
    {
      "epoch": 7.99163179916318,
      "grad_norm": 0.0038248547352850437,
      "learning_rate": 1.2008368200836822e-05,
      "loss": 0.004,
      "step": 7640
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.9941056910569106,
      "eval_f1": 0.9744830620325562,
      "eval_loss": 0.022876577451825142,
      "eval_precision": 0.9643012625163256,
      "eval_recall": 0.9848821698532682,
      "eval_runtime": 322.8434,
      "eval_samples_per_second": 60.961,
      "eval_steps_per_second": 2.543,
      "step": 7648
    },
    {
      "epoch": 8.002092050209205,
      "grad_norm": 0.03153102844953537,
      "learning_rate": 1.1997907949790797e-05,
      "loss": 0.0128,
      "step": 7650
    },
    {
      "epoch": 8.01255230125523,
      "grad_norm": 0.01200562622398138,
      "learning_rate": 1.1987447698744771e-05,
      "loss": 0.0069,
      "step": 7660
    },
    {
      "epoch": 8.023012552301255,
      "grad_norm": 0.01621711067855358,
      "learning_rate": 1.1976987447698744e-05,
      "loss": 0.0055,
      "step": 7670
    },
    {
      "epoch": 8.03347280334728,
      "grad_norm": 0.2868693768978119,
      "learning_rate": 1.1966527196652719e-05,
      "loss": 0.0106,
      "step": 7680
    },
    {
      "epoch": 8.043933054393305,
      "grad_norm": 0.0038070213049650192,
      "learning_rate": 1.1956066945606695e-05,
      "loss": 0.0052,
      "step": 7690
    },
    {
      "epoch": 8.05439330543933,
      "grad_norm": 0.8389095067977905,
      "learning_rate": 1.194560669456067e-05,
      "loss": 0.0012,
      "step": 7700
    },
    {
      "epoch": 8.064853556485355,
      "grad_norm": 0.019283831119537354,
      "learning_rate": 1.1935146443514645e-05,
      "loss": 0.0021,
      "step": 7710
    },
    {
      "epoch": 8.07531380753138,
      "grad_norm": 1.0065693855285645,
      "learning_rate": 1.192468619246862e-05,
      "loss": 0.0053,
      "step": 7720
    },
    {
      "epoch": 8.085774058577407,
      "grad_norm": 0.798679769039154,
      "learning_rate": 1.1914225941422594e-05,
      "loss": 0.0059,
      "step": 7730
    },
    {
      "epoch": 8.096234309623432,
      "grad_norm": 0.0038289071526378393,
      "learning_rate": 1.1903765690376569e-05,
      "loss": 0.0014,
      "step": 7740
    },
    {
      "epoch": 8.106694560669457,
      "grad_norm": 1.8409684896469116,
      "learning_rate": 1.1893305439330545e-05,
      "loss": 0.0097,
      "step": 7750
    },
    {
      "epoch": 8.117154811715482,
      "grad_norm": 0.1877211630344391,
      "learning_rate": 1.188284518828452e-05,
      "loss": 0.0033,
      "step": 7760
    },
    {
      "epoch": 8.127615062761507,
      "grad_norm": 0.00269103585742414,
      "learning_rate": 1.1872384937238495e-05,
      "loss": 0.0041,
      "step": 7770
    },
    {
      "epoch": 8.138075313807532,
      "grad_norm": 0.6320925354957581,
      "learning_rate": 1.186192468619247e-05,
      "loss": 0.0054,
      "step": 7780
    },
    {
      "epoch": 8.148535564853557,
      "grad_norm": 0.0006883635651320219,
      "learning_rate": 1.1851464435146444e-05,
      "loss": 0.004,
      "step": 7790
    },
    {
      "epoch": 8.158995815899582,
      "grad_norm": 0.39165976643562317,
      "learning_rate": 1.184100418410042e-05,
      "loss": 0.0029,
      "step": 7800
    },
    {
      "epoch": 8.169456066945607,
      "grad_norm": 1.4076652526855469,
      "learning_rate": 1.1830543933054395e-05,
      "loss": 0.0088,
      "step": 7810
    },
    {
      "epoch": 8.179916317991632,
      "grad_norm": 0.06527416408061981,
      "learning_rate": 1.182008368200837e-05,
      "loss": 0.0025,
      "step": 7820
    },
    {
      "epoch": 8.190376569037657,
      "grad_norm": 0.04411016404628754,
      "learning_rate": 1.1809623430962345e-05,
      "loss": 0.0032,
      "step": 7830
    },
    {
      "epoch": 8.200836820083682,
      "grad_norm": 0.006286784540861845,
      "learning_rate": 1.179916317991632e-05,
      "loss": 0.0023,
      "step": 7840
    },
    {
      "epoch": 8.211297071129707,
      "grad_norm": 0.24404311180114746,
      "learning_rate": 1.1788702928870294e-05,
      "loss": 0.003,
      "step": 7850
    },
    {
      "epoch": 8.221757322175732,
      "grad_norm": 0.013041960075497627,
      "learning_rate": 1.177824267782427e-05,
      "loss": 0.0105,
      "step": 7860
    },
    {
      "epoch": 8.232217573221757,
      "grad_norm": 0.0025945627130568027,
      "learning_rate": 1.1767782426778242e-05,
      "loss": 0.0016,
      "step": 7870
    },
    {
      "epoch": 8.242677824267782,
      "grad_norm": 0.304688036441803,
      "learning_rate": 1.1757322175732218e-05,
      "loss": 0.0039,
      "step": 7880
    },
    {
      "epoch": 8.253138075313808,
      "grad_norm": 0.0015731052262708545,
      "learning_rate": 1.1746861924686193e-05,
      "loss": 0.0045,
      "step": 7890
    },
    {
      "epoch": 8.263598326359833,
      "grad_norm": 0.3642421364784241,
      "learning_rate": 1.1736401673640168e-05,
      "loss": 0.0112,
      "step": 7900
    },
    {
      "epoch": 8.274058577405858,
      "grad_norm": 0.0036510422360152006,
      "learning_rate": 1.1725941422594142e-05,
      "loss": 0.0004,
      "step": 7910
    },
    {
      "epoch": 8.284518828451883,
      "grad_norm": 0.006933157332241535,
      "learning_rate": 1.1715481171548117e-05,
      "loss": 0.0083,
      "step": 7920
    },
    {
      "epoch": 8.294979079497908,
      "grad_norm": 13.040864944458008,
      "learning_rate": 1.1705020920502093e-05,
      "loss": 0.0111,
      "step": 7930
    },
    {
      "epoch": 8.305439330543933,
      "grad_norm": 0.014179571531713009,
      "learning_rate": 1.1694560669456068e-05,
      "loss": 0.0052,
      "step": 7940
    },
    {
      "epoch": 8.315899581589958,
      "grad_norm": 1.0513540506362915,
      "learning_rate": 1.1684100418410043e-05,
      "loss": 0.0089,
      "step": 7950
    },
    {
      "epoch": 8.326359832635983,
      "grad_norm": 0.024813877418637276,
      "learning_rate": 1.1673640167364018e-05,
      "loss": 0.0016,
      "step": 7960
    },
    {
      "epoch": 8.336820083682008,
      "grad_norm": 0.0009403016301803291,
      "learning_rate": 1.1663179916317992e-05,
      "loss": 0.0042,
      "step": 7970
    },
    {
      "epoch": 8.347280334728033,
      "grad_norm": 2.001326560974121,
      "learning_rate": 1.1652719665271967e-05,
      "loss": 0.0103,
      "step": 7980
    },
    {
      "epoch": 8.357740585774058,
      "grad_norm": 0.0037529654800891876,
      "learning_rate": 1.1642259414225943e-05,
      "loss": 0.0113,
      "step": 7990
    },
    {
      "epoch": 8.368200836820083,
      "grad_norm": 0.02348858118057251,
      "learning_rate": 1.1631799163179918e-05,
      "loss": 0.004,
      "step": 8000
    },
    {
      "epoch": 8.378661087866108,
      "grad_norm": 0.0017931493930518627,
      "learning_rate": 1.1621338912133893e-05,
      "loss": 0.0082,
      "step": 8010
    },
    {
      "epoch": 8.389121338912133,
      "grad_norm": 0.00707791605964303,
      "learning_rate": 1.1610878661087867e-05,
      "loss": 0.0043,
      "step": 8020
    },
    {
      "epoch": 8.399581589958158,
      "grad_norm": 0.0036916183307766914,
      "learning_rate": 1.1600418410041842e-05,
      "loss": 0.003,
      "step": 8030
    },
    {
      "epoch": 8.410041841004183,
      "grad_norm": 0.060981545597314835,
      "learning_rate": 1.1589958158995817e-05,
      "loss": 0.0048,
      "step": 8040
    },
    {
      "epoch": 8.42050209205021,
      "grad_norm": 0.6077410578727722,
      "learning_rate": 1.1579497907949793e-05,
      "loss": 0.0028,
      "step": 8050
    },
    {
      "epoch": 8.430962343096235,
      "grad_norm": 0.008323028683662415,
      "learning_rate": 1.1569037656903768e-05,
      "loss": 0.0053,
      "step": 8060
    },
    {
      "epoch": 8.44142259414226,
      "grad_norm": 0.030373772606253624,
      "learning_rate": 1.1558577405857741e-05,
      "loss": 0.0082,
      "step": 8070
    },
    {
      "epoch": 8.451882845188285,
      "grad_norm": 3.508272647857666,
      "learning_rate": 1.1548117154811716e-05,
      "loss": 0.0048,
      "step": 8080
    },
    {
      "epoch": 8.46234309623431,
      "grad_norm": 0.005806403234601021,
      "learning_rate": 1.153765690376569e-05,
      "loss": 0.005,
      "step": 8090
    },
    {
      "epoch": 8.472803347280335,
      "grad_norm": 0.044185519218444824,
      "learning_rate": 1.1527196652719665e-05,
      "loss": 0.0048,
      "step": 8100
    },
    {
      "epoch": 8.48326359832636,
      "grad_norm": 0.04825427755713463,
      "learning_rate": 1.151673640167364e-05,
      "loss": 0.0078,
      "step": 8110
    },
    {
      "epoch": 8.493723849372385,
      "grad_norm": 2.7387866973876953,
      "learning_rate": 1.1506276150627616e-05,
      "loss": 0.0051,
      "step": 8120
    },
    {
      "epoch": 8.50418410041841,
      "grad_norm": 0.008246365934610367,
      "learning_rate": 1.1495815899581591e-05,
      "loss": 0.0003,
      "step": 8130
    },
    {
      "epoch": 8.514644351464435,
      "grad_norm": 0.7695473432540894,
      "learning_rate": 1.1485355648535566e-05,
      "loss": 0.004,
      "step": 8140
    },
    {
      "epoch": 8.52510460251046,
      "grad_norm": 1.0046192407608032,
      "learning_rate": 1.147489539748954e-05,
      "loss": 0.0028,
      "step": 8150
    },
    {
      "epoch": 8.535564853556485,
      "grad_norm": 0.006723141297698021,
      "learning_rate": 1.1464435146443515e-05,
      "loss": 0.0052,
      "step": 8160
    },
    {
      "epoch": 8.54602510460251,
      "grad_norm": 0.03117961250245571,
      "learning_rate": 1.145397489539749e-05,
      "loss": 0.0132,
      "step": 8170
    },
    {
      "epoch": 8.556485355648535,
      "grad_norm": 0.5860963463783264,
      "learning_rate": 1.1443514644351466e-05,
      "loss": 0.006,
      "step": 8180
    },
    {
      "epoch": 8.56694560669456,
      "grad_norm": 0.13144388794898987,
      "learning_rate": 1.143305439330544e-05,
      "loss": 0.0085,
      "step": 8190
    },
    {
      "epoch": 8.577405857740585,
      "grad_norm": 0.011198168620467186,
      "learning_rate": 1.1422594142259416e-05,
      "loss": 0.003,
      "step": 8200
    },
    {
      "epoch": 8.58786610878661,
      "grad_norm": 1.2665270566940308,
      "learning_rate": 1.141213389121339e-05,
      "loss": 0.0068,
      "step": 8210
    },
    {
      "epoch": 8.598326359832637,
      "grad_norm": 0.0022698405664414167,
      "learning_rate": 1.1401673640167365e-05,
      "loss": 0.0011,
      "step": 8220
    },
    {
      "epoch": 8.608786610878662,
      "grad_norm": 0.05976713076233864,
      "learning_rate": 1.1391213389121341e-05,
      "loss": 0.0055,
      "step": 8230
    },
    {
      "epoch": 8.619246861924687,
      "grad_norm": 0.007307842373847961,
      "learning_rate": 1.1380753138075316e-05,
      "loss": 0.0028,
      "step": 8240
    },
    {
      "epoch": 8.629707112970712,
      "grad_norm": 0.0799747109413147,
      "learning_rate": 1.137029288702929e-05,
      "loss": 0.0055,
      "step": 8250
    },
    {
      "epoch": 8.640167364016737,
      "grad_norm": 0.012330112978816032,
      "learning_rate": 1.1359832635983265e-05,
      "loss": 0.0075,
      "step": 8260
    },
    {
      "epoch": 8.650627615062762,
      "grad_norm": 0.591675877571106,
      "learning_rate": 1.1349372384937238e-05,
      "loss": 0.0031,
      "step": 8270
    },
    {
      "epoch": 8.661087866108787,
      "grad_norm": 0.006078796461224556,
      "learning_rate": 1.1338912133891213e-05,
      "loss": 0.0034,
      "step": 8280
    },
    {
      "epoch": 8.671548117154812,
      "grad_norm": 0.1818075180053711,
      "learning_rate": 1.1328451882845188e-05,
      "loss": 0.0191,
      "step": 8290
    },
    {
      "epoch": 8.682008368200837,
      "grad_norm": 0.16015438735485077,
      "learning_rate": 1.1317991631799163e-05,
      "loss": 0.0024,
      "step": 8300
    },
    {
      "epoch": 8.692468619246862,
      "grad_norm": 0.9477096796035767,
      "learning_rate": 1.1307531380753139e-05,
      "loss": 0.0157,
      "step": 8310
    },
    {
      "epoch": 8.702928870292887,
      "grad_norm": 0.004772915039211512,
      "learning_rate": 1.1297071129707114e-05,
      "loss": 0.0032,
      "step": 8320
    },
    {
      "epoch": 8.713389121338912,
      "grad_norm": 0.009189721196889877,
      "learning_rate": 1.1286610878661088e-05,
      "loss": 0.0129,
      "step": 8330
    },
    {
      "epoch": 8.723849372384937,
      "grad_norm": 0.011827162466943264,
      "learning_rate": 1.1276150627615063e-05,
      "loss": 0.0013,
      "step": 8340
    },
    {
      "epoch": 8.734309623430962,
      "grad_norm": 0.05045614019036293,
      "learning_rate": 1.1265690376569038e-05,
      "loss": 0.0078,
      "step": 8350
    },
    {
      "epoch": 8.744769874476987,
      "grad_norm": 0.03616117313504219,
      "learning_rate": 1.1255230125523014e-05,
      "loss": 0.0081,
      "step": 8360
    },
    {
      "epoch": 8.755230125523013,
      "grad_norm": 0.09152396023273468,
      "learning_rate": 1.1244769874476989e-05,
      "loss": 0.0045,
      "step": 8370
    },
    {
      "epoch": 8.765690376569038,
      "grad_norm": 1.2933893203735352,
      "learning_rate": 1.1234309623430964e-05,
      "loss": 0.0044,
      "step": 8380
    },
    {
      "epoch": 8.776150627615063,
      "grad_norm": 1.563035011291504,
      "learning_rate": 1.1223849372384938e-05,
      "loss": 0.0122,
      "step": 8390
    },
    {
      "epoch": 8.786610878661088,
      "grad_norm": 0.003271488705649972,
      "learning_rate": 1.1213389121338913e-05,
      "loss": 0.0012,
      "step": 8400
    },
    {
      "epoch": 8.797071129707113,
      "grad_norm": 0.005283267702907324,
      "learning_rate": 1.1202928870292888e-05,
      "loss": 0.0035,
      "step": 8410
    },
    {
      "epoch": 8.807531380753138,
      "grad_norm": 6.1220784187316895,
      "learning_rate": 1.1192468619246864e-05,
      "loss": 0.005,
      "step": 8420
    },
    {
      "epoch": 8.817991631799163,
      "grad_norm": 0.07582399994134903,
      "learning_rate": 1.1182008368200839e-05,
      "loss": 0.0013,
      "step": 8430
    },
    {
      "epoch": 8.828451882845188,
      "grad_norm": 0.27098605036735535,
      "learning_rate": 1.1171548117154814e-05,
      "loss": 0.0046,
      "step": 8440
    },
    {
      "epoch": 8.838912133891213,
      "grad_norm": 0.008283145725727081,
      "learning_rate": 1.1161087866108788e-05,
      "loss": 0.004,
      "step": 8450
    },
    {
      "epoch": 8.849372384937238,
      "grad_norm": 0.3377106189727783,
      "learning_rate": 1.1150627615062763e-05,
      "loss": 0.0163,
      "step": 8460
    },
    {
      "epoch": 8.859832635983263,
      "grad_norm": 0.44654276967048645,
      "learning_rate": 1.1140167364016736e-05,
      "loss": 0.0204,
      "step": 8470
    },
    {
      "epoch": 8.870292887029288,
      "grad_norm": 0.24287600815296173,
      "learning_rate": 1.112970711297071e-05,
      "loss": 0.0122,
      "step": 8480
    },
    {
      "epoch": 8.880753138075313,
      "grad_norm": 1.3792752027511597,
      "learning_rate": 1.1119246861924687e-05,
      "loss": 0.0057,
      "step": 8490
    },
    {
      "epoch": 8.891213389121338,
      "grad_norm": 5.197778224945068,
      "learning_rate": 1.1108786610878662e-05,
      "loss": 0.0077,
      "step": 8500
    },
    {
      "epoch": 8.901673640167363,
      "grad_norm": 0.01991908811032772,
      "learning_rate": 1.1098326359832636e-05,
      "loss": 0.0046,
      "step": 8510
    },
    {
      "epoch": 8.91213389121339,
      "grad_norm": 0.0035904226824641228,
      "learning_rate": 1.1087866108786611e-05,
      "loss": 0.0091,
      "step": 8520
    },
    {
      "epoch": 8.922594142259415,
      "grad_norm": 0.3967740833759308,
      "learning_rate": 1.1077405857740586e-05,
      "loss": 0.006,
      "step": 8530
    },
    {
      "epoch": 8.93305439330544,
      "grad_norm": 8.347567558288574,
      "learning_rate": 1.106694560669456e-05,
      "loss": 0.0215,
      "step": 8540
    },
    {
      "epoch": 8.943514644351465,
      "grad_norm": 2.1722159385681152,
      "learning_rate": 1.1056485355648537e-05,
      "loss": 0.0252,
      "step": 8550
    },
    {
      "epoch": 8.95397489539749,
      "grad_norm": 0.026384316384792328,
      "learning_rate": 1.1046025104602512e-05,
      "loss": 0.0034,
      "step": 8560
    },
    {
      "epoch": 8.964435146443515,
      "grad_norm": 1.3971129655838013,
      "learning_rate": 1.1035564853556486e-05,
      "loss": 0.0054,
      "step": 8570
    },
    {
      "epoch": 8.97489539748954,
      "grad_norm": 0.6691856384277344,
      "learning_rate": 1.1025104602510461e-05,
      "loss": 0.0006,
      "step": 8580
    },
    {
      "epoch": 8.985355648535565,
      "grad_norm": 0.7153664231300354,
      "learning_rate": 1.1014644351464436e-05,
      "loss": 0.014,
      "step": 8590
    },
    {
      "epoch": 8.99581589958159,
      "grad_norm": 0.33710649609565735,
      "learning_rate": 1.100418410041841e-05,
      "loss": 0.0031,
      "step": 8600
    },
    {
      "epoch": 9.0,
      "eval_accuracy": 0.9945630081300812,
      "eval_f1": 0.9762907157101707,
      "eval_loss": 0.020523622632026672,
      "eval_precision": 0.9730565371024735,
      "eval_recall": 0.9795464650955981,
      "eval_runtime": 322.789,
      "eval_samples_per_second": 60.972,
      "eval_steps_per_second": 2.543,
      "step": 8604
    },
    {
      "epoch": 9.006276150627615,
      "grad_norm": 0.006316844839602709,
      "learning_rate": 1.0993723849372387e-05,
      "loss": 0.0033,
      "step": 8610
    },
    {
      "epoch": 9.01673640167364,
      "grad_norm": 0.007566506043076515,
      "learning_rate": 1.0983263598326362e-05,
      "loss": 0.0028,
      "step": 8620
    },
    {
      "epoch": 9.027196652719665,
      "grad_norm": 0.20467205345630646,
      "learning_rate": 1.0972803347280336e-05,
      "loss": 0.0007,
      "step": 8630
    },
    {
      "epoch": 9.03765690376569,
      "grad_norm": 0.014771454967558384,
      "learning_rate": 1.0962343096234311e-05,
      "loss": 0.0029,
      "step": 8640
    },
    {
      "epoch": 9.048117154811715,
      "grad_norm": 0.41265180706977844,
      "learning_rate": 1.0951882845188286e-05,
      "loss": 0.0024,
      "step": 8650
    },
    {
      "epoch": 9.05857740585774,
      "grad_norm": 0.004079367499798536,
      "learning_rate": 1.0941422594142262e-05,
      "loss": 0.0079,
      "step": 8660
    },
    {
      "epoch": 9.069037656903765,
      "grad_norm": 2.1961445808410645,
      "learning_rate": 1.0930962343096233e-05,
      "loss": 0.008,
      "step": 8670
    },
    {
      "epoch": 9.07949790794979,
      "grad_norm": 0.004714229144155979,
      "learning_rate": 1.092050209205021e-05,
      "loss": 0.0013,
      "step": 8680
    },
    {
      "epoch": 9.089958158995817,
      "grad_norm": 0.24852749705314636,
      "learning_rate": 1.0910041841004185e-05,
      "loss": 0.0042,
      "step": 8690
    },
    {
      "epoch": 9.100418410041842,
      "grad_norm": 1.138870120048523,
      "learning_rate": 1.089958158995816e-05,
      "loss": 0.0062,
      "step": 8700
    },
    {
      "epoch": 9.110878661087867,
      "grad_norm": 0.3462156057357788,
      "learning_rate": 1.0889121338912134e-05,
      "loss": 0.0182,
      "step": 8710
    },
    {
      "epoch": 9.121338912133892,
      "grad_norm": 0.024933643639087677,
      "learning_rate": 1.0878661087866109e-05,
      "loss": 0.009,
      "step": 8720
    },
    {
      "epoch": 9.131799163179917,
      "grad_norm": 0.15200527012348175,
      "learning_rate": 1.0868200836820083e-05,
      "loss": 0.0008,
      "step": 8730
    },
    {
      "epoch": 9.142259414225942,
      "grad_norm": 0.33392229676246643,
      "learning_rate": 1.085774058577406e-05,
      "loss": 0.011,
      "step": 8740
    },
    {
      "epoch": 9.152719665271967,
      "grad_norm": 0.019029630348086357,
      "learning_rate": 1.0847280334728034e-05,
      "loss": 0.0005,
      "step": 8750
    },
    {
      "epoch": 9.163179916317992,
      "grad_norm": 0.02903195470571518,
      "learning_rate": 1.083682008368201e-05,
      "loss": 0.002,
      "step": 8760
    },
    {
      "epoch": 9.173640167364017,
      "grad_norm": 3.209235429763794,
      "learning_rate": 1.0826359832635984e-05,
      "loss": 0.0045,
      "step": 8770
    },
    {
      "epoch": 9.184100418410042,
      "grad_norm": 0.023524519056081772,
      "learning_rate": 1.0815899581589959e-05,
      "loss": 0.0024,
      "step": 8780
    },
    {
      "epoch": 9.194560669456067,
      "grad_norm": 0.5792419910430908,
      "learning_rate": 1.0805439330543935e-05,
      "loss": 0.0017,
      "step": 8790
    },
    {
      "epoch": 9.205020920502092,
      "grad_norm": 2.3408119678497314,
      "learning_rate": 1.079497907949791e-05,
      "loss": 0.0112,
      "step": 8800
    },
    {
      "epoch": 9.215481171548117,
      "grad_norm": 0.02338048256933689,
      "learning_rate": 1.0784518828451884e-05,
      "loss": 0.0035,
      "step": 8810
    },
    {
      "epoch": 9.225941422594142,
      "grad_norm": 0.009831041097640991,
      "learning_rate": 1.0774058577405859e-05,
      "loss": 0.0009,
      "step": 8820
    },
    {
      "epoch": 9.236401673640167,
      "grad_norm": 0.01277223788201809,
      "learning_rate": 1.0763598326359834e-05,
      "loss": 0.0028,
      "step": 8830
    },
    {
      "epoch": 9.246861924686192,
      "grad_norm": 0.05663590505719185,
      "learning_rate": 1.0753138075313809e-05,
      "loss": 0.0108,
      "step": 8840
    },
    {
      "epoch": 9.257322175732218,
      "grad_norm": 0.871924638748169,
      "learning_rate": 1.0742677824267785e-05,
      "loss": 0.0066,
      "step": 8850
    },
    {
      "epoch": 9.267782426778243,
      "grad_norm": 0.018623221665620804,
      "learning_rate": 1.073221757322176e-05,
      "loss": 0.0038,
      "step": 8860
    },
    {
      "epoch": 9.278242677824268,
      "grad_norm": 0.4947318434715271,
      "learning_rate": 1.0721757322175733e-05,
      "loss": 0.0017,
      "step": 8870
    },
    {
      "epoch": 9.288702928870293,
      "grad_norm": 0.0035844529047608376,
      "learning_rate": 1.0711297071129707e-05,
      "loss": 0.0019,
      "step": 8880
    },
    {
      "epoch": 9.299163179916318,
      "grad_norm": 1.3849729299545288,
      "learning_rate": 1.0700836820083682e-05,
      "loss": 0.0038,
      "step": 8890
    },
    {
      "epoch": 9.309623430962343,
      "grad_norm": 0.20227444171905518,
      "learning_rate": 1.0690376569037657e-05,
      "loss": 0.0153,
      "step": 8900
    },
    {
      "epoch": 9.320083682008368,
      "grad_norm": 0.0029169106855988503,
      "learning_rate": 1.0679916317991631e-05,
      "loss": 0.0075,
      "step": 8910
    },
    {
      "epoch": 9.330543933054393,
      "grad_norm": 0.006653835531324148,
      "learning_rate": 1.0669456066945608e-05,
      "loss": 0.0087,
      "step": 8920
    },
    {
      "epoch": 9.341004184100418,
      "grad_norm": 0.00787931028753519,
      "learning_rate": 1.0658995815899583e-05,
      "loss": 0.0041,
      "step": 8930
    },
    {
      "epoch": 9.351464435146443,
      "grad_norm": 1.0206360816955566,
      "learning_rate": 1.0648535564853557e-05,
      "loss": 0.0102,
      "step": 8940
    },
    {
      "epoch": 9.361924686192468,
      "grad_norm": 0.5316330194473267,
      "learning_rate": 1.0638075313807532e-05,
      "loss": 0.0037,
      "step": 8950
    },
    {
      "epoch": 9.372384937238493,
      "grad_norm": 0.001981377834454179,
      "learning_rate": 1.0627615062761507e-05,
      "loss": 0.0048,
      "step": 8960
    },
    {
      "epoch": 9.382845188284518,
      "grad_norm": 0.003947101999074221,
      "learning_rate": 1.0617154811715481e-05,
      "loss": 0.0018,
      "step": 8970
    },
    {
      "epoch": 9.393305439330543,
      "grad_norm": 0.5898857712745667,
      "learning_rate": 1.0606694560669458e-05,
      "loss": 0.0019,
      "step": 8980
    },
    {
      "epoch": 9.403765690376568,
      "grad_norm": 0.0008817857597023249,
      "learning_rate": 1.0596234309623432e-05,
      "loss": 0.004,
      "step": 8990
    },
    {
      "epoch": 9.414225941422593,
      "grad_norm": 0.018709195777773857,
      "learning_rate": 1.0585774058577407e-05,
      "loss": 0.0008,
      "step": 9000
    },
    {
      "epoch": 9.42468619246862,
      "grad_norm": 0.3562849164009094,
      "learning_rate": 1.0575313807531382e-05,
      "loss": 0.0017,
      "step": 9010
    },
    {
      "epoch": 9.435146443514645,
      "grad_norm": 0.009480567649006844,
      "learning_rate": 1.0564853556485357e-05,
      "loss": 0.0042,
      "step": 9020
    },
    {
      "epoch": 9.44560669456067,
      "grad_norm": 0.0012195729650557041,
      "learning_rate": 1.0554393305439333e-05,
      "loss": 0.0044,
      "step": 9030
    },
    {
      "epoch": 9.456066945606695,
      "grad_norm": 0.2760177552700043,
      "learning_rate": 1.0543933054393308e-05,
      "loss": 0.0035,
      "step": 9040
    },
    {
      "epoch": 9.46652719665272,
      "grad_norm": 0.003026483580470085,
      "learning_rate": 1.0533472803347282e-05,
      "loss": 0.0016,
      "step": 9050
    },
    {
      "epoch": 9.476987447698745,
      "grad_norm": 1.6809929609298706,
      "learning_rate": 1.0523012552301257e-05,
      "loss": 0.0037,
      "step": 9060
    },
    {
      "epoch": 9.48744769874477,
      "grad_norm": 0.0021663124207407236,
      "learning_rate": 1.051255230125523e-05,
      "loss": 0.0013,
      "step": 9070
    },
    {
      "epoch": 9.497907949790795,
      "grad_norm": 0.028934411704540253,
      "learning_rate": 1.0502092050209205e-05,
      "loss": 0.0071,
      "step": 9080
    },
    {
      "epoch": 9.50836820083682,
      "grad_norm": 0.0011312449350953102,
      "learning_rate": 1.049163179916318e-05,
      "loss": 0.0082,
      "step": 9090
    },
    {
      "epoch": 9.518828451882845,
      "grad_norm": 0.01206317637115717,
      "learning_rate": 1.0481171548117154e-05,
      "loss": 0.0034,
      "step": 9100
    },
    {
      "epoch": 9.52928870292887,
      "grad_norm": 0.009551149792969227,
      "learning_rate": 1.047071129707113e-05,
      "loss": 0.0052,
      "step": 9110
    },
    {
      "epoch": 9.539748953974895,
      "grad_norm": 0.5690335631370544,
      "learning_rate": 1.0460251046025105e-05,
      "loss": 0.0083,
      "step": 9120
    },
    {
      "epoch": 9.55020920502092,
      "grad_norm": 0.0021584134083241224,
      "learning_rate": 1.044979079497908e-05,
      "loss": 0.0044,
      "step": 9130
    },
    {
      "epoch": 9.560669456066945,
      "grad_norm": 0.0010482285870239139,
      "learning_rate": 1.0439330543933055e-05,
      "loss": 0.0036,
      "step": 9140
    },
    {
      "epoch": 9.57112970711297,
      "grad_norm": 0.012491513974964619,
      "learning_rate": 1.042887029288703e-05,
      "loss": 0.0147,
      "step": 9150
    },
    {
      "epoch": 9.581589958158997,
      "grad_norm": 0.00731969578191638,
      "learning_rate": 1.0418410041841006e-05,
      "loss": 0.0001,
      "step": 9160
    },
    {
      "epoch": 9.592050209205022,
      "grad_norm": 0.6354934573173523,
      "learning_rate": 1.040794979079498e-05,
      "loss": 0.0048,
      "step": 9170
    },
    {
      "epoch": 9.602510460251047,
      "grad_norm": 0.0036124554462730885,
      "learning_rate": 1.0397489539748955e-05,
      "loss": 0.0017,
      "step": 9180
    },
    {
      "epoch": 9.612970711297072,
      "grad_norm": 0.6437332034111023,
      "learning_rate": 1.038702928870293e-05,
      "loss": 0.0137,
      "step": 9190
    },
    {
      "epoch": 9.623430962343097,
      "grad_norm": 0.007959243841469288,
      "learning_rate": 1.0376569037656905e-05,
      "loss": 0.001,
      "step": 9200
    },
    {
      "epoch": 9.633891213389122,
      "grad_norm": 0.5117673873901367,
      "learning_rate": 1.036610878661088e-05,
      "loss": 0.0015,
      "step": 9210
    },
    {
      "epoch": 9.644351464435147,
      "grad_norm": 0.01937481015920639,
      "learning_rate": 1.0355648535564856e-05,
      "loss": 0.0019,
      "step": 9220
    },
    {
      "epoch": 9.654811715481172,
      "grad_norm": 2.654437780380249,
      "learning_rate": 1.034518828451883e-05,
      "loss": 0.0065,
      "step": 9230
    },
    {
      "epoch": 9.665271966527197,
      "grad_norm": 0.0029207654297351837,
      "learning_rate": 1.0334728033472805e-05,
      "loss": 0.0008,
      "step": 9240
    },
    {
      "epoch": 9.675732217573222,
      "grad_norm": 0.16130845248699188,
      "learning_rate": 1.032426778242678e-05,
      "loss": 0.0059,
      "step": 9250
    },
    {
      "epoch": 9.686192468619247,
      "grad_norm": 0.6411452889442444,
      "learning_rate": 1.0313807531380755e-05,
      "loss": 0.0019,
      "step": 9260
    },
    {
      "epoch": 9.696652719665272,
      "grad_norm": 1.5525970458984375,
      "learning_rate": 1.0303347280334728e-05,
      "loss": 0.006,
      "step": 9270
    },
    {
      "epoch": 9.707112970711297,
      "grad_norm": 7.133182525634766,
      "learning_rate": 1.0292887029288702e-05,
      "loss": 0.0086,
      "step": 9280
    },
    {
      "epoch": 9.717573221757322,
      "grad_norm": 0.003925974480807781,
      "learning_rate": 1.0282426778242679e-05,
      "loss": 0.0061,
      "step": 9290
    },
    {
      "epoch": 9.728033472803347,
      "grad_norm": 0.051157765090465546,
      "learning_rate": 1.0271966527196653e-05,
      "loss": 0.0005,
      "step": 9300
    },
    {
      "epoch": 9.738493723849372,
      "grad_norm": 0.21807432174682617,
      "learning_rate": 1.0261506276150628e-05,
      "loss": 0.0002,
      "step": 9310
    },
    {
      "epoch": 9.748953974895397,
      "grad_norm": 0.0055092149414122105,
      "learning_rate": 1.0251046025104603e-05,
      "loss": 0.0016,
      "step": 9320
    },
    {
      "epoch": 9.759414225941423,
      "grad_norm": 0.2925810217857361,
      "learning_rate": 1.0240585774058578e-05,
      "loss": 0.0122,
      "step": 9330
    },
    {
      "epoch": 9.769874476987448,
      "grad_norm": 0.023482924327254295,
      "learning_rate": 1.0230125523012552e-05,
      "loss": 0.005,
      "step": 9340
    },
    {
      "epoch": 9.780334728033473,
      "grad_norm": 0.10105147212743759,
      "learning_rate": 1.0219665271966529e-05,
      "loss": 0.001,
      "step": 9350
    },
    {
      "epoch": 9.790794979079498,
      "grad_norm": 0.001077911932952702,
      "learning_rate": 1.0209205020920503e-05,
      "loss": 0.0048,
      "step": 9360
    },
    {
      "epoch": 9.801255230125523,
      "grad_norm": 0.001608612947165966,
      "learning_rate": 1.0198744769874478e-05,
      "loss": 0.0089,
      "step": 9370
    },
    {
      "epoch": 9.811715481171548,
      "grad_norm": 0.8764603137969971,
      "learning_rate": 1.0188284518828453e-05,
      "loss": 0.0094,
      "step": 9380
    },
    {
      "epoch": 9.822175732217573,
      "grad_norm": 0.004726395942270756,
      "learning_rate": 1.0177824267782427e-05,
      "loss": 0.0007,
      "step": 9390
    },
    {
      "epoch": 9.832635983263598,
      "grad_norm": 0.0018938066205009818,
      "learning_rate": 1.0167364016736402e-05,
      "loss": 0.0018,
      "step": 9400
    },
    {
      "epoch": 9.843096234309623,
      "grad_norm": 2.2037367820739746,
      "learning_rate": 1.0156903765690379e-05,
      "loss": 0.0096,
      "step": 9410
    },
    {
      "epoch": 9.853556485355648,
      "grad_norm": 0.004669391084462404,
      "learning_rate": 1.0146443514644353e-05,
      "loss": 0.006,
      "step": 9420
    },
    {
      "epoch": 9.864016736401673,
      "grad_norm": 0.24908342957496643,
      "learning_rate": 1.0135983263598328e-05,
      "loss": 0.005,
      "step": 9430
    },
    {
      "epoch": 9.874476987447698,
      "grad_norm": 0.0029173351358622313,
      "learning_rate": 1.0125523012552303e-05,
      "loss": 0.0076,
      "step": 9440
    },
    {
      "epoch": 9.884937238493723,
      "grad_norm": 0.021216291934251785,
      "learning_rate": 1.0115062761506277e-05,
      "loss": 0.0011,
      "step": 9450
    },
    {
      "epoch": 9.895397489539748,
      "grad_norm": 0.011886879801750183,
      "learning_rate": 1.0104602510460254e-05,
      "loss": 0.0006,
      "step": 9460
    },
    {
      "epoch": 9.905857740585773,
      "grad_norm": 1.3922011852264404,
      "learning_rate": 1.0094142259414225e-05,
      "loss": 0.002,
      "step": 9470
    },
    {
      "epoch": 9.9163179916318,
      "grad_norm": 0.05429171770811081,
      "learning_rate": 1.0083682008368201e-05,
      "loss": 0.0015,
      "step": 9480
    },
    {
      "epoch": 9.926778242677825,
      "grad_norm": 2.344876766204834,
      "learning_rate": 1.0073221757322176e-05,
      "loss": 0.0108,
      "step": 9490
    },
    {
      "epoch": 9.93723849372385,
      "grad_norm": 0.0011457523796707392,
      "learning_rate": 1.0062761506276151e-05,
      "loss": 0.0019,
      "step": 9500
    },
    {
      "epoch": 9.947698744769875,
      "grad_norm": 0.16113585233688354,
      "learning_rate": 1.0052301255230126e-05,
      "loss": 0.0049,
      "step": 9510
    },
    {
      "epoch": 9.9581589958159,
      "grad_norm": 0.020618634298443794,
      "learning_rate": 1.00418410041841e-05,
      "loss": 0.0008,
      "step": 9520
    },
    {
      "epoch": 9.968619246861925,
      "grad_norm": 0.6431591510772705,
      "learning_rate": 1.0031380753138075e-05,
      "loss": 0.0097,
      "step": 9530
    },
    {
      "epoch": 9.97907949790795,
      "grad_norm": 0.001852588844485581,
      "learning_rate": 1.0020920502092051e-05,
      "loss": 0.0016,
      "step": 9540
    },
    {
      "epoch": 9.989539748953975,
      "grad_norm": 0.0033645930234342813,
      "learning_rate": 1.0010460251046026e-05,
      "loss": 0.0101,
      "step": 9550
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.016927445307374,
      "learning_rate": 1e-05,
      "loss": 0.003,
      "step": 9560
    },
    {
      "epoch": 10.0,
      "eval_accuracy": 0.9946646341463414,
      "eval_f1": 0.976713240186294,
      "eval_loss": 0.021108906716108322,
      "eval_precision": 0.9743362831858408,
      "eval_recall": 0.9791018230324589,
      "eval_runtime": 323.526,
      "eval_samples_per_second": 60.833,
      "eval_steps_per_second": 2.538,
      "step": 9560
    },
    {
      "epoch": 10.010460251046025,
      "grad_norm": 0.0057912967167794704,
      "learning_rate": 9.989539748953976e-06,
      "loss": 0.0016,
      "step": 9570
    },
    {
      "epoch": 10.02092050209205,
      "grad_norm": 0.007797928061336279,
      "learning_rate": 9.97907949790795e-06,
      "loss": 0.0044,
      "step": 9580
    },
    {
      "epoch": 10.031380753138075,
      "grad_norm": 0.006493786349892616,
      "learning_rate": 9.968619246861927e-06,
      "loss": 0.0112,
      "step": 9590
    },
    {
      "epoch": 10.0418410041841,
      "grad_norm": 0.11581768840551376,
      "learning_rate": 9.958158995815901e-06,
      "loss": 0.0037,
      "step": 9600
    },
    {
      "epoch": 10.052301255230125,
      "grad_norm": 0.28540176153182983,
      "learning_rate": 9.947698744769874e-06,
      "loss": 0.0033,
      "step": 9610
    },
    {
      "epoch": 10.06276150627615,
      "grad_norm": 0.024044645950198174,
      "learning_rate": 9.937238493723849e-06,
      "loss": 0.0033,
      "step": 9620
    },
    {
      "epoch": 10.073221757322175,
      "grad_norm": 0.0833665132522583,
      "learning_rate": 9.926778242677825e-06,
      "loss": 0.0012,
      "step": 9630
    },
    {
      "epoch": 10.0836820083682,
      "grad_norm": 5.062504768371582,
      "learning_rate": 9.9163179916318e-06,
      "loss": 0.001,
      "step": 9640
    },
    {
      "epoch": 10.094142259414227,
      "grad_norm": 0.0031310580670833588,
      "learning_rate": 9.905857740585775e-06,
      "loss": 0.0053,
      "step": 9650
    },
    {
      "epoch": 10.104602510460252,
      "grad_norm": 0.003982011694461107,
      "learning_rate": 9.89539748953975e-06,
      "loss": 0.0006,
      "step": 9660
    },
    {
      "epoch": 10.115062761506277,
      "grad_norm": 0.864879846572876,
      "learning_rate": 9.884937238493724e-06,
      "loss": 0.0016,
      "step": 9670
    },
    {
      "epoch": 10.125523012552302,
      "grad_norm": 3.92071533203125,
      "learning_rate": 9.874476987447699e-06,
      "loss": 0.0042,
      "step": 9680
    },
    {
      "epoch": 10.135983263598327,
      "grad_norm": 0.07704935222864151,
      "learning_rate": 9.864016736401675e-06,
      "loss": 0.0024,
      "step": 9690
    },
    {
      "epoch": 10.146443514644352,
      "grad_norm": 0.061254069209098816,
      "learning_rate": 9.85355648535565e-06,
      "loss": 0.0018,
      "step": 9700
    },
    {
      "epoch": 10.156903765690377,
      "grad_norm": 0.06187230721116066,
      "learning_rate": 9.843096234309623e-06,
      "loss": 0.0029,
      "step": 9710
    },
    {
      "epoch": 10.167364016736402,
      "grad_norm": 0.30635425448417664,
      "learning_rate": 9.8326359832636e-06,
      "loss": 0.0096,
      "step": 9720
    },
    {
      "epoch": 10.177824267782427,
      "grad_norm": 0.6396790146827698,
      "learning_rate": 9.822175732217574e-06,
      "loss": 0.0099,
      "step": 9730
    },
    {
      "epoch": 10.188284518828452,
      "grad_norm": 0.0057715605944395065,
      "learning_rate": 9.811715481171549e-06,
      "loss": 0.0006,
      "step": 9740
    },
    {
      "epoch": 10.198744769874477,
      "grad_norm": 0.04827018454670906,
      "learning_rate": 9.801255230125524e-06,
      "loss": 0.0025,
      "step": 9750
    },
    {
      "epoch": 10.209205020920502,
      "grad_norm": 0.5823132395744324,
      "learning_rate": 9.790794979079498e-06,
      "loss": 0.0043,
      "step": 9760
    },
    {
      "epoch": 10.219665271966527,
      "grad_norm": 0.5727836489677429,
      "learning_rate": 9.780334728033473e-06,
      "loss": 0.005,
      "step": 9770
    },
    {
      "epoch": 10.230125523012552,
      "grad_norm": 0.035116732120513916,
      "learning_rate": 9.76987447698745e-06,
      "loss": 0.0001,
      "step": 9780
    },
    {
      "epoch": 10.240585774058577,
      "grad_norm": 0.06270967423915863,
      "learning_rate": 9.759414225941424e-06,
      "loss": 0.0046,
      "step": 9790
    },
    {
      "epoch": 10.251046025104603,
      "grad_norm": 0.0023026729468256235,
      "learning_rate": 9.748953974895399e-06,
      "loss": 0.0005,
      "step": 9800
    },
    {
      "epoch": 10.261506276150628,
      "grad_norm": 0.5992724895477295,
      "learning_rate": 9.738493723849372e-06,
      "loss": 0.0174,
      "step": 9810
    },
    {
      "epoch": 10.271966527196653,
      "grad_norm": 0.0216581542044878,
      "learning_rate": 9.728033472803348e-06,
      "loss": 0.0023,
      "step": 9820
    },
    {
      "epoch": 10.282426778242678,
      "grad_norm": 3.7532498836517334,
      "learning_rate": 9.717573221757323e-06,
      "loss": 0.0052,
      "step": 9830
    },
    {
      "epoch": 10.292887029288703,
      "grad_norm": 0.007404607720673084,
      "learning_rate": 9.707112970711298e-06,
      "loss": 0.0048,
      "step": 9840
    },
    {
      "epoch": 10.303347280334728,
      "grad_norm": 0.45104435086250305,
      "learning_rate": 9.696652719665272e-06,
      "loss": 0.0019,
      "step": 9850
    },
    {
      "epoch": 10.313807531380753,
      "grad_norm": 0.24422013759613037,
      "learning_rate": 9.686192468619247e-06,
      "loss": 0.0085,
      "step": 9860
    },
    {
      "epoch": 10.324267782426778,
      "grad_norm": 0.0018328482983633876,
      "learning_rate": 9.675732217573223e-06,
      "loss": 0.0182,
      "step": 9870
    },
    {
      "epoch": 10.334728033472803,
      "grad_norm": 0.30863046646118164,
      "learning_rate": 9.665271966527198e-06,
      "loss": 0.0006,
      "step": 9880
    },
    {
      "epoch": 10.345188284518828,
      "grad_norm": 0.5152172446250916,
      "learning_rate": 9.654811715481173e-06,
      "loss": 0.0047,
      "step": 9890
    },
    {
      "epoch": 10.355648535564853,
      "grad_norm": 0.04397159814834595,
      "learning_rate": 9.644351464435148e-06,
      "loss": 0.0166,
      "step": 9900
    },
    {
      "epoch": 10.366108786610878,
      "grad_norm": 0.41381531953811646,
      "learning_rate": 9.633891213389122e-06,
      "loss": 0.005,
      "step": 9910
    },
    {
      "epoch": 10.376569037656903,
      "grad_norm": 0.012582052499055862,
      "learning_rate": 9.623430962343097e-06,
      "loss": 0.0016,
      "step": 9920
    },
    {
      "epoch": 10.387029288702928,
      "grad_norm": 0.009052951820194721,
      "learning_rate": 9.612970711297072e-06,
      "loss": 0.0007,
      "step": 9930
    },
    {
      "epoch": 10.397489539748953,
      "grad_norm": 0.003960725851356983,
      "learning_rate": 9.602510460251046e-06,
      "loss": 0.0061,
      "step": 9940
    },
    {
      "epoch": 10.407949790794978,
      "grad_norm": 0.842512845993042,
      "learning_rate": 9.592050209205021e-06,
      "loss": 0.0037,
      "step": 9950
    },
    {
      "epoch": 10.418410041841003,
      "grad_norm": 1.1429152488708496,
      "learning_rate": 9.581589958158996e-06,
      "loss": 0.0014,
      "step": 9960
    },
    {
      "epoch": 10.42887029288703,
      "grad_norm": 0.0027251450810581446,
      "learning_rate": 9.571129707112972e-06,
      "loss": 0.0039,
      "step": 9970
    },
    {
      "epoch": 10.439330543933055,
      "grad_norm": 0.0027215159498155117,
      "learning_rate": 9.560669456066947e-06,
      "loss": 0.0015,
      "step": 9980
    },
    {
      "epoch": 10.44979079497908,
      "grad_norm": 0.0025931994896382093,
      "learning_rate": 9.550209205020922e-06,
      "loss": 0.0007,
      "step": 9990
    },
    {
      "epoch": 10.460251046025105,
      "grad_norm": 0.0022956188768148422,
      "learning_rate": 9.539748953974896e-06,
      "loss": 0.0016,
      "step": 10000
    },
    {
      "epoch": 10.47071129707113,
      "grad_norm": 0.0016370514640584588,
      "learning_rate": 9.529288702928871e-06,
      "loss": 0.0032,
      "step": 10010
    },
    {
      "epoch": 10.481171548117155,
      "grad_norm": 0.08259841799736023,
      "learning_rate": 9.518828451882846e-06,
      "loss": 0.0001,
      "step": 10020
    },
    {
      "epoch": 10.49163179916318,
      "grad_norm": 0.7688747048377991,
      "learning_rate": 9.50836820083682e-06,
      "loss": 0.0041,
      "step": 10030
    },
    {
      "epoch": 10.502092050209205,
      "grad_norm": 0.2327679842710495,
      "learning_rate": 9.497907949790795e-06,
      "loss": 0.0008,
      "step": 10040
    },
    {
      "epoch": 10.51255230125523,
      "grad_norm": 0.279587060213089,
      "learning_rate": 9.48744769874477e-06,
      "loss": 0.0049,
      "step": 10050
    },
    {
      "epoch": 10.523012552301255,
      "grad_norm": 1.4516202211380005,
      "learning_rate": 9.476987447698746e-06,
      "loss": 0.0026,
      "step": 10060
    },
    {
      "epoch": 10.53347280334728,
      "grad_norm": 0.002392539754509926,
      "learning_rate": 9.466527196652721e-06,
      "loss": 0.0132,
      "step": 10070
    },
    {
      "epoch": 10.543933054393305,
      "grad_norm": 0.008509788662195206,
      "learning_rate": 9.456066945606696e-06,
      "loss": 0.0034,
      "step": 10080
    },
    {
      "epoch": 10.55439330543933,
      "grad_norm": 0.0020179988350719213,
      "learning_rate": 9.44560669456067e-06,
      "loss": 0.0057,
      "step": 10090
    },
    {
      "epoch": 10.564853556485355,
      "grad_norm": 0.002323925495147705,
      "learning_rate": 9.435146443514645e-06,
      "loss": 0.0022,
      "step": 10100
    },
    {
      "epoch": 10.57531380753138,
      "grad_norm": 0.7083558440208435,
      "learning_rate": 9.42468619246862e-06,
      "loss": 0.0034,
      "step": 10110
    },
    {
      "epoch": 10.585774058577407,
      "grad_norm": 0.001232301234267652,
      "learning_rate": 9.414225941422594e-06,
      "loss": 0.0039,
      "step": 10120
    },
    {
      "epoch": 10.596234309623432,
      "grad_norm": 0.0012763224076479673,
      "learning_rate": 9.40376569037657e-06,
      "loss": 0.0045,
      "step": 10130
    },
    {
      "epoch": 10.606694560669457,
      "grad_norm": 0.3727673292160034,
      "learning_rate": 9.393305439330544e-06,
      "loss": 0.0089,
      "step": 10140
    },
    {
      "epoch": 10.617154811715482,
      "grad_norm": 0.5609146952629089,
      "learning_rate": 9.38284518828452e-06,
      "loss": 0.0073,
      "step": 10150
    },
    {
      "epoch": 10.627615062761507,
      "grad_norm": 0.008083169348537922,
      "learning_rate": 9.372384937238495e-06,
      "loss": 0.001,
      "step": 10160
    },
    {
      "epoch": 10.638075313807532,
      "grad_norm": 0.002359554637223482,
      "learning_rate": 9.36192468619247e-06,
      "loss": 0.0019,
      "step": 10170
    },
    {
      "epoch": 10.648535564853557,
      "grad_norm": 0.0037792599759995937,
      "learning_rate": 9.351464435146444e-06,
      "loss": 0.0031,
      "step": 10180
    },
    {
      "epoch": 10.658995815899582,
      "grad_norm": 0.001533373724669218,
      "learning_rate": 9.341004184100419e-06,
      "loss": 0.0004,
      "step": 10190
    },
    {
      "epoch": 10.669456066945607,
      "grad_norm": 0.040277332067489624,
      "learning_rate": 9.330543933054394e-06,
      "loss": 0.0043,
      "step": 10200
    },
    {
      "epoch": 10.679916317991632,
      "grad_norm": 0.01037927158176899,
      "learning_rate": 9.320083682008368e-06,
      "loss": 0.0083,
      "step": 10210
    },
    {
      "epoch": 10.690376569037657,
      "grad_norm": 0.001913833082653582,
      "learning_rate": 9.309623430962343e-06,
      "loss": 0.0091,
      "step": 10220
    },
    {
      "epoch": 10.700836820083682,
      "grad_norm": 0.04721267148852348,
      "learning_rate": 9.299163179916318e-06,
      "loss": 0.0011,
      "step": 10230
    },
    {
      "epoch": 10.711297071129707,
      "grad_norm": 0.014009444043040276,
      "learning_rate": 9.288702928870293e-06,
      "loss": 0.0016,
      "step": 10240
    },
    {
      "epoch": 10.721757322175732,
      "grad_norm": 0.0016314518870785832,
      "learning_rate": 9.278242677824269e-06,
      "loss": 0.0013,
      "step": 10250
    },
    {
      "epoch": 10.732217573221757,
      "grad_norm": 0.04189519211649895,
      "learning_rate": 9.267782426778244e-06,
      "loss": 0.0066,
      "step": 10260
    },
    {
      "epoch": 10.742677824267782,
      "grad_norm": 0.0011341486824676394,
      "learning_rate": 9.257322175732218e-06,
      "loss": 0.0014,
      "step": 10270
    },
    {
      "epoch": 10.753138075313807,
      "grad_norm": 0.042330458760261536,
      "learning_rate": 9.246861924686193e-06,
      "loss": 0.0047,
      "step": 10280
    },
    {
      "epoch": 10.763598326359833,
      "grad_norm": 0.24857349693775177,
      "learning_rate": 9.236401673640168e-06,
      "loss": 0.0024,
      "step": 10290
    },
    {
      "epoch": 10.774058577405858,
      "grad_norm": 1.2777841091156006,
      "learning_rate": 9.225941422594144e-06,
      "loss": 0.0037,
      "step": 10300
    },
    {
      "epoch": 10.784518828451883,
      "grad_norm": 0.001355856889858842,
      "learning_rate": 9.215481171548117e-06,
      "loss": 0.0049,
      "step": 10310
    },
    {
      "epoch": 10.794979079497908,
      "grad_norm": 0.007831687107682228,
      "learning_rate": 9.205020920502092e-06,
      "loss": 0.0193,
      "step": 10320
    },
    {
      "epoch": 10.805439330543933,
      "grad_norm": 0.00819280557334423,
      "learning_rate": 9.194560669456067e-06,
      "loss": 0.0018,
      "step": 10330
    },
    {
      "epoch": 10.815899581589958,
      "grad_norm": 0.00044264583266340196,
      "learning_rate": 9.184100418410043e-06,
      "loss": 0.0133,
      "step": 10340
    },
    {
      "epoch": 10.826359832635983,
      "grad_norm": 0.010587552562355995,
      "learning_rate": 9.173640167364018e-06,
      "loss": 0.0018,
      "step": 10350
    },
    {
      "epoch": 10.836820083682008,
      "grad_norm": 0.3863410949707031,
      "learning_rate": 9.163179916317992e-06,
      "loss": 0.002,
      "step": 10360
    },
    {
      "epoch": 10.847280334728033,
      "grad_norm": 0.0017945842118933797,
      "learning_rate": 9.152719665271967e-06,
      "loss": 0.0058,
      "step": 10370
    },
    {
      "epoch": 10.857740585774058,
      "grad_norm": 0.001030800980515778,
      "learning_rate": 9.142259414225942e-06,
      "loss": 0.0003,
      "step": 10380
    },
    {
      "epoch": 10.868200836820083,
      "grad_norm": 0.06092086061835289,
      "learning_rate": 9.131799163179918e-06,
      "loss": 0.0052,
      "step": 10390
    },
    {
      "epoch": 10.878661087866108,
      "grad_norm": 0.01748935505747795,
      "learning_rate": 9.121338912133893e-06,
      "loss": 0.0042,
      "step": 10400
    },
    {
      "epoch": 10.889121338912133,
      "grad_norm": 0.013568595051765442,
      "learning_rate": 9.110878661087866e-06,
      "loss": 0.0019,
      "step": 10410
    },
    {
      "epoch": 10.899581589958158,
      "grad_norm": 1.4260064363479614,
      "learning_rate": 9.10041841004184e-06,
      "loss": 0.0064,
      "step": 10420
    },
    {
      "epoch": 10.910041841004183,
      "grad_norm": 0.25169438123703003,
      "learning_rate": 9.089958158995817e-06,
      "loss": 0.0034,
      "step": 10430
    },
    {
      "epoch": 10.92050209205021,
      "grad_norm": 0.014246470294892788,
      "learning_rate": 9.079497907949792e-06,
      "loss": 0.0011,
      "step": 10440
    },
    {
      "epoch": 10.930962343096235,
      "grad_norm": 0.720231831073761,
      "learning_rate": 9.069037656903766e-06,
      "loss": 0.0025,
      "step": 10450
    },
    {
      "epoch": 10.94142259414226,
      "grad_norm": 0.20580004155635834,
      "learning_rate": 9.058577405857741e-06,
      "loss": 0.0081,
      "step": 10460
    },
    {
      "epoch": 10.951882845188285,
      "grad_norm": 0.008835704065859318,
      "learning_rate": 9.048117154811716e-06,
      "loss": 0.0016,
      "step": 10470
    },
    {
      "epoch": 10.96234309623431,
      "grad_norm": 2.74882435798645,
      "learning_rate": 9.03765690376569e-06,
      "loss": 0.0244,
      "step": 10480
    },
    {
      "epoch": 10.972803347280335,
      "grad_norm": 0.0009572763810865581,
      "learning_rate": 9.027196652719667e-06,
      "loss": 0.0019,
      "step": 10490
    },
    {
      "epoch": 10.98326359832636,
      "grad_norm": 0.0049835010431706905,
      "learning_rate": 9.016736401673642e-06,
      "loss": 0.0109,
      "step": 10500
    },
    {
      "epoch": 10.993723849372385,
      "grad_norm": 0.7095803022384644,
      "learning_rate": 9.006276150627615e-06,
      "loss": 0.0056,
      "step": 10510
    },
    {
      "epoch": 11.0,
      "eval_accuracy": 0.9942581300813008,
      "eval_f1": 0.9748273557585209,
      "eval_loss": 0.02572621963918209,
      "eval_precision": 0.9767857142857143,
      "eval_recall": 0.9728768341485104,
      "eval_runtime": 325.082,
      "eval_samples_per_second": 60.542,
      "eval_steps_per_second": 2.526,
      "step": 10516
    },
    {
      "epoch": 11.00418410041841,
      "grad_norm": 0.5945740342140198,
      "learning_rate": 8.995815899581591e-06,
      "loss": 0.0083,
      "step": 10520
    },
    {
      "epoch": 11.014644351464435,
      "grad_norm": 0.005877816583961248,
      "learning_rate": 8.985355648535566e-06,
      "loss": 0.0031,
      "step": 10530
    },
    {
      "epoch": 11.02510460251046,
      "grad_norm": 0.013348214328289032,
      "learning_rate": 8.97489539748954e-06,
      "loss": 0.0017,
      "step": 10540
    },
    {
      "epoch": 11.035564853556485,
      "grad_norm": 0.011792818084359169,
      "learning_rate": 8.964435146443515e-06,
      "loss": 0.0042,
      "step": 10550
    },
    {
      "epoch": 11.04602510460251,
      "grad_norm": 0.05883036181330681,
      "learning_rate": 8.95397489539749e-06,
      "loss": 0.0037,
      "step": 10560
    },
    {
      "epoch": 11.056485355648535,
      "grad_norm": 0.019405579194426537,
      "learning_rate": 8.943514644351465e-06,
      "loss": 0.0009,
      "step": 10570
    },
    {
      "epoch": 11.06694560669456,
      "grad_norm": 0.15380510687828064,
      "learning_rate": 8.933054393305441e-06,
      "loss": 0.0014,
      "step": 10580
    },
    {
      "epoch": 11.077405857740585,
      "grad_norm": 0.9733167886734009,
      "learning_rate": 8.922594142259416e-06,
      "loss": 0.015,
      "step": 10590
    },
    {
      "epoch": 11.087866108786612,
      "grad_norm": 0.010025231167674065,
      "learning_rate": 8.91213389121339e-06,
      "loss": 0.0026,
      "step": 10600
    },
    {
      "epoch": 11.098326359832637,
      "grad_norm": 0.022307775914669037,
      "learning_rate": 8.901673640167363e-06,
      "loss": 0.002,
      "step": 10610
    },
    {
      "epoch": 11.108786610878662,
      "grad_norm": 0.12923406064510345,
      "learning_rate": 8.89121338912134e-06,
      "loss": 0.004,
      "step": 10620
    },
    {
      "epoch": 11.119246861924687,
      "grad_norm": 0.5978710055351257,
      "learning_rate": 8.880753138075315e-06,
      "loss": 0.0035,
      "step": 10630
    },
    {
      "epoch": 11.129707112970712,
      "grad_norm": 0.1276135891675949,
      "learning_rate": 8.87029288702929e-06,
      "loss": 0.0015,
      "step": 10640
    },
    {
      "epoch": 11.140167364016737,
      "grad_norm": 0.0238341037184,
      "learning_rate": 8.859832635983264e-06,
      "loss": 0.005,
      "step": 10650
    },
    {
      "epoch": 11.150627615062762,
      "grad_norm": 0.007828913629055023,
      "learning_rate": 8.849372384937239e-06,
      "loss": 0.0027,
      "step": 10660
    },
    {
      "epoch": 11.161087866108787,
      "grad_norm": 0.4637387990951538,
      "learning_rate": 8.838912133891215e-06,
      "loss": 0.0032,
      "step": 10670
    },
    {
      "epoch": 11.171548117154812,
      "grad_norm": 0.00203125411644578,
      "learning_rate": 8.82845188284519e-06,
      "loss": 0.0031,
      "step": 10680
    },
    {
      "epoch": 11.182008368200837,
      "grad_norm": 0.024865010753273964,
      "learning_rate": 8.817991631799164e-06,
      "loss": 0.0027,
      "step": 10690
    },
    {
      "epoch": 11.192468619246862,
      "grad_norm": 0.001394270220771432,
      "learning_rate": 8.80753138075314e-06,
      "loss": 0.0078,
      "step": 10700
    },
    {
      "epoch": 11.202928870292887,
      "grad_norm": 0.32362499833106995,
      "learning_rate": 8.797071129707114e-06,
      "loss": 0.0024,
      "step": 10710
    },
    {
      "epoch": 11.213389121338912,
      "grad_norm": 0.002912163734436035,
      "learning_rate": 8.786610878661089e-06,
      "loss": 0.0039,
      "step": 10720
    },
    {
      "epoch": 11.223849372384937,
      "grad_norm": 0.0016158095095306635,
      "learning_rate": 8.776150627615063e-06,
      "loss": 0.0027,
      "step": 10730
    },
    {
      "epoch": 11.234309623430962,
      "grad_norm": 0.00640633562579751,
      "learning_rate": 8.765690376569038e-06,
      "loss": 0.0085,
      "step": 10740
    },
    {
      "epoch": 11.244769874476987,
      "grad_norm": 0.01931554079055786,
      "learning_rate": 8.755230125523013e-06,
      "loss": 0.0002,
      "step": 10750
    },
    {
      "epoch": 11.255230125523013,
      "grad_norm": 0.3561343848705292,
      "learning_rate": 8.744769874476987e-06,
      "loss": 0.0079,
      "step": 10760
    },
    {
      "epoch": 11.265690376569038,
      "grad_norm": 0.014917204156517982,
      "learning_rate": 8.734309623430964e-06,
      "loss": 0.0068,
      "step": 10770
    },
    {
      "epoch": 11.276150627615063,
      "grad_norm": 0.07613057643175125,
      "learning_rate": 8.723849372384939e-06,
      "loss": 0.0005,
      "step": 10780
    },
    {
      "epoch": 11.286610878661088,
      "grad_norm": 0.002296042861416936,
      "learning_rate": 8.713389121338913e-06,
      "loss": 0.0014,
      "step": 10790
    },
    {
      "epoch": 11.297071129707113,
      "grad_norm": 0.0016263668658211827,
      "learning_rate": 8.702928870292888e-06,
      "loss": 0.001,
      "step": 10800
    },
    {
      "epoch": 11.307531380753138,
      "grad_norm": 0.014809847809374332,
      "learning_rate": 8.692468619246863e-06,
      "loss": 0.0039,
      "step": 10810
    },
    {
      "epoch": 11.317991631799163,
      "grad_norm": 0.0037074300926178694,
      "learning_rate": 8.682008368200837e-06,
      "loss": 0.0092,
      "step": 10820
    },
    {
      "epoch": 11.328451882845188,
      "grad_norm": 0.5142701864242554,
      "learning_rate": 8.671548117154812e-06,
      "loss": 0.004,
      "step": 10830
    },
    {
      "epoch": 11.338912133891213,
      "grad_norm": 0.010796388611197472,
      "learning_rate": 8.661087866108787e-06,
      "loss": 0.007,
      "step": 10840
    },
    {
      "epoch": 11.349372384937238,
      "grad_norm": 0.0012768061133101583,
      "learning_rate": 8.650627615062761e-06,
      "loss": 0.0067,
      "step": 10850
    },
    {
      "epoch": 11.359832635983263,
      "grad_norm": 0.0016417300794273615,
      "learning_rate": 8.640167364016738e-06,
      "loss": 0.0018,
      "step": 10860
    },
    {
      "epoch": 11.370292887029288,
      "grad_norm": 0.020187996327877045,
      "learning_rate": 8.629707112970713e-06,
      "loss": 0.0009,
      "step": 10870
    },
    {
      "epoch": 11.380753138075313,
      "grad_norm": 0.009914745576679707,
      "learning_rate": 8.619246861924687e-06,
      "loss": 0.0014,
      "step": 10880
    },
    {
      "epoch": 11.391213389121338,
      "grad_norm": 0.007615764159709215,
      "learning_rate": 8.608786610878662e-06,
      "loss": 0.0038,
      "step": 10890
    },
    {
      "epoch": 11.401673640167363,
      "grad_norm": 0.021744536235928535,
      "learning_rate": 8.598326359832637e-06,
      "loss": 0.0076,
      "step": 10900
    },
    {
      "epoch": 11.412133891213388,
      "grad_norm": 0.0007745094480924308,
      "learning_rate": 8.587866108786611e-06,
      "loss": 0.006,
      "step": 10910
    },
    {
      "epoch": 11.422594142259415,
      "grad_norm": 0.0007521165534853935,
      "learning_rate": 8.577405857740586e-06,
      "loss": 0.001,
      "step": 10920
    },
    {
      "epoch": 11.43305439330544,
      "grad_norm": 0.0016572503373026848,
      "learning_rate": 8.56694560669456e-06,
      "loss": 0.0023,
      "step": 10930
    },
    {
      "epoch": 11.443514644351465,
      "grad_norm": 0.7293089032173157,
      "learning_rate": 8.556485355648536e-06,
      "loss": 0.0105,
      "step": 10940
    },
    {
      "epoch": 11.45397489539749,
      "grad_norm": 0.001995356287807226,
      "learning_rate": 8.546025104602512e-06,
      "loss": 0.0079,
      "step": 10950
    },
    {
      "epoch": 11.464435146443515,
      "grad_norm": 0.0792774111032486,
      "learning_rate": 8.535564853556487e-06,
      "loss": 0.0001,
      "step": 10960
    },
    {
      "epoch": 11.47489539748954,
      "grad_norm": 0.40030819177627563,
      "learning_rate": 8.525104602510461e-06,
      "loss": 0.0034,
      "step": 10970
    },
    {
      "epoch": 11.485355648535565,
      "grad_norm": 0.0015742891700938344,
      "learning_rate": 8.514644351464436e-06,
      "loss": 0.0009,
      "step": 10980
    },
    {
      "epoch": 11.49581589958159,
      "grad_norm": 0.10847200453281403,
      "learning_rate": 8.50418410041841e-06,
      "loss": 0.0003,
      "step": 10990
    },
    {
      "epoch": 11.506276150627615,
      "grad_norm": 0.004217387642711401,
      "learning_rate": 8.493723849372385e-06,
      "loss": 0.0008,
      "step": 11000
    },
    {
      "epoch": 11.51673640167364,
      "grad_norm": 0.004508303478360176,
      "learning_rate": 8.48326359832636e-06,
      "loss": 0.0009,
      "step": 11010
    },
    {
      "epoch": 11.527196652719665,
      "grad_norm": 0.0033183544874191284,
      "learning_rate": 8.472803347280335e-06,
      "loss": 0.017,
      "step": 11020
    },
    {
      "epoch": 11.53765690376569,
      "grad_norm": 0.004715650342404842,
      "learning_rate": 8.46234309623431e-06,
      "loss": 0.0054,
      "step": 11030
    },
    {
      "epoch": 11.548117154811715,
      "grad_norm": 0.006174218840897083,
      "learning_rate": 8.451882845188284e-06,
      "loss": 0.0013,
      "step": 11040
    },
    {
      "epoch": 11.55857740585774,
      "grad_norm": 0.0014847186394035816,
      "learning_rate": 8.44142259414226e-06,
      "loss": 0.0002,
      "step": 11050
    },
    {
      "epoch": 11.569037656903765,
      "grad_norm": 0.0013592569157481194,
      "learning_rate": 8.430962343096235e-06,
      "loss": 0.0036,
      "step": 11060
    },
    {
      "epoch": 11.57949790794979,
      "grad_norm": 0.010281875729560852,
      "learning_rate": 8.42050209205021e-06,
      "loss": 0.005,
      "step": 11070
    },
    {
      "epoch": 11.589958158995817,
      "grad_norm": 0.375369131565094,
      "learning_rate": 8.410041841004185e-06,
      "loss": 0.0009,
      "step": 11080
    },
    {
      "epoch": 11.600418410041842,
      "grad_norm": 0.00502103753387928,
      "learning_rate": 8.39958158995816e-06,
      "loss": 0.0051,
      "step": 11090
    },
    {
      "epoch": 11.610878661087867,
      "grad_norm": 1.0614850521087646,
      "learning_rate": 8.389121338912136e-06,
      "loss": 0.0039,
      "step": 11100
    },
    {
      "epoch": 11.621338912133892,
      "grad_norm": 0.9565901160240173,
      "learning_rate": 8.378661087866109e-06,
      "loss": 0.0011,
      "step": 11110
    },
    {
      "epoch": 11.631799163179917,
      "grad_norm": 0.015765011310577393,
      "learning_rate": 8.368200836820084e-06,
      "loss": 0.002,
      "step": 11120
    },
    {
      "epoch": 11.642259414225942,
      "grad_norm": 0.6341187357902527,
      "learning_rate": 8.357740585774058e-06,
      "loss": 0.014,
      "step": 11130
    },
    {
      "epoch": 11.652719665271967,
      "grad_norm": 0.38056159019470215,
      "learning_rate": 8.347280334728035e-06,
      "loss": 0.0048,
      "step": 11140
    },
    {
      "epoch": 11.663179916317992,
      "grad_norm": 0.060266364365816116,
      "learning_rate": 8.33682008368201e-06,
      "loss": 0.0036,
      "step": 11150
    },
    {
      "epoch": 11.673640167364017,
      "grad_norm": 1.3135260343551636,
      "learning_rate": 8.326359832635984e-06,
      "loss": 0.0047,
      "step": 11160
    },
    {
      "epoch": 11.684100418410042,
      "grad_norm": 0.15831874310970306,
      "learning_rate": 8.315899581589959e-06,
      "loss": 0.0011,
      "step": 11170
    },
    {
      "epoch": 11.694560669456067,
      "grad_norm": 0.005496697966009378,
      "learning_rate": 8.305439330543934e-06,
      "loss": 0.0024,
      "step": 11180
    },
    {
      "epoch": 11.705020920502092,
      "grad_norm": 0.04331463947892189,
      "learning_rate": 8.294979079497908e-06,
      "loss": 0.0043,
      "step": 11190
    },
    {
      "epoch": 11.715481171548117,
      "grad_norm": 0.0021442482247948647,
      "learning_rate": 8.284518828451885e-06,
      "loss": 0.0006,
      "step": 11200
    },
    {
      "epoch": 11.725941422594142,
      "grad_norm": 0.004639600869268179,
      "learning_rate": 8.274058577405858e-06,
      "loss": 0.0031,
      "step": 11210
    },
    {
      "epoch": 11.736401673640167,
      "grad_norm": 7.123814105987549,
      "learning_rate": 8.263598326359832e-06,
      "loss": 0.0063,
      "step": 11220
    },
    {
      "epoch": 11.746861924686193,
      "grad_norm": 0.010109570808708668,
      "learning_rate": 8.253138075313809e-06,
      "loss": 0.0018,
      "step": 11230
    },
    {
      "epoch": 11.757322175732218,
      "grad_norm": 0.48581695556640625,
      "learning_rate": 8.242677824267783e-06,
      "loss": 0.0081,
      "step": 11240
    },
    {
      "epoch": 11.767782426778243,
      "grad_norm": 0.014666486531496048,
      "learning_rate": 8.232217573221758e-06,
      "loss": 0.0001,
      "step": 11250
    },
    {
      "epoch": 11.778242677824268,
      "grad_norm": 0.011043819598853588,
      "learning_rate": 8.221757322175733e-06,
      "loss": 0.0042,
      "step": 11260
    },
    {
      "epoch": 11.788702928870293,
      "grad_norm": 0.006918179802596569,
      "learning_rate": 8.211297071129708e-06,
      "loss": 0.0021,
      "step": 11270
    },
    {
      "epoch": 11.799163179916318,
      "grad_norm": 0.013315306976437569,
      "learning_rate": 8.200836820083682e-06,
      "loss": 0.0012,
      "step": 11280
    },
    {
      "epoch": 11.809623430962343,
      "grad_norm": 0.0018932520179077983,
      "learning_rate": 8.190376569037659e-06,
      "loss": 0.0026,
      "step": 11290
    },
    {
      "epoch": 11.820083682008368,
      "grad_norm": 1.1664395332336426,
      "learning_rate": 8.179916317991633e-06,
      "loss": 0.0011,
      "step": 11300
    },
    {
      "epoch": 11.830543933054393,
      "grad_norm": 0.32097646594047546,
      "learning_rate": 8.169456066945606e-06,
      "loss": 0.0026,
      "step": 11310
    },
    {
      "epoch": 11.841004184100418,
      "grad_norm": 0.004503720905631781,
      "learning_rate": 8.158995815899581e-06,
      "loss": 0.005,
      "step": 11320
    },
    {
      "epoch": 11.851464435146443,
      "grad_norm": 0.0044519719667732716,
      "learning_rate": 8.148535564853557e-06,
      "loss": 0.0019,
      "step": 11330
    },
    {
      "epoch": 11.861924686192468,
      "grad_norm": 0.597915768623352,
      "learning_rate": 8.138075313807532e-06,
      "loss": 0.0055,
      "step": 11340
    },
    {
      "epoch": 11.872384937238493,
      "grad_norm": 0.0025614388287067413,
      "learning_rate": 8.127615062761507e-06,
      "loss": 0.0104,
      "step": 11350
    },
    {
      "epoch": 11.882845188284518,
      "grad_norm": 0.002811030950397253,
      "learning_rate": 8.117154811715482e-06,
      "loss": 0.0079,
      "step": 11360
    },
    {
      "epoch": 11.893305439330543,
      "grad_norm": 0.00610695406794548,
      "learning_rate": 8.106694560669456e-06,
      "loss": 0.0025,
      "step": 11370
    },
    {
      "epoch": 11.903765690376568,
      "grad_norm": 0.9367243051528931,
      "learning_rate": 8.096234309623433e-06,
      "loss": 0.0028,
      "step": 11380
    },
    {
      "epoch": 11.914225941422593,
      "grad_norm": 0.4324547350406647,
      "learning_rate": 8.085774058577407e-06,
      "loss": 0.0032,
      "step": 11390
    },
    {
      "epoch": 11.92468619246862,
      "grad_norm": 0.07873857766389847,
      "learning_rate": 8.075313807531382e-06,
      "loss": 0.003,
      "step": 11400
    },
    {
      "epoch": 11.935146443514645,
      "grad_norm": 0.3789440393447876,
      "learning_rate": 8.064853556485355e-06,
      "loss": 0.0041,
      "step": 11410
    },
    {
      "epoch": 11.94560669456067,
      "grad_norm": 0.015337267890572548,
      "learning_rate": 8.054393305439332e-06,
      "loss": 0.0017,
      "step": 11420
    },
    {
      "epoch": 11.956066945606695,
      "grad_norm": 0.8132628798484802,
      "learning_rate": 8.043933054393306e-06,
      "loss": 0.0102,
      "step": 11430
    },
    {
      "epoch": 11.96652719665272,
      "grad_norm": 0.5670746564865112,
      "learning_rate": 8.033472803347281e-06,
      "loss": 0.0033,
      "step": 11440
    },
    {
      "epoch": 11.976987447698745,
      "grad_norm": 0.0023839380592107773,
      "learning_rate": 8.023012552301256e-06,
      "loss": 0.0008,
      "step": 11450
    },
    {
      "epoch": 11.98744769874477,
      "grad_norm": 0.0035650029312819242,
      "learning_rate": 8.01255230125523e-06,
      "loss": 0.0062,
      "step": 11460
    },
    {
      "epoch": 11.997907949790795,
      "grad_norm": 0.7529591917991638,
      "learning_rate": 8.002092050209205e-06,
      "loss": 0.011,
      "step": 11470
    },
    {
      "epoch": 12.0,
      "eval_accuracy": 0.9947154471544716,
      "eval_f1": 0.9769605671245016,
      "eval_loss": 0.022911807522177696,
      "eval_precision": 0.9735099337748344,
      "eval_recall": 0.9804357492218764,
      "eval_runtime": 324.995,
      "eval_samples_per_second": 60.558,
      "eval_steps_per_second": 2.526,
      "step": 11472
    },
    {
      "epoch": 12.00836820083682,
      "grad_norm": 11.962410926818848,
      "learning_rate": 7.991631799163181e-06,
      "loss": 0.0028,
      "step": 11480
    },
    {
      "epoch": 12.018828451882845,
      "grad_norm": 0.0017108529573306441,
      "learning_rate": 7.981171548117156e-06,
      "loss": 0.0018,
      "step": 11490
    },
    {
      "epoch": 12.02928870292887,
      "grad_norm": 0.5904381275177002,
      "learning_rate": 7.97071129707113e-06,
      "loss": 0.0023,
      "step": 11500
    },
    {
      "epoch": 12.039748953974895,
      "grad_norm": 0.0016435509314760566,
      "learning_rate": 7.960251046025106e-06,
      "loss": 0.0006,
      "step": 11510
    },
    {
      "epoch": 12.05020920502092,
      "grad_norm": 0.001957477768883109,
      "learning_rate": 7.94979079497908e-06,
      "loss": 0.0027,
      "step": 11520
    },
    {
      "epoch": 12.060669456066945,
      "grad_norm": 2.7592523097991943,
      "learning_rate": 7.939330543933055e-06,
      "loss": 0.0153,
      "step": 11530
    },
    {
      "epoch": 12.07112970711297,
      "grad_norm": 0.25333791971206665,
      "learning_rate": 7.92887029288703e-06,
      "loss": 0.0009,
      "step": 11540
    },
    {
      "epoch": 12.081589958158995,
      "grad_norm": 0.5494646430015564,
      "learning_rate": 7.918410041841004e-06,
      "loss": 0.0037,
      "step": 11550
    },
    {
      "epoch": 12.092050209205022,
      "grad_norm": 0.006546847056597471,
      "learning_rate": 7.907949790794979e-06,
      "loss": 0.0011,
      "step": 11560
    },
    {
      "epoch": 12.102510460251047,
      "grad_norm": 0.006065938621759415,
      "learning_rate": 7.897489539748955e-06,
      "loss": 0.0017,
      "step": 11570
    },
    {
      "epoch": 12.112970711297072,
      "grad_norm": 0.01289374940097332,
      "learning_rate": 7.88702928870293e-06,
      "loss": 0.0011,
      "step": 11580
    },
    {
      "epoch": 12.123430962343097,
      "grad_norm": 0.01216045394539833,
      "learning_rate": 7.876569037656905e-06,
      "loss": 0.0023,
      "step": 11590
    },
    {
      "epoch": 12.133891213389122,
      "grad_norm": 0.0012649348936975002,
      "learning_rate": 7.86610878661088e-06,
      "loss": 0.0103,
      "step": 11600
    },
    {
      "epoch": 12.144351464435147,
      "grad_norm": 0.05449824035167694,
      "learning_rate": 7.855648535564854e-06,
      "loss": 0.0013,
      "step": 11610
    },
    {
      "epoch": 12.154811715481172,
      "grad_norm": 0.020347818732261658,
      "learning_rate": 7.845188284518829e-06,
      "loss": 0.0032,
      "step": 11620
    },
    {
      "epoch": 12.165271966527197,
      "grad_norm": 0.16633838415145874,
      "learning_rate": 7.834728033472804e-06,
      "loss": 0.0009,
      "step": 11630
    },
    {
      "epoch": 12.175732217573222,
      "grad_norm": 0.003104311879724264,
      "learning_rate": 7.824267782426778e-06,
      "loss": 0.001,
      "step": 11640
    },
    {
      "epoch": 12.186192468619247,
      "grad_norm": 0.0017247680807486176,
      "learning_rate": 7.813807531380753e-06,
      "loss": 0.0094,
      "step": 11650
    },
    {
      "epoch": 12.196652719665272,
      "grad_norm": 0.004205387085676193,
      "learning_rate": 7.80334728033473e-06,
      "loss": 0.002,
      "step": 11660
    },
    {
      "epoch": 12.207112970711297,
      "grad_norm": 0.0016154503682628274,
      "learning_rate": 7.792887029288704e-06,
      "loss": 0.0036,
      "step": 11670
    },
    {
      "epoch": 12.217573221757322,
      "grad_norm": 0.002808790188282728,
      "learning_rate": 7.782426778242679e-06,
      "loss": 0.0013,
      "step": 11680
    },
    {
      "epoch": 12.228033472803347,
      "grad_norm": 0.050079986453056335,
      "learning_rate": 7.771966527196654e-06,
      "loss": 0.0042,
      "step": 11690
    },
    {
      "epoch": 12.238493723849372,
      "grad_norm": 0.30618271231651306,
      "learning_rate": 7.761506276150628e-06,
      "loss": 0.0032,
      "step": 11700
    },
    {
      "epoch": 12.248953974895397,
      "grad_norm": 0.0016608356963843107,
      "learning_rate": 7.751046025104603e-06,
      "loss": 0.0013,
      "step": 11710
    },
    {
      "epoch": 12.259414225941423,
      "grad_norm": 0.011516198515892029,
      "learning_rate": 7.740585774058578e-06,
      "loss": 0.0068,
      "step": 11720
    },
    {
      "epoch": 12.269874476987448,
      "grad_norm": 0.0029756007716059685,
      "learning_rate": 7.730125523012552e-06,
      "loss": 0.0056,
      "step": 11730
    },
    {
      "epoch": 12.280334728033473,
      "grad_norm": 0.06814765185117722,
      "learning_rate": 7.719665271966527e-06,
      "loss": 0.0016,
      "step": 11740
    },
    {
      "epoch": 12.290794979079498,
      "grad_norm": 0.0011318712495267391,
      "learning_rate": 7.709205020920504e-06,
      "loss": 0.0021,
      "step": 11750
    },
    {
      "epoch": 12.301255230125523,
      "grad_norm": 0.002164784586057067,
      "learning_rate": 7.698744769874478e-06,
      "loss": 0.0024,
      "step": 11760
    },
    {
      "epoch": 12.311715481171548,
      "grad_norm": 0.0012718596262857318,
      "learning_rate": 7.688284518828453e-06,
      "loss": 0.0015,
      "step": 11770
    },
    {
      "epoch": 12.322175732217573,
      "grad_norm": 0.596798837184906,
      "learning_rate": 7.677824267782428e-06,
      "loss": 0.0061,
      "step": 11780
    },
    {
      "epoch": 12.332635983263598,
      "grad_norm": 0.01687983050942421,
      "learning_rate": 7.667364016736402e-06,
      "loss": 0.0029,
      "step": 11790
    },
    {
      "epoch": 12.343096234309623,
      "grad_norm": 0.016435882076621056,
      "learning_rate": 7.656903765690377e-06,
      "loss": 0.0053,
      "step": 11800
    },
    {
      "epoch": 12.353556485355648,
      "grad_norm": 3.573029041290283,
      "learning_rate": 7.646443514644352e-06,
      "loss": 0.0125,
      "step": 11810
    },
    {
      "epoch": 12.364016736401673,
      "grad_norm": 10.997804641723633,
      "learning_rate": 7.635983263598326e-06,
      "loss": 0.0058,
      "step": 11820
    },
    {
      "epoch": 12.374476987447698,
      "grad_norm": 0.0025608104187995195,
      "learning_rate": 7.625523012552302e-06,
      "loss": 0.0014,
      "step": 11830
    },
    {
      "epoch": 12.384937238493723,
      "grad_norm": 0.004162598866969347,
      "learning_rate": 7.615062761506277e-06,
      "loss": 0.0026,
      "step": 11840
    },
    {
      "epoch": 12.395397489539748,
      "grad_norm": 0.003915916197001934,
      "learning_rate": 7.6046025104602514e-06,
      "loss": 0.0002,
      "step": 11850
    },
    {
      "epoch": 12.405857740585773,
      "grad_norm": 0.0011180732399225235,
      "learning_rate": 7.594142259414227e-06,
      "loss": 0.0008,
      "step": 11860
    },
    {
      "epoch": 12.4163179916318,
      "grad_norm": 0.0010106851113960147,
      "learning_rate": 7.583682008368202e-06,
      "loss": 0.0025,
      "step": 11870
    },
    {
      "epoch": 12.426778242677825,
      "grad_norm": 0.028459638357162476,
      "learning_rate": 7.573221757322176e-06,
      "loss": 0.0018,
      "step": 11880
    },
    {
      "epoch": 12.43723849372385,
      "grad_norm": 0.0029612730722874403,
      "learning_rate": 7.562761506276152e-06,
      "loss": 0.0005,
      "step": 11890
    },
    {
      "epoch": 12.447698744769875,
      "grad_norm": 0.0010238937102258205,
      "learning_rate": 7.552301255230127e-06,
      "loss": 0.0065,
      "step": 11900
    },
    {
      "epoch": 12.4581589958159,
      "grad_norm": 0.001289108651690185,
      "learning_rate": 7.5418410041841005e-06,
      "loss": 0.0035,
      "step": 11910
    },
    {
      "epoch": 12.468619246861925,
      "grad_norm": 0.52070152759552,
      "learning_rate": 7.531380753138075e-06,
      "loss": 0.0039,
      "step": 11920
    },
    {
      "epoch": 12.47907949790795,
      "grad_norm": 0.5998429656028748,
      "learning_rate": 7.520920502092051e-06,
      "loss": 0.009,
      "step": 11930
    },
    {
      "epoch": 12.489539748953975,
      "grad_norm": 0.0019885508809238672,
      "learning_rate": 7.5104602510460255e-06,
      "loss": 0.0022,
      "step": 11940
    },
    {
      "epoch": 12.5,
      "grad_norm": 0.2055187076330185,
      "learning_rate": 7.500000000000001e-06,
      "loss": 0.0023,
      "step": 11950
    },
    {
      "epoch": 12.510460251046025,
      "grad_norm": 0.021052544936537743,
      "learning_rate": 7.489539748953976e-06,
      "loss": 0.0024,
      "step": 11960
    },
    {
      "epoch": 12.52092050209205,
      "grad_norm": 0.0035545011050999165,
      "learning_rate": 7.4790794979079504e-06,
      "loss": 0.0009,
      "step": 11970
    },
    {
      "epoch": 12.531380753138075,
      "grad_norm": 0.04807558283209801,
      "learning_rate": 7.468619246861926e-06,
      "loss": 0.0018,
      "step": 11980
    },
    {
      "epoch": 12.5418410041841,
      "grad_norm": 0.2983674108982086,
      "learning_rate": 7.458158995815901e-06,
      "loss": 0.0008,
      "step": 11990
    },
    {
      "epoch": 12.552301255230125,
      "grad_norm": 0.011930989101529121,
      "learning_rate": 7.4476987447698746e-06,
      "loss": 0.0044,
      "step": 12000
    },
    {
      "epoch": 12.56276150627615,
      "grad_norm": 0.003968909382820129,
      "learning_rate": 7.437238493723849e-06,
      "loss": 0.0098,
      "step": 12010
    },
    {
      "epoch": 12.573221757322175,
      "grad_norm": 0.5258284211158752,
      "learning_rate": 7.426778242677825e-06,
      "loss": 0.0014,
      "step": 12020
    },
    {
      "epoch": 12.5836820083682,
      "grad_norm": 0.004405669867992401,
      "learning_rate": 7.4163179916317995e-06,
      "loss": 0.0087,
      "step": 12030
    },
    {
      "epoch": 12.594142259414227,
      "grad_norm": 0.06213514134287834,
      "learning_rate": 7.405857740585774e-06,
      "loss": 0.0058,
      "step": 12040
    },
    {
      "epoch": 12.604602510460252,
      "grad_norm": 0.0011400380171835423,
      "learning_rate": 7.39539748953975e-06,
      "loss": 0.0018,
      "step": 12050
    },
    {
      "epoch": 12.615062761506277,
      "grad_norm": 0.005104091949760914,
      "learning_rate": 7.3849372384937245e-06,
      "loss": 0.0002,
      "step": 12060
    },
    {
      "epoch": 12.625523012552302,
      "grad_norm": 0.0010607619769871235,
      "learning_rate": 7.374476987447699e-06,
      "loss": 0.0001,
      "step": 12070
    },
    {
      "epoch": 12.635983263598327,
      "grad_norm": 0.06945820897817612,
      "learning_rate": 7.364016736401675e-06,
      "loss": 0.0034,
      "step": 12080
    },
    {
      "epoch": 12.646443514644352,
      "grad_norm": 0.00619461527094245,
      "learning_rate": 7.3535564853556494e-06,
      "loss": 0.0049,
      "step": 12090
    },
    {
      "epoch": 12.656903765690377,
      "grad_norm": 0.6567928791046143,
      "learning_rate": 7.343096234309623e-06,
      "loss": 0.0046,
      "step": 12100
    },
    {
      "epoch": 12.667364016736402,
      "grad_norm": 0.0013644566060975194,
      "learning_rate": 7.332635983263599e-06,
      "loss": 0.0007,
      "step": 12110
    },
    {
      "epoch": 12.677824267782427,
      "grad_norm": 0.04108716920018196,
      "learning_rate": 7.3221757322175736e-06,
      "loss": 0.0091,
      "step": 12120
    },
    {
      "epoch": 12.688284518828452,
      "grad_norm": 0.011211246252059937,
      "learning_rate": 7.311715481171548e-06,
      "loss": 0.0025,
      "step": 12130
    },
    {
      "epoch": 12.698744769874477,
      "grad_norm": 0.626124918460846,
      "learning_rate": 7.301255230125524e-06,
      "loss": 0.0141,
      "step": 12140
    },
    {
      "epoch": 12.709205020920502,
      "grad_norm": 0.0036809996236115694,
      "learning_rate": 7.2907949790794985e-06,
      "loss": 0.0039,
      "step": 12150
    },
    {
      "epoch": 12.719665271966527,
      "grad_norm": 0.0020499294623732567,
      "learning_rate": 7.280334728033473e-06,
      "loss": 0.009,
      "step": 12160
    },
    {
      "epoch": 12.730125523012552,
      "grad_norm": 0.05238892138004303,
      "learning_rate": 7.269874476987449e-06,
      "loss": 0.0026,
      "step": 12170
    },
    {
      "epoch": 12.740585774058577,
      "grad_norm": 0.00744970329105854,
      "learning_rate": 7.2594142259414235e-06,
      "loss": 0.0004,
      "step": 12180
    },
    {
      "epoch": 12.751046025104603,
      "grad_norm": 0.003677712520584464,
      "learning_rate": 7.248953974895398e-06,
      "loss": 0.0004,
      "step": 12190
    },
    {
      "epoch": 12.761506276150628,
      "grad_norm": 0.009895882569253445,
      "learning_rate": 7.238493723849372e-06,
      "loss": 0.0028,
      "step": 12200
    },
    {
      "epoch": 12.771966527196653,
      "grad_norm": 0.17636838555335999,
      "learning_rate": 7.228033472803348e-06,
      "loss": 0.0012,
      "step": 12210
    },
    {
      "epoch": 12.782426778242678,
      "grad_norm": 0.00494078965857625,
      "learning_rate": 7.217573221757322e-06,
      "loss": 0.0012,
      "step": 12220
    },
    {
      "epoch": 12.792887029288703,
      "grad_norm": 0.0034601211082190275,
      "learning_rate": 7.207112970711298e-06,
      "loss": 0.0029,
      "step": 12230
    },
    {
      "epoch": 12.803347280334728,
      "grad_norm": 0.42673832178115845,
      "learning_rate": 7.1966527196652726e-06,
      "loss": 0.0013,
      "step": 12240
    },
    {
      "epoch": 12.813807531380753,
      "grad_norm": 0.001421024790033698,
      "learning_rate": 7.186192468619247e-06,
      "loss": 0.0018,
      "step": 12250
    },
    {
      "epoch": 12.824267782426778,
      "grad_norm": 0.0012018552515655756,
      "learning_rate": 7.175732217573223e-06,
      "loss": 0.0029,
      "step": 12260
    },
    {
      "epoch": 12.834728033472803,
      "grad_norm": 0.0032297931611537933,
      "learning_rate": 7.1652719665271975e-06,
      "loss": 0.0015,
      "step": 12270
    },
    {
      "epoch": 12.845188284518828,
      "grad_norm": 0.0022366091143339872,
      "learning_rate": 7.154811715481172e-06,
      "loss": 0.0035,
      "step": 12280
    },
    {
      "epoch": 12.855648535564853,
      "grad_norm": 0.002192625543102622,
      "learning_rate": 7.144351464435148e-06,
      "loss": 0.0016,
      "step": 12290
    },
    {
      "epoch": 12.866108786610878,
      "grad_norm": 0.0019106253748759627,
      "learning_rate": 7.133891213389122e-06,
      "loss": 0.0012,
      "step": 12300
    },
    {
      "epoch": 12.876569037656903,
      "grad_norm": 0.3598614037036896,
      "learning_rate": 7.123430962343096e-06,
      "loss": 0.002,
      "step": 12310
    },
    {
      "epoch": 12.887029288702928,
      "grad_norm": 0.609213650226593,
      "learning_rate": 7.112970711297071e-06,
      "loss": 0.0053,
      "step": 12320
    },
    {
      "epoch": 12.897489539748953,
      "grad_norm": 10.803927421569824,
      "learning_rate": 7.102510460251047e-06,
      "loss": 0.0048,
      "step": 12330
    },
    {
      "epoch": 12.907949790794978,
      "grad_norm": 0.5688014030456543,
      "learning_rate": 7.092050209205021e-06,
      "loss": 0.0058,
      "step": 12340
    },
    {
      "epoch": 12.918410041841003,
      "grad_norm": 0.0016045389929786325,
      "learning_rate": 7.081589958158996e-06,
      "loss": 0.0009,
      "step": 12350
    },
    {
      "epoch": 12.92887029288703,
      "grad_norm": 0.8344583511352539,
      "learning_rate": 7.0711297071129716e-06,
      "loss": 0.0046,
      "step": 12360
    },
    {
      "epoch": 12.939330543933055,
      "grad_norm": 0.260588675737381,
      "learning_rate": 7.060669456066946e-06,
      "loss": 0.0096,
      "step": 12370
    },
    {
      "epoch": 12.94979079497908,
      "grad_norm": 0.23156733810901642,
      "learning_rate": 7.050209205020922e-06,
      "loss": 0.0048,
      "step": 12380
    },
    {
      "epoch": 12.960251046025105,
      "grad_norm": 0.6258500218391418,
      "learning_rate": 7.0397489539748965e-06,
      "loss": 0.0056,
      "step": 12390
    },
    {
      "epoch": 12.97071129707113,
      "grad_norm": 0.0018335876520723104,
      "learning_rate": 7.02928870292887e-06,
      "loss": 0.0026,
      "step": 12400
    },
    {
      "epoch": 12.981171548117155,
      "grad_norm": 0.3342655599117279,
      "learning_rate": 7.018828451882845e-06,
      "loss": 0.001,
      "step": 12410
    },
    {
      "epoch": 12.99163179916318,
      "grad_norm": 0.003930902574211359,
      "learning_rate": 7.008368200836821e-06,
      "loss": 0.0063,
      "step": 12420
    },
    {
      "epoch": 13.0,
      "eval_accuracy": 0.9947154471544716,
      "eval_f1": 0.9769911504424779,
      "eval_loss": 0.020110856741666794,
      "eval_precision": 0.9722589167767504,
      "eval_recall": 0.9817696754112939,
      "eval_runtime": 322.7976,
      "eval_samples_per_second": 60.97,
      "eval_steps_per_second": 2.543,
      "step": 12428
    },
    {
      "epoch": 13.002092050209205,
      "grad_norm": 0.605671226978302,
      "learning_rate": 6.997907949790795e-06,
      "loss": 0.0011,
      "step": 12430
    },
    {
      "epoch": 13.01255230125523,
      "grad_norm": 0.0070203328505158424,
      "learning_rate": 6.98744769874477e-06,
      "loss": 0.0001,
      "step": 12440
    },
    {
      "epoch": 13.023012552301255,
      "grad_norm": 0.004600913263857365,
      "learning_rate": 6.976987447698746e-06,
      "loss": 0.0095,
      "step": 12450
    },
    {
      "epoch": 13.03347280334728,
      "grad_norm": 0.001342799630947411,
      "learning_rate": 6.96652719665272e-06,
      "loss": 0.0021,
      "step": 12460
    },
    {
      "epoch": 13.043933054393305,
      "grad_norm": 0.060664549469947815,
      "learning_rate": 6.956066945606695e-06,
      "loss": 0.0029,
      "step": 12470
    },
    {
      "epoch": 13.05439330543933,
      "grad_norm": 5.22585391998291,
      "learning_rate": 6.9456066945606706e-06,
      "loss": 0.0025,
      "step": 12480
    },
    {
      "epoch": 13.064853556485355,
      "grad_norm": 0.30163466930389404,
      "learning_rate": 6.935146443514645e-06,
      "loss": 0.0007,
      "step": 12490
    },
    {
      "epoch": 13.07531380753138,
      "grad_norm": 0.0018873180961236358,
      "learning_rate": 6.924686192468619e-06,
      "loss": 0.0007,
      "step": 12500
    },
    {
      "epoch": 13.085774058577407,
      "grad_norm": 0.005154725629836321,
      "learning_rate": 6.914225941422595e-06,
      "loss": 0.002,
      "step": 12510
    },
    {
      "epoch": 13.096234309623432,
      "grad_norm": 1.758294939994812,
      "learning_rate": 6.903765690376569e-06,
      "loss": 0.0141,
      "step": 12520
    },
    {
      "epoch": 13.106694560669457,
      "grad_norm": 0.0015110706444829702,
      "learning_rate": 6.893305439330544e-06,
      "loss": 0.0039,
      "step": 12530
    },
    {
      "epoch": 13.117154811715482,
      "grad_norm": 0.0010007591918110847,
      "learning_rate": 6.88284518828452e-06,
      "loss": 0.0053,
      "step": 12540
    },
    {
      "epoch": 13.127615062761507,
      "grad_norm": 0.007008938584476709,
      "learning_rate": 6.872384937238494e-06,
      "loss": 0.0014,
      "step": 12550
    },
    {
      "epoch": 13.138075313807532,
      "grad_norm": 0.0018285854021087289,
      "learning_rate": 6.861924686192469e-06,
      "loss": 0.0076,
      "step": 12560
    },
    {
      "epoch": 13.148535564853557,
      "grad_norm": 0.0033008253667503595,
      "learning_rate": 6.851464435146445e-06,
      "loss": 0.0013,
      "step": 12570
    },
    {
      "epoch": 13.158995815899582,
      "grad_norm": 0.3773033022880554,
      "learning_rate": 6.841004184100419e-06,
      "loss": 0.0038,
      "step": 12580
    },
    {
      "epoch": 13.169456066945607,
      "grad_norm": 0.001221735612489283,
      "learning_rate": 6.830543933054394e-06,
      "loss": 0.0026,
      "step": 12590
    },
    {
      "epoch": 13.179916317991632,
      "grad_norm": 0.0017715445719659328,
      "learning_rate": 6.820083682008368e-06,
      "loss": 0.0001,
      "step": 12600
    },
    {
      "epoch": 13.190376569037657,
      "grad_norm": 0.25778883695602417,
      "learning_rate": 6.809623430962343e-06,
      "loss": 0.0009,
      "step": 12610
    },
    {
      "epoch": 13.200836820083682,
      "grad_norm": 0.05460285022854805,
      "learning_rate": 6.799163179916318e-06,
      "loss": 0.0039,
      "step": 12620
    },
    {
      "epoch": 13.211297071129707,
      "grad_norm": 0.0009194862213917077,
      "learning_rate": 6.788702928870293e-06,
      "loss": 0.0006,
      "step": 12630
    },
    {
      "epoch": 13.221757322175732,
      "grad_norm": 0.0022349045611917973,
      "learning_rate": 6.778242677824268e-06,
      "loss": 0.0021,
      "step": 12640
    },
    {
      "epoch": 13.232217573221757,
      "grad_norm": 0.00139425543602556,
      "learning_rate": 6.767782426778243e-06,
      "loss": 0.0005,
      "step": 12650
    },
    {
      "epoch": 13.242677824267782,
      "grad_norm": 0.0020610718056559563,
      "learning_rate": 6.757322175732219e-06,
      "loss": 0.0045,
      "step": 12660
    },
    {
      "epoch": 13.253138075313808,
      "grad_norm": 0.002618654165416956,
      "learning_rate": 6.746861924686193e-06,
      "loss": 0.0031,
      "step": 12670
    },
    {
      "epoch": 13.263598326359833,
      "grad_norm": 0.0014691619435325265,
      "learning_rate": 6.736401673640168e-06,
      "loss": 0.0065,
      "step": 12680
    },
    {
      "epoch": 13.274058577405858,
      "grad_norm": 0.003964290022850037,
      "learning_rate": 6.725941422594144e-06,
      "loss": 0.0014,
      "step": 12690
    },
    {
      "epoch": 13.284518828451883,
      "grad_norm": 0.3325393497943878,
      "learning_rate": 6.7154811715481175e-06,
      "loss": 0.0017,
      "step": 12700
    },
    {
      "epoch": 13.294979079497908,
      "grad_norm": 0.00648903613910079,
      "learning_rate": 6.705020920502092e-06,
      "loss": 0.0044,
      "step": 12710
    },
    {
      "epoch": 13.305439330543933,
      "grad_norm": 0.0008060889667831361,
      "learning_rate": 6.694560669456067e-06,
      "loss": 0.0051,
      "step": 12720
    },
    {
      "epoch": 13.315899581589958,
      "grad_norm": 0.0028590112924575806,
      "learning_rate": 6.684100418410042e-06,
      "loss": 0.0027,
      "step": 12730
    },
    {
      "epoch": 13.326359832635983,
      "grad_norm": 4.4106621742248535,
      "learning_rate": 6.673640167364017e-06,
      "loss": 0.0082,
      "step": 12740
    },
    {
      "epoch": 13.336820083682008,
      "grad_norm": 1.7445534467697144,
      "learning_rate": 6.663179916317992e-06,
      "loss": 0.003,
      "step": 12750
    },
    {
      "epoch": 13.347280334728033,
      "grad_norm": 0.0021607556845992804,
      "learning_rate": 6.652719665271967e-06,
      "loss": 0.0027,
      "step": 12760
    },
    {
      "epoch": 13.357740585774058,
      "grad_norm": 0.018894124776124954,
      "learning_rate": 6.642259414225942e-06,
      "loss": 0.0017,
      "step": 12770
    },
    {
      "epoch": 13.368200836820083,
      "grad_norm": 0.023785412311553955,
      "learning_rate": 6.631799163179918e-06,
      "loss": 0.0033,
      "step": 12780
    },
    {
      "epoch": 13.378661087866108,
      "grad_norm": 0.006553844083100557,
      "learning_rate": 6.621338912133892e-06,
      "loss": 0.0017,
      "step": 12790
    },
    {
      "epoch": 13.389121338912133,
      "grad_norm": 0.0017925382126122713,
      "learning_rate": 6.610878661087866e-06,
      "loss": 0.0028,
      "step": 12800
    },
    {
      "epoch": 13.399581589958158,
      "grad_norm": 0.0023517559748142958,
      "learning_rate": 6.600418410041841e-06,
      "loss": 0.0005,
      "step": 12810
    },
    {
      "epoch": 13.410041841004183,
      "grad_norm": 0.0018283652607351542,
      "learning_rate": 6.5899581589958165e-06,
      "loss": 0.0007,
      "step": 12820
    },
    {
      "epoch": 13.42050209205021,
      "grad_norm": 0.0023282032925635576,
      "learning_rate": 6.579497907949791e-06,
      "loss": 0.0016,
      "step": 12830
    },
    {
      "epoch": 13.430962343096235,
      "grad_norm": 0.11187448352575302,
      "learning_rate": 6.569037656903766e-06,
      "loss": 0.0027,
      "step": 12840
    },
    {
      "epoch": 13.44142259414226,
      "grad_norm": 0.01974320225417614,
      "learning_rate": 6.558577405857741e-06,
      "loss": 0.0015,
      "step": 12850
    },
    {
      "epoch": 13.451882845188285,
      "grad_norm": 0.0012609426630660892,
      "learning_rate": 6.548117154811716e-06,
      "loss": 0.0052,
      "step": 12860
    },
    {
      "epoch": 13.46234309623431,
      "grad_norm": 0.0009454675018787384,
      "learning_rate": 6.537656903765691e-06,
      "loss": 0.0004,
      "step": 12870
    },
    {
      "epoch": 13.472803347280335,
      "grad_norm": 0.006622704677283764,
      "learning_rate": 6.527196652719666e-06,
      "loss": 0.0046,
      "step": 12880
    },
    {
      "epoch": 13.48326359832636,
      "grad_norm": 0.0007449856493622065,
      "learning_rate": 6.516736401673641e-06,
      "loss": 0.0,
      "step": 12890
    },
    {
      "epoch": 13.493723849372385,
      "grad_norm": 0.007118677254766226,
      "learning_rate": 6.506276150627615e-06,
      "loss": 0.0037,
      "step": 12900
    },
    {
      "epoch": 13.50418410041841,
      "grad_norm": 0.003054623957723379,
      "learning_rate": 6.4958158995815905e-06,
      "loss": 0.0012,
      "step": 12910
    },
    {
      "epoch": 13.514644351464435,
      "grad_norm": 0.000989457592368126,
      "learning_rate": 6.485355648535565e-06,
      "loss": 0.0021,
      "step": 12920
    },
    {
      "epoch": 13.52510460251046,
      "grad_norm": 0.2517068386077881,
      "learning_rate": 6.47489539748954e-06,
      "loss": 0.005,
      "step": 12930
    },
    {
      "epoch": 13.535564853556485,
      "grad_norm": 1.3756415843963623,
      "learning_rate": 6.4644351464435155e-06,
      "loss": 0.0006,
      "step": 12940
    },
    {
      "epoch": 13.54602510460251,
      "grad_norm": 0.001524770283140242,
      "learning_rate": 6.45397489539749e-06,
      "loss": 0.002,
      "step": 12950
    },
    {
      "epoch": 13.556485355648535,
      "grad_norm": 2.586544990539551,
      "learning_rate": 6.443514644351465e-06,
      "loss": 0.0016,
      "step": 12960
    },
    {
      "epoch": 13.56694560669456,
      "grad_norm": 0.0013255849480628967,
      "learning_rate": 6.43305439330544e-06,
      "loss": 0.0024,
      "step": 12970
    },
    {
      "epoch": 13.577405857740585,
      "grad_norm": 0.0004863772774115205,
      "learning_rate": 6.422594142259415e-06,
      "loss": 0.0041,
      "step": 12980
    },
    {
      "epoch": 13.58786610878661,
      "grad_norm": 0.4886970818042755,
      "learning_rate": 6.41213389121339e-06,
      "loss": 0.0027,
      "step": 12990
    },
    {
      "epoch": 13.598326359832637,
      "grad_norm": 0.0007132375612854958,
      "learning_rate": 6.401673640167364e-06,
      "loss": 0.004,
      "step": 13000
    },
    {
      "epoch": 13.608786610878662,
      "grad_norm": 0.0005758629413321614,
      "learning_rate": 6.391213389121339e-06,
      "loss": 0.0125,
      "step": 13010
    },
    {
      "epoch": 13.619246861924687,
      "grad_norm": 0.35238948464393616,
      "learning_rate": 6.380753138075314e-06,
      "loss": 0.0027,
      "step": 13020
    },
    {
      "epoch": 13.629707112970712,
      "grad_norm": 0.0008598401327617466,
      "learning_rate": 6.370292887029289e-06,
      "loss": 0.0019,
      "step": 13030
    },
    {
      "epoch": 13.640167364016737,
      "grad_norm": 0.0006427425541915,
      "learning_rate": 6.359832635983264e-06,
      "loss": 0.0032,
      "step": 13040
    },
    {
      "epoch": 13.650627615062762,
      "grad_norm": 1.1764545440673828,
      "learning_rate": 6.349372384937239e-06,
      "loss": 0.0151,
      "step": 13050
    },
    {
      "epoch": 13.661087866108787,
      "grad_norm": 0.5929921269416809,
      "learning_rate": 6.3389121338912145e-06,
      "loss": 0.0041,
      "step": 13060
    },
    {
      "epoch": 13.671548117154812,
      "grad_norm": 7.402414321899414,
      "learning_rate": 6.328451882845189e-06,
      "loss": 0.0068,
      "step": 13070
    },
    {
      "epoch": 13.682008368200837,
      "grad_norm": 0.0030852353665977716,
      "learning_rate": 6.317991631799164e-06,
      "loss": 0.0037,
      "step": 13080
    },
    {
      "epoch": 13.692468619246862,
      "grad_norm": 0.0014233195688575506,
      "learning_rate": 6.307531380753139e-06,
      "loss": 0.0002,
      "step": 13090
    },
    {
      "epoch": 13.702928870292887,
      "grad_norm": 0.0022801540326327085,
      "learning_rate": 6.297071129707113e-06,
      "loss": 0.0026,
      "step": 13100
    },
    {
      "epoch": 13.713389121338912,
      "grad_norm": 0.017168721184134483,
      "learning_rate": 6.286610878661088e-06,
      "loss": 0.0013,
      "step": 13110
    },
    {
      "epoch": 13.723849372384937,
      "grad_norm": 0.001709559466689825,
      "learning_rate": 6.276150627615063e-06,
      "loss": 0.0027,
      "step": 13120
    },
    {
      "epoch": 13.734309623430962,
      "grad_norm": 0.0037536779418587685,
      "learning_rate": 6.265690376569038e-06,
      "loss": 0.0004,
      "step": 13130
    },
    {
      "epoch": 13.744769874476987,
      "grad_norm": 0.002535173436626792,
      "learning_rate": 6.255230125523013e-06,
      "loss": 0.0008,
      "step": 13140
    },
    {
      "epoch": 13.755230125523013,
      "grad_norm": 0.01181229017674923,
      "learning_rate": 6.244769874476988e-06,
      "loss": 0.0012,
      "step": 13150
    },
    {
      "epoch": 13.765690376569038,
      "grad_norm": 0.003369465470314026,
      "learning_rate": 6.234309623430963e-06,
      "loss": 0.0048,
      "step": 13160
    },
    {
      "epoch": 13.776150627615063,
      "grad_norm": 0.003049363847821951,
      "learning_rate": 6.223849372384938e-06,
      "loss": 0.002,
      "step": 13170
    },
    {
      "epoch": 13.786610878661088,
      "grad_norm": 0.0019957234617322683,
      "learning_rate": 6.213389121338913e-06,
      "loss": 0.0009,
      "step": 13180
    },
    {
      "epoch": 13.797071129707113,
      "grad_norm": 0.0033129402436316013,
      "learning_rate": 6.202928870292888e-06,
      "loss": 0.0036,
      "step": 13190
    },
    {
      "epoch": 13.807531380753138,
      "grad_norm": 0.004000755492597818,
      "learning_rate": 6.192468619246862e-06,
      "loss": 0.006,
      "step": 13200
    },
    {
      "epoch": 13.817991631799163,
      "grad_norm": 0.020137613639235497,
      "learning_rate": 6.182008368200837e-06,
      "loss": 0.0028,
      "step": 13210
    },
    {
      "epoch": 13.828451882845188,
      "grad_norm": 0.001349608413875103,
      "learning_rate": 6.171548117154812e-06,
      "loss": 0.0074,
      "step": 13220
    },
    {
      "epoch": 13.838912133891213,
      "grad_norm": 0.0018174723954871297,
      "learning_rate": 6.161087866108787e-06,
      "loss": 0.0006,
      "step": 13230
    },
    {
      "epoch": 13.849372384937238,
      "grad_norm": 0.0015284770634025335,
      "learning_rate": 6.150627615062762e-06,
      "loss": 0.0011,
      "step": 13240
    },
    {
      "epoch": 13.859832635983263,
      "grad_norm": 0.011408326216042042,
      "learning_rate": 6.140167364016737e-06,
      "loss": 0.0004,
      "step": 13250
    },
    {
      "epoch": 13.870292887029288,
      "grad_norm": 0.008589541539549828,
      "learning_rate": 6.129707112970712e-06,
      "loss": 0.0025,
      "step": 13260
    },
    {
      "epoch": 13.880753138075313,
      "grad_norm": 0.0007094620377756655,
      "learning_rate": 6.119246861924687e-06,
      "loss": 0.0011,
      "step": 13270
    },
    {
      "epoch": 13.891213389121338,
      "grad_norm": 0.0013950098073109984,
      "learning_rate": 6.108786610878662e-06,
      "loss": 0.0011,
      "step": 13280
    },
    {
      "epoch": 13.901673640167363,
      "grad_norm": 0.0059870462864637375,
      "learning_rate": 6.098326359832637e-06,
      "loss": 0.0028,
      "step": 13290
    },
    {
      "epoch": 13.91213389121339,
      "grad_norm": 0.033877111971378326,
      "learning_rate": 6.087866108786611e-06,
      "loss": 0.004,
      "step": 13300
    },
    {
      "epoch": 13.922594142259415,
      "grad_norm": 0.0006154275615699589,
      "learning_rate": 6.0774058577405855e-06,
      "loss": 0.0035,
      "step": 13310
    },
    {
      "epoch": 13.93305439330544,
      "grad_norm": 0.0016889638500288129,
      "learning_rate": 6.066945606694561e-06,
      "loss": 0.0012,
      "step": 13320
    },
    {
      "epoch": 13.943514644351465,
      "grad_norm": 0.001057060551829636,
      "learning_rate": 6.056485355648536e-06,
      "loss": 0.0014,
      "step": 13330
    },
    {
      "epoch": 13.95397489539749,
      "grad_norm": 0.01355618704110384,
      "learning_rate": 6.046025104602511e-06,
      "loss": 0.001,
      "step": 13340
    },
    {
      "epoch": 13.964435146443515,
      "grad_norm": 0.0005233267438597977,
      "learning_rate": 6.035564853556486e-06,
      "loss": 0.0034,
      "step": 13350
    },
    {
      "epoch": 13.97489539748954,
      "grad_norm": 0.10582322627305984,
      "learning_rate": 6.025104602510461e-06,
      "loss": 0.0031,
      "step": 13360
    },
    {
      "epoch": 13.985355648535565,
      "grad_norm": 0.014014857821166515,
      "learning_rate": 6.014644351464436e-06,
      "loss": 0.0014,
      "step": 13370
    },
    {
      "epoch": 13.99581589958159,
      "grad_norm": 0.0033130908850580454,
      "learning_rate": 6.004184100418411e-06,
      "loss": 0.0001,
      "step": 13380
    },
    {
      "epoch": 14.0,
      "eval_accuracy": 0.9947662601626016,
      "eval_f1": 0.9772174297721744,
      "eval_loss": 0.02212991565465927,
      "eval_precision": 0.9722711267605634,
      "eval_recall": 0.9822143174744331,
      "eval_runtime": 323.5429,
      "eval_samples_per_second": 60.83,
      "eval_steps_per_second": 2.538,
      "step": 13384
    },
    {
      "epoch": 14.006276150627615,
      "grad_norm": 0.0031371084041893482,
      "learning_rate": 5.993723849372386e-06,
      "loss": 0.001,
      "step": 13390
    },
    {
      "epoch": 14.01673640167364,
      "grad_norm": 0.0036308260168880224,
      "learning_rate": 5.9832635983263595e-06,
      "loss": 0.0011,
      "step": 13400
    },
    {
      "epoch": 14.027196652719665,
      "grad_norm": 0.0065322862938046455,
      "learning_rate": 5.972803347280335e-06,
      "loss": 0.0031,
      "step": 13410
    },
    {
      "epoch": 14.03765690376569,
      "grad_norm": 0.0013774767285212874,
      "learning_rate": 5.96234309623431e-06,
      "loss": 0.002,
      "step": 13420
    },
    {
      "epoch": 14.048117154811715,
      "grad_norm": 0.0020115345250815153,
      "learning_rate": 5.9518828451882845e-06,
      "loss": 0.0038,
      "step": 13430
    },
    {
      "epoch": 14.05857740585774,
      "grad_norm": 0.001279950374737382,
      "learning_rate": 5.94142259414226e-06,
      "loss": 0.0001,
      "step": 13440
    },
    {
      "epoch": 14.069037656903765,
      "grad_norm": 0.0013200989924371243,
      "learning_rate": 5.930962343096235e-06,
      "loss": 0.0002,
      "step": 13450
    },
    {
      "epoch": 14.07949790794979,
      "grad_norm": 0.0022228690795600414,
      "learning_rate": 5.92050209205021e-06,
      "loss": 0.0016,
      "step": 13460
    },
    {
      "epoch": 14.089958158995817,
      "grad_norm": 0.001049719867296517,
      "learning_rate": 5.910041841004185e-06,
      "loss": 0.0052,
      "step": 13470
    },
    {
      "epoch": 14.100418410041842,
      "grad_norm": 0.2946113646030426,
      "learning_rate": 5.89958158995816e-06,
      "loss": 0.0021,
      "step": 13480
    },
    {
      "epoch": 14.110878661087867,
      "grad_norm": 3.028658390045166,
      "learning_rate": 5.889121338912135e-06,
      "loss": 0.0034,
      "step": 13490
    },
    {
      "epoch": 14.121338912133892,
      "grad_norm": 0.4276787042617798,
      "learning_rate": 5.878661087866109e-06,
      "loss": 0.0013,
      "step": 13500
    },
    {
      "epoch": 14.131799163179917,
      "grad_norm": 5.9522786140441895,
      "learning_rate": 5.868200836820084e-06,
      "loss": 0.0031,
      "step": 13510
    },
    {
      "epoch": 14.142259414225942,
      "grad_norm": 0.0014140094863250852,
      "learning_rate": 5.8577405857740585e-06,
      "loss": 0.0017,
      "step": 13520
    },
    {
      "epoch": 14.152719665271967,
      "grad_norm": 0.0010538558708503842,
      "learning_rate": 5.847280334728034e-06,
      "loss": 0.0025,
      "step": 13530
    },
    {
      "epoch": 14.163179916317992,
      "grad_norm": 0.4792097508907318,
      "learning_rate": 5.836820083682009e-06,
      "loss": 0.002,
      "step": 13540
    },
    {
      "epoch": 14.173640167364017,
      "grad_norm": 0.0013817066792398691,
      "learning_rate": 5.8263598326359835e-06,
      "loss": 0.0,
      "step": 13550
    },
    {
      "epoch": 14.184100418410042,
      "grad_norm": 0.33961021900177,
      "learning_rate": 5.815899581589959e-06,
      "loss": 0.0047,
      "step": 13560
    },
    {
      "epoch": 14.194560669456067,
      "grad_norm": 0.001404859242029488,
      "learning_rate": 5.805439330543934e-06,
      "loss": 0.0021,
      "step": 13570
    },
    {
      "epoch": 14.205020920502092,
      "grad_norm": 0.5644888877868652,
      "learning_rate": 5.7949790794979084e-06,
      "loss": 0.003,
      "step": 13580
    },
    {
      "epoch": 14.215481171548117,
      "grad_norm": 3.0258333683013916,
      "learning_rate": 5.784518828451884e-06,
      "loss": 0.0047,
      "step": 13590
    },
    {
      "epoch": 14.225941422594142,
      "grad_norm": 0.0003920638409908861,
      "learning_rate": 5.774058577405858e-06,
      "loss": 0.0029,
      "step": 13600
    },
    {
      "epoch": 14.236401673640167,
      "grad_norm": 0.00026694650296121836,
      "learning_rate": 5.7635983263598325e-06,
      "loss": 0.0016,
      "step": 13610
    },
    {
      "epoch": 14.246861924686192,
      "grad_norm": 0.00035247058258391917,
      "learning_rate": 5.753138075313808e-06,
      "loss": 0.002,
      "step": 13620
    },
    {
      "epoch": 14.257322175732218,
      "grad_norm": 0.04079865291714668,
      "learning_rate": 5.742677824267783e-06,
      "loss": 0.002,
      "step": 13630
    },
    {
      "epoch": 14.267782426778243,
      "grad_norm": 0.7560418844223022,
      "learning_rate": 5.7322175732217575e-06,
      "loss": 0.0028,
      "step": 13640
    },
    {
      "epoch": 14.278242677824268,
      "grad_norm": 0.5718390941619873,
      "learning_rate": 5.721757322175733e-06,
      "loss": 0.0036,
      "step": 13650
    },
    {
      "epoch": 14.288702928870293,
      "grad_norm": 0.005060538649559021,
      "learning_rate": 5.711297071129708e-06,
      "loss": 0.0027,
      "step": 13660
    },
    {
      "epoch": 14.299163179916318,
      "grad_norm": 1.9312744140625,
      "learning_rate": 5.7008368200836825e-06,
      "loss": 0.0011,
      "step": 13670
    },
    {
      "epoch": 14.309623430962343,
      "grad_norm": 0.0014884712873026729,
      "learning_rate": 5.690376569037658e-06,
      "loss": 0.0032,
      "step": 13680
    },
    {
      "epoch": 14.320083682008368,
      "grad_norm": 0.04901024326682091,
      "learning_rate": 5.679916317991633e-06,
      "loss": 0.0,
      "step": 13690
    },
    {
      "epoch": 14.330543933054393,
      "grad_norm": 0.004444244783371687,
      "learning_rate": 5.669456066945607e-06,
      "loss": 0.0012,
      "step": 13700
    },
    {
      "epoch": 14.341004184100418,
      "grad_norm": 0.008762485347688198,
      "learning_rate": 5.658995815899581e-06,
      "loss": 0.0047,
      "step": 13710
    },
    {
      "epoch": 14.351464435146443,
      "grad_norm": 0.007436207961291075,
      "learning_rate": 5.648535564853557e-06,
      "loss": 0.0036,
      "step": 13720
    },
    {
      "epoch": 14.361924686192468,
      "grad_norm": 0.07152541726827621,
      "learning_rate": 5.6380753138075315e-06,
      "loss": 0.0013,
      "step": 13730
    },
    {
      "epoch": 14.372384937238493,
      "grad_norm": 0.39643150568008423,
      "learning_rate": 5.627615062761507e-06,
      "loss": 0.0014,
      "step": 13740
    },
    {
      "epoch": 14.382845188284518,
      "grad_norm": 0.0006901665474288166,
      "learning_rate": 5.617154811715482e-06,
      "loss": 0.0,
      "step": 13750
    },
    {
      "epoch": 14.393305439330543,
      "grad_norm": 0.00199110247194767,
      "learning_rate": 5.6066945606694565e-06,
      "loss": 0.0012,
      "step": 13760
    },
    {
      "epoch": 14.403765690376568,
      "grad_norm": 0.001676549087278545,
      "learning_rate": 5.596234309623432e-06,
      "loss": 0.0063,
      "step": 13770
    },
    {
      "epoch": 14.414225941422593,
      "grad_norm": 6.960324764251709,
      "learning_rate": 5.585774058577407e-06,
      "loss": 0.0026,
      "step": 13780
    },
    {
      "epoch": 14.42468619246862,
      "grad_norm": 0.0005126834730617702,
      "learning_rate": 5.5753138075313815e-06,
      "loss": 0.003,
      "step": 13790
    },
    {
      "epoch": 14.435146443514645,
      "grad_norm": 0.0015569363022223115,
      "learning_rate": 5.564853556485355e-06,
      "loss": 0.001,
      "step": 13800
    },
    {
      "epoch": 14.44560669456067,
      "grad_norm": 0.5067769885063171,
      "learning_rate": 5.554393305439331e-06,
      "loss": 0.0015,
      "step": 13810
    },
    {
      "epoch": 14.456066945606695,
      "grad_norm": 0.011236051097512245,
      "learning_rate": 5.543933054393306e-06,
      "loss": 0.0013,
      "step": 13820
    },
    {
      "epoch": 14.46652719665272,
      "grad_norm": 0.0012296204222366214,
      "learning_rate": 5.53347280334728e-06,
      "loss": 0.0018,
      "step": 13830
    },
    {
      "epoch": 14.476987447698745,
      "grad_norm": 0.000714982277713716,
      "learning_rate": 5.523012552301256e-06,
      "loss": 0.0008,
      "step": 13840
    },
    {
      "epoch": 14.48744769874477,
      "grad_norm": 0.43358007073402405,
      "learning_rate": 5.5125523012552305e-06,
      "loss": 0.004,
      "step": 13850
    },
    {
      "epoch": 14.497907949790795,
      "grad_norm": 0.00407668249681592,
      "learning_rate": 5.502092050209205e-06,
      "loss": 0.0007,
      "step": 13860
    },
    {
      "epoch": 14.50836820083682,
      "grad_norm": 0.001705107861198485,
      "learning_rate": 5.491631799163181e-06,
      "loss": 0.0065,
      "step": 13870
    },
    {
      "epoch": 14.518828451882845,
      "grad_norm": 0.23019395768642426,
      "learning_rate": 5.4811715481171555e-06,
      "loss": 0.0024,
      "step": 13880
    },
    {
      "epoch": 14.52928870292887,
      "grad_norm": 0.0005585121689364314,
      "learning_rate": 5.470711297071131e-06,
      "loss": 0.0013,
      "step": 13890
    },
    {
      "epoch": 14.539748953974895,
      "grad_norm": 0.000583793327677995,
      "learning_rate": 5.460251046025105e-06,
      "loss": 0.005,
      "step": 13900
    },
    {
      "epoch": 14.55020920502092,
      "grad_norm": 0.001130367862060666,
      "learning_rate": 5.44979079497908e-06,
      "loss": 0.0111,
      "step": 13910
    },
    {
      "epoch": 14.560669456066945,
      "grad_norm": 0.027170445770025253,
      "learning_rate": 5.439330543933054e-06,
      "loss": 0.0016,
      "step": 13920
    },
    {
      "epoch": 14.57112970711297,
      "grad_norm": 0.0005414853803813457,
      "learning_rate": 5.42887029288703e-06,
      "loss": 0.01,
      "step": 13930
    },
    {
      "epoch": 14.581589958158997,
      "grad_norm": 0.0018199034966528416,
      "learning_rate": 5.418410041841005e-06,
      "loss": 0.0025,
      "step": 13940
    },
    {
      "epoch": 14.592050209205022,
      "grad_norm": 0.0012890711659565568,
      "learning_rate": 5.407949790794979e-06,
      "loss": 0.0011,
      "step": 13950
    },
    {
      "epoch": 14.602510460251047,
      "grad_norm": 0.00326856249012053,
      "learning_rate": 5.397489539748955e-06,
      "loss": 0.0016,
      "step": 13960
    },
    {
      "epoch": 14.612970711297072,
      "grad_norm": 0.45731207728385925,
      "learning_rate": 5.3870292887029296e-06,
      "loss": 0.0045,
      "step": 13970
    },
    {
      "epoch": 14.623430962343097,
      "grad_norm": 0.6211620569229126,
      "learning_rate": 5.376569037656904e-06,
      "loss": 0.0013,
      "step": 13980
    },
    {
      "epoch": 14.633891213389122,
      "grad_norm": 0.0007421728223562241,
      "learning_rate": 5.36610878661088e-06,
      "loss": 0.0055,
      "step": 13990
    },
    {
      "epoch": 14.644351464435147,
      "grad_norm": 0.7421160340309143,
      "learning_rate": 5.355648535564854e-06,
      "loss": 0.003,
      "step": 14000
    },
    {
      "epoch": 14.654811715481172,
      "grad_norm": 0.006070103496313095,
      "learning_rate": 5.345188284518828e-06,
      "loss": 0.0019,
      "step": 14010
    },
    {
      "epoch": 14.665271966527197,
      "grad_norm": 0.5618448853492737,
      "learning_rate": 5.334728033472804e-06,
      "loss": 0.004,
      "step": 14020
    },
    {
      "epoch": 14.675732217573222,
      "grad_norm": 0.1607927829027176,
      "learning_rate": 5.324267782426779e-06,
      "loss": 0.0014,
      "step": 14030
    },
    {
      "epoch": 14.686192468619247,
      "grad_norm": 0.005318174604326487,
      "learning_rate": 5.313807531380753e-06,
      "loss": 0.0009,
      "step": 14040
    },
    {
      "epoch": 14.696652719665272,
      "grad_norm": 0.0003765782166738063,
      "learning_rate": 5.303347280334729e-06,
      "loss": 0.0006,
      "step": 14050
    },
    {
      "epoch": 14.707112970711297,
      "grad_norm": 0.016472335904836655,
      "learning_rate": 5.292887029288704e-06,
      "loss": 0.0021,
      "step": 14060
    },
    {
      "epoch": 14.717573221757322,
      "grad_norm": 0.1394435614347458,
      "learning_rate": 5.282426778242678e-06,
      "loss": 0.0001,
      "step": 14070
    },
    {
      "epoch": 14.728033472803347,
      "grad_norm": 0.0005229823873378336,
      "learning_rate": 5.271966527196654e-06,
      "loss": 0.0028,
      "step": 14080
    },
    {
      "epoch": 14.738493723849372,
      "grad_norm": 0.003450467949733138,
      "learning_rate": 5.2615062761506286e-06,
      "loss": 0.0023,
      "step": 14090
    },
    {
      "epoch": 14.748953974895397,
      "grad_norm": 0.3207431435585022,
      "learning_rate": 5.251046025104602e-06,
      "loss": 0.0061,
      "step": 14100
    },
    {
      "epoch": 14.759414225941423,
      "grad_norm": 0.0005819759680889547,
      "learning_rate": 5.240585774058577e-06,
      "loss": 0.0012,
      "step": 14110
    },
    {
      "epoch": 14.769874476987448,
      "grad_norm": 0.6872633695602417,
      "learning_rate": 5.230125523012553e-06,
      "loss": 0.0033,
      "step": 14120
    },
    {
      "epoch": 14.780334728033473,
      "grad_norm": 0.08406958729028702,
      "learning_rate": 5.219665271966527e-06,
      "loss": 0.0014,
      "step": 14130
    },
    {
      "epoch": 14.790794979079498,
      "grad_norm": 0.0007906105020083487,
      "learning_rate": 5.209205020920503e-06,
      "loss": 0.0014,
      "step": 14140
    },
    {
      "epoch": 14.801255230125523,
      "grad_norm": 0.0023249853402376175,
      "learning_rate": 5.198744769874478e-06,
      "loss": 0.001,
      "step": 14150
    },
    {
      "epoch": 14.811715481171548,
      "grad_norm": 0.8500617146492004,
      "learning_rate": 5.188284518828452e-06,
      "loss": 0.0039,
      "step": 14160
    },
    {
      "epoch": 14.822175732217573,
      "grad_norm": 0.0008029480231925845,
      "learning_rate": 5.177824267782428e-06,
      "loss": 0.0,
      "step": 14170
    },
    {
      "epoch": 14.832635983263598,
      "grad_norm": 0.0023978573735803366,
      "learning_rate": 5.167364016736403e-06,
      "loss": 0.0024,
      "step": 14180
    },
    {
      "epoch": 14.843096234309623,
      "grad_norm": 0.000537186162546277,
      "learning_rate": 5.156903765690377e-06,
      "loss": 0.0023,
      "step": 14190
    },
    {
      "epoch": 14.853556485355648,
      "grad_norm": 0.00040082159102894366,
      "learning_rate": 5.146443514644351e-06,
      "loss": 0.0022,
      "step": 14200
    },
    {
      "epoch": 14.864016736401673,
      "grad_norm": 0.0005540752317756414,
      "learning_rate": 5.135983263598327e-06,
      "loss": 0.0007,
      "step": 14210
    },
    {
      "epoch": 14.874476987447698,
      "grad_norm": 0.43565380573272705,
      "learning_rate": 5.125523012552301e-06,
      "loss": 0.0012,
      "step": 14220
    },
    {
      "epoch": 14.884937238493723,
      "grad_norm": 0.0007911826833151281,
      "learning_rate": 5.115062761506276e-06,
      "loss": 0.0022,
      "step": 14230
    },
    {
      "epoch": 14.895397489539748,
      "grad_norm": 0.9313806891441345,
      "learning_rate": 5.104602510460252e-06,
      "loss": 0.0033,
      "step": 14240
    },
    {
      "epoch": 14.905857740585773,
      "grad_norm": 0.0010078164050355554,
      "learning_rate": 5.094142259414226e-06,
      "loss": 0.0006,
      "step": 14250
    },
    {
      "epoch": 14.9163179916318,
      "grad_norm": 3.1442158222198486,
      "learning_rate": 5.083682008368201e-06,
      "loss": 0.0007,
      "step": 14260
    },
    {
      "epoch": 14.926778242677825,
      "grad_norm": 0.6077865958213806,
      "learning_rate": 5.073221757322177e-06,
      "loss": 0.0024,
      "step": 14270
    },
    {
      "epoch": 14.93723849372385,
      "grad_norm": 0.3407031297683716,
      "learning_rate": 5.062761506276151e-06,
      "loss": 0.0017,
      "step": 14280
    },
    {
      "epoch": 14.947698744769875,
      "grad_norm": 0.003608366474509239,
      "learning_rate": 5.052301255230127e-06,
      "loss": 0.0013,
      "step": 14290
    },
    {
      "epoch": 14.9581589958159,
      "grad_norm": 0.000251743447734043,
      "learning_rate": 5.041841004184101e-06,
      "loss": 0.0083,
      "step": 14300
    },
    {
      "epoch": 14.968619246861925,
      "grad_norm": 0.773269534111023,
      "learning_rate": 5.0313807531380754e-06,
      "loss": 0.0091,
      "step": 14310
    },
    {
      "epoch": 14.97907949790795,
      "grad_norm": 0.0005830649752169847,
      "learning_rate": 5.02092050209205e-06,
      "loss": 0.0048,
      "step": 14320
    },
    {
      "epoch": 14.989539748953975,
      "grad_norm": 0.0005461298278532922,
      "learning_rate": 5.010460251046026e-06,
      "loss": 0.0089,
      "step": 14330
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.0010703119914978743,
      "learning_rate": 5e-06,
      "loss": 0.0005,
      "step": 14340
    },
    {
      "epoch": 15.0,
      "eval_accuracy": 0.9945630081300812,
      "eval_f1": 0.9763953231855282,
      "eval_loss": 0.02275431528687477,
      "eval_precision": 0.9689141856392294,
      "eval_recall": 0.9839928857269897,
      "eval_runtime": 323.4441,
      "eval_samples_per_second": 60.848,
      "eval_steps_per_second": 2.538,
      "step": 14340
    },
    {
      "epoch": 15.010460251046025,
      "grad_norm": 0.005795057397335768,
      "learning_rate": 4.989539748953975e-06,
      "loss": 0.0009,
      "step": 14350
    },
    {
      "epoch": 15.02092050209205,
      "grad_norm": 0.019781820476055145,
      "learning_rate": 4.979079497907951e-06,
      "loss": 0.0006,
      "step": 14360
    },
    {
      "epoch": 15.031380753138075,
      "grad_norm": 0.14051848649978638,
      "learning_rate": 4.9686192468619245e-06,
      "loss": 0.0023,
      "step": 14370
    },
    {
      "epoch": 15.0418410041841,
      "grad_norm": 0.00048534810775890946,
      "learning_rate": 4.9581589958159e-06,
      "loss": 0.0007,
      "step": 14380
    },
    {
      "epoch": 15.052301255230125,
      "grad_norm": 0.00066621124278754,
      "learning_rate": 4.947698744769875e-06,
      "loss": 0.0013,
      "step": 14390
    },
    {
      "epoch": 15.06276150627615,
      "grad_norm": 0.000377496617147699,
      "learning_rate": 4.9372384937238495e-06,
      "loss": 0.0049,
      "step": 14400
    },
    {
      "epoch": 15.073221757322175,
      "grad_norm": 0.015212414786219597,
      "learning_rate": 4.926778242677825e-06,
      "loss": 0.0006,
      "step": 14410
    },
    {
      "epoch": 15.0836820083682,
      "grad_norm": 0.4813319742679596,
      "learning_rate": 4.9163179916318e-06,
      "loss": 0.0065,
      "step": 14420
    },
    {
      "epoch": 15.094142259414227,
      "grad_norm": 0.000471452804049477,
      "learning_rate": 4.9058577405857744e-06,
      "loss": 0.0014,
      "step": 14430
    },
    {
      "epoch": 15.104602510460252,
      "grad_norm": 0.2828526198863983,
      "learning_rate": 4.895397489539749e-06,
      "loss": 0.0015,
      "step": 14440
    },
    {
      "epoch": 15.115062761506277,
      "grad_norm": 0.000352174392901361,
      "learning_rate": 4.884937238493725e-06,
      "loss": 0.0017,
      "step": 14450
    },
    {
      "epoch": 15.125523012552302,
      "grad_norm": 0.0014755212469026446,
      "learning_rate": 4.874476987447699e-06,
      "loss": 0.0009,
      "step": 14460
    },
    {
      "epoch": 15.135983263598327,
      "grad_norm": 0.0005398356588557363,
      "learning_rate": 4.864016736401674e-06,
      "loss": 0.0012,
      "step": 14470
    },
    {
      "epoch": 15.146443514644352,
      "grad_norm": 0.003156506922096014,
      "learning_rate": 4.853556485355649e-06,
      "loss": 0.0001,
      "step": 14480
    },
    {
      "epoch": 15.156903765690377,
      "grad_norm": 0.3020938038825989,
      "learning_rate": 4.8430962343096235e-06,
      "loss": 0.0042,
      "step": 14490
    },
    {
      "epoch": 15.167364016736402,
      "grad_norm": 0.0007261274731718004,
      "learning_rate": 4.832635983263599e-06,
      "loss": 0.0012,
      "step": 14500
    },
    {
      "epoch": 15.177824267782427,
      "grad_norm": 0.0015004202723503113,
      "learning_rate": 4.822175732217574e-06,
      "loss": 0.0019,
      "step": 14510
    },
    {
      "epoch": 15.188284518828452,
      "grad_norm": 0.008441919460892677,
      "learning_rate": 4.8117154811715485e-06,
      "loss": 0.004,
      "step": 14520
    },
    {
      "epoch": 15.198744769874477,
      "grad_norm": 0.0006186789250932634,
      "learning_rate": 4.801255230125523e-06,
      "loss": 0.0008,
      "step": 14530
    },
    {
      "epoch": 15.209205020920502,
      "grad_norm": 0.0002489527396392077,
      "learning_rate": 4.790794979079498e-06,
      "loss": 0.0058,
      "step": 14540
    },
    {
      "epoch": 15.219665271966527,
      "grad_norm": 0.0005246507935225964,
      "learning_rate": 4.7803347280334734e-06,
      "loss": 0.0093,
      "step": 14550
    },
    {
      "epoch": 15.230125523012552,
      "grad_norm": 0.0021076519042253494,
      "learning_rate": 4.769874476987448e-06,
      "loss": 0.0014,
      "step": 14560
    },
    {
      "epoch": 15.240585774058577,
      "grad_norm": 0.5976952314376831,
      "learning_rate": 4.759414225941423e-06,
      "loss": 0.0065,
      "step": 14570
    },
    {
      "epoch": 15.251046025104603,
      "grad_norm": 0.0004128008149564266,
      "learning_rate": 4.7489539748953976e-06,
      "loss": 0.0005,
      "step": 14580
    },
    {
      "epoch": 15.261506276150628,
      "grad_norm": 0.805025041103363,
      "learning_rate": 4.738493723849373e-06,
      "loss": 0.0053,
      "step": 14590
    },
    {
      "epoch": 15.271966527196653,
      "grad_norm": 1.200577974319458,
      "learning_rate": 4.728033472803348e-06,
      "loss": 0.0027,
      "step": 14600
    },
    {
      "epoch": 15.282426778242678,
      "grad_norm": 0.000410381646361202,
      "learning_rate": 4.7175732217573225e-06,
      "loss": 0.0059,
      "step": 14610
    },
    {
      "epoch": 15.292887029288703,
      "grad_norm": 0.0028196144849061966,
      "learning_rate": 4.707112970711297e-06,
      "loss": 0.0008,
      "step": 14620
    },
    {
      "epoch": 15.303347280334728,
      "grad_norm": 5.63673210144043,
      "learning_rate": 4.696652719665272e-06,
      "loss": 0.0027,
      "step": 14630
    },
    {
      "epoch": 15.313807531380753,
      "grad_norm": 0.00042025657603517175,
      "learning_rate": 4.6861924686192475e-06,
      "loss": 0.0011,
      "step": 14640
    },
    {
      "epoch": 15.324267782426778,
      "grad_norm": 0.00040158777846954763,
      "learning_rate": 4.675732217573222e-06,
      "loss": 0.002,
      "step": 14650
    },
    {
      "epoch": 15.334728033472803,
      "grad_norm": 0.0009026002371683717,
      "learning_rate": 4.665271966527197e-06,
      "loss": 0.0002,
      "step": 14660
    },
    {
      "epoch": 15.345188284518828,
      "grad_norm": 0.3716665506362915,
      "learning_rate": 4.654811715481172e-06,
      "loss": 0.0011,
      "step": 14670
    },
    {
      "epoch": 15.355648535564853,
      "grad_norm": 0.0005237270379438996,
      "learning_rate": 4.644351464435146e-06,
      "loss": 0.0024,
      "step": 14680
    },
    {
      "epoch": 15.366108786610878,
      "grad_norm": 0.1497332602739334,
      "learning_rate": 4.633891213389122e-06,
      "loss": 0.0028,
      "step": 14690
    },
    {
      "epoch": 15.376569037656903,
      "grad_norm": 0.03903912007808685,
      "learning_rate": 4.6234309623430966e-06,
      "loss": 0.0132,
      "step": 14700
    },
    {
      "epoch": 15.387029288702928,
      "grad_norm": 0.020033037289977074,
      "learning_rate": 4.612970711297072e-06,
      "loss": 0.0014,
      "step": 14710
    },
    {
      "epoch": 15.397489539748953,
      "grad_norm": 0.0022358251735568047,
      "learning_rate": 4.602510460251046e-06,
      "loss": 0.0005,
      "step": 14720
    },
    {
      "epoch": 15.407949790794978,
      "grad_norm": 0.002329968847334385,
      "learning_rate": 4.5920502092050215e-06,
      "loss": 0.0011,
      "step": 14730
    },
    {
      "epoch": 15.418410041841003,
      "grad_norm": 0.0004862665955442935,
      "learning_rate": 4.581589958158996e-06,
      "loss": 0.004,
      "step": 14740
    },
    {
      "epoch": 15.42887029288703,
      "grad_norm": 0.0012363981222733855,
      "learning_rate": 4.571129707112971e-06,
      "loss": 0.001,
      "step": 14750
    },
    {
      "epoch": 15.439330543933055,
      "grad_norm": 0.0003159402695018798,
      "learning_rate": 4.5606694560669465e-06,
      "loss": 0.0025,
      "step": 14760
    },
    {
      "epoch": 15.44979079497908,
      "grad_norm": 0.00494660809636116,
      "learning_rate": 4.55020920502092e-06,
      "loss": 0.0008,
      "step": 14770
    },
    {
      "epoch": 15.460251046025105,
      "grad_norm": 0.0007311071385629475,
      "learning_rate": 4.539748953974896e-06,
      "loss": 0.0032,
      "step": 14780
    },
    {
      "epoch": 15.47071129707113,
      "grad_norm": 0.0016461072955280542,
      "learning_rate": 4.529288702928871e-06,
      "loss": 0.0008,
      "step": 14790
    },
    {
      "epoch": 15.481171548117155,
      "grad_norm": 0.3590894341468811,
      "learning_rate": 4.518828451882845e-06,
      "loss": 0.0006,
      "step": 14800
    },
    {
      "epoch": 15.49163179916318,
      "grad_norm": 0.36207592487335205,
      "learning_rate": 4.508368200836821e-06,
      "loss": 0.0038,
      "step": 14810
    },
    {
      "epoch": 15.502092050209205,
      "grad_norm": 0.6180776357650757,
      "learning_rate": 4.4979079497907956e-06,
      "loss": 0.0021,
      "step": 14820
    },
    {
      "epoch": 15.51255230125523,
      "grad_norm": 0.004048068076372147,
      "learning_rate": 4.48744769874477e-06,
      "loss": 0.002,
      "step": 14830
    },
    {
      "epoch": 15.523012552301255,
      "grad_norm": 0.052348073571920395,
      "learning_rate": 4.476987447698745e-06,
      "loss": 0.0032,
      "step": 14840
    },
    {
      "epoch": 15.53347280334728,
      "grad_norm": 0.0005307053797878325,
      "learning_rate": 4.4665271966527205e-06,
      "loss": 0.0038,
      "step": 14850
    },
    {
      "epoch": 15.543933054393305,
      "grad_norm": 0.2898978888988495,
      "learning_rate": 4.456066945606695e-06,
      "loss": 0.0095,
      "step": 14860
    },
    {
      "epoch": 15.55439330543933,
      "grad_norm": 0.6492432951927185,
      "learning_rate": 4.44560669456067e-06,
      "loss": 0.0027,
      "step": 14870
    },
    {
      "epoch": 15.564853556485355,
      "grad_norm": 0.0003869151696562767,
      "learning_rate": 4.435146443514645e-06,
      "loss": 0.0049,
      "step": 14880
    },
    {
      "epoch": 15.57531380753138,
      "grad_norm": 0.00029857855406589806,
      "learning_rate": 4.424686192468619e-06,
      "loss": 0.0004,
      "step": 14890
    },
    {
      "epoch": 15.585774058577407,
      "grad_norm": 0.0003775235381908715,
      "learning_rate": 4.414225941422595e-06,
      "loss": 0.001,
      "step": 14900
    },
    {
      "epoch": 15.596234309623432,
      "grad_norm": 0.0006544091738760471,
      "learning_rate": 4.40376569037657e-06,
      "loss": 0.004,
      "step": 14910
    },
    {
      "epoch": 15.606694560669457,
      "grad_norm": 0.001039387658238411,
      "learning_rate": 4.393305439330544e-06,
      "loss": 0.0005,
      "step": 14920
    },
    {
      "epoch": 15.617154811715482,
      "grad_norm": 0.0008852994651533663,
      "learning_rate": 4.382845188284519e-06,
      "loss": 0.0004,
      "step": 14930
    },
    {
      "epoch": 15.627615062761507,
      "grad_norm": 0.0014263363555073738,
      "learning_rate": 4.372384937238494e-06,
      "loss": 0.0014,
      "step": 14940
    },
    {
      "epoch": 15.638075313807532,
      "grad_norm": 0.002068042056635022,
      "learning_rate": 4.361924686192469e-06,
      "loss": 0.0026,
      "step": 14950
    },
    {
      "epoch": 15.648535564853557,
      "grad_norm": 0.8028711676597595,
      "learning_rate": 4.351464435146444e-06,
      "loss": 0.0025,
      "step": 14960
    },
    {
      "epoch": 15.658995815899582,
      "grad_norm": 0.01169145293533802,
      "learning_rate": 4.341004184100419e-06,
      "loss": 0.0017,
      "step": 14970
    },
    {
      "epoch": 15.669456066945607,
      "grad_norm": 0.48482707142829895,
      "learning_rate": 4.330543933054393e-06,
      "loss": 0.0035,
      "step": 14980
    },
    {
      "epoch": 15.679916317991632,
      "grad_norm": 0.00027007886092178524,
      "learning_rate": 4.320083682008369e-06,
      "loss": 0.0,
      "step": 14990
    },
    {
      "epoch": 15.690376569037657,
      "grad_norm": 0.00025922307395376265,
      "learning_rate": 4.309623430962344e-06,
      "loss": 0.0023,
      "step": 15000
    },
    {
      "epoch": 15.700836820083682,
      "grad_norm": 0.0059886351227760315,
      "learning_rate": 4.299163179916318e-06,
      "loss": 0.0009,
      "step": 15010
    },
    {
      "epoch": 15.711297071129707,
      "grad_norm": 0.00045649652020074427,
      "learning_rate": 4.288702928870293e-06,
      "loss": 0.0014,
      "step": 15020
    },
    {
      "epoch": 15.721757322175732,
      "grad_norm": 0.0004143717233091593,
      "learning_rate": 4.278242677824268e-06,
      "loss": 0.0036,
      "step": 15030
    },
    {
      "epoch": 15.732217573221757,
      "grad_norm": 0.28718942403793335,
      "learning_rate": 4.267782426778243e-06,
      "loss": 0.0029,
      "step": 15040
    },
    {
      "epoch": 15.742677824267782,
      "grad_norm": 0.7289984822273254,
      "learning_rate": 4.257322175732218e-06,
      "loss": 0.0019,
      "step": 15050
    },
    {
      "epoch": 15.753138075313807,
      "grad_norm": 0.000561768130864948,
      "learning_rate": 4.246861924686193e-06,
      "loss": 0.0016,
      "step": 15060
    },
    {
      "epoch": 15.763598326359833,
      "grad_norm": 0.0018668557750061154,
      "learning_rate": 4.236401673640167e-06,
      "loss": 0.0041,
      "step": 15070
    },
    {
      "epoch": 15.774058577405858,
      "grad_norm": 0.0010028509423136711,
      "learning_rate": 4.225941422594142e-06,
      "loss": 0.0015,
      "step": 15080
    },
    {
      "epoch": 15.784518828451883,
      "grad_norm": 0.0005074554355815053,
      "learning_rate": 4.215481171548118e-06,
      "loss": 0.0032,
      "step": 15090
    },
    {
      "epoch": 15.794979079497908,
      "grad_norm": 0.28038328886032104,
      "learning_rate": 4.205020920502092e-06,
      "loss": 0.002,
      "step": 15100
    },
    {
      "epoch": 15.805439330543933,
      "grad_norm": 0.0002681094338186085,
      "learning_rate": 4.194560669456068e-06,
      "loss": 0.0006,
      "step": 15110
    },
    {
      "epoch": 15.815899581589958,
      "grad_norm": 0.0007316881092265248,
      "learning_rate": 4.184100418410042e-06,
      "loss": 0.0037,
      "step": 15120
    },
    {
      "epoch": 15.826359832635983,
      "grad_norm": 0.01069769449532032,
      "learning_rate": 4.173640167364017e-06,
      "loss": 0.004,
      "step": 15130
    },
    {
      "epoch": 15.836820083682008,
      "grad_norm": 0.02827523648738861,
      "learning_rate": 4.163179916317992e-06,
      "loss": 0.0015,
      "step": 15140
    },
    {
      "epoch": 15.847280334728033,
      "grad_norm": 0.0008390144794248044,
      "learning_rate": 4.152719665271967e-06,
      "loss": 0.0026,
      "step": 15150
    },
    {
      "epoch": 15.857740585774058,
      "grad_norm": 0.0010955182369798422,
      "learning_rate": 4.142259414225942e-06,
      "loss": 0.0013,
      "step": 15160
    },
    {
      "epoch": 15.868200836820083,
      "grad_norm": 0.0003472741227596998,
      "learning_rate": 4.131799163179916e-06,
      "loss": 0.002,
      "step": 15170
    },
    {
      "epoch": 15.878661087866108,
      "grad_norm": 0.0008623043540865183,
      "learning_rate": 4.121338912133892e-06,
      "loss": 0.0013,
      "step": 15180
    },
    {
      "epoch": 15.889121338912133,
      "grad_norm": 0.003347032004967332,
      "learning_rate": 4.110878661087866e-06,
      "loss": 0.001,
      "step": 15190
    },
    {
      "epoch": 15.899581589958158,
      "grad_norm": 1.9917105436325073,
      "learning_rate": 4.100418410041841e-06,
      "loss": 0.013,
      "step": 15200
    },
    {
      "epoch": 15.910041841004183,
      "grad_norm": 0.23738394677639008,
      "learning_rate": 4.089958158995817e-06,
      "loss": 0.0012,
      "step": 15210
    },
    {
      "epoch": 15.92050209205021,
      "grad_norm": 0.00022399728186428547,
      "learning_rate": 4.0794979079497905e-06,
      "loss": 0.0034,
      "step": 15220
    },
    {
      "epoch": 15.930962343096235,
      "grad_norm": 0.0016436581499874592,
      "learning_rate": 4.069037656903766e-06,
      "loss": 0.003,
      "step": 15230
    },
    {
      "epoch": 15.94142259414226,
      "grad_norm": 0.0010111797600984573,
      "learning_rate": 4.058577405857741e-06,
      "loss": 0.0007,
      "step": 15240
    },
    {
      "epoch": 15.951882845188285,
      "grad_norm": 0.44166693091392517,
      "learning_rate": 4.048117154811716e-06,
      "loss": 0.0021,
      "step": 15250
    },
    {
      "epoch": 15.96234309623431,
      "grad_norm": 0.0006579163600690663,
      "learning_rate": 4.037656903765691e-06,
      "loss": 0.0037,
      "step": 15260
    },
    {
      "epoch": 15.972803347280335,
      "grad_norm": 0.0014461121754720807,
      "learning_rate": 4.027196652719666e-06,
      "loss": 0.0028,
      "step": 15270
    },
    {
      "epoch": 15.98326359832636,
      "grad_norm": 0.000977269490249455,
      "learning_rate": 4.0167364016736405e-06,
      "loss": 0.0023,
      "step": 15280
    },
    {
      "epoch": 15.993723849372385,
      "grad_norm": 0.015884310007095337,
      "learning_rate": 4.006276150627615e-06,
      "loss": 0.001,
      "step": 15290
    },
    {
      "epoch": 16.0,
      "eval_accuracy": 0.9954268292682927,
      "eval_f1": 0.98,
      "eval_loss": 0.022011546418070793,
      "eval_precision": 0.9795646379386939,
      "eval_recall": 0.9804357492218764,
      "eval_runtime": 323.3691,
      "eval_samples_per_second": 60.862,
      "eval_steps_per_second": 2.539,
      "step": 15296
    }
  ],
  "logging_steps": 10,
  "max_steps": 19120,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.521674408880045e+17,
  "train_batch_size": 96,
  "trial_name": null,
  "trial_params": null
}
