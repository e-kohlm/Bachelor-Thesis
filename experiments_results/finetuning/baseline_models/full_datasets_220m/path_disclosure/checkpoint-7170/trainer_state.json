{
  "best_metric": 0.9778466991581745,
  "best_model_checkpoint": "../saved_models/path_disclosure/checkpoint-7170",
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 7170,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001394700139470014,
      "grad_norm": 84.01358795166016,
      "learning_rate": 1.999721059972106e-05,
      "loss": 0.792,
      "step": 1
    },
    {
      "epoch": 0.01394700139470014,
      "grad_norm": 10.376057624816895,
      "learning_rate": 1.9972105997210603e-05,
      "loss": 0.5241,
      "step": 10
    },
    {
      "epoch": 0.02789400278940028,
      "grad_norm": 9.80446720123291,
      "learning_rate": 1.99442119944212e-05,
      "loss": 0.3855,
      "step": 20
    },
    {
      "epoch": 0.04184100418410042,
      "grad_norm": 8.701587677001953,
      "learning_rate": 1.9916317991631803e-05,
      "loss": 0.4084,
      "step": 30
    },
    {
      "epoch": 0.05578800557880056,
      "grad_norm": 13.229002952575684,
      "learning_rate": 1.98884239888424e-05,
      "loss": 0.4253,
      "step": 40
    },
    {
      "epoch": 0.0697350069735007,
      "grad_norm": 10.754326820373535,
      "learning_rate": 1.9860529986053002e-05,
      "loss": 0.3474,
      "step": 50
    },
    {
      "epoch": 0.08368200836820083,
      "grad_norm": 24.268630981445312,
      "learning_rate": 1.98326359832636e-05,
      "loss": 0.3959,
      "step": 60
    },
    {
      "epoch": 0.09762900976290098,
      "grad_norm": 9.84748649597168,
      "learning_rate": 1.9804741980474202e-05,
      "loss": 0.3514,
      "step": 70
    },
    {
      "epoch": 0.11157601115760112,
      "grad_norm": 8.821394920349121,
      "learning_rate": 1.97768479776848e-05,
      "loss": 0.3558,
      "step": 80
    },
    {
      "epoch": 0.12552301255230125,
      "grad_norm": 8.148404121398926,
      "learning_rate": 1.9748953974895398e-05,
      "loss": 0.3577,
      "step": 90
    },
    {
      "epoch": 0.1394700139470014,
      "grad_norm": 7.731862545013428,
      "learning_rate": 1.9721059972106e-05,
      "loss": 0.3602,
      "step": 100
    },
    {
      "epoch": 0.15341701534170155,
      "grad_norm": 10.063288688659668,
      "learning_rate": 1.9693165969316597e-05,
      "loss": 0.3219,
      "step": 110
    },
    {
      "epoch": 0.16736401673640167,
      "grad_norm": 6.373303413391113,
      "learning_rate": 1.96652719665272e-05,
      "loss": 0.3289,
      "step": 120
    },
    {
      "epoch": 0.18131101813110181,
      "grad_norm": 5.114632606506348,
      "learning_rate": 1.9637377963737797e-05,
      "loss": 0.3053,
      "step": 130
    },
    {
      "epoch": 0.19525801952580196,
      "grad_norm": 7.78073787689209,
      "learning_rate": 1.9609483960948395e-05,
      "loss": 0.352,
      "step": 140
    },
    {
      "epoch": 0.20920502092050208,
      "grad_norm": 5.3754377365112305,
      "learning_rate": 1.9581589958158997e-05,
      "loss": 0.3003,
      "step": 150
    },
    {
      "epoch": 0.22315202231520223,
      "grad_norm": 4.478102684020996,
      "learning_rate": 1.9553695955369598e-05,
      "loss": 0.2718,
      "step": 160
    },
    {
      "epoch": 0.23709902370990238,
      "grad_norm": 4.791820049285889,
      "learning_rate": 1.9525801952580196e-05,
      "loss": 0.3099,
      "step": 170
    },
    {
      "epoch": 0.2510460251046025,
      "grad_norm": 4.790005683898926,
      "learning_rate": 1.9497907949790798e-05,
      "loss": 0.2752,
      "step": 180
    },
    {
      "epoch": 0.2649930264993027,
      "grad_norm": 8.127106666564941,
      "learning_rate": 1.9470013947001396e-05,
      "loss": 0.2915,
      "step": 190
    },
    {
      "epoch": 0.2789400278940028,
      "grad_norm": 4.3169355392456055,
      "learning_rate": 1.9442119944211997e-05,
      "loss": 0.3007,
      "step": 200
    },
    {
      "epoch": 0.2928870292887029,
      "grad_norm": 3.9885811805725098,
      "learning_rate": 1.9414225941422595e-05,
      "loss": 0.2727,
      "step": 210
    },
    {
      "epoch": 0.3068340306834031,
      "grad_norm": 11.778096199035645,
      "learning_rate": 1.9386331938633197e-05,
      "loss": 0.2808,
      "step": 220
    },
    {
      "epoch": 0.3207810320781032,
      "grad_norm": 4.025723934173584,
      "learning_rate": 1.9358437935843795e-05,
      "loss": 0.2719,
      "step": 230
    },
    {
      "epoch": 0.33472803347280333,
      "grad_norm": 4.661413669586182,
      "learning_rate": 1.9330543933054396e-05,
      "loss": 0.299,
      "step": 240
    },
    {
      "epoch": 0.3486750348675035,
      "grad_norm": 4.526686668395996,
      "learning_rate": 1.9302649930264994e-05,
      "loss": 0.278,
      "step": 250
    },
    {
      "epoch": 0.36262203626220363,
      "grad_norm": 3.4911136627197266,
      "learning_rate": 1.9274755927475596e-05,
      "loss": 0.2268,
      "step": 260
    },
    {
      "epoch": 0.37656903765690375,
      "grad_norm": 2.9370975494384766,
      "learning_rate": 1.9246861924686194e-05,
      "loss": 0.2452,
      "step": 270
    },
    {
      "epoch": 0.3905160390516039,
      "grad_norm": 3.6291255950927734,
      "learning_rate": 1.9218967921896795e-05,
      "loss": 0.224,
      "step": 280
    },
    {
      "epoch": 0.40446304044630405,
      "grad_norm": 3.4383368492126465,
      "learning_rate": 1.9191073919107393e-05,
      "loss": 0.2224,
      "step": 290
    },
    {
      "epoch": 0.41841004184100417,
      "grad_norm": 4.0369343757629395,
      "learning_rate": 1.916317991631799e-05,
      "loss": 0.2045,
      "step": 300
    },
    {
      "epoch": 0.43235704323570434,
      "grad_norm": 4.1642351150512695,
      "learning_rate": 1.9135285913528593e-05,
      "loss": 0.228,
      "step": 310
    },
    {
      "epoch": 0.44630404463040446,
      "grad_norm": 4.849725246429443,
      "learning_rate": 1.910739191073919e-05,
      "loss": 0.2045,
      "step": 320
    },
    {
      "epoch": 0.4602510460251046,
      "grad_norm": 4.764532566070557,
      "learning_rate": 1.9079497907949793e-05,
      "loss": 0.1931,
      "step": 330
    },
    {
      "epoch": 0.47419804741980476,
      "grad_norm": 3.7834219932556152,
      "learning_rate": 1.905160390516039e-05,
      "loss": 0.2241,
      "step": 340
    },
    {
      "epoch": 0.4881450488145049,
      "grad_norm": 2.6587462425231934,
      "learning_rate": 1.9023709902370992e-05,
      "loss": 0.199,
      "step": 350
    },
    {
      "epoch": 0.502092050209205,
      "grad_norm": 3.6149771213531494,
      "learning_rate": 1.899581589958159e-05,
      "loss": 0.1633,
      "step": 360
    },
    {
      "epoch": 0.5160390516039052,
      "grad_norm": 4.3440632820129395,
      "learning_rate": 1.8967921896792192e-05,
      "loss": 0.2009,
      "step": 370
    },
    {
      "epoch": 0.5299860529986054,
      "grad_norm": 3.4671058654785156,
      "learning_rate": 1.894002789400279e-05,
      "loss": 0.2005,
      "step": 380
    },
    {
      "epoch": 0.5439330543933054,
      "grad_norm": 2.8459503650665283,
      "learning_rate": 1.891213389121339e-05,
      "loss": 0.165,
      "step": 390
    },
    {
      "epoch": 0.5578800557880056,
      "grad_norm": 3.3826258182525635,
      "learning_rate": 1.888423988842399e-05,
      "loss": 0.1951,
      "step": 400
    },
    {
      "epoch": 0.5718270571827058,
      "grad_norm": 4.165107250213623,
      "learning_rate": 1.885634588563459e-05,
      "loss": 0.2007,
      "step": 410
    },
    {
      "epoch": 0.5857740585774058,
      "grad_norm": 4.188710689544678,
      "learning_rate": 1.882845188284519e-05,
      "loss": 0.1527,
      "step": 420
    },
    {
      "epoch": 0.599721059972106,
      "grad_norm": 7.025703430175781,
      "learning_rate": 1.880055788005579e-05,
      "loss": 0.2166,
      "step": 430
    },
    {
      "epoch": 0.6136680613668062,
      "grad_norm": 3.671794891357422,
      "learning_rate": 1.877266387726639e-05,
      "loss": 0.1688,
      "step": 440
    },
    {
      "epoch": 0.6276150627615062,
      "grad_norm": 2.8007993698120117,
      "learning_rate": 1.874476987447699e-05,
      "loss": 0.1549,
      "step": 450
    },
    {
      "epoch": 0.6415620641562064,
      "grad_norm": 3.6256632804870605,
      "learning_rate": 1.871687587168759e-05,
      "loss": 0.1496,
      "step": 460
    },
    {
      "epoch": 0.6555090655509066,
      "grad_norm": 2.6087710857391357,
      "learning_rate": 1.868898186889819e-05,
      "loss": 0.1198,
      "step": 470
    },
    {
      "epoch": 0.6694560669456067,
      "grad_norm": 6.200826644897461,
      "learning_rate": 1.8661087866108788e-05,
      "loss": 0.1731,
      "step": 480
    },
    {
      "epoch": 0.6834030683403068,
      "grad_norm": 6.069821834564209,
      "learning_rate": 1.863319386331939e-05,
      "loss": 0.141,
      "step": 490
    },
    {
      "epoch": 0.697350069735007,
      "grad_norm": 3.595304489135742,
      "learning_rate": 1.8605299860529987e-05,
      "loss": 0.153,
      "step": 500
    },
    {
      "epoch": 0.7112970711297071,
      "grad_norm": 1.922656774520874,
      "learning_rate": 1.8577405857740585e-05,
      "loss": 0.1489,
      "step": 510
    },
    {
      "epoch": 0.7252440725244073,
      "grad_norm": 5.104182720184326,
      "learning_rate": 1.8549511854951187e-05,
      "loss": 0.1377,
      "step": 520
    },
    {
      "epoch": 0.7391910739191074,
      "grad_norm": 4.432780742645264,
      "learning_rate": 1.8521617852161785e-05,
      "loss": 0.1246,
      "step": 530
    },
    {
      "epoch": 0.7531380753138075,
      "grad_norm": 3.1378273963928223,
      "learning_rate": 1.8493723849372386e-05,
      "loss": 0.1149,
      "step": 540
    },
    {
      "epoch": 0.7670850767085077,
      "grad_norm": 4.10233211517334,
      "learning_rate": 1.8465829846582984e-05,
      "loss": 0.1375,
      "step": 550
    },
    {
      "epoch": 0.7810320781032078,
      "grad_norm": 3.2562568187713623,
      "learning_rate": 1.8437935843793586e-05,
      "loss": 0.1222,
      "step": 560
    },
    {
      "epoch": 0.7949790794979079,
      "grad_norm": 4.026324272155762,
      "learning_rate": 1.8410041841004184e-05,
      "loss": 0.1172,
      "step": 570
    },
    {
      "epoch": 0.8089260808926081,
      "grad_norm": 4.589270114898682,
      "learning_rate": 1.8382147838214785e-05,
      "loss": 0.1342,
      "step": 580
    },
    {
      "epoch": 0.8228730822873083,
      "grad_norm": 10.651972770690918,
      "learning_rate": 1.8354253835425383e-05,
      "loss": 0.1196,
      "step": 590
    },
    {
      "epoch": 0.8368200836820083,
      "grad_norm": 4.2188720703125,
      "learning_rate": 1.8326359832635985e-05,
      "loss": 0.1345,
      "step": 600
    },
    {
      "epoch": 0.8507670850767085,
      "grad_norm": 4.791040897369385,
      "learning_rate": 1.8298465829846583e-05,
      "loss": 0.1343,
      "step": 610
    },
    {
      "epoch": 0.8647140864714087,
      "grad_norm": 2.747316360473633,
      "learning_rate": 1.8270571827057184e-05,
      "loss": 0.1044,
      "step": 620
    },
    {
      "epoch": 0.8786610878661087,
      "grad_norm": 4.027476787567139,
      "learning_rate": 1.8242677824267786e-05,
      "loss": 0.0821,
      "step": 630
    },
    {
      "epoch": 0.8926080892608089,
      "grad_norm": 4.539304256439209,
      "learning_rate": 1.8214783821478384e-05,
      "loss": 0.0914,
      "step": 640
    },
    {
      "epoch": 0.9065550906555091,
      "grad_norm": 5.738148212432861,
      "learning_rate": 1.8186889818688986e-05,
      "loss": 0.098,
      "step": 650
    },
    {
      "epoch": 0.9205020920502092,
      "grad_norm": 3.917536735534668,
      "learning_rate": 1.8158995815899584e-05,
      "loss": 0.0993,
      "step": 660
    },
    {
      "epoch": 0.9344490934449093,
      "grad_norm": 3.8664183616638184,
      "learning_rate": 1.8131101813110185e-05,
      "loss": 0.1148,
      "step": 670
    },
    {
      "epoch": 0.9483960948396095,
      "grad_norm": 3.4459424018859863,
      "learning_rate": 1.8103207810320783e-05,
      "loss": 0.0775,
      "step": 680
    },
    {
      "epoch": 0.9623430962343096,
      "grad_norm": 4.891317367553711,
      "learning_rate": 1.807531380753138e-05,
      "loss": 0.0915,
      "step": 690
    },
    {
      "epoch": 0.9762900976290098,
      "grad_norm": 4.88082218170166,
      "learning_rate": 1.8047419804741983e-05,
      "loss": 0.0877,
      "step": 700
    },
    {
      "epoch": 0.9902370990237099,
      "grad_norm": 1.8239941596984863,
      "learning_rate": 1.801952580195258e-05,
      "loss": 0.0605,
      "step": 710
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9728150406504065,
      "eval_f1": 0.8819245199735157,
      "eval_loss": 0.07262490689754486,
      "eval_precision": 0.8755477651183172,
      "eval_recall": 0.8883948421520675,
      "eval_runtime": 274.4973,
      "eval_samples_per_second": 71.698,
      "eval_steps_per_second": 2.244,
      "step": 717
    },
    {
      "epoch": 1.00418410041841,
      "grad_norm": 3.248720169067383,
      "learning_rate": 1.7991631799163182e-05,
      "loss": 0.0745,
      "step": 720
    },
    {
      "epoch": 1.0181311018131103,
      "grad_norm": 3.3879361152648926,
      "learning_rate": 1.796373779637378e-05,
      "loss": 0.073,
      "step": 730
    },
    {
      "epoch": 1.0320781032078103,
      "grad_norm": 3.915745496749878,
      "learning_rate": 1.793584379358438e-05,
      "loss": 0.0765,
      "step": 740
    },
    {
      "epoch": 1.0460251046025104,
      "grad_norm": 8.971283912658691,
      "learning_rate": 1.790794979079498e-05,
      "loss": 0.0705,
      "step": 750
    },
    {
      "epoch": 1.0599721059972107,
      "grad_norm": 3.4693307876586914,
      "learning_rate": 1.7880055788005578e-05,
      "loss": 0.1021,
      "step": 760
    },
    {
      "epoch": 1.0739191073919108,
      "grad_norm": 3.6498544216156006,
      "learning_rate": 1.785216178521618e-05,
      "loss": 0.0788,
      "step": 770
    },
    {
      "epoch": 1.0878661087866108,
      "grad_norm": 4.758431434631348,
      "learning_rate": 1.782426778242678e-05,
      "loss": 0.0549,
      "step": 780
    },
    {
      "epoch": 1.1018131101813111,
      "grad_norm": 2.1259682178497314,
      "learning_rate": 1.779637377963738e-05,
      "loss": 0.0598,
      "step": 790
    },
    {
      "epoch": 1.1157601115760112,
      "grad_norm": 4.971304893493652,
      "learning_rate": 1.776847977684798e-05,
      "loss": 0.103,
      "step": 800
    },
    {
      "epoch": 1.1297071129707112,
      "grad_norm": 2.781179428100586,
      "learning_rate": 1.774058577405858e-05,
      "loss": 0.0748,
      "step": 810
    },
    {
      "epoch": 1.1436541143654115,
      "grad_norm": 1.2622170448303223,
      "learning_rate": 1.771269177126918e-05,
      "loss": 0.0467,
      "step": 820
    },
    {
      "epoch": 1.1576011157601116,
      "grad_norm": 4.502941608428955,
      "learning_rate": 1.7684797768479778e-05,
      "loss": 0.0593,
      "step": 830
    },
    {
      "epoch": 1.1715481171548117,
      "grad_norm": 3.7528812885284424,
      "learning_rate": 1.765690376569038e-05,
      "loss": 0.0579,
      "step": 840
    },
    {
      "epoch": 1.185495118549512,
      "grad_norm": 2.238372802734375,
      "learning_rate": 1.7629009762900978e-05,
      "loss": 0.0466,
      "step": 850
    },
    {
      "epoch": 1.199442119944212,
      "grad_norm": 3.0598294734954834,
      "learning_rate": 1.760111576011158e-05,
      "loss": 0.0669,
      "step": 860
    },
    {
      "epoch": 1.213389121338912,
      "grad_norm": 1.8972396850585938,
      "learning_rate": 1.7573221757322177e-05,
      "loss": 0.0522,
      "step": 870
    },
    {
      "epoch": 1.2273361227336124,
      "grad_norm": 2.3243768215179443,
      "learning_rate": 1.754532775453278e-05,
      "loss": 0.0678,
      "step": 880
    },
    {
      "epoch": 1.2412831241283124,
      "grad_norm": 3.2772178649902344,
      "learning_rate": 1.7517433751743377e-05,
      "loss": 0.0444,
      "step": 890
    },
    {
      "epoch": 1.2552301255230125,
      "grad_norm": 4.128870964050293,
      "learning_rate": 1.7489539748953975e-05,
      "loss": 0.0716,
      "step": 900
    },
    {
      "epoch": 1.2691771269177128,
      "grad_norm": 1.827104091644287,
      "learning_rate": 1.7461645746164576e-05,
      "loss": 0.0639,
      "step": 910
    },
    {
      "epoch": 1.2831241283124128,
      "grad_norm": 3.359415292739868,
      "learning_rate": 1.7433751743375174e-05,
      "loss": 0.0646,
      "step": 920
    },
    {
      "epoch": 1.297071129707113,
      "grad_norm": 3.1035494804382324,
      "learning_rate": 1.7405857740585776e-05,
      "loss": 0.0741,
      "step": 930
    },
    {
      "epoch": 1.3110181311018132,
      "grad_norm": 3.066162586212158,
      "learning_rate": 1.7377963737796374e-05,
      "loss": 0.0541,
      "step": 940
    },
    {
      "epoch": 1.3249651324965133,
      "grad_norm": 3.0756983757019043,
      "learning_rate": 1.7350069735006975e-05,
      "loss": 0.0573,
      "step": 950
    },
    {
      "epoch": 1.3389121338912133,
      "grad_norm": 2.7455551624298096,
      "learning_rate": 1.7322175732217574e-05,
      "loss": 0.0775,
      "step": 960
    },
    {
      "epoch": 1.3528591352859136,
      "grad_norm": 3.0772721767425537,
      "learning_rate": 1.7294281729428175e-05,
      "loss": 0.0487,
      "step": 970
    },
    {
      "epoch": 1.3668061366806137,
      "grad_norm": 2.026576042175293,
      "learning_rate": 1.7266387726638773e-05,
      "loss": 0.0552,
      "step": 980
    },
    {
      "epoch": 1.3807531380753137,
      "grad_norm": 6.381761074066162,
      "learning_rate": 1.7238493723849375e-05,
      "loss": 0.0686,
      "step": 990
    },
    {
      "epoch": 1.394700139470014,
      "grad_norm": 1.7102476358413696,
      "learning_rate": 1.7210599721059973e-05,
      "loss": 0.0602,
      "step": 1000
    },
    {
      "epoch": 1.408647140864714,
      "grad_norm": 2.10491943359375,
      "learning_rate": 1.7182705718270574e-05,
      "loss": 0.0722,
      "step": 1010
    },
    {
      "epoch": 1.4225941422594142,
      "grad_norm": 1.7025809288024902,
      "learning_rate": 1.7154811715481172e-05,
      "loss": 0.0586,
      "step": 1020
    },
    {
      "epoch": 1.4365411436541144,
      "grad_norm": 1.5636217594146729,
      "learning_rate": 1.7126917712691774e-05,
      "loss": 0.0561,
      "step": 1030
    },
    {
      "epoch": 1.4504881450488145,
      "grad_norm": 2.4885597229003906,
      "learning_rate": 1.7099023709902372e-05,
      "loss": 0.0661,
      "step": 1040
    },
    {
      "epoch": 1.4644351464435146,
      "grad_norm": 3.7196948528289795,
      "learning_rate": 1.7071129707112973e-05,
      "loss": 0.0621,
      "step": 1050
    },
    {
      "epoch": 1.4783821478382149,
      "grad_norm": 3.7268638610839844,
      "learning_rate": 1.704323570432357e-05,
      "loss": 0.054,
      "step": 1060
    },
    {
      "epoch": 1.492329149232915,
      "grad_norm": 3.777031183242798,
      "learning_rate": 1.7015341701534173e-05,
      "loss": 0.053,
      "step": 1070
    },
    {
      "epoch": 1.506276150627615,
      "grad_norm": 1.7232592105865479,
      "learning_rate": 1.698744769874477e-05,
      "loss": 0.059,
      "step": 1080
    },
    {
      "epoch": 1.5202231520223153,
      "grad_norm": 4.983209133148193,
      "learning_rate": 1.6959553695955372e-05,
      "loss": 0.0742,
      "step": 1090
    },
    {
      "epoch": 1.5341701534170153,
      "grad_norm": 3.2505064010620117,
      "learning_rate": 1.693165969316597e-05,
      "loss": 0.0486,
      "step": 1100
    },
    {
      "epoch": 1.5481171548117154,
      "grad_norm": 3.177755355834961,
      "learning_rate": 1.690376569037657e-05,
      "loss": 0.0373,
      "step": 1110
    },
    {
      "epoch": 1.5620641562064157,
      "grad_norm": 3.6042754650115967,
      "learning_rate": 1.687587168758717e-05,
      "loss": 0.0457,
      "step": 1120
    },
    {
      "epoch": 1.5760111576011158,
      "grad_norm": 2.336559295654297,
      "learning_rate": 1.6847977684797768e-05,
      "loss": 0.0519,
      "step": 1130
    },
    {
      "epoch": 1.5899581589958158,
      "grad_norm": 4.35693359375,
      "learning_rate": 1.682008368200837e-05,
      "loss": 0.0664,
      "step": 1140
    },
    {
      "epoch": 1.6039051603905161,
      "grad_norm": 2.6376125812530518,
      "learning_rate": 1.6792189679218968e-05,
      "loss": 0.0484,
      "step": 1150
    },
    {
      "epoch": 1.6178521617852162,
      "grad_norm": 1.9082268476486206,
      "learning_rate": 1.676429567642957e-05,
      "loss": 0.0465,
      "step": 1160
    },
    {
      "epoch": 1.6317991631799162,
      "grad_norm": 4.179577350616455,
      "learning_rate": 1.6736401673640167e-05,
      "loss": 0.0374,
      "step": 1170
    },
    {
      "epoch": 1.6457461645746165,
      "grad_norm": 1.4058759212493896,
      "learning_rate": 1.670850767085077e-05,
      "loss": 0.0333,
      "step": 1180
    },
    {
      "epoch": 1.6596931659693166,
      "grad_norm": 2.6023573875427246,
      "learning_rate": 1.6680613668061367e-05,
      "loss": 0.0576,
      "step": 1190
    },
    {
      "epoch": 1.6736401673640167,
      "grad_norm": 2.1578307151794434,
      "learning_rate": 1.6652719665271968e-05,
      "loss": 0.0388,
      "step": 1200
    },
    {
      "epoch": 1.687587168758717,
      "grad_norm": 2.5048940181732178,
      "learning_rate": 1.6624825662482566e-05,
      "loss": 0.0376,
      "step": 1210
    },
    {
      "epoch": 1.701534170153417,
      "grad_norm": 5.315274238586426,
      "learning_rate": 1.6596931659693168e-05,
      "loss": 0.0396,
      "step": 1220
    },
    {
      "epoch": 1.715481171548117,
      "grad_norm": 1.1060208082199097,
      "learning_rate": 1.656903765690377e-05,
      "loss": 0.0402,
      "step": 1230
    },
    {
      "epoch": 1.7294281729428174,
      "grad_norm": 3.253359794616699,
      "learning_rate": 1.6541143654114367e-05,
      "loss": 0.051,
      "step": 1240
    },
    {
      "epoch": 1.7433751743375174,
      "grad_norm": 1.840212345123291,
      "learning_rate": 1.651324965132497e-05,
      "loss": 0.0443,
      "step": 1250
    },
    {
      "epoch": 1.7573221757322175,
      "grad_norm": 1.3138936758041382,
      "learning_rate": 1.6485355648535567e-05,
      "loss": 0.0307,
      "step": 1260
    },
    {
      "epoch": 1.7712691771269178,
      "grad_norm": 5.658222198486328,
      "learning_rate": 1.645746164574617e-05,
      "loss": 0.051,
      "step": 1270
    },
    {
      "epoch": 1.7852161785216178,
      "grad_norm": 3.096787691116333,
      "learning_rate": 1.6429567642956766e-05,
      "loss": 0.0513,
      "step": 1280
    },
    {
      "epoch": 1.799163179916318,
      "grad_norm": 3.6060075759887695,
      "learning_rate": 1.6401673640167365e-05,
      "loss": 0.0468,
      "step": 1290
    },
    {
      "epoch": 1.8131101813110182,
      "grad_norm": 1.7710363864898682,
      "learning_rate": 1.6373779637377966e-05,
      "loss": 0.025,
      "step": 1300
    },
    {
      "epoch": 1.8270571827057183,
      "grad_norm": 1.7581018209457397,
      "learning_rate": 1.6345885634588564e-05,
      "loss": 0.04,
      "step": 1310
    },
    {
      "epoch": 1.8410041841004183,
      "grad_norm": 3.5614399909973145,
      "learning_rate": 1.6317991631799162e-05,
      "loss": 0.0409,
      "step": 1320
    },
    {
      "epoch": 1.8549511854951186,
      "grad_norm": 1.0203502178192139,
      "learning_rate": 1.6290097629009764e-05,
      "loss": 0.0314,
      "step": 1330
    },
    {
      "epoch": 1.8688981868898187,
      "grad_norm": 2.4871575832366943,
      "learning_rate": 1.6262203626220362e-05,
      "loss": 0.0586,
      "step": 1340
    },
    {
      "epoch": 1.8828451882845187,
      "grad_norm": 2.203070640563965,
      "learning_rate": 1.6234309623430963e-05,
      "loss": 0.0327,
      "step": 1350
    },
    {
      "epoch": 1.896792189679219,
      "grad_norm": 3.134233236312866,
      "learning_rate": 1.620641562064156e-05,
      "loss": 0.0237,
      "step": 1360
    },
    {
      "epoch": 1.910739191073919,
      "grad_norm": 3.2871317863464355,
      "learning_rate": 1.6178521617852163e-05,
      "loss": 0.0555,
      "step": 1370
    },
    {
      "epoch": 1.9246861924686192,
      "grad_norm": 1.968050479888916,
      "learning_rate": 1.6150627615062764e-05,
      "loss": 0.0474,
      "step": 1380
    },
    {
      "epoch": 1.9386331938633194,
      "grad_norm": 2.2234139442443848,
      "learning_rate": 1.6122733612273362e-05,
      "loss": 0.0385,
      "step": 1390
    },
    {
      "epoch": 1.9525801952580195,
      "grad_norm": 3.1550779342651367,
      "learning_rate": 1.6094839609483964e-05,
      "loss": 0.0256,
      "step": 1400
    },
    {
      "epoch": 1.9665271966527196,
      "grad_norm": 3.1670827865600586,
      "learning_rate": 1.6066945606694562e-05,
      "loss": 0.0497,
      "step": 1410
    },
    {
      "epoch": 1.9804741980474199,
      "grad_norm": 1.9465910196304321,
      "learning_rate": 1.6039051603905163e-05,
      "loss": 0.035,
      "step": 1420
    },
    {
      "epoch": 1.99442119944212,
      "grad_norm": 2.91137957572937,
      "learning_rate": 1.601115760111576e-05,
      "loss": 0.0519,
      "step": 1430
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.989430894308943,
      "eval_f1": 0.9533632286995516,
      "eval_loss": 0.030795380473136902,
      "eval_precision": 0.9615558570782451,
      "eval_recall": 0.9453090262338817,
      "eval_runtime": 253.4916,
      "eval_samples_per_second": 77.64,
      "eval_steps_per_second": 2.43,
      "step": 1434
    },
    {
      "epoch": 2.00836820083682,
      "grad_norm": 2.598578929901123,
      "learning_rate": 1.5983263598326363e-05,
      "loss": 0.0564,
      "step": 1440
    },
    {
      "epoch": 2.0223152022315203,
      "grad_norm": 1.041955590248108,
      "learning_rate": 1.595536959553696e-05,
      "loss": 0.0244,
      "step": 1450
    },
    {
      "epoch": 2.0362622036262206,
      "grad_norm": 3.8542470932006836,
      "learning_rate": 1.5927475592747562e-05,
      "loss": 0.043,
      "step": 1460
    },
    {
      "epoch": 2.0502092050209204,
      "grad_norm": 2.368194341659546,
      "learning_rate": 1.589958158995816e-05,
      "loss": 0.0462,
      "step": 1470
    },
    {
      "epoch": 2.0641562064156207,
      "grad_norm": 2.0393288135528564,
      "learning_rate": 1.5871687587168762e-05,
      "loss": 0.0269,
      "step": 1480
    },
    {
      "epoch": 2.078103207810321,
      "grad_norm": 1.418185830116272,
      "learning_rate": 1.584379358437936e-05,
      "loss": 0.0288,
      "step": 1490
    },
    {
      "epoch": 2.092050209205021,
      "grad_norm": 2.3824691772460938,
      "learning_rate": 1.5815899581589958e-05,
      "loss": 0.0287,
      "step": 1500
    },
    {
      "epoch": 2.105997210599721,
      "grad_norm": 1.1874029636383057,
      "learning_rate": 1.578800557880056e-05,
      "loss": 0.0304,
      "step": 1510
    },
    {
      "epoch": 2.1199442119944214,
      "grad_norm": 1.264107584953308,
      "learning_rate": 1.5760111576011158e-05,
      "loss": 0.025,
      "step": 1520
    },
    {
      "epoch": 2.1338912133891212,
      "grad_norm": 1.8922122716903687,
      "learning_rate": 1.573221757322176e-05,
      "loss": 0.0346,
      "step": 1530
    },
    {
      "epoch": 2.1478382147838215,
      "grad_norm": 2.4553403854370117,
      "learning_rate": 1.5704323570432357e-05,
      "loss": 0.0238,
      "step": 1540
    },
    {
      "epoch": 2.161785216178522,
      "grad_norm": 1.6876029968261719,
      "learning_rate": 1.567642956764296e-05,
      "loss": 0.0145,
      "step": 1550
    },
    {
      "epoch": 2.1757322175732217,
      "grad_norm": 2.480003595352173,
      "learning_rate": 1.5648535564853557e-05,
      "loss": 0.0255,
      "step": 1560
    },
    {
      "epoch": 2.189679218967922,
      "grad_norm": 0.5879560112953186,
      "learning_rate": 1.5620641562064158e-05,
      "loss": 0.0305,
      "step": 1570
    },
    {
      "epoch": 2.2036262203626222,
      "grad_norm": 1.950254201889038,
      "learning_rate": 1.5592747559274756e-05,
      "loss": 0.0249,
      "step": 1580
    },
    {
      "epoch": 2.217573221757322,
      "grad_norm": 0.6706158518791199,
      "learning_rate": 1.5564853556485358e-05,
      "loss": 0.0232,
      "step": 1590
    },
    {
      "epoch": 2.2315202231520224,
      "grad_norm": 2.4010419845581055,
      "learning_rate": 1.5536959553695956e-05,
      "loss": 0.0417,
      "step": 1600
    },
    {
      "epoch": 2.2454672245467227,
      "grad_norm": 2.246631622314453,
      "learning_rate": 1.5509065550906557e-05,
      "loss": 0.0301,
      "step": 1610
    },
    {
      "epoch": 2.2594142259414225,
      "grad_norm": 2.752089023590088,
      "learning_rate": 1.5481171548117155e-05,
      "loss": 0.0285,
      "step": 1620
    },
    {
      "epoch": 2.273361227336123,
      "grad_norm": 1.1230792999267578,
      "learning_rate": 1.5453277545327757e-05,
      "loss": 0.0276,
      "step": 1630
    },
    {
      "epoch": 2.287308228730823,
      "grad_norm": 2.5496819019317627,
      "learning_rate": 1.5425383542538355e-05,
      "loss": 0.0264,
      "step": 1640
    },
    {
      "epoch": 2.301255230125523,
      "grad_norm": 0.3606833219528198,
      "learning_rate": 1.5397489539748957e-05,
      "loss": 0.0319,
      "step": 1650
    },
    {
      "epoch": 2.315202231520223,
      "grad_norm": 3.279980421066284,
      "learning_rate": 1.5369595536959555e-05,
      "loss": 0.0305,
      "step": 1660
    },
    {
      "epoch": 2.3291492329149235,
      "grad_norm": 0.8505197167396545,
      "learning_rate": 1.5341701534170156e-05,
      "loss": 0.0377,
      "step": 1670
    },
    {
      "epoch": 2.3430962343096233,
      "grad_norm": 1.129953145980835,
      "learning_rate": 1.5313807531380754e-05,
      "loss": 0.0325,
      "step": 1680
    },
    {
      "epoch": 2.3570432357043236,
      "grad_norm": 5.36334753036499,
      "learning_rate": 1.5285913528591356e-05,
      "loss": 0.0271,
      "step": 1690
    },
    {
      "epoch": 2.370990237099024,
      "grad_norm": 1.035342812538147,
      "learning_rate": 1.5258019525801955e-05,
      "loss": 0.0379,
      "step": 1700
    },
    {
      "epoch": 2.3849372384937237,
      "grad_norm": 2.1195387840270996,
      "learning_rate": 1.5230125523012553e-05,
      "loss": 0.0273,
      "step": 1710
    },
    {
      "epoch": 2.398884239888424,
      "grad_norm": 0.3425799310207367,
      "learning_rate": 1.5202231520223153e-05,
      "loss": 0.0288,
      "step": 1720
    },
    {
      "epoch": 2.4128312412831243,
      "grad_norm": 2.2944977283477783,
      "learning_rate": 1.5174337517433753e-05,
      "loss": 0.0147,
      "step": 1730
    },
    {
      "epoch": 2.426778242677824,
      "grad_norm": 4.488933086395264,
      "learning_rate": 1.5146443514644353e-05,
      "loss": 0.0393,
      "step": 1740
    },
    {
      "epoch": 2.4407252440725244,
      "grad_norm": 10.51984691619873,
      "learning_rate": 1.5118549511854953e-05,
      "loss": 0.0417,
      "step": 1750
    },
    {
      "epoch": 2.4546722454672247,
      "grad_norm": 0.18719814717769623,
      "learning_rate": 1.5090655509065552e-05,
      "loss": 0.0252,
      "step": 1760
    },
    {
      "epoch": 2.4686192468619246,
      "grad_norm": 2.6530513763427734,
      "learning_rate": 1.506276150627615e-05,
      "loss": 0.0343,
      "step": 1770
    },
    {
      "epoch": 2.482566248256625,
      "grad_norm": 2.131481170654297,
      "learning_rate": 1.5034867503486752e-05,
      "loss": 0.0454,
      "step": 1780
    },
    {
      "epoch": 2.496513249651325,
      "grad_norm": 0.6451980471611023,
      "learning_rate": 1.500697350069735e-05,
      "loss": 0.0237,
      "step": 1790
    },
    {
      "epoch": 2.510460251046025,
      "grad_norm": 4.215994834899902,
      "learning_rate": 1.4979079497907951e-05,
      "loss": 0.0438,
      "step": 1800
    },
    {
      "epoch": 2.5244072524407253,
      "grad_norm": 6.753211498260498,
      "learning_rate": 1.495118549511855e-05,
      "loss": 0.0245,
      "step": 1810
    },
    {
      "epoch": 2.5383542538354256,
      "grad_norm": 0.08745383471250534,
      "learning_rate": 1.4923291492329151e-05,
      "loss": 0.0129,
      "step": 1820
    },
    {
      "epoch": 2.5523012552301254,
      "grad_norm": 1.5862823724746704,
      "learning_rate": 1.4895397489539749e-05,
      "loss": 0.0244,
      "step": 1830
    },
    {
      "epoch": 2.5662482566248257,
      "grad_norm": 0.20747894048690796,
      "learning_rate": 1.486750348675035e-05,
      "loss": 0.0217,
      "step": 1840
    },
    {
      "epoch": 2.5801952580195255,
      "grad_norm": 0.23028260469436646,
      "learning_rate": 1.483960948396095e-05,
      "loss": 0.0268,
      "step": 1850
    },
    {
      "epoch": 2.594142259414226,
      "grad_norm": 1.7992593050003052,
      "learning_rate": 1.4811715481171548e-05,
      "loss": 0.02,
      "step": 1860
    },
    {
      "epoch": 2.608089260808926,
      "grad_norm": 1.7689611911773682,
      "learning_rate": 1.478382147838215e-05,
      "loss": 0.0249,
      "step": 1870
    },
    {
      "epoch": 2.6220362622036264,
      "grad_norm": 3.161846876144409,
      "learning_rate": 1.4755927475592748e-05,
      "loss": 0.0292,
      "step": 1880
    },
    {
      "epoch": 2.6359832635983262,
      "grad_norm": 1.7770419120788574,
      "learning_rate": 1.472803347280335e-05,
      "loss": 0.0194,
      "step": 1890
    },
    {
      "epoch": 2.6499302649930265,
      "grad_norm": 3.488950252532959,
      "learning_rate": 1.4700139470013948e-05,
      "loss": 0.0206,
      "step": 1900
    },
    {
      "epoch": 2.6638772663877264,
      "grad_norm": 2.094789505004883,
      "learning_rate": 1.4672245467224549e-05,
      "loss": 0.0308,
      "step": 1910
    },
    {
      "epoch": 2.6778242677824267,
      "grad_norm": 0.25962141156196594,
      "learning_rate": 1.4644351464435147e-05,
      "loss": 0.0242,
      "step": 1920
    },
    {
      "epoch": 2.691771269177127,
      "grad_norm": 0.47607871890068054,
      "learning_rate": 1.4616457461645747e-05,
      "loss": 0.0316,
      "step": 1930
    },
    {
      "epoch": 2.7057182705718272,
      "grad_norm": 0.7894746661186218,
      "learning_rate": 1.4588563458856347e-05,
      "loss": 0.0282,
      "step": 1940
    },
    {
      "epoch": 2.719665271966527,
      "grad_norm": 2.558196544647217,
      "learning_rate": 1.4560669456066946e-05,
      "loss": 0.0422,
      "step": 1950
    },
    {
      "epoch": 2.7336122733612274,
      "grad_norm": 0.5729178190231323,
      "learning_rate": 1.4532775453277546e-05,
      "loss": 0.0106,
      "step": 1960
    },
    {
      "epoch": 2.747559274755927,
      "grad_norm": 2.218397855758667,
      "learning_rate": 1.4504881450488146e-05,
      "loss": 0.0175,
      "step": 1970
    },
    {
      "epoch": 2.7615062761506275,
      "grad_norm": 1.5336941480636597,
      "learning_rate": 1.4476987447698744e-05,
      "loss": 0.0223,
      "step": 1980
    },
    {
      "epoch": 2.775453277545328,
      "grad_norm": 3.042720317840576,
      "learning_rate": 1.4449093444909346e-05,
      "loss": 0.0301,
      "step": 1990
    },
    {
      "epoch": 2.789400278940028,
      "grad_norm": 1.6546729803085327,
      "learning_rate": 1.4421199442119947e-05,
      "loss": 0.0194,
      "step": 2000
    },
    {
      "epoch": 2.803347280334728,
      "grad_norm": 2.472884178161621,
      "learning_rate": 1.4393305439330545e-05,
      "loss": 0.0161,
      "step": 2010
    },
    {
      "epoch": 2.817294281729428,
      "grad_norm": 4.162240028381348,
      "learning_rate": 1.4365411436541145e-05,
      "loss": 0.0255,
      "step": 2020
    },
    {
      "epoch": 2.831241283124128,
      "grad_norm": 3.873025417327881,
      "learning_rate": 1.4337517433751745e-05,
      "loss": 0.0244,
      "step": 2030
    },
    {
      "epoch": 2.8451882845188283,
      "grad_norm": 3.045571804046631,
      "learning_rate": 1.4309623430962344e-05,
      "loss": 0.0224,
      "step": 2040
    },
    {
      "epoch": 2.8591352859135286,
      "grad_norm": 1.2610762119293213,
      "learning_rate": 1.4281729428172944e-05,
      "loss": 0.0121,
      "step": 2050
    },
    {
      "epoch": 2.873082287308229,
      "grad_norm": 2.774986982345581,
      "learning_rate": 1.4253835425383544e-05,
      "loss": 0.0402,
      "step": 2060
    },
    {
      "epoch": 2.8870292887029287,
      "grad_norm": 2.7818520069122314,
      "learning_rate": 1.4225941422594142e-05,
      "loss": 0.0455,
      "step": 2070
    },
    {
      "epoch": 2.900976290097629,
      "grad_norm": 0.15286099910736084,
      "learning_rate": 1.4198047419804744e-05,
      "loss": 0.0243,
      "step": 2080
    },
    {
      "epoch": 2.914923291492329,
      "grad_norm": 1.9180853366851807,
      "learning_rate": 1.4170153417015342e-05,
      "loss": 0.0262,
      "step": 2090
    },
    {
      "epoch": 2.928870292887029,
      "grad_norm": 0.12029436975717545,
      "learning_rate": 1.4142259414225943e-05,
      "loss": 0.0205,
      "step": 2100
    },
    {
      "epoch": 2.9428172942817294,
      "grad_norm": 4.315433025360107,
      "learning_rate": 1.4114365411436541e-05,
      "loss": 0.0237,
      "step": 2110
    },
    {
      "epoch": 2.9567642956764297,
      "grad_norm": 0.08458749204874039,
      "learning_rate": 1.4086471408647143e-05,
      "loss": 0.0283,
      "step": 2120
    },
    {
      "epoch": 2.9707112970711296,
      "grad_norm": 0.5681735873222351,
      "learning_rate": 1.405857740585774e-05,
      "loss": 0.0356,
      "step": 2130
    },
    {
      "epoch": 2.98465829846583,
      "grad_norm": 0.7643511891365051,
      "learning_rate": 1.4030683403068342e-05,
      "loss": 0.0278,
      "step": 2140
    },
    {
      "epoch": 2.99860529986053,
      "grad_norm": 0.4725630581378937,
      "learning_rate": 1.4002789400278942e-05,
      "loss": 0.0197,
      "step": 2150
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.990599593495935,
      "eval_f1": 0.9592780101254677,
      "eval_loss": 0.027610452845692635,
      "eval_precision": 0.9498692240627724,
      "eval_recall": 0.9688750555802579,
      "eval_runtime": 278.651,
      "eval_samples_per_second": 70.63,
      "eval_steps_per_second": 2.211,
      "step": 2151
    },
    {
      "epoch": 3.01255230125523,
      "grad_norm": 0.2421783059835434,
      "learning_rate": 1.397489539748954e-05,
      "loss": 0.0211,
      "step": 2160
    },
    {
      "epoch": 3.0264993026499303,
      "grad_norm": 2.818358898162842,
      "learning_rate": 1.3947001394700142e-05,
      "loss": 0.0216,
      "step": 2170
    },
    {
      "epoch": 3.0404463040446306,
      "grad_norm": 0.07509764283895493,
      "learning_rate": 1.391910739191074e-05,
      "loss": 0.0178,
      "step": 2180
    },
    {
      "epoch": 3.0543933054393304,
      "grad_norm": 1.1019092798233032,
      "learning_rate": 1.3891213389121341e-05,
      "loss": 0.0082,
      "step": 2190
    },
    {
      "epoch": 3.0683403068340307,
      "grad_norm": 4.657718658447266,
      "learning_rate": 1.386331938633194e-05,
      "loss": 0.0264,
      "step": 2200
    },
    {
      "epoch": 3.082287308228731,
      "grad_norm": 0.7019718289375305,
      "learning_rate": 1.383542538354254e-05,
      "loss": 0.0342,
      "step": 2210
    },
    {
      "epoch": 3.096234309623431,
      "grad_norm": 0.8041576147079468,
      "learning_rate": 1.3807531380753139e-05,
      "loss": 0.0211,
      "step": 2220
    },
    {
      "epoch": 3.110181311018131,
      "grad_norm": 0.8853076100349426,
      "learning_rate": 1.3779637377963739e-05,
      "loss": 0.0071,
      "step": 2230
    },
    {
      "epoch": 3.1241283124128314,
      "grad_norm": 0.06635182350873947,
      "learning_rate": 1.3751743375174338e-05,
      "loss": 0.0107,
      "step": 2240
    },
    {
      "epoch": 3.1380753138075312,
      "grad_norm": 0.48780304193496704,
      "learning_rate": 1.3723849372384938e-05,
      "loss": 0.0211,
      "step": 2250
    },
    {
      "epoch": 3.1520223152022315,
      "grad_norm": 2.6552724838256836,
      "learning_rate": 1.3695955369595538e-05,
      "loss": 0.0154,
      "step": 2260
    },
    {
      "epoch": 3.165969316596932,
      "grad_norm": 0.11213412135839462,
      "learning_rate": 1.3668061366806138e-05,
      "loss": 0.009,
      "step": 2270
    },
    {
      "epoch": 3.1799163179916317,
      "grad_norm": 1.771400809288025,
      "learning_rate": 1.3640167364016736e-05,
      "loss": 0.0109,
      "step": 2280
    },
    {
      "epoch": 3.193863319386332,
      "grad_norm": 1.697243094444275,
      "learning_rate": 1.3612273361227337e-05,
      "loss": 0.0202,
      "step": 2290
    },
    {
      "epoch": 3.2078103207810322,
      "grad_norm": 2.518815279006958,
      "learning_rate": 1.3584379358437939e-05,
      "loss": 0.0201,
      "step": 2300
    },
    {
      "epoch": 3.221757322175732,
      "grad_norm": 1.3360759019851685,
      "learning_rate": 1.3556485355648537e-05,
      "loss": 0.0139,
      "step": 2310
    },
    {
      "epoch": 3.2357043235704324,
      "grad_norm": 0.4885372519493103,
      "learning_rate": 1.3528591352859137e-05,
      "loss": 0.0117,
      "step": 2320
    },
    {
      "epoch": 3.2496513249651326,
      "grad_norm": 0.37567099928855896,
      "learning_rate": 1.3500697350069736e-05,
      "loss": 0.0151,
      "step": 2330
    },
    {
      "epoch": 3.2635983263598325,
      "grad_norm": 0.45556050539016724,
      "learning_rate": 1.3472803347280336e-05,
      "loss": 0.0118,
      "step": 2340
    },
    {
      "epoch": 3.2775453277545328,
      "grad_norm": 1.212725043296814,
      "learning_rate": 1.3444909344490936e-05,
      "loss": 0.025,
      "step": 2350
    },
    {
      "epoch": 3.291492329149233,
      "grad_norm": 1.7717835903167725,
      "learning_rate": 1.3417015341701536e-05,
      "loss": 0.0117,
      "step": 2360
    },
    {
      "epoch": 3.305439330543933,
      "grad_norm": 0.3807908296585083,
      "learning_rate": 1.3389121338912134e-05,
      "loss": 0.019,
      "step": 2370
    },
    {
      "epoch": 3.319386331938633,
      "grad_norm": 2.0537185668945312,
      "learning_rate": 1.3361227336122735e-05,
      "loss": 0.0243,
      "step": 2380
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 0.388047456741333,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0252,
      "step": 2390
    },
    {
      "epoch": 3.3472803347280333,
      "grad_norm": 2.7461585998535156,
      "learning_rate": 1.3305439330543935e-05,
      "loss": 0.0137,
      "step": 2400
    },
    {
      "epoch": 3.3612273361227336,
      "grad_norm": 0.24165678024291992,
      "learning_rate": 1.3277545327754533e-05,
      "loss": 0.0287,
      "step": 2410
    },
    {
      "epoch": 3.375174337517434,
      "grad_norm": 0.7686974406242371,
      "learning_rate": 1.3249651324965134e-05,
      "loss": 0.0185,
      "step": 2420
    },
    {
      "epoch": 3.3891213389121337,
      "grad_norm": 0.1364087611436844,
      "learning_rate": 1.3221757322175732e-05,
      "loss": 0.0245,
      "step": 2430
    },
    {
      "epoch": 3.403068340306834,
      "grad_norm": 0.7799211144447327,
      "learning_rate": 1.3193863319386332e-05,
      "loss": 0.0224,
      "step": 2440
    },
    {
      "epoch": 3.4170153417015343,
      "grad_norm": 1.8519158363342285,
      "learning_rate": 1.3165969316596934e-05,
      "loss": 0.028,
      "step": 2450
    },
    {
      "epoch": 3.430962343096234,
      "grad_norm": 0.6538898348808289,
      "learning_rate": 1.3138075313807532e-05,
      "loss": 0.0201,
      "step": 2460
    },
    {
      "epoch": 3.4449093444909344,
      "grad_norm": 5.0084381103515625,
      "learning_rate": 1.3110181311018133e-05,
      "loss": 0.0238,
      "step": 2470
    },
    {
      "epoch": 3.4588563458856347,
      "grad_norm": 1.2359118461608887,
      "learning_rate": 1.3082287308228731e-05,
      "loss": 0.025,
      "step": 2480
    },
    {
      "epoch": 3.4728033472803346,
      "grad_norm": 1.4806896448135376,
      "learning_rate": 1.3054393305439333e-05,
      "loss": 0.0096,
      "step": 2490
    },
    {
      "epoch": 3.486750348675035,
      "grad_norm": 3.517350912094116,
      "learning_rate": 1.3026499302649931e-05,
      "loss": 0.024,
      "step": 2500
    },
    {
      "epoch": 3.500697350069735,
      "grad_norm": 0.9751824736595154,
      "learning_rate": 1.2998605299860532e-05,
      "loss": 0.0204,
      "step": 2510
    },
    {
      "epoch": 3.514644351464435,
      "grad_norm": 0.3418366611003876,
      "learning_rate": 1.297071129707113e-05,
      "loss": 0.0154,
      "step": 2520
    },
    {
      "epoch": 3.5285913528591353,
      "grad_norm": 0.34558355808258057,
      "learning_rate": 1.294281729428173e-05,
      "loss": 0.0141,
      "step": 2530
    },
    {
      "epoch": 3.5425383542538356,
      "grad_norm": 1.9680157899856567,
      "learning_rate": 1.291492329149233e-05,
      "loss": 0.0147,
      "step": 2540
    },
    {
      "epoch": 3.5564853556485354,
      "grad_norm": 2.641212224960327,
      "learning_rate": 1.288702928870293e-05,
      "loss": 0.0176,
      "step": 2550
    },
    {
      "epoch": 3.5704323570432357,
      "grad_norm": 1.6573816537857056,
      "learning_rate": 1.285913528591353e-05,
      "loss": 0.0121,
      "step": 2560
    },
    {
      "epoch": 3.584379358437936,
      "grad_norm": 2.6428260803222656,
      "learning_rate": 1.283124128312413e-05,
      "loss": 0.0278,
      "step": 2570
    },
    {
      "epoch": 3.598326359832636,
      "grad_norm": 1.3036900758743286,
      "learning_rate": 1.2803347280334727e-05,
      "loss": 0.0147,
      "step": 2580
    },
    {
      "epoch": 3.612273361227336,
      "grad_norm": 1.3841474056243896,
      "learning_rate": 1.2775453277545329e-05,
      "loss": 0.0281,
      "step": 2590
    },
    {
      "epoch": 3.6262203626220364,
      "grad_norm": 2.0588130950927734,
      "learning_rate": 1.274755927475593e-05,
      "loss": 0.0154,
      "step": 2600
    },
    {
      "epoch": 3.6401673640167362,
      "grad_norm": 1.4600170850753784,
      "learning_rate": 1.2719665271966528e-05,
      "loss": 0.0416,
      "step": 2610
    },
    {
      "epoch": 3.6541143654114365,
      "grad_norm": 0.06448771059513092,
      "learning_rate": 1.2691771269177128e-05,
      "loss": 0.02,
      "step": 2620
    },
    {
      "epoch": 3.668061366806137,
      "grad_norm": 2.817124605178833,
      "learning_rate": 1.2663877266387728e-05,
      "loss": 0.0331,
      "step": 2630
    },
    {
      "epoch": 3.6820083682008367,
      "grad_norm": 2.007983684539795,
      "learning_rate": 1.2635983263598328e-05,
      "loss": 0.013,
      "step": 2640
    },
    {
      "epoch": 3.695955369595537,
      "grad_norm": 4.004310607910156,
      "learning_rate": 1.2608089260808928e-05,
      "loss": 0.019,
      "step": 2650
    },
    {
      "epoch": 3.7099023709902372,
      "grad_norm": 2.0062527656555176,
      "learning_rate": 1.2580195258019527e-05,
      "loss": 0.0135,
      "step": 2660
    },
    {
      "epoch": 3.723849372384937,
      "grad_norm": 0.24097684025764465,
      "learning_rate": 1.2552301255230125e-05,
      "loss": 0.0204,
      "step": 2670
    },
    {
      "epoch": 3.7377963737796374,
      "grad_norm": 3.1339645385742188,
      "learning_rate": 1.2524407252440727e-05,
      "loss": 0.0255,
      "step": 2680
    },
    {
      "epoch": 3.7517433751743376,
      "grad_norm": 3.5627942085266113,
      "learning_rate": 1.2496513249651325e-05,
      "loss": 0.0137,
      "step": 2690
    },
    {
      "epoch": 3.7656903765690375,
      "grad_norm": 0.9152580499649048,
      "learning_rate": 1.2468619246861926e-05,
      "loss": 0.0118,
      "step": 2700
    },
    {
      "epoch": 3.7796373779637378,
      "grad_norm": 1.2523505687713623,
      "learning_rate": 1.2440725244072524e-05,
      "loss": 0.0147,
      "step": 2710
    },
    {
      "epoch": 3.793584379358438,
      "grad_norm": 1.3529068231582642,
      "learning_rate": 1.2412831241283126e-05,
      "loss": 0.015,
      "step": 2720
    },
    {
      "epoch": 3.8075313807531384,
      "grad_norm": 0.5823209285736084,
      "learning_rate": 1.2384937238493724e-05,
      "loss": 0.0091,
      "step": 2730
    },
    {
      "epoch": 3.821478382147838,
      "grad_norm": 1.6694433689117432,
      "learning_rate": 1.2357043235704324e-05,
      "loss": 0.0261,
      "step": 2740
    },
    {
      "epoch": 3.8354253835425385,
      "grad_norm": 0.39228591322898865,
      "learning_rate": 1.2329149232914925e-05,
      "loss": 0.0158,
      "step": 2750
    },
    {
      "epoch": 3.8493723849372383,
      "grad_norm": 0.9959983825683594,
      "learning_rate": 1.2301255230125523e-05,
      "loss": 0.0069,
      "step": 2760
    },
    {
      "epoch": 3.8633193863319386,
      "grad_norm": 2.09320068359375,
      "learning_rate": 1.2273361227336125e-05,
      "loss": 0.0222,
      "step": 2770
    },
    {
      "epoch": 3.877266387726639,
      "grad_norm": 4.61893892288208,
      "learning_rate": 1.2245467224546723e-05,
      "loss": 0.0172,
      "step": 2780
    },
    {
      "epoch": 3.891213389121339,
      "grad_norm": 1.3634169101715088,
      "learning_rate": 1.2217573221757324e-05,
      "loss": 0.0088,
      "step": 2790
    },
    {
      "epoch": 3.905160390516039,
      "grad_norm": 3.303529977798462,
      "learning_rate": 1.2189679218967922e-05,
      "loss": 0.0312,
      "step": 2800
    },
    {
      "epoch": 3.9191073919107393,
      "grad_norm": 0.21824166178703308,
      "learning_rate": 1.2161785216178524e-05,
      "loss": 0.0173,
      "step": 2810
    },
    {
      "epoch": 3.933054393305439,
      "grad_norm": 0.13058525323867798,
      "learning_rate": 1.2133891213389122e-05,
      "loss": 0.0138,
      "step": 2820
    },
    {
      "epoch": 3.9470013947001394,
      "grad_norm": 4.162825584411621,
      "learning_rate": 1.2105997210599722e-05,
      "loss": 0.0154,
      "step": 2830
    },
    {
      "epoch": 3.9609483960948397,
      "grad_norm": 2.0677952766418457,
      "learning_rate": 1.2078103207810322e-05,
      "loss": 0.0176,
      "step": 2840
    },
    {
      "epoch": 3.97489539748954,
      "grad_norm": 1.0993648767471313,
      "learning_rate": 1.2050209205020921e-05,
      "loss": 0.0136,
      "step": 2850
    },
    {
      "epoch": 3.98884239888424,
      "grad_norm": 1.8692467212677002,
      "learning_rate": 1.2022315202231521e-05,
      "loss": 0.0151,
      "step": 2860
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.9919715447154471,
      "eval_f1": 0.9652747252747252,
      "eval_loss": 0.028260601684451103,
      "eval_precision": 0.954367666232073,
      "eval_recall": 0.9764339706536238,
      "eval_runtime": 281.287,
      "eval_samples_per_second": 69.968,
      "eval_steps_per_second": 2.19,
      "step": 2868
    },
    {
      "epoch": 4.00278940027894,
      "grad_norm": 4.279382228851318,
      "learning_rate": 1.1994421199442121e-05,
      "loss": 0.0218,
      "step": 2870
    },
    {
      "epoch": 4.01673640167364,
      "grad_norm": 0.7858643531799316,
      "learning_rate": 1.1966527196652719e-05,
      "loss": 0.01,
      "step": 2880
    },
    {
      "epoch": 4.03068340306834,
      "grad_norm": 1.6759719848632812,
      "learning_rate": 1.193863319386332e-05,
      "loss": 0.0202,
      "step": 2890
    },
    {
      "epoch": 4.044630404463041,
      "grad_norm": 1.1469736099243164,
      "learning_rate": 1.1910739191073922e-05,
      "loss": 0.007,
      "step": 2900
    },
    {
      "epoch": 4.058577405857741,
      "grad_norm": 0.9899379014968872,
      "learning_rate": 1.188284518828452e-05,
      "loss": 0.007,
      "step": 2910
    },
    {
      "epoch": 4.072524407252441,
      "grad_norm": 0.22759923338890076,
      "learning_rate": 1.185495118549512e-05,
      "loss": 0.0115,
      "step": 2920
    },
    {
      "epoch": 4.0864714086471405,
      "grad_norm": 0.034604236483573914,
      "learning_rate": 1.182705718270572e-05,
      "loss": 0.0108,
      "step": 2930
    },
    {
      "epoch": 4.100418410041841,
      "grad_norm": 1.8263787031173706,
      "learning_rate": 1.179916317991632e-05,
      "loss": 0.0042,
      "step": 2940
    },
    {
      "epoch": 4.114365411436541,
      "grad_norm": 0.055677857249975204,
      "learning_rate": 1.1771269177126917e-05,
      "loss": 0.0069,
      "step": 2950
    },
    {
      "epoch": 4.128312412831241,
      "grad_norm": 0.06823903322219849,
      "learning_rate": 1.1743375174337519e-05,
      "loss": 0.0152,
      "step": 2960
    },
    {
      "epoch": 4.142259414225942,
      "grad_norm": 0.12226226180791855,
      "learning_rate": 1.1715481171548117e-05,
      "loss": 0.0136,
      "step": 2970
    },
    {
      "epoch": 4.156206415620642,
      "grad_norm": 3.7261009216308594,
      "learning_rate": 1.1687587168758718e-05,
      "loss": 0.0169,
      "step": 2980
    },
    {
      "epoch": 4.170153417015341,
      "grad_norm": 0.21641531586647034,
      "learning_rate": 1.1659693165969317e-05,
      "loss": 0.0148,
      "step": 2990
    },
    {
      "epoch": 4.184100418410042,
      "grad_norm": 0.10880794376134872,
      "learning_rate": 1.1631799163179918e-05,
      "loss": 0.0129,
      "step": 3000
    },
    {
      "epoch": 4.198047419804742,
      "grad_norm": 0.12900924682617188,
      "learning_rate": 1.1603905160390516e-05,
      "loss": 0.0051,
      "step": 3010
    },
    {
      "epoch": 4.211994421199442,
      "grad_norm": 0.03371737524867058,
      "learning_rate": 1.1576011157601118e-05,
      "loss": 0.016,
      "step": 3020
    },
    {
      "epoch": 4.2259414225941425,
      "grad_norm": 0.1890646070241928,
      "learning_rate": 1.1548117154811716e-05,
      "loss": 0.0105,
      "step": 3030
    },
    {
      "epoch": 4.239888423988843,
      "grad_norm": 6.198490619659424,
      "learning_rate": 1.1520223152022315e-05,
      "loss": 0.005,
      "step": 3040
    },
    {
      "epoch": 4.253835425383542,
      "grad_norm": 0.23366917669773102,
      "learning_rate": 1.1492329149232915e-05,
      "loss": 0.0089,
      "step": 3050
    },
    {
      "epoch": 4.2677824267782425,
      "grad_norm": 1.2497159242630005,
      "learning_rate": 1.1464435146443515e-05,
      "loss": 0.0083,
      "step": 3060
    },
    {
      "epoch": 4.281729428172943,
      "grad_norm": 0.09439717233181,
      "learning_rate": 1.1436541143654116e-05,
      "loss": 0.0113,
      "step": 3070
    },
    {
      "epoch": 4.295676429567643,
      "grad_norm": 1.9609819650650024,
      "learning_rate": 1.1408647140864715e-05,
      "loss": 0.0168,
      "step": 3080
    },
    {
      "epoch": 4.309623430962343,
      "grad_norm": 1.3599574565887451,
      "learning_rate": 1.1380753138075316e-05,
      "loss": 0.0096,
      "step": 3090
    },
    {
      "epoch": 4.323570432357044,
      "grad_norm": 0.8270103335380554,
      "learning_rate": 1.1352859135285914e-05,
      "loss": 0.0062,
      "step": 3100
    },
    {
      "epoch": 4.337517433751743,
      "grad_norm": 0.794647216796875,
      "learning_rate": 1.1324965132496516e-05,
      "loss": 0.0083,
      "step": 3110
    },
    {
      "epoch": 4.351464435146443,
      "grad_norm": 0.052881065756082535,
      "learning_rate": 1.1297071129707114e-05,
      "loss": 0.0228,
      "step": 3120
    },
    {
      "epoch": 4.365411436541144,
      "grad_norm": 0.45213449001312256,
      "learning_rate": 1.1269177126917713e-05,
      "loss": 0.0134,
      "step": 3130
    },
    {
      "epoch": 4.379358437935844,
      "grad_norm": 0.07234801352024078,
      "learning_rate": 1.1241283124128313e-05,
      "loss": 0.0123,
      "step": 3140
    },
    {
      "epoch": 4.393305439330544,
      "grad_norm": 0.26651227474212646,
      "learning_rate": 1.1213389121338913e-05,
      "loss": 0.0064,
      "step": 3150
    },
    {
      "epoch": 4.4072524407252445,
      "grad_norm": 0.4235111474990845,
      "learning_rate": 1.1185495118549513e-05,
      "loss": 0.0053,
      "step": 3160
    },
    {
      "epoch": 4.421199442119944,
      "grad_norm": 2.6142375469207764,
      "learning_rate": 1.1157601115760113e-05,
      "loss": 0.0073,
      "step": 3170
    },
    {
      "epoch": 4.435146443514644,
      "grad_norm": 1.4009675979614258,
      "learning_rate": 1.112970711297071e-05,
      "loss": 0.0137,
      "step": 3180
    },
    {
      "epoch": 4.449093444909344,
      "grad_norm": 0.33765965700149536,
      "learning_rate": 1.1101813110181312e-05,
      "loss": 0.0106,
      "step": 3190
    },
    {
      "epoch": 4.463040446304045,
      "grad_norm": 0.04191507771611214,
      "learning_rate": 1.107391910739191e-05,
      "loss": 0.0069,
      "step": 3200
    },
    {
      "epoch": 4.476987447698745,
      "grad_norm": 0.7595380544662476,
      "learning_rate": 1.1046025104602512e-05,
      "loss": 0.0092,
      "step": 3210
    },
    {
      "epoch": 4.490934449093445,
      "grad_norm": 0.062127597630023956,
      "learning_rate": 1.1018131101813111e-05,
      "loss": 0.0018,
      "step": 3220
    },
    {
      "epoch": 4.504881450488145,
      "grad_norm": 2.2319061756134033,
      "learning_rate": 1.0990237099023711e-05,
      "loss": 0.0091,
      "step": 3230
    },
    {
      "epoch": 4.518828451882845,
      "grad_norm": 3.290994167327881,
      "learning_rate": 1.0962343096234311e-05,
      "loss": 0.0055,
      "step": 3240
    },
    {
      "epoch": 4.532775453277545,
      "grad_norm": 0.058180905878543854,
      "learning_rate": 1.0934449093444909e-05,
      "loss": 0.0123,
      "step": 3250
    },
    {
      "epoch": 4.546722454672246,
      "grad_norm": 6.332539081573486,
      "learning_rate": 1.090655509065551e-05,
      "loss": 0.0178,
      "step": 3260
    },
    {
      "epoch": 4.560669456066946,
      "grad_norm": 5.191953182220459,
      "learning_rate": 1.0878661087866109e-05,
      "loss": 0.024,
      "step": 3270
    },
    {
      "epoch": 4.574616457461646,
      "grad_norm": 1.1005725860595703,
      "learning_rate": 1.085076708507671e-05,
      "loss": 0.0176,
      "step": 3280
    },
    {
      "epoch": 4.5885634588563455,
      "grad_norm": 3.660923480987549,
      "learning_rate": 1.0822873082287308e-05,
      "loss": 0.0153,
      "step": 3290
    },
    {
      "epoch": 4.602510460251046,
      "grad_norm": 1.4061567783355713,
      "learning_rate": 1.079497907949791e-05,
      "loss": 0.0138,
      "step": 3300
    },
    {
      "epoch": 4.616457461645746,
      "grad_norm": 0.17104217410087585,
      "learning_rate": 1.0767085076708508e-05,
      "loss": 0.0245,
      "step": 3310
    },
    {
      "epoch": 4.630404463040446,
      "grad_norm": 1.126923680305481,
      "learning_rate": 1.073919107391911e-05,
      "loss": 0.0066,
      "step": 3320
    },
    {
      "epoch": 4.644351464435147,
      "grad_norm": 0.030535155907273293,
      "learning_rate": 1.0711297071129707e-05,
      "loss": 0.0134,
      "step": 3330
    },
    {
      "epoch": 4.658298465829847,
      "grad_norm": 3.949070930480957,
      "learning_rate": 1.0683403068340307e-05,
      "loss": 0.029,
      "step": 3340
    },
    {
      "epoch": 4.672245467224546,
      "grad_norm": 2.342012643814087,
      "learning_rate": 1.0655509065550907e-05,
      "loss": 0.0177,
      "step": 3350
    },
    {
      "epoch": 4.686192468619247,
      "grad_norm": 1.6700530052185059,
      "learning_rate": 1.0627615062761507e-05,
      "loss": 0.0241,
      "step": 3360
    },
    {
      "epoch": 4.700139470013947,
      "grad_norm": 0.5901055335998535,
      "learning_rate": 1.0599721059972108e-05,
      "loss": 0.0088,
      "step": 3370
    },
    {
      "epoch": 4.714086471408647,
      "grad_norm": 0.23388172686100006,
      "learning_rate": 1.0571827057182706e-05,
      "loss": 0.0075,
      "step": 3380
    },
    {
      "epoch": 4.7280334728033475,
      "grad_norm": 1.1487469673156738,
      "learning_rate": 1.0543933054393308e-05,
      "loss": 0.0093,
      "step": 3390
    },
    {
      "epoch": 4.741980474198048,
      "grad_norm": 0.19632072746753693,
      "learning_rate": 1.0516039051603906e-05,
      "loss": 0.012,
      "step": 3400
    },
    {
      "epoch": 4.755927475592747,
      "grad_norm": 1.2749669551849365,
      "learning_rate": 1.0488145048814507e-05,
      "loss": 0.0069,
      "step": 3410
    },
    {
      "epoch": 4.7698744769874475,
      "grad_norm": 1.2036583423614502,
      "learning_rate": 1.0460251046025105e-05,
      "loss": 0.0203,
      "step": 3420
    },
    {
      "epoch": 4.783821478382148,
      "grad_norm": 0.21418815851211548,
      "learning_rate": 1.0432357043235705e-05,
      "loss": 0.0113,
      "step": 3430
    },
    {
      "epoch": 4.797768479776848,
      "grad_norm": 1.187900185585022,
      "learning_rate": 1.0404463040446305e-05,
      "loss": 0.0063,
      "step": 3440
    },
    {
      "epoch": 4.811715481171548,
      "grad_norm": 3.2229161262512207,
      "learning_rate": 1.0376569037656905e-05,
      "loss": 0.0301,
      "step": 3450
    },
    {
      "epoch": 4.825662482566249,
      "grad_norm": 1.841607689857483,
      "learning_rate": 1.0348675034867503e-05,
      "loss": 0.0136,
      "step": 3460
    },
    {
      "epoch": 4.839609483960948,
      "grad_norm": 0.18270404636859894,
      "learning_rate": 1.0320781032078104e-05,
      "loss": 0.0171,
      "step": 3470
    },
    {
      "epoch": 4.853556485355648,
      "grad_norm": 0.06433646380901337,
      "learning_rate": 1.0292887029288702e-05,
      "loss": 0.0147,
      "step": 3480
    },
    {
      "epoch": 4.867503486750349,
      "grad_norm": 0.08547468483448029,
      "learning_rate": 1.0264993026499304e-05,
      "loss": 0.0134,
      "step": 3490
    },
    {
      "epoch": 4.881450488145049,
      "grad_norm": 0.9363776445388794,
      "learning_rate": 1.0237099023709902e-05,
      "loss": 0.0062,
      "step": 3500
    },
    {
      "epoch": 4.895397489539749,
      "grad_norm": 1.7721753120422363,
      "learning_rate": 1.0209205020920503e-05,
      "loss": 0.0141,
      "step": 3510
    },
    {
      "epoch": 4.9093444909344495,
      "grad_norm": 0.17318347096443176,
      "learning_rate": 1.0181311018131103e-05,
      "loss": 0.0087,
      "step": 3520
    },
    {
      "epoch": 4.923291492329149,
      "grad_norm": 0.8607273697853088,
      "learning_rate": 1.0153417015341703e-05,
      "loss": 0.0088,
      "step": 3530
    },
    {
      "epoch": 4.937238493723849,
      "grad_norm": 2.2601823806762695,
      "learning_rate": 1.0125523012552303e-05,
      "loss": 0.0103,
      "step": 3540
    },
    {
      "epoch": 4.951185495118549,
      "grad_norm": 2.844405174255371,
      "learning_rate": 1.00976290097629e-05,
      "loss": 0.0092,
      "step": 3550
    },
    {
      "epoch": 4.96513249651325,
      "grad_norm": 0.14576998353004456,
      "learning_rate": 1.0069735006973502e-05,
      "loss": 0.0248,
      "step": 3560
    },
    {
      "epoch": 4.97907949790795,
      "grad_norm": 0.07118240743875504,
      "learning_rate": 1.00418410041841e-05,
      "loss": 0.0135,
      "step": 3570
    },
    {
      "epoch": 4.99302649930265,
      "grad_norm": 1.9179130792617798,
      "learning_rate": 1.0013947001394702e-05,
      "loss": 0.0053,
      "step": 3580
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.9936991869918699,
      "eval_f1": 0.9723830734966594,
      "eval_loss": 0.021764181554317474,
      "eval_precision": 0.9741186970102633,
      "eval_recall": 0.9706536238328146,
      "eval_runtime": 246.8211,
      "eval_samples_per_second": 79.738,
      "eval_steps_per_second": 2.496,
      "step": 3585
    },
    {
      "epoch": 5.00697350069735,
      "grad_norm": 1.0066591501235962,
      "learning_rate": 9.986052998605302e-06,
      "loss": 0.0045,
      "step": 3590
    },
    {
      "epoch": 5.02092050209205,
      "grad_norm": 1.3447682857513428,
      "learning_rate": 9.958158995815901e-06,
      "loss": 0.0041,
      "step": 3600
    },
    {
      "epoch": 5.03486750348675,
      "grad_norm": 0.3493168354034424,
      "learning_rate": 9.930264993026501e-06,
      "loss": 0.0033,
      "step": 3610
    },
    {
      "epoch": 5.048814504881451,
      "grad_norm": 1.212352991104126,
      "learning_rate": 9.902370990237101e-06,
      "loss": 0.012,
      "step": 3620
    },
    {
      "epoch": 5.062761506276151,
      "grad_norm": 0.005399730056524277,
      "learning_rate": 9.874476987447699e-06,
      "loss": 0.0076,
      "step": 3630
    },
    {
      "epoch": 5.076708507670851,
      "grad_norm": 0.3637447655200958,
      "learning_rate": 9.846582984658299e-06,
      "loss": 0.0065,
      "step": 3640
    },
    {
      "epoch": 5.0906555090655505,
      "grad_norm": 0.0038138183299452066,
      "learning_rate": 9.818688981868899e-06,
      "loss": 0.0061,
      "step": 3650
    },
    {
      "epoch": 5.104602510460251,
      "grad_norm": 4.082736492156982,
      "learning_rate": 9.790794979079498e-06,
      "loss": 0.0118,
      "step": 3660
    },
    {
      "epoch": 5.118549511854951,
      "grad_norm": 0.47017666697502136,
      "learning_rate": 9.762900976290098e-06,
      "loss": 0.01,
      "step": 3670
    },
    {
      "epoch": 5.132496513249651,
      "grad_norm": 0.8115451335906982,
      "learning_rate": 9.735006973500698e-06,
      "loss": 0.0037,
      "step": 3680
    },
    {
      "epoch": 5.146443514644352,
      "grad_norm": 0.6838297247886658,
      "learning_rate": 9.707112970711298e-06,
      "loss": 0.0077,
      "step": 3690
    },
    {
      "epoch": 5.160390516039052,
      "grad_norm": 8.231048583984375,
      "learning_rate": 9.679218967921897e-06,
      "loss": 0.0056,
      "step": 3700
    },
    {
      "epoch": 5.174337517433751,
      "grad_norm": 0.0446716845035553,
      "learning_rate": 9.651324965132497e-06,
      "loss": 0.0148,
      "step": 3710
    },
    {
      "epoch": 5.188284518828452,
      "grad_norm": 0.2881154417991638,
      "learning_rate": 9.623430962343097e-06,
      "loss": 0.0035,
      "step": 3720
    },
    {
      "epoch": 5.202231520223152,
      "grad_norm": 0.002786905039101839,
      "learning_rate": 9.595536959553697e-06,
      "loss": 0.005,
      "step": 3730
    },
    {
      "epoch": 5.216178521617852,
      "grad_norm": 0.5509651899337769,
      "learning_rate": 9.567642956764297e-06,
      "loss": 0.0101,
      "step": 3740
    },
    {
      "epoch": 5.2301255230125525,
      "grad_norm": 0.05007585883140564,
      "learning_rate": 9.539748953974896e-06,
      "loss": 0.0098,
      "step": 3750
    },
    {
      "epoch": 5.244072524407253,
      "grad_norm": 1.7808743715286255,
      "learning_rate": 9.511854951185496e-06,
      "loss": 0.0079,
      "step": 3760
    },
    {
      "epoch": 5.258019525801952,
      "grad_norm": 0.019765052944421768,
      "learning_rate": 9.483960948396096e-06,
      "loss": 0.0089,
      "step": 3770
    },
    {
      "epoch": 5.2719665271966525,
      "grad_norm": 0.4757174253463745,
      "learning_rate": 9.456066945606696e-06,
      "loss": 0.01,
      "step": 3780
    },
    {
      "epoch": 5.285913528591353,
      "grad_norm": 0.603591799736023,
      "learning_rate": 9.428172942817295e-06,
      "loss": 0.0115,
      "step": 3790
    },
    {
      "epoch": 5.299860529986053,
      "grad_norm": 0.1351752132177353,
      "learning_rate": 9.400278940027895e-06,
      "loss": 0.0021,
      "step": 3800
    },
    {
      "epoch": 5.313807531380753,
      "grad_norm": 3.828801393508911,
      "learning_rate": 9.372384937238495e-06,
      "loss": 0.0114,
      "step": 3810
    },
    {
      "epoch": 5.327754532775454,
      "grad_norm": 0.667027473449707,
      "learning_rate": 9.344490934449095e-06,
      "loss": 0.0108,
      "step": 3820
    },
    {
      "epoch": 5.341701534170153,
      "grad_norm": 0.2701655924320221,
      "learning_rate": 9.316596931659695e-06,
      "loss": 0.0056,
      "step": 3830
    },
    {
      "epoch": 5.355648535564853,
      "grad_norm": 0.41894859075546265,
      "learning_rate": 9.288702928870293e-06,
      "loss": 0.0068,
      "step": 3840
    },
    {
      "epoch": 5.369595536959554,
      "grad_norm": 0.3125480115413666,
      "learning_rate": 9.260808926080892e-06,
      "loss": 0.0151,
      "step": 3850
    },
    {
      "epoch": 5.383542538354254,
      "grad_norm": 0.10397185385227203,
      "learning_rate": 9.232914923291492e-06,
      "loss": 0.0181,
      "step": 3860
    },
    {
      "epoch": 5.397489539748954,
      "grad_norm": 2.200453519821167,
      "learning_rate": 9.205020920502092e-06,
      "loss": 0.0058,
      "step": 3870
    },
    {
      "epoch": 5.4114365411436545,
      "grad_norm": 1.3846886157989502,
      "learning_rate": 9.177126917712692e-06,
      "loss": 0.0092,
      "step": 3880
    },
    {
      "epoch": 5.425383542538354,
      "grad_norm": 0.5226691961288452,
      "learning_rate": 9.149232914923292e-06,
      "loss": 0.0093,
      "step": 3890
    },
    {
      "epoch": 5.439330543933054,
      "grad_norm": 0.11022493988275528,
      "learning_rate": 9.121338912133893e-06,
      "loss": 0.0076,
      "step": 3900
    },
    {
      "epoch": 5.453277545327754,
      "grad_norm": 1.1759190559387207,
      "learning_rate": 9.093444909344493e-06,
      "loss": 0.0163,
      "step": 3910
    },
    {
      "epoch": 5.467224546722455,
      "grad_norm": 0.10490567237138748,
      "learning_rate": 9.065550906555093e-06,
      "loss": 0.0078,
      "step": 3920
    },
    {
      "epoch": 5.481171548117155,
      "grad_norm": 0.563797116279602,
      "learning_rate": 9.03765690376569e-06,
      "loss": 0.0082,
      "step": 3930
    },
    {
      "epoch": 5.495118549511855,
      "grad_norm": 0.13703793287277222,
      "learning_rate": 9.00976290097629e-06,
      "loss": 0.0061,
      "step": 3940
    },
    {
      "epoch": 5.509065550906556,
      "grad_norm": 0.6734590530395508,
      "learning_rate": 8.98186889818689e-06,
      "loss": 0.0093,
      "step": 3950
    },
    {
      "epoch": 5.523012552301255,
      "grad_norm": 0.1408083289861679,
      "learning_rate": 8.95397489539749e-06,
      "loss": 0.0193,
      "step": 3960
    },
    {
      "epoch": 5.536959553695955,
      "grad_norm": 0.045703355222940445,
      "learning_rate": 8.92608089260809e-06,
      "loss": 0.0099,
      "step": 3970
    },
    {
      "epoch": 5.550906555090656,
      "grad_norm": 0.19768844544887543,
      "learning_rate": 8.89818688981869e-06,
      "loss": 0.0056,
      "step": 3980
    },
    {
      "epoch": 5.564853556485356,
      "grad_norm": 0.07546484470367432,
      "learning_rate": 8.87029288702929e-06,
      "loss": 0.014,
      "step": 3990
    },
    {
      "epoch": 5.578800557880056,
      "grad_norm": 2.0046236515045166,
      "learning_rate": 8.842398884239889e-06,
      "loss": 0.0057,
      "step": 4000
    },
    {
      "epoch": 5.5927475592747555,
      "grad_norm": 0.59581059217453,
      "learning_rate": 8.814504881450489e-06,
      "loss": 0.0073,
      "step": 4010
    },
    {
      "epoch": 5.606694560669456,
      "grad_norm": 0.027224810793995857,
      "learning_rate": 8.786610878661089e-06,
      "loss": 0.0032,
      "step": 4020
    },
    {
      "epoch": 5.620641562064156,
      "grad_norm": 2.4953927993774414,
      "learning_rate": 8.758716875871688e-06,
      "loss": 0.0148,
      "step": 4030
    },
    {
      "epoch": 5.634588563458856,
      "grad_norm": 3.5835554599761963,
      "learning_rate": 8.730822873082288e-06,
      "loss": 0.0114,
      "step": 4040
    },
    {
      "epoch": 5.648535564853557,
      "grad_norm": 0.2627483010292053,
      "learning_rate": 8.702928870292888e-06,
      "loss": 0.006,
      "step": 4050
    },
    {
      "epoch": 5.662482566248257,
      "grad_norm": 0.010564275085926056,
      "learning_rate": 8.675034867503488e-06,
      "loss": 0.0081,
      "step": 4060
    },
    {
      "epoch": 5.676429567642957,
      "grad_norm": 1.777307391166687,
      "learning_rate": 8.647140864714088e-06,
      "loss": 0.0111,
      "step": 4070
    },
    {
      "epoch": 5.690376569037657,
      "grad_norm": 0.5131472945213318,
      "learning_rate": 8.619246861924687e-06,
      "loss": 0.0124,
      "step": 4080
    },
    {
      "epoch": 5.704323570432357,
      "grad_norm": 2.754464864730835,
      "learning_rate": 8.591352859135287e-06,
      "loss": 0.0108,
      "step": 4090
    },
    {
      "epoch": 5.718270571827057,
      "grad_norm": 5.443126201629639,
      "learning_rate": 8.563458856345887e-06,
      "loss": 0.0176,
      "step": 4100
    },
    {
      "epoch": 5.7322175732217575,
      "grad_norm": 6.948451519012451,
      "learning_rate": 8.535564853556487e-06,
      "loss": 0.0064,
      "step": 4110
    },
    {
      "epoch": 5.746164574616458,
      "grad_norm": 0.014122324995696545,
      "learning_rate": 8.507670850767086e-06,
      "loss": 0.0175,
      "step": 4120
    },
    {
      "epoch": 5.760111576011157,
      "grad_norm": 1.218454360961914,
      "learning_rate": 8.479776847977686e-06,
      "loss": 0.0078,
      "step": 4130
    },
    {
      "epoch": 5.7740585774058575,
      "grad_norm": 0.4707831144332886,
      "learning_rate": 8.451882845188284e-06,
      "loss": 0.0077,
      "step": 4140
    },
    {
      "epoch": 5.788005578800558,
      "grad_norm": 0.030731044709682465,
      "learning_rate": 8.423988842398884e-06,
      "loss": 0.005,
      "step": 4150
    },
    {
      "epoch": 5.801952580195258,
      "grad_norm": 0.9243633151054382,
      "learning_rate": 8.396094839609484e-06,
      "loss": 0.0083,
      "step": 4160
    },
    {
      "epoch": 5.815899581589958,
      "grad_norm": 0.016248956322669983,
      "learning_rate": 8.368200836820084e-06,
      "loss": 0.0059,
      "step": 4170
    },
    {
      "epoch": 5.829846582984659,
      "grad_norm": 0.04823729768395424,
      "learning_rate": 8.340306834030683e-06,
      "loss": 0.0041,
      "step": 4180
    },
    {
      "epoch": 5.843793584379359,
      "grad_norm": 3.552276372909546,
      "learning_rate": 8.312412831241283e-06,
      "loss": 0.0117,
      "step": 4190
    },
    {
      "epoch": 5.857740585774058,
      "grad_norm": 2.3228442668914795,
      "learning_rate": 8.284518828451885e-06,
      "loss": 0.0148,
      "step": 4200
    },
    {
      "epoch": 5.871687587168759,
      "grad_norm": 0.7105805277824402,
      "learning_rate": 8.256624825662484e-06,
      "loss": 0.014,
      "step": 4210
    },
    {
      "epoch": 5.885634588563459,
      "grad_norm": 2.7564146518707275,
      "learning_rate": 8.228730822873084e-06,
      "loss": 0.0102,
      "step": 4220
    },
    {
      "epoch": 5.899581589958159,
      "grad_norm": 1.3572651147842407,
      "learning_rate": 8.200836820083682e-06,
      "loss": 0.0141,
      "step": 4230
    },
    {
      "epoch": 5.9135285913528595,
      "grad_norm": 0.15127553045749664,
      "learning_rate": 8.172942817294282e-06,
      "loss": 0.0107,
      "step": 4240
    },
    {
      "epoch": 5.927475592747559,
      "grad_norm": 2.3104331493377686,
      "learning_rate": 8.145048814504882e-06,
      "loss": 0.0159,
      "step": 4250
    },
    {
      "epoch": 5.941422594142259,
      "grad_norm": 0.42987892031669617,
      "learning_rate": 8.117154811715482e-06,
      "loss": 0.0047,
      "step": 4260
    },
    {
      "epoch": 5.955369595536959,
      "grad_norm": 0.0859665796160698,
      "learning_rate": 8.089260808926081e-06,
      "loss": 0.0041,
      "step": 4270
    },
    {
      "epoch": 5.96931659693166,
      "grad_norm": 1.1948554515838623,
      "learning_rate": 8.061366806136681e-06,
      "loss": 0.0079,
      "step": 4280
    },
    {
      "epoch": 5.98326359832636,
      "grad_norm": 3.486279249191284,
      "learning_rate": 8.033472803347281e-06,
      "loss": 0.0127,
      "step": 4290
    },
    {
      "epoch": 5.99721059972106,
      "grad_norm": 0.5178748369216919,
      "learning_rate": 8.00557880055788e-06,
      "loss": 0.0063,
      "step": 4300
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.9944613821138212,
      "eval_f1": 0.9759009506964403,
      "eval_loss": 0.020991329103708267,
      "eval_precision": 0.9705364995602462,
      "eval_recall": 0.9813250333481547,
      "eval_runtime": 259.9525,
      "eval_samples_per_second": 75.71,
      "eval_steps_per_second": 2.37,
      "step": 4302
    },
    {
      "epoch": 6.01115760111576,
      "grad_norm": 0.22603784501552582,
      "learning_rate": 7.97768479776848e-06,
      "loss": 0.0085,
      "step": 4310
    },
    {
      "epoch": 6.02510460251046,
      "grad_norm": 0.18318134546279907,
      "learning_rate": 7.94979079497908e-06,
      "loss": 0.0078,
      "step": 4320
    },
    {
      "epoch": 6.03905160390516,
      "grad_norm": 0.1368444561958313,
      "learning_rate": 7.92189679218968e-06,
      "loss": 0.0108,
      "step": 4330
    },
    {
      "epoch": 6.052998605299861,
      "grad_norm": 0.9255832433700562,
      "learning_rate": 7.89400278940028e-06,
      "loss": 0.0084,
      "step": 4340
    },
    {
      "epoch": 6.066945606694561,
      "grad_norm": 1.0272183418273926,
      "learning_rate": 7.86610878661088e-06,
      "loss": 0.0038,
      "step": 4350
    },
    {
      "epoch": 6.080892608089261,
      "grad_norm": 2.068122386932373,
      "learning_rate": 7.83821478382148e-06,
      "loss": 0.0126,
      "step": 4360
    },
    {
      "epoch": 6.0948396094839605,
      "grad_norm": 0.36994364857673645,
      "learning_rate": 7.810320781032079e-06,
      "loss": 0.0061,
      "step": 4370
    },
    {
      "epoch": 6.108786610878661,
      "grad_norm": 0.03234897926449776,
      "learning_rate": 7.782426778242679e-06,
      "loss": 0.0061,
      "step": 4380
    },
    {
      "epoch": 6.122733612273361,
      "grad_norm": 0.07335441559553146,
      "learning_rate": 7.754532775453279e-06,
      "loss": 0.0053,
      "step": 4390
    },
    {
      "epoch": 6.136680613668061,
      "grad_norm": 1.097739815711975,
      "learning_rate": 7.726638772663878e-06,
      "loss": 0.0028,
      "step": 4400
    },
    {
      "epoch": 6.150627615062762,
      "grad_norm": 0.014180457219481468,
      "learning_rate": 7.698744769874478e-06,
      "loss": 0.0069,
      "step": 4410
    },
    {
      "epoch": 6.164574616457462,
      "grad_norm": 0.0394798144698143,
      "learning_rate": 7.670850767085078e-06,
      "loss": 0.0103,
      "step": 4420
    },
    {
      "epoch": 6.178521617852161,
      "grad_norm": 0.5778579711914062,
      "learning_rate": 7.642956764295678e-06,
      "loss": 0.0053,
      "step": 4430
    },
    {
      "epoch": 6.192468619246862,
      "grad_norm": 0.38527992367744446,
      "learning_rate": 7.615062761506277e-06,
      "loss": 0.0067,
      "step": 4440
    },
    {
      "epoch": 6.206415620641562,
      "grad_norm": 0.05847766995429993,
      "learning_rate": 7.5871687587168765e-06,
      "loss": 0.0018,
      "step": 4450
    },
    {
      "epoch": 6.220362622036262,
      "grad_norm": 0.21694745123386383,
      "learning_rate": 7.559274755927476e-06,
      "loss": 0.005,
      "step": 4460
    },
    {
      "epoch": 6.2343096234309625,
      "grad_norm": 0.10077738761901855,
      "learning_rate": 7.531380753138075e-06,
      "loss": 0.0068,
      "step": 4470
    },
    {
      "epoch": 6.248256624825663,
      "grad_norm": 0.16625598073005676,
      "learning_rate": 7.503486750348675e-06,
      "loss": 0.0047,
      "step": 4480
    },
    {
      "epoch": 6.262203626220362,
      "grad_norm": 0.3058856129646301,
      "learning_rate": 7.475592747559275e-06,
      "loss": 0.0061,
      "step": 4490
    },
    {
      "epoch": 6.2761506276150625,
      "grad_norm": 2.172966480255127,
      "learning_rate": 7.4476987447698746e-06,
      "loss": 0.0062,
      "step": 4500
    },
    {
      "epoch": 6.290097629009763,
      "grad_norm": 2.0590436458587646,
      "learning_rate": 7.419804741980475e-06,
      "loss": 0.0058,
      "step": 4510
    },
    {
      "epoch": 6.304044630404463,
      "grad_norm": 0.038078904151916504,
      "learning_rate": 7.391910739191075e-06,
      "loss": 0.0026,
      "step": 4520
    },
    {
      "epoch": 6.317991631799163,
      "grad_norm": 2.530505657196045,
      "learning_rate": 7.364016736401675e-06,
      "loss": 0.0042,
      "step": 4530
    },
    {
      "epoch": 6.331938633193864,
      "grad_norm": 1.174665093421936,
      "learning_rate": 7.3361227336122745e-06,
      "loss": 0.0029,
      "step": 4540
    },
    {
      "epoch": 6.345885634588564,
      "grad_norm": 0.030565818771719933,
      "learning_rate": 7.3082287308228735e-06,
      "loss": 0.0052,
      "step": 4550
    },
    {
      "epoch": 6.359832635983263,
      "grad_norm": 0.056594058871269226,
      "learning_rate": 7.280334728033473e-06,
      "loss": 0.009,
      "step": 4560
    },
    {
      "epoch": 6.373779637377964,
      "grad_norm": 0.022033799439668655,
      "learning_rate": 7.252440725244073e-06,
      "loss": 0.0061,
      "step": 4570
    },
    {
      "epoch": 6.387726638772664,
      "grad_norm": 6.796726703643799,
      "learning_rate": 7.224546722454673e-06,
      "loss": 0.0167,
      "step": 4580
    },
    {
      "epoch": 6.401673640167364,
      "grad_norm": 0.6456677317619324,
      "learning_rate": 7.1966527196652726e-06,
      "loss": 0.003,
      "step": 4590
    },
    {
      "epoch": 6.4156206415620645,
      "grad_norm": 2.107211112976074,
      "learning_rate": 7.168758716875872e-06,
      "loss": 0.0042,
      "step": 4600
    },
    {
      "epoch": 6.429567642956764,
      "grad_norm": 0.005793309770524502,
      "learning_rate": 7.140864714086472e-06,
      "loss": 0.0041,
      "step": 4610
    },
    {
      "epoch": 6.443514644351464,
      "grad_norm": 0.49162545800209045,
      "learning_rate": 7.112970711297071e-06,
      "loss": 0.0141,
      "step": 4620
    },
    {
      "epoch": 6.457461645746164,
      "grad_norm": 0.7679216265678406,
      "learning_rate": 7.085076708507671e-06,
      "loss": 0.0037,
      "step": 4630
    },
    {
      "epoch": 6.471408647140865,
      "grad_norm": 0.003409006167203188,
      "learning_rate": 7.057182705718271e-06,
      "loss": 0.0004,
      "step": 4640
    },
    {
      "epoch": 6.485355648535565,
      "grad_norm": 0.14144322276115417,
      "learning_rate": 7.02928870292887e-06,
      "loss": 0.0064,
      "step": 4650
    },
    {
      "epoch": 6.499302649930265,
      "grad_norm": 0.7580954432487488,
      "learning_rate": 7.001394700139471e-06,
      "loss": 0.0094,
      "step": 4660
    },
    {
      "epoch": 6.513249651324966,
      "grad_norm": 0.01928865909576416,
      "learning_rate": 6.973500697350071e-06,
      "loss": 0.004,
      "step": 4670
    },
    {
      "epoch": 6.527196652719665,
      "grad_norm": 0.01638295128941536,
      "learning_rate": 6.9456066945606706e-06,
      "loss": 0.0043,
      "step": 4680
    },
    {
      "epoch": 6.541143654114365,
      "grad_norm": 2.088611125946045,
      "learning_rate": 6.91771269177127e-06,
      "loss": 0.005,
      "step": 4690
    },
    {
      "epoch": 6.5550906555090656,
      "grad_norm": 1.8949646949768066,
      "learning_rate": 6.889818688981869e-06,
      "loss": 0.006,
      "step": 4700
    },
    {
      "epoch": 6.569037656903766,
      "grad_norm": 0.8470786809921265,
      "learning_rate": 6.861924686192469e-06,
      "loss": 0.0041,
      "step": 4710
    },
    {
      "epoch": 6.582984658298466,
      "grad_norm": 0.7038528323173523,
      "learning_rate": 6.834030683403069e-06,
      "loss": 0.0088,
      "step": 4720
    },
    {
      "epoch": 6.5969316596931655,
      "grad_norm": 0.26899442076683044,
      "learning_rate": 6.806136680613669e-06,
      "loss": 0.003,
      "step": 4730
    },
    {
      "epoch": 6.610878661087866,
      "grad_norm": 0.251977801322937,
      "learning_rate": 6.778242677824268e-06,
      "loss": 0.004,
      "step": 4740
    },
    {
      "epoch": 6.624825662482566,
      "grad_norm": 0.1736530363559723,
      "learning_rate": 6.750348675034868e-06,
      "loss": 0.0041,
      "step": 4750
    },
    {
      "epoch": 6.638772663877266,
      "grad_norm": 0.06834931671619415,
      "learning_rate": 6.722454672245468e-06,
      "loss": 0.0146,
      "step": 4760
    },
    {
      "epoch": 6.652719665271967,
      "grad_norm": 0.5773891806602478,
      "learning_rate": 6.694560669456067e-06,
      "loss": 0.0095,
      "step": 4770
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 0.05985427647829056,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.004,
      "step": 4780
    },
    {
      "epoch": 6.680613668061367,
      "grad_norm": 0.04016346111893654,
      "learning_rate": 6.6387726638772664e-06,
      "loss": 0.0014,
      "step": 4790
    },
    {
      "epoch": 6.694560669456067,
      "grad_norm": 0.6974655389785767,
      "learning_rate": 6.610878661087866e-06,
      "loss": 0.0071,
      "step": 4800
    },
    {
      "epoch": 6.708507670850767,
      "grad_norm": 0.916353702545166,
      "learning_rate": 6.582984658298467e-06,
      "loss": 0.0033,
      "step": 4810
    },
    {
      "epoch": 6.722454672245467,
      "grad_norm": 0.022000079974532127,
      "learning_rate": 6.555090655509067e-06,
      "loss": 0.0016,
      "step": 4820
    },
    {
      "epoch": 6.7364016736401675,
      "grad_norm": 0.032055485993623734,
      "learning_rate": 6.527196652719666e-06,
      "loss": 0.005,
      "step": 4830
    },
    {
      "epoch": 6.750348675034868,
      "grad_norm": 1.7696009874343872,
      "learning_rate": 6.499302649930266e-06,
      "loss": 0.0047,
      "step": 4840
    },
    {
      "epoch": 6.764295676429567,
      "grad_norm": 0.01719910465180874,
      "learning_rate": 6.471408647140865e-06,
      "loss": 0.0014,
      "step": 4850
    },
    {
      "epoch": 6.7782426778242675,
      "grad_norm": 0.015743451192975044,
      "learning_rate": 6.443514644351465e-06,
      "loss": 0.0068,
      "step": 4860
    },
    {
      "epoch": 6.792189679218968,
      "grad_norm": 0.049910448491573334,
      "learning_rate": 6.415620641562065e-06,
      "loss": 0.0007,
      "step": 4870
    },
    {
      "epoch": 6.806136680613668,
      "grad_norm": 1.8238269090652466,
      "learning_rate": 6.3877266387726644e-06,
      "loss": 0.003,
      "step": 4880
    },
    {
      "epoch": 6.820083682008368,
      "grad_norm": 0.14954501390457153,
      "learning_rate": 6.359832635983264e-06,
      "loss": 0.0083,
      "step": 4890
    },
    {
      "epoch": 6.834030683403069,
      "grad_norm": 0.27459025382995605,
      "learning_rate": 6.331938633193864e-06,
      "loss": 0.0025,
      "step": 4900
    },
    {
      "epoch": 6.847977684797769,
      "grad_norm": 0.5979804992675781,
      "learning_rate": 6.304044630404464e-06,
      "loss": 0.0059,
      "step": 4910
    },
    {
      "epoch": 6.861924686192468,
      "grad_norm": 0.025159606710076332,
      "learning_rate": 6.276150627615063e-06,
      "loss": 0.0081,
      "step": 4920
    },
    {
      "epoch": 6.875871687587169,
      "grad_norm": 0.023497916758060455,
      "learning_rate": 6.2482566248256625e-06,
      "loss": 0.0054,
      "step": 4930
    },
    {
      "epoch": 6.889818688981869,
      "grad_norm": 0.37449169158935547,
      "learning_rate": 6.220362622036262e-06,
      "loss": 0.0122,
      "step": 4940
    },
    {
      "epoch": 6.903765690376569,
      "grad_norm": 0.03164828568696976,
      "learning_rate": 6.192468619246862e-06,
      "loss": 0.0027,
      "step": 4950
    },
    {
      "epoch": 6.9177126917712695,
      "grad_norm": 0.14376913011074066,
      "learning_rate": 6.164574616457463e-06,
      "loss": 0.003,
      "step": 4960
    },
    {
      "epoch": 6.931659693165969,
      "grad_norm": 1.9514656066894531,
      "learning_rate": 6.1366806136680624e-06,
      "loss": 0.0048,
      "step": 4970
    },
    {
      "epoch": 6.945606694560669,
      "grad_norm": 0.03970819711685181,
      "learning_rate": 6.108786610878662e-06,
      "loss": 0.0086,
      "step": 4980
    },
    {
      "epoch": 6.959553695955369,
      "grad_norm": 1.0221185684204102,
      "learning_rate": 6.080892608089262e-06,
      "loss": 0.0025,
      "step": 4990
    },
    {
      "epoch": 6.97350069735007,
      "grad_norm": 0.052980937063694,
      "learning_rate": 6.052998605299861e-06,
      "loss": 0.0085,
      "step": 5000
    },
    {
      "epoch": 6.98744769874477,
      "grad_norm": 0.03497942537069321,
      "learning_rate": 6.025104602510461e-06,
      "loss": 0.0113,
      "step": 5010
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.9939532520325203,
      "eval_f1": 0.9735379141649989,
      "eval_loss": 0.02181939221918583,
      "eval_precision": 0.9737544483985765,
      "eval_recall": 0.9733214762116497,
      "eval_runtime": 249.5004,
      "eval_samples_per_second": 78.882,
      "eval_steps_per_second": 2.469,
      "step": 5019
    },
    {
      "epoch": 7.00139470013947,
      "grad_norm": 0.14736178517341614,
      "learning_rate": 5.9972105997210605e-06,
      "loss": 0.0062,
      "step": 5020
    },
    {
      "epoch": 7.015341701534171,
      "grad_norm": 0.0056775715202093124,
      "learning_rate": 5.96931659693166e-06,
      "loss": 0.0004,
      "step": 5030
    },
    {
      "epoch": 7.02928870292887,
      "grad_norm": 1.272586703300476,
      "learning_rate": 5.94142259414226e-06,
      "loss": 0.003,
      "step": 5040
    },
    {
      "epoch": 7.04323570432357,
      "grad_norm": 5.36946439743042,
      "learning_rate": 5.91352859135286e-06,
      "loss": 0.007,
      "step": 5050
    },
    {
      "epoch": 7.0571827057182706,
      "grad_norm": 0.010613912716507912,
      "learning_rate": 5.885634588563459e-06,
      "loss": 0.0053,
      "step": 5060
    },
    {
      "epoch": 7.071129707112971,
      "grad_norm": 0.08808841556310654,
      "learning_rate": 5.8577405857740585e-06,
      "loss": 0.0024,
      "step": 5070
    },
    {
      "epoch": 7.085076708507671,
      "grad_norm": 0.14453500509262085,
      "learning_rate": 5.829846582984658e-06,
      "loss": 0.0014,
      "step": 5080
    },
    {
      "epoch": 7.099023709902371,
      "grad_norm": 0.2602320909500122,
      "learning_rate": 5.801952580195258e-06,
      "loss": 0.0071,
      "step": 5090
    },
    {
      "epoch": 7.112970711297071,
      "grad_norm": 0.5196231007575989,
      "learning_rate": 5.774058577405858e-06,
      "loss": 0.0063,
      "step": 5100
    },
    {
      "epoch": 7.126917712691771,
      "grad_norm": 0.0070594074204564095,
      "learning_rate": 5.746164574616458e-06,
      "loss": 0.0063,
      "step": 5110
    },
    {
      "epoch": 7.140864714086471,
      "grad_norm": 0.03312598541378975,
      "learning_rate": 5.718270571827058e-06,
      "loss": 0.0014,
      "step": 5120
    },
    {
      "epoch": 7.154811715481172,
      "grad_norm": 0.0024463979061692953,
      "learning_rate": 5.690376569037658e-06,
      "loss": 0.0075,
      "step": 5130
    },
    {
      "epoch": 7.168758716875872,
      "grad_norm": 0.008834744803607464,
      "learning_rate": 5.662482566248258e-06,
      "loss": 0.0044,
      "step": 5140
    },
    {
      "epoch": 7.182705718270572,
      "grad_norm": 0.0054518189281225204,
      "learning_rate": 5.634588563458857e-06,
      "loss": 0.0096,
      "step": 5150
    },
    {
      "epoch": 7.196652719665272,
      "grad_norm": 1.5097975730895996,
      "learning_rate": 5.6066945606694565e-06,
      "loss": 0.0102,
      "step": 5160
    },
    {
      "epoch": 7.210599721059972,
      "grad_norm": 0.19946014881134033,
      "learning_rate": 5.578800557880056e-06,
      "loss": 0.0048,
      "step": 5170
    },
    {
      "epoch": 7.224546722454672,
      "grad_norm": 0.044533733278512955,
      "learning_rate": 5.550906555090656e-06,
      "loss": 0.0002,
      "step": 5180
    },
    {
      "epoch": 7.2384937238493725,
      "grad_norm": 0.20525230467319489,
      "learning_rate": 5.523012552301256e-06,
      "loss": 0.0007,
      "step": 5190
    },
    {
      "epoch": 7.252440725244073,
      "grad_norm": 0.15823917090892792,
      "learning_rate": 5.495118549511856e-06,
      "loss": 0.0016,
      "step": 5200
    },
    {
      "epoch": 7.266387726638772,
      "grad_norm": 0.1641676425933838,
      "learning_rate": 5.4672245467224546e-06,
      "loss": 0.0024,
      "step": 5210
    },
    {
      "epoch": 7.2803347280334725,
      "grad_norm": 0.047102756798267365,
      "learning_rate": 5.439330543933054e-06,
      "loss": 0.0032,
      "step": 5220
    },
    {
      "epoch": 7.294281729428173,
      "grad_norm": 1.006273627281189,
      "learning_rate": 5.411436541143654e-06,
      "loss": 0.0047,
      "step": 5230
    },
    {
      "epoch": 7.308228730822873,
      "grad_norm": 0.1383180171251297,
      "learning_rate": 5.383542538354254e-06,
      "loss": 0.0083,
      "step": 5240
    },
    {
      "epoch": 7.322175732217573,
      "grad_norm": 0.38670405745506287,
      "learning_rate": 5.355648535564854e-06,
      "loss": 0.0012,
      "step": 5250
    },
    {
      "epoch": 7.336122733612274,
      "grad_norm": 0.010465976782143116,
      "learning_rate": 5.3277545327754534e-06,
      "loss": 0.0015,
      "step": 5260
    },
    {
      "epoch": 7.350069735006974,
      "grad_norm": 0.020292209461331367,
      "learning_rate": 5.299860529986054e-06,
      "loss": 0.0038,
      "step": 5270
    },
    {
      "epoch": 7.364016736401673,
      "grad_norm": 4.342037200927734,
      "learning_rate": 5.271966527196654e-06,
      "loss": 0.0072,
      "step": 5280
    },
    {
      "epoch": 7.377963737796374,
      "grad_norm": 0.0026085604913532734,
      "learning_rate": 5.244072524407254e-06,
      "loss": 0.0028,
      "step": 5290
    },
    {
      "epoch": 7.391910739191074,
      "grad_norm": 0.6351506114006042,
      "learning_rate": 5.2161785216178526e-06,
      "loss": 0.0074,
      "step": 5300
    },
    {
      "epoch": 7.405857740585774,
      "grad_norm": 0.0023649802897125483,
      "learning_rate": 5.188284518828452e-06,
      "loss": 0.0025,
      "step": 5310
    },
    {
      "epoch": 7.4198047419804745,
      "grad_norm": 0.18486355245113373,
      "learning_rate": 5.160390516039052e-06,
      "loss": 0.0063,
      "step": 5320
    },
    {
      "epoch": 7.433751743375175,
      "grad_norm": 1.0944797992706299,
      "learning_rate": 5.132496513249652e-06,
      "loss": 0.0098,
      "step": 5330
    },
    {
      "epoch": 7.447698744769874,
      "grad_norm": 0.17425157129764557,
      "learning_rate": 5.104602510460252e-06,
      "loss": 0.0018,
      "step": 5340
    },
    {
      "epoch": 7.461645746164574,
      "grad_norm": 0.30188873410224915,
      "learning_rate": 5.0767085076708514e-06,
      "loss": 0.0113,
      "step": 5350
    },
    {
      "epoch": 7.475592747559275,
      "grad_norm": 0.038590677082538605,
      "learning_rate": 5.04881450488145e-06,
      "loss": 0.003,
      "step": 5360
    },
    {
      "epoch": 7.489539748953975,
      "grad_norm": 0.6165459156036377,
      "learning_rate": 5.02092050209205e-06,
      "loss": 0.0008,
      "step": 5370
    },
    {
      "epoch": 7.503486750348675,
      "grad_norm": 0.027121195569634438,
      "learning_rate": 4.993026499302651e-06,
      "loss": 0.0015,
      "step": 5380
    },
    {
      "epoch": 7.517433751743376,
      "grad_norm": 0.544467031955719,
      "learning_rate": 4.9651324965132506e-06,
      "loss": 0.0059,
      "step": 5390
    },
    {
      "epoch": 7.531380753138075,
      "grad_norm": 0.004008714109659195,
      "learning_rate": 4.9372384937238495e-06,
      "loss": 0.0035,
      "step": 5400
    },
    {
      "epoch": 7.545327754532775,
      "grad_norm": 0.14644823968410492,
      "learning_rate": 4.909344490934449e-06,
      "loss": 0.0011,
      "step": 5410
    },
    {
      "epoch": 7.5592747559274756,
      "grad_norm": 0.00526181235909462,
      "learning_rate": 4.881450488145049e-06,
      "loss": 0.0024,
      "step": 5420
    },
    {
      "epoch": 7.573221757322176,
      "grad_norm": 0.4424709379673004,
      "learning_rate": 4.853556485355649e-06,
      "loss": 0.0028,
      "step": 5430
    },
    {
      "epoch": 7.587168758716876,
      "grad_norm": 1.3538243770599365,
      "learning_rate": 4.825662482566249e-06,
      "loss": 0.0093,
      "step": 5440
    },
    {
      "epoch": 7.6011157601115755,
      "grad_norm": 2.453819990158081,
      "learning_rate": 4.797768479776848e-06,
      "loss": 0.0085,
      "step": 5450
    },
    {
      "epoch": 7.615062761506276,
      "grad_norm": 0.07050478458404541,
      "learning_rate": 4.769874476987448e-06,
      "loss": 0.0069,
      "step": 5460
    },
    {
      "epoch": 7.629009762900976,
      "grad_norm": 1.0046474933624268,
      "learning_rate": 4.741980474198048e-06,
      "loss": 0.0055,
      "step": 5470
    },
    {
      "epoch": 7.642956764295676,
      "grad_norm": 0.08538272976875305,
      "learning_rate": 4.714086471408648e-06,
      "loss": 0.0028,
      "step": 5480
    },
    {
      "epoch": 7.656903765690377,
      "grad_norm": 0.004651055671274662,
      "learning_rate": 4.6861924686192475e-06,
      "loss": 0.0073,
      "step": 5490
    },
    {
      "epoch": 7.670850767085077,
      "grad_norm": 1.0780781507492065,
      "learning_rate": 4.658298465829847e-06,
      "loss": 0.0026,
      "step": 5500
    },
    {
      "epoch": 7.684797768479777,
      "grad_norm": 5.163690090179443,
      "learning_rate": 4.630404463040446e-06,
      "loss": 0.0029,
      "step": 5510
    },
    {
      "epoch": 7.698744769874477,
      "grad_norm": 0.9720237255096436,
      "learning_rate": 4.602510460251046e-06,
      "loss": 0.0006,
      "step": 5520
    },
    {
      "epoch": 7.712691771269177,
      "grad_norm": 0.01999012939631939,
      "learning_rate": 4.574616457461646e-06,
      "loss": 0.0071,
      "step": 5530
    },
    {
      "epoch": 7.726638772663877,
      "grad_norm": 0.08044282346963882,
      "learning_rate": 4.546722454672246e-06,
      "loss": 0.0076,
      "step": 5540
    },
    {
      "epoch": 7.7405857740585775,
      "grad_norm": 0.013959868811070919,
      "learning_rate": 4.518828451882845e-06,
      "loss": 0.0074,
      "step": 5550
    },
    {
      "epoch": 7.754532775453278,
      "grad_norm": 0.41463136672973633,
      "learning_rate": 4.490934449093445e-06,
      "loss": 0.0015,
      "step": 5560
    },
    {
      "epoch": 7.768479776847977,
      "grad_norm": 0.020356876775622368,
      "learning_rate": 4.463040446304045e-06,
      "loss": 0.0039,
      "step": 5570
    },
    {
      "epoch": 7.7824267782426775,
      "grad_norm": 0.272702693939209,
      "learning_rate": 4.435146443514645e-06,
      "loss": 0.0088,
      "step": 5580
    },
    {
      "epoch": 7.796373779637378,
      "grad_norm": 1.4201678037643433,
      "learning_rate": 4.407252440725244e-06,
      "loss": 0.007,
      "step": 5590
    },
    {
      "epoch": 7.810320781032078,
      "grad_norm": 0.07738903164863586,
      "learning_rate": 4.379358437935844e-06,
      "loss": 0.0039,
      "step": 5600
    },
    {
      "epoch": 7.824267782426778,
      "grad_norm": 0.0046279155649244785,
      "learning_rate": 4.351464435146444e-06,
      "loss": 0.0058,
      "step": 5610
    },
    {
      "epoch": 7.838214783821479,
      "grad_norm": 0.013399063609540462,
      "learning_rate": 4.323570432357044e-06,
      "loss": 0.0059,
      "step": 5620
    },
    {
      "epoch": 7.852161785216179,
      "grad_norm": 0.63754802942276,
      "learning_rate": 4.2956764295676435e-06,
      "loss": 0.002,
      "step": 5630
    },
    {
      "epoch": 7.866108786610878,
      "grad_norm": 0.4253852367401123,
      "learning_rate": 4.267782426778243e-06,
      "loss": 0.0042,
      "step": 5640
    },
    {
      "epoch": 7.880055788005579,
      "grad_norm": 0.48548492789268494,
      "learning_rate": 4.239888423988843e-06,
      "loss": 0.0015,
      "step": 5650
    },
    {
      "epoch": 7.894002789400279,
      "grad_norm": 0.3118671178817749,
      "learning_rate": 4.211994421199442e-06,
      "loss": 0.003,
      "step": 5660
    },
    {
      "epoch": 7.907949790794979,
      "grad_norm": 0.10022500157356262,
      "learning_rate": 4.184100418410042e-06,
      "loss": 0.004,
      "step": 5670
    },
    {
      "epoch": 7.9218967921896795,
      "grad_norm": 0.008798746392130852,
      "learning_rate": 4.1562064156206416e-06,
      "loss": 0.0038,
      "step": 5680
    },
    {
      "epoch": 7.93584379358438,
      "grad_norm": 0.28315988183021545,
      "learning_rate": 4.128312412831242e-06,
      "loss": 0.0066,
      "step": 5690
    },
    {
      "epoch": 7.949790794979079,
      "grad_norm": 0.15357835590839386,
      "learning_rate": 4.100418410041841e-06,
      "loss": 0.0022,
      "step": 5700
    },
    {
      "epoch": 7.963737796373779,
      "grad_norm": 0.0061750346794724464,
      "learning_rate": 4.072524407252441e-06,
      "loss": 0.0027,
      "step": 5710
    },
    {
      "epoch": 7.97768479776848,
      "grad_norm": 1.403652310371399,
      "learning_rate": 4.044630404463041e-06,
      "loss": 0.0041,
      "step": 5720
    },
    {
      "epoch": 7.99163179916318,
      "grad_norm": 0.0011612809030339122,
      "learning_rate": 4.0167364016736405e-06,
      "loss": 0.003,
      "step": 5730
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.9945121951219512,
      "eval_f1": 0.9759786476868328,
      "eval_loss": 0.021405600011348724,
      "eval_precision": 0.9764129951045839,
      "eval_recall": 0.9755446865273455,
      "eval_runtime": 244.2634,
      "eval_samples_per_second": 80.573,
      "eval_steps_per_second": 2.522,
      "step": 5736
    },
    {
      "epoch": 8.00557880055788,
      "grad_norm": 1.848227858543396,
      "learning_rate": 3.98884239888424e-06,
      "loss": 0.0035,
      "step": 5740
    },
    {
      "epoch": 8.01952580195258,
      "grad_norm": 1.7649534940719604,
      "learning_rate": 3.96094839609484e-06,
      "loss": 0.0048,
      "step": 5750
    },
    {
      "epoch": 8.03347280334728,
      "grad_norm": 0.35011762380599976,
      "learning_rate": 3.93305439330544e-06,
      "loss": 0.0057,
      "step": 5760
    },
    {
      "epoch": 8.047419804741981,
      "grad_norm": 0.20313461124897003,
      "learning_rate": 3.9051603905160396e-06,
      "loss": 0.0052,
      "step": 5770
    },
    {
      "epoch": 8.06136680613668,
      "grad_norm": 0.8384640216827393,
      "learning_rate": 3.877266387726639e-06,
      "loss": 0.0029,
      "step": 5780
    },
    {
      "epoch": 8.07531380753138,
      "grad_norm": 0.4053376317024231,
      "learning_rate": 3.849372384937239e-06,
      "loss": 0.0043,
      "step": 5790
    },
    {
      "epoch": 8.089260808926081,
      "grad_norm": 0.23094309866428375,
      "learning_rate": 3.821478382147839e-06,
      "loss": 0.005,
      "step": 5800
    },
    {
      "epoch": 8.10320781032078,
      "grad_norm": 0.008571453392505646,
      "learning_rate": 3.7935843793584383e-06,
      "loss": 0.0021,
      "step": 5810
    },
    {
      "epoch": 8.117154811715482,
      "grad_norm": 0.06571957468986511,
      "learning_rate": 3.7656903765690376e-06,
      "loss": 0.0035,
      "step": 5820
    },
    {
      "epoch": 8.131101813110181,
      "grad_norm": 0.010092401877045631,
      "learning_rate": 3.7377963737796374e-06,
      "loss": 0.0005,
      "step": 5830
    },
    {
      "epoch": 8.145048814504882,
      "grad_norm": 0.10007792711257935,
      "learning_rate": 3.7099023709902376e-06,
      "loss": 0.002,
      "step": 5840
    },
    {
      "epoch": 8.158995815899582,
      "grad_norm": 0.3773503601551056,
      "learning_rate": 3.6820083682008374e-06,
      "loss": 0.0033,
      "step": 5850
    },
    {
      "epoch": 8.172942817294281,
      "grad_norm": 0.4018493592739105,
      "learning_rate": 3.6541143654114367e-06,
      "loss": 0.007,
      "step": 5860
    },
    {
      "epoch": 8.186889818688982,
      "grad_norm": 0.0014957570238038898,
      "learning_rate": 3.6262203626220365e-06,
      "loss": 0.0028,
      "step": 5870
    },
    {
      "epoch": 8.200836820083682,
      "grad_norm": 1.658091425895691,
      "learning_rate": 3.5983263598326363e-06,
      "loss": 0.0028,
      "step": 5880
    },
    {
      "epoch": 8.214783821478383,
      "grad_norm": 0.02102692611515522,
      "learning_rate": 3.570432357043236e-06,
      "loss": 0.0072,
      "step": 5890
    },
    {
      "epoch": 8.228730822873082,
      "grad_norm": 0.0078174052760005,
      "learning_rate": 3.5425383542538354e-06,
      "loss": 0.003,
      "step": 5900
    },
    {
      "epoch": 8.242677824267782,
      "grad_norm": 4.832366466522217,
      "learning_rate": 3.514644351464435e-06,
      "loss": 0.0068,
      "step": 5910
    },
    {
      "epoch": 8.256624825662483,
      "grad_norm": 0.014498780481517315,
      "learning_rate": 3.4867503486750354e-06,
      "loss": 0.0028,
      "step": 5920
    },
    {
      "epoch": 8.270571827057182,
      "grad_norm": 0.2863450050354004,
      "learning_rate": 3.458856345885635e-06,
      "loss": 0.003,
      "step": 5930
    },
    {
      "epoch": 8.284518828451883,
      "grad_norm": 0.0059140934608876705,
      "learning_rate": 3.4309623430962345e-06,
      "loss": 0.0015,
      "step": 5940
    },
    {
      "epoch": 8.298465829846583,
      "grad_norm": 0.03529870882630348,
      "learning_rate": 3.4030683403068343e-06,
      "loss": 0.003,
      "step": 5950
    },
    {
      "epoch": 8.312412831241284,
      "grad_norm": 0.11444800347089767,
      "learning_rate": 3.375174337517434e-06,
      "loss": 0.0041,
      "step": 5960
    },
    {
      "epoch": 8.326359832635983,
      "grad_norm": 0.00961575098335743,
      "learning_rate": 3.3472803347280334e-06,
      "loss": 0.0031,
      "step": 5970
    },
    {
      "epoch": 8.340306834030683,
      "grad_norm": 0.009013029746711254,
      "learning_rate": 3.3193863319386332e-06,
      "loss": 0.0023,
      "step": 5980
    },
    {
      "epoch": 8.354253835425384,
      "grad_norm": 0.030993904918432236,
      "learning_rate": 3.2914923291492334e-06,
      "loss": 0.001,
      "step": 5990
    },
    {
      "epoch": 8.368200836820083,
      "grad_norm": 0.0038378420285880566,
      "learning_rate": 3.263598326359833e-06,
      "loss": 0.0012,
      "step": 6000
    },
    {
      "epoch": 8.382147838214784,
      "grad_norm": 0.21542757749557495,
      "learning_rate": 3.2357043235704325e-06,
      "loss": 0.0019,
      "step": 6010
    },
    {
      "epoch": 8.396094839609484,
      "grad_norm": 0.04842488095164299,
      "learning_rate": 3.2078103207810323e-06,
      "loss": 0.0041,
      "step": 6020
    },
    {
      "epoch": 8.410041841004183,
      "grad_norm": 0.9613530039787292,
      "learning_rate": 3.179916317991632e-06,
      "loss": 0.0076,
      "step": 6030
    },
    {
      "epoch": 8.423988842398884,
      "grad_norm": 0.008633799850940704,
      "learning_rate": 3.152022315202232e-06,
      "loss": 0.0035,
      "step": 6040
    },
    {
      "epoch": 8.437935843793584,
      "grad_norm": 0.06885585188865662,
      "learning_rate": 3.1241283124128312e-06,
      "loss": 0.0024,
      "step": 6050
    },
    {
      "epoch": 8.451882845188285,
      "grad_norm": 0.14181353151798248,
      "learning_rate": 3.096234309623431e-06,
      "loss": 0.003,
      "step": 6060
    },
    {
      "epoch": 8.465829846582984,
      "grad_norm": 0.0032576867379248142,
      "learning_rate": 3.0683403068340312e-06,
      "loss": 0.0012,
      "step": 6070
    },
    {
      "epoch": 8.479776847977686,
      "grad_norm": 0.5235759615898132,
      "learning_rate": 3.040446304044631e-06,
      "loss": 0.0026,
      "step": 6080
    },
    {
      "epoch": 8.493723849372385,
      "grad_norm": 0.16039906442165375,
      "learning_rate": 3.0125523012552303e-06,
      "loss": 0.0019,
      "step": 6090
    },
    {
      "epoch": 8.507670850767084,
      "grad_norm": 0.058370646089315414,
      "learning_rate": 2.98465829846583e-06,
      "loss": 0.0008,
      "step": 6100
    },
    {
      "epoch": 8.521617852161786,
      "grad_norm": 0.23256585001945496,
      "learning_rate": 2.95676429567643e-06,
      "loss": 0.0018,
      "step": 6110
    },
    {
      "epoch": 8.535564853556485,
      "grad_norm": 0.4689863920211792,
      "learning_rate": 2.9288702928870293e-06,
      "loss": 0.0023,
      "step": 6120
    },
    {
      "epoch": 8.549511854951186,
      "grad_norm": 0.22678402066230774,
      "learning_rate": 2.900976290097629e-06,
      "loss": 0.0048,
      "step": 6130
    },
    {
      "epoch": 8.563458856345886,
      "grad_norm": 0.06508547067642212,
      "learning_rate": 2.873082287308229e-06,
      "loss": 0.0037,
      "step": 6140
    },
    {
      "epoch": 8.577405857740585,
      "grad_norm": 0.0006148713291622698,
      "learning_rate": 2.845188284518829e-06,
      "loss": 0.0003,
      "step": 6150
    },
    {
      "epoch": 8.591352859135286,
      "grad_norm": 0.0272928848862648,
      "learning_rate": 2.8172942817294284e-06,
      "loss": 0.0033,
      "step": 6160
    },
    {
      "epoch": 8.605299860529986,
      "grad_norm": 0.01703384891152382,
      "learning_rate": 2.789400278940028e-06,
      "loss": 0.0014,
      "step": 6170
    },
    {
      "epoch": 8.619246861924687,
      "grad_norm": 0.10386446118354797,
      "learning_rate": 2.761506276150628e-06,
      "loss": 0.0024,
      "step": 6180
    },
    {
      "epoch": 8.633193863319386,
      "grad_norm": 0.00464066443964839,
      "learning_rate": 2.7336122733612273e-06,
      "loss": 0.0017,
      "step": 6190
    },
    {
      "epoch": 8.647140864714087,
      "grad_norm": 0.0012087495997548103,
      "learning_rate": 2.705718270571827e-06,
      "loss": 0.0019,
      "step": 6200
    },
    {
      "epoch": 8.661087866108787,
      "grad_norm": 0.00595420366153121,
      "learning_rate": 2.677824267782427e-06,
      "loss": 0.0027,
      "step": 6210
    },
    {
      "epoch": 8.675034867503486,
      "grad_norm": 0.010853511281311512,
      "learning_rate": 2.649930264993027e-06,
      "loss": 0.0044,
      "step": 6220
    },
    {
      "epoch": 8.688981868898187,
      "grad_norm": 1.999620795249939,
      "learning_rate": 2.622036262203627e-06,
      "loss": 0.0117,
      "step": 6230
    },
    {
      "epoch": 8.702928870292887,
      "grad_norm": 0.1705266535282135,
      "learning_rate": 2.594142259414226e-06,
      "loss": 0.0028,
      "step": 6240
    },
    {
      "epoch": 8.716875871687588,
      "grad_norm": 0.0022464182693511248,
      "learning_rate": 2.566248256624826e-06,
      "loss": 0.0058,
      "step": 6250
    },
    {
      "epoch": 8.730822873082287,
      "grad_norm": 0.8251380920410156,
      "learning_rate": 2.5383542538354257e-06,
      "loss": 0.0009,
      "step": 6260
    },
    {
      "epoch": 8.744769874476987,
      "grad_norm": 0.0033503291197121143,
      "learning_rate": 2.510460251046025e-06,
      "loss": 0.0082,
      "step": 6270
    },
    {
      "epoch": 8.758716875871688,
      "grad_norm": 0.005801900755614042,
      "learning_rate": 2.4825662482566253e-06,
      "loss": 0.0037,
      "step": 6280
    },
    {
      "epoch": 8.772663877266387,
      "grad_norm": 0.060935016721487045,
      "learning_rate": 2.4546722454672246e-06,
      "loss": 0.0036,
      "step": 6290
    },
    {
      "epoch": 8.786610878661088,
      "grad_norm": 0.23235943913459778,
      "learning_rate": 2.4267782426778244e-06,
      "loss": 0.0031,
      "step": 6300
    },
    {
      "epoch": 8.800557880055788,
      "grad_norm": 0.008992315270006657,
      "learning_rate": 2.398884239888424e-06,
      "loss": 0.0095,
      "step": 6310
    },
    {
      "epoch": 8.814504881450489,
      "grad_norm": 0.049963418394327164,
      "learning_rate": 2.370990237099024e-06,
      "loss": 0.0051,
      "step": 6320
    },
    {
      "epoch": 8.828451882845188,
      "grad_norm": 1.0566167831420898,
      "learning_rate": 2.3430962343096237e-06,
      "loss": 0.0061,
      "step": 6330
    },
    {
      "epoch": 8.842398884239888,
      "grad_norm": 0.39021801948547363,
      "learning_rate": 2.315202231520223e-06,
      "loss": 0.0039,
      "step": 6340
    },
    {
      "epoch": 8.856345885634589,
      "grad_norm": 0.006639461498707533,
      "learning_rate": 2.287308228730823e-06,
      "loss": 0.0083,
      "step": 6350
    },
    {
      "epoch": 8.870292887029288,
      "grad_norm": 0.4747683107852936,
      "learning_rate": 2.2594142259414227e-06,
      "loss": 0.0133,
      "step": 6360
    },
    {
      "epoch": 8.88423988842399,
      "grad_norm": 1.3549010753631592,
      "learning_rate": 2.2315202231520224e-06,
      "loss": 0.0017,
      "step": 6370
    },
    {
      "epoch": 8.898186889818689,
      "grad_norm": 0.05701521411538124,
      "learning_rate": 2.203626220362622e-06,
      "loss": 0.0054,
      "step": 6380
    },
    {
      "epoch": 8.91213389121339,
      "grad_norm": 0.0023216423578560352,
      "learning_rate": 2.175732217573222e-06,
      "loss": 0.0005,
      "step": 6390
    },
    {
      "epoch": 8.92608089260809,
      "grad_norm": 0.0022735665552318096,
      "learning_rate": 2.1478382147838218e-06,
      "loss": 0.0061,
      "step": 6400
    },
    {
      "epoch": 8.940027894002789,
      "grad_norm": 0.004587192554026842,
      "learning_rate": 2.1199442119944215e-06,
      "loss": 0.0041,
      "step": 6410
    },
    {
      "epoch": 8.95397489539749,
      "grad_norm": 0.011033453978598118,
      "learning_rate": 2.092050209205021e-06,
      "loss": 0.0063,
      "step": 6420
    },
    {
      "epoch": 8.96792189679219,
      "grad_norm": 0.010999204590916634,
      "learning_rate": 2.064156206415621e-06,
      "loss": 0.001,
      "step": 6430
    },
    {
      "epoch": 8.98186889818689,
      "grad_norm": 0.0010820217430591583,
      "learning_rate": 2.0362622036262205e-06,
      "loss": 0.001,
      "step": 6440
    },
    {
      "epoch": 8.99581589958159,
      "grad_norm": 0.7134702801704407,
      "learning_rate": 2.0083682008368202e-06,
      "loss": 0.0092,
      "step": 6450
    },
    {
      "epoch": 9.0,
      "eval_accuracy": 0.9946646341463414,
      "eval_f1": 0.9767235646198182,
      "eval_loss": 0.02090141549706459,
      "eval_precision": 0.9739168877099912,
      "eval_recall": 0.9795464650955981,
      "eval_runtime": 255.8563,
      "eval_samples_per_second": 76.922,
      "eval_steps_per_second": 2.408,
      "step": 6453
    },
    {
      "epoch": 9.00976290097629,
      "grad_norm": 0.004606776405125856,
      "learning_rate": 1.98047419804742e-06,
      "loss": 0.0013,
      "step": 6460
    },
    {
      "epoch": 9.02370990237099,
      "grad_norm": 0.011997356079518795,
      "learning_rate": 1.9525801952580198e-06,
      "loss": 0.0015,
      "step": 6470
    },
    {
      "epoch": 9.03765690376569,
      "grad_norm": 0.040860071778297424,
      "learning_rate": 1.9246861924686196e-06,
      "loss": 0.004,
      "step": 6480
    },
    {
      "epoch": 9.051603905160391,
      "grad_norm": 0.00242214254103601,
      "learning_rate": 1.8967921896792191e-06,
      "loss": 0.0021,
      "step": 6490
    },
    {
      "epoch": 9.06555090655509,
      "grad_norm": 0.6574864387512207,
      "learning_rate": 1.8688981868898187e-06,
      "loss": 0.0026,
      "step": 6500
    },
    {
      "epoch": 9.07949790794979,
      "grad_norm": 0.0007938251947052777,
      "learning_rate": 1.8410041841004187e-06,
      "loss": 0.0012,
      "step": 6510
    },
    {
      "epoch": 9.093444909344491,
      "grad_norm": 0.036886002868413925,
      "learning_rate": 1.8131101813110183e-06,
      "loss": 0.0005,
      "step": 6520
    },
    {
      "epoch": 9.10739191073919,
      "grad_norm": 0.017188208177685738,
      "learning_rate": 1.785216178521618e-06,
      "loss": 0.0052,
      "step": 6530
    },
    {
      "epoch": 9.121338912133892,
      "grad_norm": 0.007813787087798119,
      "learning_rate": 1.7573221757322176e-06,
      "loss": 0.0054,
      "step": 6540
    },
    {
      "epoch": 9.135285913528591,
      "grad_norm": 0.003958242479711771,
      "learning_rate": 1.7294281729428176e-06,
      "loss": 0.0006,
      "step": 6550
    },
    {
      "epoch": 9.149232914923292,
      "grad_norm": 0.0012531982501968741,
      "learning_rate": 1.7015341701534172e-06,
      "loss": 0.0023,
      "step": 6560
    },
    {
      "epoch": 9.163179916317992,
      "grad_norm": 0.013885658234357834,
      "learning_rate": 1.6736401673640167e-06,
      "loss": 0.0008,
      "step": 6570
    },
    {
      "epoch": 9.177126917712691,
      "grad_norm": 1.2364798784255981,
      "learning_rate": 1.6457461645746167e-06,
      "loss": 0.0068,
      "step": 6580
    },
    {
      "epoch": 9.191073919107392,
      "grad_norm": 0.07758141309022903,
      "learning_rate": 1.6178521617852163e-06,
      "loss": 0.0002,
      "step": 6590
    },
    {
      "epoch": 9.205020920502092,
      "grad_norm": 0.0070921932347118855,
      "learning_rate": 1.589958158995816e-06,
      "loss": 0.0004,
      "step": 6600
    },
    {
      "epoch": 9.218967921896793,
      "grad_norm": 0.4285903573036194,
      "learning_rate": 1.5620641562064156e-06,
      "loss": 0.0007,
      "step": 6610
    },
    {
      "epoch": 9.232914923291492,
      "grad_norm": 1.3146005868911743,
      "learning_rate": 1.5341701534170156e-06,
      "loss": 0.0023,
      "step": 6620
    },
    {
      "epoch": 9.246861924686192,
      "grad_norm": 1.845930576324463,
      "learning_rate": 1.5062761506276152e-06,
      "loss": 0.0052,
      "step": 6630
    },
    {
      "epoch": 9.260808926080893,
      "grad_norm": 0.09557295590639114,
      "learning_rate": 1.478382147838215e-06,
      "loss": 0.0056,
      "step": 6640
    },
    {
      "epoch": 9.274755927475592,
      "grad_norm": 0.0028424751944839954,
      "learning_rate": 1.4504881450488145e-06,
      "loss": 0.0011,
      "step": 6650
    },
    {
      "epoch": 9.288702928870293,
      "grad_norm": 0.012007910758256912,
      "learning_rate": 1.4225941422594145e-06,
      "loss": 0.0022,
      "step": 6660
    },
    {
      "epoch": 9.302649930264993,
      "grad_norm": 1.7762900590896606,
      "learning_rate": 1.394700139470014e-06,
      "loss": 0.0043,
      "step": 6670
    },
    {
      "epoch": 9.316596931659694,
      "grad_norm": 0.0012802464189007878,
      "learning_rate": 1.3668061366806136e-06,
      "loss": 0.0019,
      "step": 6680
    },
    {
      "epoch": 9.330543933054393,
      "grad_norm": 0.009961741045117378,
      "learning_rate": 1.3389121338912134e-06,
      "loss": 0.0034,
      "step": 6690
    },
    {
      "epoch": 9.344490934449093,
      "grad_norm": 0.5484119653701782,
      "learning_rate": 1.3110181311018134e-06,
      "loss": 0.0025,
      "step": 6700
    },
    {
      "epoch": 9.358437935843794,
      "grad_norm": 0.25485435128211975,
      "learning_rate": 1.283124128312413e-06,
      "loss": 0.0054,
      "step": 6710
    },
    {
      "epoch": 9.372384937238493,
      "grad_norm": 0.2743892967700958,
      "learning_rate": 1.2552301255230125e-06,
      "loss": 0.0017,
      "step": 6720
    },
    {
      "epoch": 9.386331938633194,
      "grad_norm": 0.0007802208419889212,
      "learning_rate": 1.2273361227336123e-06,
      "loss": 0.002,
      "step": 6730
    },
    {
      "epoch": 9.400278940027894,
      "grad_norm": 0.39249059557914734,
      "learning_rate": 1.199442119944212e-06,
      "loss": 0.0043,
      "step": 6740
    },
    {
      "epoch": 9.414225941422593,
      "grad_norm": 0.011575489304959774,
      "learning_rate": 1.1715481171548119e-06,
      "loss": 0.001,
      "step": 6750
    },
    {
      "epoch": 9.428172942817294,
      "grad_norm": 0.3608829975128174,
      "learning_rate": 1.1436541143654114e-06,
      "loss": 0.0015,
      "step": 6760
    },
    {
      "epoch": 9.442119944211994,
      "grad_norm": 0.011292138136923313,
      "learning_rate": 1.1157601115760112e-06,
      "loss": 0.003,
      "step": 6770
    },
    {
      "epoch": 9.456066945606695,
      "grad_norm": 0.27499300241470337,
      "learning_rate": 1.087866108786611e-06,
      "loss": 0.0034,
      "step": 6780
    },
    {
      "epoch": 9.470013947001394,
      "grad_norm": 0.49136459827423096,
      "learning_rate": 1.0599721059972108e-06,
      "loss": 0.0018,
      "step": 6790
    },
    {
      "epoch": 9.483960948396096,
      "grad_norm": 0.0015310946619138122,
      "learning_rate": 1.0320781032078105e-06,
      "loss": 0.0018,
      "step": 6800
    },
    {
      "epoch": 9.497907949790795,
      "grad_norm": 0.6681292653083801,
      "learning_rate": 1.0041841004184101e-06,
      "loss": 0.0033,
      "step": 6810
    },
    {
      "epoch": 9.511854951185494,
      "grad_norm": 0.013574284501373768,
      "learning_rate": 9.762900976290099e-07,
      "loss": 0.0013,
      "step": 6820
    },
    {
      "epoch": 9.525801952580196,
      "grad_norm": 0.060613375157117844,
      "learning_rate": 9.483960948396096e-07,
      "loss": 0.0022,
      "step": 6830
    },
    {
      "epoch": 9.539748953974895,
      "grad_norm": 0.629391074180603,
      "learning_rate": 9.205020920502093e-07,
      "loss": 0.0034,
      "step": 6840
    },
    {
      "epoch": 9.553695955369596,
      "grad_norm": 0.6333954334259033,
      "learning_rate": 8.92608089260809e-07,
      "loss": 0.002,
      "step": 6850
    },
    {
      "epoch": 9.567642956764296,
      "grad_norm": 1.4609026908874512,
      "learning_rate": 8.647140864714088e-07,
      "loss": 0.0024,
      "step": 6860
    },
    {
      "epoch": 9.581589958158997,
      "grad_norm": 0.0064620450139045715,
      "learning_rate": 8.368200836820084e-07,
      "loss": 0.0003,
      "step": 6870
    },
    {
      "epoch": 9.595536959553696,
      "grad_norm": 0.5769689679145813,
      "learning_rate": 8.089260808926081e-07,
      "loss": 0.004,
      "step": 6880
    },
    {
      "epoch": 9.609483960948396,
      "grad_norm": 0.0019097253680229187,
      "learning_rate": 7.810320781032078e-07,
      "loss": 0.0001,
      "step": 6890
    },
    {
      "epoch": 9.623430962343097,
      "grad_norm": 1.8930734395980835,
      "learning_rate": 7.531380753138076e-07,
      "loss": 0.0038,
      "step": 6900
    },
    {
      "epoch": 9.637377963737796,
      "grad_norm": 0.017071658745408058,
      "learning_rate": 7.252440725244073e-07,
      "loss": 0.001,
      "step": 6910
    },
    {
      "epoch": 9.651324965132497,
      "grad_norm": 0.6011569499969482,
      "learning_rate": 6.97350069735007e-07,
      "loss": 0.0018,
      "step": 6920
    },
    {
      "epoch": 9.665271966527197,
      "grad_norm": 0.11737540364265442,
      "learning_rate": 6.694560669456067e-07,
      "loss": 0.0054,
      "step": 6930
    },
    {
      "epoch": 9.679218967921896,
      "grad_norm": 0.00930263102054596,
      "learning_rate": 6.415620641562065e-07,
      "loss": 0.0021,
      "step": 6940
    },
    {
      "epoch": 9.693165969316597,
      "grad_norm": 0.3959777355194092,
      "learning_rate": 6.136680613668062e-07,
      "loss": 0.0038,
      "step": 6950
    },
    {
      "epoch": 9.707112970711297,
      "grad_norm": 1.1594529151916504,
      "learning_rate": 5.857740585774059e-07,
      "loss": 0.0038,
      "step": 6960
    },
    {
      "epoch": 9.721059972105998,
      "grad_norm": 0.00897553376853466,
      "learning_rate": 5.578800557880056e-07,
      "loss": 0.004,
      "step": 6970
    },
    {
      "epoch": 9.735006973500697,
      "grad_norm": 0.19357699155807495,
      "learning_rate": 5.299860529986054e-07,
      "loss": 0.0014,
      "step": 6980
    },
    {
      "epoch": 9.748953974895397,
      "grad_norm": 0.013308349996805191,
      "learning_rate": 5.020920502092051e-07,
      "loss": 0.0,
      "step": 6990
    },
    {
      "epoch": 9.762900976290098,
      "grad_norm": 0.0009070574888028204,
      "learning_rate": 4.741980474198048e-07,
      "loss": 0.0025,
      "step": 7000
    },
    {
      "epoch": 9.776847977684797,
      "grad_norm": 0.012701446190476418,
      "learning_rate": 4.463040446304045e-07,
      "loss": 0.0003,
      "step": 7010
    },
    {
      "epoch": 9.790794979079498,
      "grad_norm": 0.001054003369063139,
      "learning_rate": 4.184100418410042e-07,
      "loss": 0.0016,
      "step": 7020
    },
    {
      "epoch": 9.804741980474198,
      "grad_norm": 0.0024573770351707935,
      "learning_rate": 3.905160390516039e-07,
      "loss": 0.0047,
      "step": 7030
    },
    {
      "epoch": 9.818688981868899,
      "grad_norm": 0.24390360713005066,
      "learning_rate": 3.6262203626220363e-07,
      "loss": 0.0015,
      "step": 7040
    },
    {
      "epoch": 9.832635983263598,
      "grad_norm": 0.00906198937445879,
      "learning_rate": 3.3472803347280335e-07,
      "loss": 0.0013,
      "step": 7050
    },
    {
      "epoch": 9.846582984658298,
      "grad_norm": 0.0129243778064847,
      "learning_rate": 3.068340306834031e-07,
      "loss": 0.0048,
      "step": 7060
    },
    {
      "epoch": 9.860529986052999,
      "grad_norm": 0.017724432051181793,
      "learning_rate": 2.789400278940028e-07,
      "loss": 0.0028,
      "step": 7070
    },
    {
      "epoch": 9.874476987447698,
      "grad_norm": 0.11622685939073563,
      "learning_rate": 2.5104602510460253e-07,
      "loss": 0.0019,
      "step": 7080
    },
    {
      "epoch": 9.8884239888424,
      "grad_norm": 0.0008600939181633294,
      "learning_rate": 2.2315202231520225e-07,
      "loss": 0.0004,
      "step": 7090
    },
    {
      "epoch": 9.902370990237099,
      "grad_norm": 0.141362726688385,
      "learning_rate": 1.9525801952580195e-07,
      "loss": 0.0012,
      "step": 7100
    },
    {
      "epoch": 9.9163179916318,
      "grad_norm": 0.0037795105017721653,
      "learning_rate": 1.6736401673640168e-07,
      "loss": 0.0013,
      "step": 7110
    },
    {
      "epoch": 9.9302649930265,
      "grad_norm": 0.09313526004552841,
      "learning_rate": 1.394700139470014e-07,
      "loss": 0.0068,
      "step": 7120
    },
    {
      "epoch": 9.944211994421199,
      "grad_norm": 0.0035092695616185665,
      "learning_rate": 1.1157601115760113e-07,
      "loss": 0.0024,
      "step": 7130
    },
    {
      "epoch": 9.9581589958159,
      "grad_norm": 0.001645237090997398,
      "learning_rate": 8.368200836820084e-08,
      "loss": 0.0027,
      "step": 7140
    },
    {
      "epoch": 9.9721059972106,
      "grad_norm": 0.012717155739665031,
      "learning_rate": 5.5788005578800563e-08,
      "loss": 0.0034,
      "step": 7150
    },
    {
      "epoch": 9.9860529986053,
      "grad_norm": 0.8256656527519226,
      "learning_rate": 2.7894002789400282e-08,
      "loss": 0.0026,
      "step": 7160
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.0030619760509580374,
      "learning_rate": 0.0,
      "loss": 0.0006,
      "step": 7170
    },
    {
      "epoch": 10.0,
      "eval_accuracy": 0.9949186991869918,
      "eval_f1": 0.9778466991581745,
      "eval_loss": 0.02134736441075802,
      "eval_precision": 0.97439293598234,
      "eval_recall": 0.9813250333481547,
      "eval_runtime": 242.8338,
      "eval_samples_per_second": 81.047,
      "eval_steps_per_second": 2.537,
      "step": 7170
    }
  ],
  "logging_steps": 10,
  "max_steps": 7170,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.2288181023734528e+17,
  "train_batch_size": 128,
  "trial_name": null,
  "trial_params": null
}
