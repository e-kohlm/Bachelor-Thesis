{
  "best_metric": 0.9765886287625418,
  "best_model_checkpoint": "../saved_models/patch_disclosure/checkpoint-5736",
  "epoch": 6.0,
  "eval_steps": 500,
  "global_step": 5736,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0010460251046025104,
      "grad_norm": 25.464141845703125,
      "learning_rate": 1.9997907949790796e-05,
      "loss": 0.9762,
      "step": 1
    },
    {
      "epoch": 0.010460251046025104,
      "grad_norm": 13.877843856811523,
      "learning_rate": 1.997907949790795e-05,
      "loss": 0.5154,
      "step": 10
    },
    {
      "epoch": 0.02092050209205021,
      "grad_norm": 9.923380851745605,
      "learning_rate": 1.99581589958159e-05,
      "loss": 0.4396,
      "step": 20
    },
    {
      "epoch": 0.03138075313807531,
      "grad_norm": 9.070516586303711,
      "learning_rate": 1.9937238493723853e-05,
      "loss": 0.3729,
      "step": 30
    },
    {
      "epoch": 0.04184100418410042,
      "grad_norm": 9.374723434448242,
      "learning_rate": 1.9916317991631803e-05,
      "loss": 0.4079,
      "step": 40
    },
    {
      "epoch": 0.05230125523012552,
      "grad_norm": 11.2484712600708,
      "learning_rate": 1.989539748953975e-05,
      "loss": 0.4139,
      "step": 50
    },
    {
      "epoch": 0.06276150627615062,
      "grad_norm": 7.519649505615234,
      "learning_rate": 1.9874476987447698e-05,
      "loss": 0.3621,
      "step": 60
    },
    {
      "epoch": 0.07322175732217573,
      "grad_norm": 10.883294105529785,
      "learning_rate": 1.985355648535565e-05,
      "loss": 0.3364,
      "step": 70
    },
    {
      "epoch": 0.08368200836820083,
      "grad_norm": 7.820545196533203,
      "learning_rate": 1.98326359832636e-05,
      "loss": 0.3566,
      "step": 80
    },
    {
      "epoch": 0.09414225941422594,
      "grad_norm": 8.263158798217773,
      "learning_rate": 1.981171548117155e-05,
      "loss": 0.3461,
      "step": 90
    },
    {
      "epoch": 0.10460251046025104,
      "grad_norm": 7.8223557472229,
      "learning_rate": 1.97907949790795e-05,
      "loss": 0.3192,
      "step": 100
    },
    {
      "epoch": 0.11506276150627615,
      "grad_norm": 7.489069938659668,
      "learning_rate": 1.976987447698745e-05,
      "loss": 0.3629,
      "step": 110
    },
    {
      "epoch": 0.12552301255230125,
      "grad_norm": 7.086619853973389,
      "learning_rate": 1.9748953974895398e-05,
      "loss": 0.3055,
      "step": 120
    },
    {
      "epoch": 0.13598326359832635,
      "grad_norm": 9.531622886657715,
      "learning_rate": 1.972803347280335e-05,
      "loss": 0.342,
      "step": 130
    },
    {
      "epoch": 0.14644351464435146,
      "grad_norm": 6.838603973388672,
      "learning_rate": 1.97071129707113e-05,
      "loss": 0.2613,
      "step": 140
    },
    {
      "epoch": 0.15690376569037656,
      "grad_norm": 6.873779773712158,
      "learning_rate": 1.9686192468619246e-05,
      "loss": 0.3025,
      "step": 150
    },
    {
      "epoch": 0.16736401673640167,
      "grad_norm": 5.259975910186768,
      "learning_rate": 1.96652719665272e-05,
      "loss": 0.2805,
      "step": 160
    },
    {
      "epoch": 0.17782426778242677,
      "grad_norm": 6.87125825881958,
      "learning_rate": 1.964435146443515e-05,
      "loss": 0.2684,
      "step": 170
    },
    {
      "epoch": 0.18828451882845187,
      "grad_norm": 4.734991550445557,
      "learning_rate": 1.9623430962343098e-05,
      "loss": 0.2982,
      "step": 180
    },
    {
      "epoch": 0.19874476987447698,
      "grad_norm": 4.960405349731445,
      "learning_rate": 1.9602510460251047e-05,
      "loss": 0.3027,
      "step": 190
    },
    {
      "epoch": 0.20920502092050208,
      "grad_norm": 4.660483360290527,
      "learning_rate": 1.9581589958158997e-05,
      "loss": 0.2465,
      "step": 200
    },
    {
      "epoch": 0.2196652719665272,
      "grad_norm": 15.604190826416016,
      "learning_rate": 1.9560669456066946e-05,
      "loss": 0.2063,
      "step": 210
    },
    {
      "epoch": 0.2301255230125523,
      "grad_norm": 11.107707977294922,
      "learning_rate": 1.95397489539749e-05,
      "loss": 0.2398,
      "step": 220
    },
    {
      "epoch": 0.2405857740585774,
      "grad_norm": 5.76301908493042,
      "learning_rate": 1.9518828451882848e-05,
      "loss": 0.2523,
      "step": 230
    },
    {
      "epoch": 0.2510460251046025,
      "grad_norm": 6.189260959625244,
      "learning_rate": 1.9497907949790798e-05,
      "loss": 0.2381,
      "step": 240
    },
    {
      "epoch": 0.2615062761506276,
      "grad_norm": 6.187424182891846,
      "learning_rate": 1.9476987447698744e-05,
      "loss": 0.2371,
      "step": 250
    },
    {
      "epoch": 0.2719665271966527,
      "grad_norm": 8.342443466186523,
      "learning_rate": 1.9456066945606696e-05,
      "loss": 0.2672,
      "step": 260
    },
    {
      "epoch": 0.2824267782426778,
      "grad_norm": 6.59968376159668,
      "learning_rate": 1.9435146443514646e-05,
      "loss": 0.2745,
      "step": 270
    },
    {
      "epoch": 0.2928870292887029,
      "grad_norm": 11.79394817352295,
      "learning_rate": 1.9414225941422595e-05,
      "loss": 0.1827,
      "step": 280
    },
    {
      "epoch": 0.303347280334728,
      "grad_norm": 3.7250871658325195,
      "learning_rate": 1.9393305439330545e-05,
      "loss": 0.1812,
      "step": 290
    },
    {
      "epoch": 0.3138075313807531,
      "grad_norm": 5.359461307525635,
      "learning_rate": 1.9372384937238494e-05,
      "loss": 0.2414,
      "step": 300
    },
    {
      "epoch": 0.32426778242677823,
      "grad_norm": 9.51794147491455,
      "learning_rate": 1.9351464435146447e-05,
      "loss": 0.2257,
      "step": 310
    },
    {
      "epoch": 0.33472803347280333,
      "grad_norm": 6.352901458740234,
      "learning_rate": 1.9330543933054396e-05,
      "loss": 0.2068,
      "step": 320
    },
    {
      "epoch": 0.34518828451882844,
      "grad_norm": 6.220390796661377,
      "learning_rate": 1.9309623430962346e-05,
      "loss": 0.2159,
      "step": 330
    },
    {
      "epoch": 0.35564853556485354,
      "grad_norm": 4.869706630706787,
      "learning_rate": 1.9288702928870295e-05,
      "loss": 0.2039,
      "step": 340
    },
    {
      "epoch": 0.36610878661087864,
      "grad_norm": 4.96921968460083,
      "learning_rate": 1.9267782426778245e-05,
      "loss": 0.182,
      "step": 350
    },
    {
      "epoch": 0.37656903765690375,
      "grad_norm": 5.087534427642822,
      "learning_rate": 1.9246861924686194e-05,
      "loss": 0.1922,
      "step": 360
    },
    {
      "epoch": 0.38702928870292885,
      "grad_norm": 9.800554275512695,
      "learning_rate": 1.9225941422594143e-05,
      "loss": 0.1803,
      "step": 370
    },
    {
      "epoch": 0.39748953974895396,
      "grad_norm": 5.780218124389648,
      "learning_rate": 1.9205020920502093e-05,
      "loss": 0.2166,
      "step": 380
    },
    {
      "epoch": 0.40794979079497906,
      "grad_norm": 4.209971904754639,
      "learning_rate": 1.9184100418410042e-05,
      "loss": 0.1771,
      "step": 390
    },
    {
      "epoch": 0.41841004184100417,
      "grad_norm": 4.5452117919921875,
      "learning_rate": 1.916317991631799e-05,
      "loss": 0.132,
      "step": 400
    },
    {
      "epoch": 0.42887029288702927,
      "grad_norm": 5.671407222747803,
      "learning_rate": 1.9142259414225944e-05,
      "loss": 0.1717,
      "step": 410
    },
    {
      "epoch": 0.4393305439330544,
      "grad_norm": 5.222231864929199,
      "learning_rate": 1.9121338912133894e-05,
      "loss": 0.1525,
      "step": 420
    },
    {
      "epoch": 0.4497907949790795,
      "grad_norm": 3.999760866165161,
      "learning_rate": 1.9100418410041843e-05,
      "loss": 0.1404,
      "step": 430
    },
    {
      "epoch": 0.4602510460251046,
      "grad_norm": 5.003997325897217,
      "learning_rate": 1.9079497907949793e-05,
      "loss": 0.1584,
      "step": 440
    },
    {
      "epoch": 0.4707112970711297,
      "grad_norm": 6.011810779571533,
      "learning_rate": 1.9058577405857742e-05,
      "loss": 0.1679,
      "step": 450
    },
    {
      "epoch": 0.4811715481171548,
      "grad_norm": 5.431471347808838,
      "learning_rate": 1.903765690376569e-05,
      "loss": 0.1544,
      "step": 460
    },
    {
      "epoch": 0.4916317991631799,
      "grad_norm": 4.308587074279785,
      "learning_rate": 1.901673640167364e-05,
      "loss": 0.1172,
      "step": 470
    },
    {
      "epoch": 0.502092050209205,
      "grad_norm": 6.722345352172852,
      "learning_rate": 1.899581589958159e-05,
      "loss": 0.1334,
      "step": 480
    },
    {
      "epoch": 0.5125523012552301,
      "grad_norm": 8.260644912719727,
      "learning_rate": 1.897489539748954e-05,
      "loss": 0.136,
      "step": 490
    },
    {
      "epoch": 0.5230125523012552,
      "grad_norm": 4.412312030792236,
      "learning_rate": 1.8953974895397492e-05,
      "loss": 0.1627,
      "step": 500
    },
    {
      "epoch": 0.5334728033472803,
      "grad_norm": 4.4785475730896,
      "learning_rate": 1.8933054393305442e-05,
      "loss": 0.1186,
      "step": 510
    },
    {
      "epoch": 0.5439330543933054,
      "grad_norm": 5.396129131317139,
      "learning_rate": 1.891213389121339e-05,
      "loss": 0.1166,
      "step": 520
    },
    {
      "epoch": 0.5543933054393305,
      "grad_norm": 3.706223726272583,
      "learning_rate": 1.889121338912134e-05,
      "loss": 0.15,
      "step": 530
    },
    {
      "epoch": 0.5648535564853556,
      "grad_norm": 6.559443473815918,
      "learning_rate": 1.887029288702929e-05,
      "loss": 0.1495,
      "step": 540
    },
    {
      "epoch": 0.5753138075313807,
      "grad_norm": 3.5618255138397217,
      "learning_rate": 1.884937238493724e-05,
      "loss": 0.1225,
      "step": 550
    },
    {
      "epoch": 0.5857740585774058,
      "grad_norm": 4.508875846862793,
      "learning_rate": 1.882845188284519e-05,
      "loss": 0.1296,
      "step": 560
    },
    {
      "epoch": 0.5962343096234309,
      "grad_norm": 8.377374649047852,
      "learning_rate": 1.880753138075314e-05,
      "loss": 0.1355,
      "step": 570
    },
    {
      "epoch": 0.606694560669456,
      "grad_norm": 4.815222263336182,
      "learning_rate": 1.8786610878661088e-05,
      "loss": 0.1426,
      "step": 580
    },
    {
      "epoch": 0.6171548117154811,
      "grad_norm": 4.795587539672852,
      "learning_rate": 1.876569037656904e-05,
      "loss": 0.1141,
      "step": 590
    },
    {
      "epoch": 0.6276150627615062,
      "grad_norm": 2.179347515106201,
      "learning_rate": 1.874476987447699e-05,
      "loss": 0.123,
      "step": 600
    },
    {
      "epoch": 0.6380753138075314,
      "grad_norm": 7.852956295013428,
      "learning_rate": 1.872384937238494e-05,
      "loss": 0.1053,
      "step": 610
    },
    {
      "epoch": 0.6485355648535565,
      "grad_norm": 6.560999870300293,
      "learning_rate": 1.870292887029289e-05,
      "loss": 0.1351,
      "step": 620
    },
    {
      "epoch": 0.6589958158995816,
      "grad_norm": 2.765834331512451,
      "learning_rate": 1.8682008368200838e-05,
      "loss": 0.0921,
      "step": 630
    },
    {
      "epoch": 0.6694560669456067,
      "grad_norm": 2.1553568840026855,
      "learning_rate": 1.8661087866108788e-05,
      "loss": 0.0972,
      "step": 640
    },
    {
      "epoch": 0.6799163179916318,
      "grad_norm": 9.337538719177246,
      "learning_rate": 1.8640167364016737e-05,
      "loss": 0.1182,
      "step": 650
    },
    {
      "epoch": 0.6903765690376569,
      "grad_norm": 4.321845054626465,
      "learning_rate": 1.8619246861924686e-05,
      "loss": 0.0934,
      "step": 660
    },
    {
      "epoch": 0.700836820083682,
      "grad_norm": 6.364386558532715,
      "learning_rate": 1.8598326359832636e-05,
      "loss": 0.0939,
      "step": 670
    },
    {
      "epoch": 0.7112970711297071,
      "grad_norm": 3.3023762702941895,
      "learning_rate": 1.8577405857740585e-05,
      "loss": 0.1126,
      "step": 680
    },
    {
      "epoch": 0.7217573221757322,
      "grad_norm": 3.176781415939331,
      "learning_rate": 1.8556485355648538e-05,
      "loss": 0.0925,
      "step": 690
    },
    {
      "epoch": 0.7322175732217573,
      "grad_norm": 4.281322479248047,
      "learning_rate": 1.8535564853556487e-05,
      "loss": 0.0904,
      "step": 700
    },
    {
      "epoch": 0.7426778242677824,
      "grad_norm": 2.7257747650146484,
      "learning_rate": 1.8514644351464437e-05,
      "loss": 0.0752,
      "step": 710
    },
    {
      "epoch": 0.7531380753138075,
      "grad_norm": 3.868561267852783,
      "learning_rate": 1.8493723849372386e-05,
      "loss": 0.0835,
      "step": 720
    },
    {
      "epoch": 0.7635983263598326,
      "grad_norm": 3.2376575469970703,
      "learning_rate": 1.8472803347280336e-05,
      "loss": 0.093,
      "step": 730
    },
    {
      "epoch": 0.7740585774058577,
      "grad_norm": 7.145227432250977,
      "learning_rate": 1.845188284518829e-05,
      "loss": 0.083,
      "step": 740
    },
    {
      "epoch": 0.7845188284518828,
      "grad_norm": 2.9098658561706543,
      "learning_rate": 1.8430962343096234e-05,
      "loss": 0.0848,
      "step": 750
    },
    {
      "epoch": 0.7949790794979079,
      "grad_norm": 4.747364044189453,
      "learning_rate": 1.8410041841004184e-05,
      "loss": 0.0767,
      "step": 760
    },
    {
      "epoch": 0.805439330543933,
      "grad_norm": 3.668341636657715,
      "learning_rate": 1.8389121338912133e-05,
      "loss": 0.0784,
      "step": 770
    },
    {
      "epoch": 0.8158995815899581,
      "grad_norm": 6.374900817871094,
      "learning_rate": 1.8368200836820086e-05,
      "loss": 0.0786,
      "step": 780
    },
    {
      "epoch": 0.8263598326359832,
      "grad_norm": 6.561157703399658,
      "learning_rate": 1.8347280334728036e-05,
      "loss": 0.0664,
      "step": 790
    },
    {
      "epoch": 0.8368200836820083,
      "grad_norm": 11.228549003601074,
      "learning_rate": 1.8326359832635985e-05,
      "loss": 0.1119,
      "step": 800
    },
    {
      "epoch": 0.8472803347280334,
      "grad_norm": 3.8586037158966064,
      "learning_rate": 1.8305439330543934e-05,
      "loss": 0.0649,
      "step": 810
    },
    {
      "epoch": 0.8577405857740585,
      "grad_norm": 4.297605037689209,
      "learning_rate": 1.8284518828451884e-05,
      "loss": 0.0573,
      "step": 820
    },
    {
      "epoch": 0.8682008368200836,
      "grad_norm": 2.7833287715911865,
      "learning_rate": 1.8263598326359837e-05,
      "loss": 0.045,
      "step": 830
    },
    {
      "epoch": 0.8786610878661087,
      "grad_norm": 3.526463747024536,
      "learning_rate": 1.8242677824267786e-05,
      "loss": 0.0853,
      "step": 840
    },
    {
      "epoch": 0.8891213389121339,
      "grad_norm": 5.430390357971191,
      "learning_rate": 1.8221757322175732e-05,
      "loss": 0.0578,
      "step": 850
    },
    {
      "epoch": 0.899581589958159,
      "grad_norm": 7.999726295471191,
      "learning_rate": 1.820083682008368e-05,
      "loss": 0.0895,
      "step": 860
    },
    {
      "epoch": 0.9100418410041841,
      "grad_norm": 5.907155990600586,
      "learning_rate": 1.8179916317991634e-05,
      "loss": 0.1045,
      "step": 870
    },
    {
      "epoch": 0.9205020920502092,
      "grad_norm": 4.121922969818115,
      "learning_rate": 1.8158995815899584e-05,
      "loss": 0.0887,
      "step": 880
    },
    {
      "epoch": 0.9309623430962343,
      "grad_norm": 4.217957019805908,
      "learning_rate": 1.8138075313807533e-05,
      "loss": 0.0612,
      "step": 890
    },
    {
      "epoch": 0.9414225941422594,
      "grad_norm": 7.617128372192383,
      "learning_rate": 1.8117154811715482e-05,
      "loss": 0.0736,
      "step": 900
    },
    {
      "epoch": 0.9518828451882845,
      "grad_norm": 1.9508875608444214,
      "learning_rate": 1.8096234309623432e-05,
      "loss": 0.0524,
      "step": 910
    },
    {
      "epoch": 0.9623430962343096,
      "grad_norm": 3.168414354324341,
      "learning_rate": 1.807531380753138e-05,
      "loss": 0.0842,
      "step": 920
    },
    {
      "epoch": 0.9728033472803347,
      "grad_norm": 6.510635852813721,
      "learning_rate": 1.8054393305439334e-05,
      "loss": 0.0554,
      "step": 930
    },
    {
      "epoch": 0.9832635983263598,
      "grad_norm": 6.0063862800598145,
      "learning_rate": 1.8033472803347283e-05,
      "loss": 0.0623,
      "step": 940
    },
    {
      "epoch": 0.9937238493723849,
      "grad_norm": 3.4484572410583496,
      "learning_rate": 1.801255230125523e-05,
      "loss": 0.0921,
      "step": 950
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9813516260162601,
      "eval_f1": 0.9170246439068506,
      "eval_loss": 0.05376163125038147,
      "eval_precision": 0.9328426862925483,
      "eval_recall": 0.9017341040462428,
      "eval_runtime": 629.3309,
      "eval_samples_per_second": 31.273,
      "eval_steps_per_second": 1.305,
      "step": 956
    },
    {
      "epoch": 1.00418410041841,
      "grad_norm": 3.6786274909973145,
      "learning_rate": 1.7991631799163182e-05,
      "loss": 0.0721,
      "step": 960
    },
    {
      "epoch": 1.0146443514644352,
      "grad_norm": 6.938756465911865,
      "learning_rate": 1.797071129707113e-05,
      "loss": 0.0429,
      "step": 970
    },
    {
      "epoch": 1.0251046025104602,
      "grad_norm": 5.303813934326172,
      "learning_rate": 1.794979079497908e-05,
      "loss": 0.0621,
      "step": 980
    },
    {
      "epoch": 1.0355648535564854,
      "grad_norm": 3.7797486782073975,
      "learning_rate": 1.792887029288703e-05,
      "loss": 0.054,
      "step": 990
    },
    {
      "epoch": 1.0460251046025104,
      "grad_norm": 4.65890645980835,
      "learning_rate": 1.790794979079498e-05,
      "loss": 0.0531,
      "step": 1000
    },
    {
      "epoch": 1.0564853556485356,
      "grad_norm": 3.537916421890259,
      "learning_rate": 1.788702928870293e-05,
      "loss": 0.0471,
      "step": 1010
    },
    {
      "epoch": 1.0669456066945606,
      "grad_norm": 5.031717777252197,
      "learning_rate": 1.7866108786610882e-05,
      "loss": 0.0654,
      "step": 1020
    },
    {
      "epoch": 1.0774058577405858,
      "grad_norm": 5.078134536743164,
      "learning_rate": 1.784518828451883e-05,
      "loss": 0.0959,
      "step": 1030
    },
    {
      "epoch": 1.0878661087866108,
      "grad_norm": 3.8373465538024902,
      "learning_rate": 1.782426778242678e-05,
      "loss": 0.0367,
      "step": 1040
    },
    {
      "epoch": 1.098326359832636,
      "grad_norm": 6.39166259765625,
      "learning_rate": 1.7803347280334727e-05,
      "loss": 0.066,
      "step": 1050
    },
    {
      "epoch": 1.108786610878661,
      "grad_norm": 4.2762908935546875,
      "learning_rate": 1.778242677824268e-05,
      "loss": 0.0422,
      "step": 1060
    },
    {
      "epoch": 1.1192468619246863,
      "grad_norm": 5.454625129699707,
      "learning_rate": 1.776150627615063e-05,
      "loss": 0.071,
      "step": 1070
    },
    {
      "epoch": 1.1297071129707112,
      "grad_norm": 2.564221143722534,
      "learning_rate": 1.774058577405858e-05,
      "loss": 0.0492,
      "step": 1080
    },
    {
      "epoch": 1.1401673640167365,
      "grad_norm": 0.5681511759757996,
      "learning_rate": 1.7719665271966528e-05,
      "loss": 0.0272,
      "step": 1090
    },
    {
      "epoch": 1.1506276150627615,
      "grad_norm": 4.234516143798828,
      "learning_rate": 1.7698744769874477e-05,
      "loss": 0.0537,
      "step": 1100
    },
    {
      "epoch": 1.1610878661087867,
      "grad_norm": 1.5904648303985596,
      "learning_rate": 1.767782426778243e-05,
      "loss": 0.0505,
      "step": 1110
    },
    {
      "epoch": 1.1715481171548117,
      "grad_norm": 4.679670810699463,
      "learning_rate": 1.765690376569038e-05,
      "loss": 0.055,
      "step": 1120
    },
    {
      "epoch": 1.1820083682008369,
      "grad_norm": 4.2891974449157715,
      "learning_rate": 1.763598326359833e-05,
      "loss": 0.0465,
      "step": 1130
    },
    {
      "epoch": 1.1924686192468619,
      "grad_norm": 1.3842781782150269,
      "learning_rate": 1.761506276150628e-05,
      "loss": 0.0469,
      "step": 1140
    },
    {
      "epoch": 1.202928870292887,
      "grad_norm": 2.005150079727173,
      "learning_rate": 1.7594142259414228e-05,
      "loss": 0.049,
      "step": 1150
    },
    {
      "epoch": 1.213389121338912,
      "grad_norm": 2.935671806335449,
      "learning_rate": 1.7573221757322177e-05,
      "loss": 0.0265,
      "step": 1160
    },
    {
      "epoch": 1.2238493723849373,
      "grad_norm": 2.619431495666504,
      "learning_rate": 1.7552301255230127e-05,
      "loss": 0.0339,
      "step": 1170
    },
    {
      "epoch": 1.2343096234309623,
      "grad_norm": 5.150984287261963,
      "learning_rate": 1.7531380753138076e-05,
      "loss": 0.0374,
      "step": 1180
    },
    {
      "epoch": 1.2447698744769875,
      "grad_norm": 5.288716793060303,
      "learning_rate": 1.7510460251046025e-05,
      "loss": 0.026,
      "step": 1190
    },
    {
      "epoch": 1.2552301255230125,
      "grad_norm": 4.305659294128418,
      "learning_rate": 1.7489539748953975e-05,
      "loss": 0.0796,
      "step": 1200
    },
    {
      "epoch": 1.2656903765690377,
      "grad_norm": 16.259281158447266,
      "learning_rate": 1.7468619246861928e-05,
      "loss": 0.0572,
      "step": 1210
    },
    {
      "epoch": 1.2761506276150627,
      "grad_norm": 2.2823452949523926,
      "learning_rate": 1.7447698744769877e-05,
      "loss": 0.0296,
      "step": 1220
    },
    {
      "epoch": 1.286610878661088,
      "grad_norm": 4.228870868682861,
      "learning_rate": 1.7426778242677826e-05,
      "loss": 0.0703,
      "step": 1230
    },
    {
      "epoch": 1.297071129707113,
      "grad_norm": 3.06766676902771,
      "learning_rate": 1.7405857740585776e-05,
      "loss": 0.0589,
      "step": 1240
    },
    {
      "epoch": 1.3075313807531381,
      "grad_norm": 1.879549503326416,
      "learning_rate": 1.7384937238493725e-05,
      "loss": 0.0284,
      "step": 1250
    },
    {
      "epoch": 1.3179916317991631,
      "grad_norm": 0.9908604621887207,
      "learning_rate": 1.7364016736401675e-05,
      "loss": 0.0239,
      "step": 1260
    },
    {
      "epoch": 1.3284518828451883,
      "grad_norm": 1.8347331285476685,
      "learning_rate": 1.7343096234309624e-05,
      "loss": 0.095,
      "step": 1270
    },
    {
      "epoch": 1.3389121338912133,
      "grad_norm": 2.769907236099243,
      "learning_rate": 1.7322175732217574e-05,
      "loss": 0.0515,
      "step": 1280
    },
    {
      "epoch": 1.3493723849372385,
      "grad_norm": 4.172111988067627,
      "learning_rate": 1.7301255230125523e-05,
      "loss": 0.0467,
      "step": 1290
    },
    {
      "epoch": 1.3598326359832635,
      "grad_norm": 2.605531692504883,
      "learning_rate": 1.7280334728033476e-05,
      "loss": 0.0435,
      "step": 1300
    },
    {
      "epoch": 1.3702928870292888,
      "grad_norm": 2.0710666179656982,
      "learning_rate": 1.7259414225941425e-05,
      "loss": 0.0447,
      "step": 1310
    },
    {
      "epoch": 1.3807531380753137,
      "grad_norm": 2.180695056915283,
      "learning_rate": 1.7238493723849375e-05,
      "loss": 0.054,
      "step": 1320
    },
    {
      "epoch": 1.391213389121339,
      "grad_norm": 0.8261520862579346,
      "learning_rate": 1.7217573221757324e-05,
      "loss": 0.046,
      "step": 1330
    },
    {
      "epoch": 1.401673640167364,
      "grad_norm": 2.6479713916778564,
      "learning_rate": 1.7196652719665273e-05,
      "loss": 0.0438,
      "step": 1340
    },
    {
      "epoch": 1.4121338912133892,
      "grad_norm": 1.5417391061782837,
      "learning_rate": 1.7175732217573223e-05,
      "loss": 0.0332,
      "step": 1350
    },
    {
      "epoch": 1.4225941422594142,
      "grad_norm": 2.392327308654785,
      "learning_rate": 1.7154811715481172e-05,
      "loss": 0.041,
      "step": 1360
    },
    {
      "epoch": 1.4330543933054394,
      "grad_norm": 2.584752082824707,
      "learning_rate": 1.713389121338912e-05,
      "loss": 0.0436,
      "step": 1370
    },
    {
      "epoch": 1.4435146443514644,
      "grad_norm": 0.6454452276229858,
      "learning_rate": 1.711297071129707e-05,
      "loss": 0.0298,
      "step": 1380
    },
    {
      "epoch": 1.4539748953974896,
      "grad_norm": 3.4886093139648438,
      "learning_rate": 1.7092050209205024e-05,
      "loss": 0.0534,
      "step": 1390
    },
    {
      "epoch": 1.4644351464435146,
      "grad_norm": 2.202897548675537,
      "learning_rate": 1.7071129707112973e-05,
      "loss": 0.0269,
      "step": 1400
    },
    {
      "epoch": 1.4748953974895398,
      "grad_norm": 4.606436252593994,
      "learning_rate": 1.7050209205020923e-05,
      "loss": 0.0484,
      "step": 1410
    },
    {
      "epoch": 1.4853556485355648,
      "grad_norm": 2.912609100341797,
      "learning_rate": 1.7029288702928872e-05,
      "loss": 0.0414,
      "step": 1420
    },
    {
      "epoch": 1.49581589958159,
      "grad_norm": 2.561939001083374,
      "learning_rate": 1.700836820083682e-05,
      "loss": 0.0316,
      "step": 1430
    },
    {
      "epoch": 1.506276150627615,
      "grad_norm": 3.478628635406494,
      "learning_rate": 1.698744769874477e-05,
      "loss": 0.0304,
      "step": 1440
    },
    {
      "epoch": 1.5167364016736402,
      "grad_norm": 4.612081527709961,
      "learning_rate": 1.696652719665272e-05,
      "loss": 0.0564,
      "step": 1450
    },
    {
      "epoch": 1.5271966527196654,
      "grad_norm": 0.7511904835700989,
      "learning_rate": 1.694560669456067e-05,
      "loss": 0.0554,
      "step": 1460
    },
    {
      "epoch": 1.5376569037656904,
      "grad_norm": 8.96230697631836,
      "learning_rate": 1.692468619246862e-05,
      "loss": 0.0267,
      "step": 1470
    },
    {
      "epoch": 1.5481171548117154,
      "grad_norm": 4.0009002685546875,
      "learning_rate": 1.690376569037657e-05,
      "loss": 0.0466,
      "step": 1480
    },
    {
      "epoch": 1.5585774058577406,
      "grad_norm": 0.8099043369293213,
      "learning_rate": 1.688284518828452e-05,
      "loss": 0.0445,
      "step": 1490
    },
    {
      "epoch": 1.5690376569037658,
      "grad_norm": 5.540468692779541,
      "learning_rate": 1.686192468619247e-05,
      "loss": 0.0435,
      "step": 1500
    },
    {
      "epoch": 1.5794979079497908,
      "grad_norm": 2.0110952854156494,
      "learning_rate": 1.684100418410042e-05,
      "loss": 0.0215,
      "step": 1510
    },
    {
      "epoch": 1.5899581589958158,
      "grad_norm": 2.038330316543579,
      "learning_rate": 1.682008368200837e-05,
      "loss": 0.0566,
      "step": 1520
    },
    {
      "epoch": 1.600418410041841,
      "grad_norm": 2.267075300216675,
      "learning_rate": 1.679916317991632e-05,
      "loss": 0.038,
      "step": 1530
    },
    {
      "epoch": 1.6108786610878663,
      "grad_norm": 0.9498194456100464,
      "learning_rate": 1.6778242677824272e-05,
      "loss": 0.0327,
      "step": 1540
    },
    {
      "epoch": 1.6213389121338913,
      "grad_norm": 0.5855558514595032,
      "learning_rate": 1.6757322175732218e-05,
      "loss": 0.042,
      "step": 1550
    },
    {
      "epoch": 1.6317991631799162,
      "grad_norm": 2.240596294403076,
      "learning_rate": 1.6736401673640167e-05,
      "loss": 0.0476,
      "step": 1560
    },
    {
      "epoch": 1.6422594142259415,
      "grad_norm": 5.034034252166748,
      "learning_rate": 1.6715481171548117e-05,
      "loss": 0.0274,
      "step": 1570
    },
    {
      "epoch": 1.6527196652719667,
      "grad_norm": 3.919858932495117,
      "learning_rate": 1.669456066945607e-05,
      "loss": 0.0413,
      "step": 1580
    },
    {
      "epoch": 1.6631799163179917,
      "grad_norm": 0.273915559053421,
      "learning_rate": 1.667364016736402e-05,
      "loss": 0.033,
      "step": 1590
    },
    {
      "epoch": 1.6736401673640167,
      "grad_norm": 3.156489372253418,
      "learning_rate": 1.6652719665271968e-05,
      "loss": 0.0222,
      "step": 1600
    },
    {
      "epoch": 1.6841004184100419,
      "grad_norm": 1.1159707307815552,
      "learning_rate": 1.6631799163179918e-05,
      "loss": 0.0433,
      "step": 1610
    },
    {
      "epoch": 1.694560669456067,
      "grad_norm": 0.15077248215675354,
      "learning_rate": 1.6610878661087867e-05,
      "loss": 0.0288,
      "step": 1620
    },
    {
      "epoch": 1.705020920502092,
      "grad_norm": 3.475707769393921,
      "learning_rate": 1.6589958158995816e-05,
      "loss": 0.0295,
      "step": 1630
    },
    {
      "epoch": 1.715481171548117,
      "grad_norm": 3.109114646911621,
      "learning_rate": 1.656903765690377e-05,
      "loss": 0.0299,
      "step": 1640
    },
    {
      "epoch": 1.7259414225941423,
      "grad_norm": 3.7358345985412598,
      "learning_rate": 1.6548117154811715e-05,
      "loss": 0.068,
      "step": 1650
    },
    {
      "epoch": 1.7364016736401675,
      "grad_norm": 6.791064262390137,
      "learning_rate": 1.6527196652719665e-05,
      "loss": 0.0251,
      "step": 1660
    },
    {
      "epoch": 1.7468619246861925,
      "grad_norm": 3.861114501953125,
      "learning_rate": 1.6506276150627617e-05,
      "loss": 0.0421,
      "step": 1670
    },
    {
      "epoch": 1.7573221757322175,
      "grad_norm": 18.436248779296875,
      "learning_rate": 1.6485355648535567e-05,
      "loss": 0.0283,
      "step": 1680
    },
    {
      "epoch": 1.7677824267782427,
      "grad_norm": 0.4819495677947998,
      "learning_rate": 1.6464435146443516e-05,
      "loss": 0.0473,
      "step": 1690
    },
    {
      "epoch": 1.778242677824268,
      "grad_norm": 0.7755247354507446,
      "learning_rate": 1.6443514644351466e-05,
      "loss": 0.0367,
      "step": 1700
    },
    {
      "epoch": 1.788702928870293,
      "grad_norm": 5.618616104125977,
      "learning_rate": 1.6422594142259415e-05,
      "loss": 0.0194,
      "step": 1710
    },
    {
      "epoch": 1.799163179916318,
      "grad_norm": 6.277303218841553,
      "learning_rate": 1.6401673640167365e-05,
      "loss": 0.0332,
      "step": 1720
    },
    {
      "epoch": 1.8096234309623431,
      "grad_norm": 0.5524811744689941,
      "learning_rate": 1.6380753138075317e-05,
      "loss": 0.0545,
      "step": 1730
    },
    {
      "epoch": 1.8200836820083683,
      "grad_norm": 1.2393696308135986,
      "learning_rate": 1.6359832635983267e-05,
      "loss": 0.022,
      "step": 1740
    },
    {
      "epoch": 1.8305439330543933,
      "grad_norm": 2.221176862716675,
      "learning_rate": 1.6338912133891213e-05,
      "loss": 0.0241,
      "step": 1750
    },
    {
      "epoch": 1.8410041841004183,
      "grad_norm": 3.741030216217041,
      "learning_rate": 1.6317991631799162e-05,
      "loss": 0.0244,
      "step": 1760
    },
    {
      "epoch": 1.8514644351464435,
      "grad_norm": 0.10844293981790543,
      "learning_rate": 1.6297071129707115e-05,
      "loss": 0.0183,
      "step": 1770
    },
    {
      "epoch": 1.8619246861924688,
      "grad_norm": 3.2416203022003174,
      "learning_rate": 1.6276150627615064e-05,
      "loss": 0.0282,
      "step": 1780
    },
    {
      "epoch": 1.8723849372384938,
      "grad_norm": 2.8503546714782715,
      "learning_rate": 1.6255230125523014e-05,
      "loss": 0.0355,
      "step": 1790
    },
    {
      "epoch": 1.8828451882845187,
      "grad_norm": 3.545569896697998,
      "learning_rate": 1.6234309623430963e-05,
      "loss": 0.0336,
      "step": 1800
    },
    {
      "epoch": 1.893305439330544,
      "grad_norm": 2.0901875495910645,
      "learning_rate": 1.6213389121338913e-05,
      "loss": 0.0375,
      "step": 1810
    },
    {
      "epoch": 1.9037656903765692,
      "grad_norm": 4.2479634284973145,
      "learning_rate": 1.6192468619246865e-05,
      "loss": 0.0288,
      "step": 1820
    },
    {
      "epoch": 1.9142259414225942,
      "grad_norm": 1.505963683128357,
      "learning_rate": 1.6171548117154815e-05,
      "loss": 0.0325,
      "step": 1830
    },
    {
      "epoch": 1.9246861924686192,
      "grad_norm": 1.71701979637146,
      "learning_rate": 1.6150627615062764e-05,
      "loss": 0.0385,
      "step": 1840
    },
    {
      "epoch": 1.9351464435146444,
      "grad_norm": 3.919980525970459,
      "learning_rate": 1.612970711297071e-05,
      "loss": 0.0401,
      "step": 1850
    },
    {
      "epoch": 1.9456066945606696,
      "grad_norm": 4.160606384277344,
      "learning_rate": 1.6108786610878663e-05,
      "loss": 0.0596,
      "step": 1860
    },
    {
      "epoch": 1.9560669456066946,
      "grad_norm": 4.607587814331055,
      "learning_rate": 1.6087866108786612e-05,
      "loss": 0.0441,
      "step": 1870
    },
    {
      "epoch": 1.9665271966527196,
      "grad_norm": 0.41367363929748535,
      "learning_rate": 1.6066945606694562e-05,
      "loss": 0.042,
      "step": 1880
    },
    {
      "epoch": 1.9769874476987448,
      "grad_norm": 0.14492250978946686,
      "learning_rate": 1.604602510460251e-05,
      "loss": 0.0429,
      "step": 1890
    },
    {
      "epoch": 1.98744769874477,
      "grad_norm": 2.825993299484253,
      "learning_rate": 1.602510460251046e-05,
      "loss": 0.0418,
      "step": 1900
    },
    {
      "epoch": 1.997907949790795,
      "grad_norm": 5.953324794769287,
      "learning_rate": 1.600418410041841e-05,
      "loss": 0.0557,
      "step": 1910
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9904471544715447,
      "eval_f1": 0.9583517944173682,
      "eval_loss": 0.02910219132900238,
      "eval_precision": 0.9549668874172186,
      "eval_recall": 0.9617607825700312,
      "eval_runtime": 629.6654,
      "eval_samples_per_second": 31.256,
      "eval_steps_per_second": 1.304,
      "step": 1912
    },
    {
      "epoch": 2.00836820083682,
      "grad_norm": 3.7233285903930664,
      "learning_rate": 1.5983263598326363e-05,
      "loss": 0.0303,
      "step": 1920
    },
    {
      "epoch": 2.018828451882845,
      "grad_norm": 0.764598548412323,
      "learning_rate": 1.5962343096234312e-05,
      "loss": 0.0216,
      "step": 1930
    },
    {
      "epoch": 2.0292887029288704,
      "grad_norm": 0.21843069791793823,
      "learning_rate": 1.594142259414226e-05,
      "loss": 0.0186,
      "step": 1940
    },
    {
      "epoch": 2.0397489539748954,
      "grad_norm": 0.11575643718242645,
      "learning_rate": 1.592050209205021e-05,
      "loss": 0.0351,
      "step": 1950
    },
    {
      "epoch": 2.0502092050209204,
      "grad_norm": 0.8260313272476196,
      "learning_rate": 1.589958158995816e-05,
      "loss": 0.0137,
      "step": 1960
    },
    {
      "epoch": 2.0606694560669454,
      "grad_norm": 1.2006323337554932,
      "learning_rate": 1.587866108786611e-05,
      "loss": 0.0229,
      "step": 1970
    },
    {
      "epoch": 2.071129707112971,
      "grad_norm": 0.4439125061035156,
      "learning_rate": 1.585774058577406e-05,
      "loss": 0.0115,
      "step": 1980
    },
    {
      "epoch": 2.081589958158996,
      "grad_norm": 0.05704062432050705,
      "learning_rate": 1.583682008368201e-05,
      "loss": 0.017,
      "step": 1990
    },
    {
      "epoch": 2.092050209205021,
      "grad_norm": 1.6363345384597778,
      "learning_rate": 1.5815899581589958e-05,
      "loss": 0.0381,
      "step": 2000
    },
    {
      "epoch": 2.102510460251046,
      "grad_norm": 0.2571846842765808,
      "learning_rate": 1.579497907949791e-05,
      "loss": 0.0234,
      "step": 2010
    },
    {
      "epoch": 2.1129707112970713,
      "grad_norm": 1.5389553308486938,
      "learning_rate": 1.577405857740586e-05,
      "loss": 0.0104,
      "step": 2020
    },
    {
      "epoch": 2.1234309623430963,
      "grad_norm": 1.7119324207305908,
      "learning_rate": 1.575313807531381e-05,
      "loss": 0.0359,
      "step": 2030
    },
    {
      "epoch": 2.1338912133891212,
      "grad_norm": 0.27557602524757385,
      "learning_rate": 1.573221757322176e-05,
      "loss": 0.0323,
      "step": 2040
    },
    {
      "epoch": 2.1443514644351462,
      "grad_norm": 2.721985340118408,
      "learning_rate": 1.571129707112971e-05,
      "loss": 0.0137,
      "step": 2050
    },
    {
      "epoch": 2.1548117154811717,
      "grad_norm": 0.39619097113609314,
      "learning_rate": 1.5690376569037658e-05,
      "loss": 0.0057,
      "step": 2060
    },
    {
      "epoch": 2.1652719665271967,
      "grad_norm": 5.20142126083374,
      "learning_rate": 1.5669456066945607e-05,
      "loss": 0.0224,
      "step": 2070
    },
    {
      "epoch": 2.1757322175732217,
      "grad_norm": 0.10897988826036453,
      "learning_rate": 1.5648535564853557e-05,
      "loss": 0.0147,
      "step": 2080
    },
    {
      "epoch": 2.1861924686192467,
      "grad_norm": 2.1016178131103516,
      "learning_rate": 1.5627615062761506e-05,
      "loss": 0.0303,
      "step": 2090
    },
    {
      "epoch": 2.196652719665272,
      "grad_norm": 2.422758102416992,
      "learning_rate": 1.560669456066946e-05,
      "loss": 0.0318,
      "step": 2100
    },
    {
      "epoch": 2.207112970711297,
      "grad_norm": 0.044810570776462555,
      "learning_rate": 1.558577405857741e-05,
      "loss": 0.0172,
      "step": 2110
    },
    {
      "epoch": 2.217573221757322,
      "grad_norm": 3.2929184436798096,
      "learning_rate": 1.5564853556485358e-05,
      "loss": 0.015,
      "step": 2120
    },
    {
      "epoch": 2.2280334728033475,
      "grad_norm": 1.4062939882278442,
      "learning_rate": 1.5543933054393307e-05,
      "loss": 0.0221,
      "step": 2130
    },
    {
      "epoch": 2.2384937238493725,
      "grad_norm": 4.096166133880615,
      "learning_rate": 1.5523012552301257e-05,
      "loss": 0.0205,
      "step": 2140
    },
    {
      "epoch": 2.2489539748953975,
      "grad_norm": 1.288605809211731,
      "learning_rate": 1.5502092050209206e-05,
      "loss": 0.0352,
      "step": 2150
    },
    {
      "epoch": 2.2594142259414225,
      "grad_norm": 4.790052890777588,
      "learning_rate": 1.5481171548117155e-05,
      "loss": 0.0365,
      "step": 2160
    },
    {
      "epoch": 2.2698744769874475,
      "grad_norm": 2.1150355339050293,
      "learning_rate": 1.5460251046025105e-05,
      "loss": 0.0172,
      "step": 2170
    },
    {
      "epoch": 2.280334728033473,
      "grad_norm": 2.5088400840759277,
      "learning_rate": 1.5439330543933054e-05,
      "loss": 0.0198,
      "step": 2180
    },
    {
      "epoch": 2.290794979079498,
      "grad_norm": 1.7126376628875732,
      "learning_rate": 1.5418410041841007e-05,
      "loss": 0.0337,
      "step": 2190
    },
    {
      "epoch": 2.301255230125523,
      "grad_norm": 2.0024523735046387,
      "learning_rate": 1.5397489539748957e-05,
      "loss": 0.0298,
      "step": 2200
    },
    {
      "epoch": 2.3117154811715483,
      "grad_norm": 0.5904170274734497,
      "learning_rate": 1.5376569037656906e-05,
      "loss": 0.0246,
      "step": 2210
    },
    {
      "epoch": 2.3221757322175733,
      "grad_norm": 2.917262315750122,
      "learning_rate": 1.5355648535564855e-05,
      "loss": 0.0411,
      "step": 2220
    },
    {
      "epoch": 2.3326359832635983,
      "grad_norm": 1.005569338798523,
      "learning_rate": 1.5334728033472805e-05,
      "loss": 0.0168,
      "step": 2230
    },
    {
      "epoch": 2.3430962343096233,
      "grad_norm": 0.6018978357315063,
      "learning_rate": 1.5313807531380754e-05,
      "loss": 0.0257,
      "step": 2240
    },
    {
      "epoch": 2.3535564853556483,
      "grad_norm": 3.1588947772979736,
      "learning_rate": 1.5292887029288704e-05,
      "loss": 0.0226,
      "step": 2250
    },
    {
      "epoch": 2.3640167364016738,
      "grad_norm": 0.03716243430972099,
      "learning_rate": 1.5271966527196653e-05,
      "loss": 0.0175,
      "step": 2260
    },
    {
      "epoch": 2.3744769874476988,
      "grad_norm": 0.31939685344696045,
      "learning_rate": 1.5251046025104604e-05,
      "loss": 0.0206,
      "step": 2270
    },
    {
      "epoch": 2.3849372384937237,
      "grad_norm": 1.3916244506835938,
      "learning_rate": 1.5230125523012553e-05,
      "loss": 0.0117,
      "step": 2280
    },
    {
      "epoch": 2.395397489539749,
      "grad_norm": 2.831700325012207,
      "learning_rate": 1.5209205020920503e-05,
      "loss": 0.0126,
      "step": 2290
    },
    {
      "epoch": 2.405857740585774,
      "grad_norm": 0.12534260749816895,
      "learning_rate": 1.5188284518828454e-05,
      "loss": 0.0129,
      "step": 2300
    },
    {
      "epoch": 2.416317991631799,
      "grad_norm": 3.1223654747009277,
      "learning_rate": 1.5167364016736403e-05,
      "loss": 0.0105,
      "step": 2310
    },
    {
      "epoch": 2.426778242677824,
      "grad_norm": 5.023980140686035,
      "learning_rate": 1.5146443514644353e-05,
      "loss": 0.0359,
      "step": 2320
    },
    {
      "epoch": 2.437238493723849,
      "grad_norm": 6.178700923919678,
      "learning_rate": 1.5125523012552304e-05,
      "loss": 0.0234,
      "step": 2330
    },
    {
      "epoch": 2.4476987447698746,
      "grad_norm": 3.8853230476379395,
      "learning_rate": 1.5104602510460253e-05,
      "loss": 0.019,
      "step": 2340
    },
    {
      "epoch": 2.4581589958158996,
      "grad_norm": 0.0456361286342144,
      "learning_rate": 1.5083682008368201e-05,
      "loss": 0.0234,
      "step": 2350
    },
    {
      "epoch": 2.4686192468619246,
      "grad_norm": 5.795137882232666,
      "learning_rate": 1.506276150627615e-05,
      "loss": 0.0313,
      "step": 2360
    },
    {
      "epoch": 2.47907949790795,
      "grad_norm": 3.342573642730713,
      "learning_rate": 1.5041841004184102e-05,
      "loss": 0.034,
      "step": 2370
    },
    {
      "epoch": 2.489539748953975,
      "grad_norm": 3.1879818439483643,
      "learning_rate": 1.5020920502092051e-05,
      "loss": 0.024,
      "step": 2380
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.7108169198036194,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 0.0083,
      "step": 2390
    },
    {
      "epoch": 2.510460251046025,
      "grad_norm": 0.16712558269500732,
      "learning_rate": 1.4979079497907951e-05,
      "loss": 0.0331,
      "step": 2400
    },
    {
      "epoch": 2.52092050209205,
      "grad_norm": 1.766182541847229,
      "learning_rate": 1.4958158995815901e-05,
      "loss": 0.0226,
      "step": 2410
    },
    {
      "epoch": 2.5313807531380754,
      "grad_norm": 0.07462663948535919,
      "learning_rate": 1.4937238493723852e-05,
      "loss": 0.0112,
      "step": 2420
    },
    {
      "epoch": 2.5418410041841004,
      "grad_norm": 3.053060293197632,
      "learning_rate": 1.4916317991631801e-05,
      "loss": 0.0185,
      "step": 2430
    },
    {
      "epoch": 2.5523012552301254,
      "grad_norm": 0.10594533383846283,
      "learning_rate": 1.4895397489539749e-05,
      "loss": 0.012,
      "step": 2440
    },
    {
      "epoch": 2.562761506276151,
      "grad_norm": 0.6288102865219116,
      "learning_rate": 1.4874476987447699e-05,
      "loss": 0.0148,
      "step": 2450
    },
    {
      "epoch": 2.573221757322176,
      "grad_norm": 2.010591983795166,
      "learning_rate": 1.485355648535565e-05,
      "loss": 0.0167,
      "step": 2460
    },
    {
      "epoch": 2.583682008368201,
      "grad_norm": 3.262636423110962,
      "learning_rate": 1.4832635983263599e-05,
      "loss": 0.0124,
      "step": 2470
    },
    {
      "epoch": 2.594142259414226,
      "grad_norm": 3.752941846847534,
      "learning_rate": 1.4811715481171548e-05,
      "loss": 0.0335,
      "step": 2480
    },
    {
      "epoch": 2.604602510460251,
      "grad_norm": 1.6537160873413086,
      "learning_rate": 1.47907949790795e-05,
      "loss": 0.0251,
      "step": 2490
    },
    {
      "epoch": 2.6150627615062763,
      "grad_norm": 2.572826623916626,
      "learning_rate": 1.4769874476987449e-05,
      "loss": 0.0348,
      "step": 2500
    },
    {
      "epoch": 2.6255230125523012,
      "grad_norm": 2.290531873703003,
      "learning_rate": 1.4748953974895398e-05,
      "loss": 0.0121,
      "step": 2510
    },
    {
      "epoch": 2.6359832635983262,
      "grad_norm": 2.635524034500122,
      "learning_rate": 1.472803347280335e-05,
      "loss": 0.0287,
      "step": 2520
    },
    {
      "epoch": 2.6464435146443517,
      "grad_norm": 2.657064199447632,
      "learning_rate": 1.4707112970711299e-05,
      "loss": 0.0163,
      "step": 2530
    },
    {
      "epoch": 2.6569037656903767,
      "grad_norm": 5.652294635772705,
      "learning_rate": 1.4686192468619247e-05,
      "loss": 0.0289,
      "step": 2540
    },
    {
      "epoch": 2.6673640167364017,
      "grad_norm": 3.0451014041900635,
      "learning_rate": 1.4665271966527198e-05,
      "loss": 0.0156,
      "step": 2550
    },
    {
      "epoch": 2.6778242677824267,
      "grad_norm": 0.26542791724205017,
      "learning_rate": 1.4644351464435147e-05,
      "loss": 0.0397,
      "step": 2560
    },
    {
      "epoch": 2.6882845188284517,
      "grad_norm": 3.0868825912475586,
      "learning_rate": 1.4623430962343097e-05,
      "loss": 0.0386,
      "step": 2570
    },
    {
      "epoch": 2.698744769874477,
      "grad_norm": 0.9102142453193665,
      "learning_rate": 1.4602510460251048e-05,
      "loss": 0.017,
      "step": 2580
    },
    {
      "epoch": 2.709205020920502,
      "grad_norm": 2.0266551971435547,
      "learning_rate": 1.4581589958158997e-05,
      "loss": 0.0123,
      "step": 2590
    },
    {
      "epoch": 2.719665271966527,
      "grad_norm": 5.879830360412598,
      "learning_rate": 1.4560669456066946e-05,
      "loss": 0.0229,
      "step": 2600
    },
    {
      "epoch": 2.7301255230125525,
      "grad_norm": 2.7043936252593994,
      "learning_rate": 1.4539748953974898e-05,
      "loss": 0.0129,
      "step": 2610
    },
    {
      "epoch": 2.7405857740585775,
      "grad_norm": 0.7801550030708313,
      "learning_rate": 1.4518828451882847e-05,
      "loss": 0.0099,
      "step": 2620
    },
    {
      "epoch": 2.7510460251046025,
      "grad_norm": 2.915773630142212,
      "learning_rate": 1.4497907949790796e-05,
      "loss": 0.0219,
      "step": 2630
    },
    {
      "epoch": 2.7615062761506275,
      "grad_norm": 0.0655442327260971,
      "learning_rate": 1.4476987447698744e-05,
      "loss": 0.0153,
      "step": 2640
    },
    {
      "epoch": 2.7719665271966525,
      "grad_norm": 0.7229580879211426,
      "learning_rate": 1.4456066945606695e-05,
      "loss": 0.0244,
      "step": 2650
    },
    {
      "epoch": 2.782426778242678,
      "grad_norm": 0.7628738880157471,
      "learning_rate": 1.4435146443514645e-05,
      "loss": 0.0113,
      "step": 2660
    },
    {
      "epoch": 2.792887029288703,
      "grad_norm": 0.11725448071956635,
      "learning_rate": 1.4414225941422596e-05,
      "loss": 0.0299,
      "step": 2670
    },
    {
      "epoch": 2.803347280334728,
      "grad_norm": 2.6409084796905518,
      "learning_rate": 1.4393305439330545e-05,
      "loss": 0.023,
      "step": 2680
    },
    {
      "epoch": 2.8138075313807533,
      "grad_norm": 0.37213122844696045,
      "learning_rate": 1.4372384937238495e-05,
      "loss": 0.0143,
      "step": 2690
    },
    {
      "epoch": 2.8242677824267783,
      "grad_norm": 0.04766860231757164,
      "learning_rate": 1.4351464435146446e-05,
      "loss": 0.0165,
      "step": 2700
    },
    {
      "epoch": 2.8347280334728033,
      "grad_norm": 0.41092216968536377,
      "learning_rate": 1.4330543933054395e-05,
      "loss": 0.0155,
      "step": 2710
    },
    {
      "epoch": 2.8451882845188283,
      "grad_norm": 1.4277740716934204,
      "learning_rate": 1.4309623430962344e-05,
      "loss": 0.0259,
      "step": 2720
    },
    {
      "epoch": 2.8556485355648533,
      "grad_norm": 3.3762168884277344,
      "learning_rate": 1.4288702928870296e-05,
      "loss": 0.013,
      "step": 2730
    },
    {
      "epoch": 2.8661087866108788,
      "grad_norm": 0.020242437720298767,
      "learning_rate": 1.4267782426778243e-05,
      "loss": 0.0171,
      "step": 2740
    },
    {
      "epoch": 2.8765690376569037,
      "grad_norm": 2.6227924823760986,
      "learning_rate": 1.4246861924686193e-05,
      "loss": 0.0209,
      "step": 2750
    },
    {
      "epoch": 2.8870292887029287,
      "grad_norm": 5.221381187438965,
      "learning_rate": 1.4225941422594142e-05,
      "loss": 0.0332,
      "step": 2760
    },
    {
      "epoch": 2.897489539748954,
      "grad_norm": 3.118340015411377,
      "learning_rate": 1.4205020920502093e-05,
      "loss": 0.0332,
      "step": 2770
    },
    {
      "epoch": 2.907949790794979,
      "grad_norm": 0.3805070221424103,
      "learning_rate": 1.4184100418410043e-05,
      "loss": 0.017,
      "step": 2780
    },
    {
      "epoch": 2.918410041841004,
      "grad_norm": 1.6447898149490356,
      "learning_rate": 1.4163179916317992e-05,
      "loss": 0.0243,
      "step": 2790
    },
    {
      "epoch": 2.928870292887029,
      "grad_norm": 0.18603044748306274,
      "learning_rate": 1.4142259414225943e-05,
      "loss": 0.0111,
      "step": 2800
    },
    {
      "epoch": 2.939330543933054,
      "grad_norm": 7.987096786499023,
      "learning_rate": 1.4121338912133893e-05,
      "loss": 0.0219,
      "step": 2810
    },
    {
      "epoch": 2.9497907949790796,
      "grad_norm": 3.083449125289917,
      "learning_rate": 1.4100418410041844e-05,
      "loss": 0.0359,
      "step": 2820
    },
    {
      "epoch": 2.9602510460251046,
      "grad_norm": 1.914328932762146,
      "learning_rate": 1.4079497907949793e-05,
      "loss": 0.0207,
      "step": 2830
    },
    {
      "epoch": 2.9707112970711296,
      "grad_norm": 0.0498964861035347,
      "learning_rate": 1.405857740585774e-05,
      "loss": 0.0217,
      "step": 2840
    },
    {
      "epoch": 2.981171548117155,
      "grad_norm": 0.12264344096183777,
      "learning_rate": 1.403765690376569e-05,
      "loss": 0.0177,
      "step": 2850
    },
    {
      "epoch": 2.99163179916318,
      "grad_norm": 0.04233561083674431,
      "learning_rate": 1.4016736401673641e-05,
      "loss": 0.0264,
      "step": 2860
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.9919715447154471,
      "eval_f1": 0.9650442477876107,
      "eval_loss": 0.029608089476823807,
      "eval_precision": 0.9603698811096433,
      "eval_recall": 0.9697643397065362,
      "eval_runtime": 723.0934,
      "eval_samples_per_second": 27.218,
      "eval_steps_per_second": 1.135,
      "step": 2868
    },
    {
      "epoch": 3.002092050209205,
      "grad_norm": 1.2474316358566284,
      "learning_rate": 1.399581589958159e-05,
      "loss": 0.0198,
      "step": 2870
    },
    {
      "epoch": 3.01255230125523,
      "grad_norm": 0.3211485743522644,
      "learning_rate": 1.397489539748954e-05,
      "loss": 0.0136,
      "step": 2880
    },
    {
      "epoch": 3.0230125523012554,
      "grad_norm": 2.0581345558166504,
      "learning_rate": 1.3953974895397491e-05,
      "loss": 0.012,
      "step": 2890
    },
    {
      "epoch": 3.0334728033472804,
      "grad_norm": 0.4754520654678345,
      "learning_rate": 1.393305439330544e-05,
      "loss": 0.0063,
      "step": 2900
    },
    {
      "epoch": 3.0439330543933054,
      "grad_norm": 5.944728374481201,
      "learning_rate": 1.391213389121339e-05,
      "loss": 0.0232,
      "step": 2910
    },
    {
      "epoch": 3.0543933054393304,
      "grad_norm": 1.0688698291778564,
      "learning_rate": 1.3891213389121341e-05,
      "loss": 0.0091,
      "step": 2920
    },
    {
      "epoch": 3.064853556485356,
      "grad_norm": 0.6262282133102417,
      "learning_rate": 1.387029288702929e-05,
      "loss": 0.0111,
      "step": 2930
    },
    {
      "epoch": 3.075313807531381,
      "grad_norm": 0.14421559870243073,
      "learning_rate": 1.3849372384937238e-05,
      "loss": 0.0175,
      "step": 2940
    },
    {
      "epoch": 3.085774058577406,
      "grad_norm": 0.21054302155971527,
      "learning_rate": 1.382845188284519e-05,
      "loss": 0.0095,
      "step": 2950
    },
    {
      "epoch": 3.096234309623431,
      "grad_norm": 0.1771978735923767,
      "learning_rate": 1.3807531380753139e-05,
      "loss": 0.022,
      "step": 2960
    },
    {
      "epoch": 3.1066945606694563,
      "grad_norm": 0.03088253177702427,
      "learning_rate": 1.3786610878661088e-05,
      "loss": 0.0042,
      "step": 2970
    },
    {
      "epoch": 3.1171548117154813,
      "grad_norm": 1.6936969757080078,
      "learning_rate": 1.376569037656904e-05,
      "loss": 0.0134,
      "step": 2980
    },
    {
      "epoch": 3.1276150627615062,
      "grad_norm": 0.6612001657485962,
      "learning_rate": 1.3744769874476989e-05,
      "loss": 0.0117,
      "step": 2990
    },
    {
      "epoch": 3.1380753138075312,
      "grad_norm": 0.11239336431026459,
      "learning_rate": 1.3723849372384938e-05,
      "loss": 0.0109,
      "step": 3000
    },
    {
      "epoch": 3.1485355648535567,
      "grad_norm": 0.19786202907562256,
      "learning_rate": 1.370292887029289e-05,
      "loss": 0.0079,
      "step": 3010
    },
    {
      "epoch": 3.1589958158995817,
      "grad_norm": 1.4793598651885986,
      "learning_rate": 1.3682008368200839e-05,
      "loss": 0.0198,
      "step": 3020
    },
    {
      "epoch": 3.1694560669456067,
      "grad_norm": 0.02976839803159237,
      "learning_rate": 1.3661087866108788e-05,
      "loss": 0.0072,
      "step": 3030
    },
    {
      "epoch": 3.1799163179916317,
      "grad_norm": 0.022356044501066208,
      "learning_rate": 1.3640167364016736e-05,
      "loss": 0.0077,
      "step": 3040
    },
    {
      "epoch": 3.190376569037657,
      "grad_norm": 0.9369667172431946,
      "learning_rate": 1.3619246861924687e-05,
      "loss": 0.0169,
      "step": 3050
    },
    {
      "epoch": 3.200836820083682,
      "grad_norm": 1.8197051286697388,
      "learning_rate": 1.3598326359832636e-05,
      "loss": 0.0086,
      "step": 3060
    },
    {
      "epoch": 3.211297071129707,
      "grad_norm": 1.212454915046692,
      "learning_rate": 1.3577405857740586e-05,
      "loss": 0.0109,
      "step": 3070
    },
    {
      "epoch": 3.221757322175732,
      "grad_norm": 1.9464935064315796,
      "learning_rate": 1.3556485355648537e-05,
      "loss": 0.0197,
      "step": 3080
    },
    {
      "epoch": 3.2322175732217575,
      "grad_norm": 0.6904953718185425,
      "learning_rate": 1.3535564853556486e-05,
      "loss": 0.0062,
      "step": 3090
    },
    {
      "epoch": 3.2426778242677825,
      "grad_norm": 0.5561779737472534,
      "learning_rate": 1.3514644351464437e-05,
      "loss": 0.0175,
      "step": 3100
    },
    {
      "epoch": 3.2531380753138075,
      "grad_norm": 2.742424964904785,
      "learning_rate": 1.3493723849372387e-05,
      "loss": 0.0167,
      "step": 3110
    },
    {
      "epoch": 3.2635983263598325,
      "grad_norm": 0.12365826219320297,
      "learning_rate": 1.3472803347280336e-05,
      "loss": 0.0076,
      "step": 3120
    },
    {
      "epoch": 3.274058577405858,
      "grad_norm": 2.5897865295410156,
      "learning_rate": 1.3451882845188287e-05,
      "loss": 0.0106,
      "step": 3130
    },
    {
      "epoch": 3.284518828451883,
      "grad_norm": 0.014281190000474453,
      "learning_rate": 1.3430962343096235e-05,
      "loss": 0.0225,
      "step": 3140
    },
    {
      "epoch": 3.294979079497908,
      "grad_norm": 3.779320240020752,
      "learning_rate": 1.3410041841004184e-05,
      "loss": 0.013,
      "step": 3150
    },
    {
      "epoch": 3.305439330543933,
      "grad_norm": 2.505500078201294,
      "learning_rate": 1.3389121338912134e-05,
      "loss": 0.0262,
      "step": 3160
    },
    {
      "epoch": 3.3158995815899583,
      "grad_norm": 5.765445232391357,
      "learning_rate": 1.3368200836820085e-05,
      "loss": 0.0307,
      "step": 3170
    },
    {
      "epoch": 3.3263598326359833,
      "grad_norm": 0.11651976406574249,
      "learning_rate": 1.3347280334728034e-05,
      "loss": 0.0199,
      "step": 3180
    },
    {
      "epoch": 3.3368200836820083,
      "grad_norm": 0.03522137179970741,
      "learning_rate": 1.3326359832635984e-05,
      "loss": 0.0164,
      "step": 3190
    },
    {
      "epoch": 3.3472803347280333,
      "grad_norm": 4.459024906158447,
      "learning_rate": 1.3305439330543935e-05,
      "loss": 0.01,
      "step": 3200
    },
    {
      "epoch": 3.3577405857740588,
      "grad_norm": 0.2589527368545532,
      "learning_rate": 1.3284518828451884e-05,
      "loss": 0.0209,
      "step": 3210
    },
    {
      "epoch": 3.3682008368200838,
      "grad_norm": 0.7164319157600403,
      "learning_rate": 1.3263598326359835e-05,
      "loss": 0.009,
      "step": 3220
    },
    {
      "epoch": 3.3786610878661087,
      "grad_norm": 2.126858949661255,
      "learning_rate": 1.3242677824267785e-05,
      "loss": 0.0208,
      "step": 3230
    },
    {
      "epoch": 3.3891213389121337,
      "grad_norm": 2.2262918949127197,
      "learning_rate": 1.3221757322175732e-05,
      "loss": 0.0081,
      "step": 3240
    },
    {
      "epoch": 3.399581589958159,
      "grad_norm": 3.9647903442382812,
      "learning_rate": 1.3200836820083682e-05,
      "loss": 0.0164,
      "step": 3250
    },
    {
      "epoch": 3.410041841004184,
      "grad_norm": 3.885805368423462,
      "learning_rate": 1.3179916317991633e-05,
      "loss": 0.0106,
      "step": 3260
    },
    {
      "epoch": 3.420502092050209,
      "grad_norm": 2.0104668140411377,
      "learning_rate": 1.3158995815899582e-05,
      "loss": 0.0148,
      "step": 3270
    },
    {
      "epoch": 3.430962343096234,
      "grad_norm": 3.987152576446533,
      "learning_rate": 1.3138075313807532e-05,
      "loss": 0.0077,
      "step": 3280
    },
    {
      "epoch": 3.4414225941422596,
      "grad_norm": 0.46018508076667786,
      "learning_rate": 1.3117154811715483e-05,
      "loss": 0.0086,
      "step": 3290
    },
    {
      "epoch": 3.4518828451882846,
      "grad_norm": 1.7602299451828003,
      "learning_rate": 1.3096234309623432e-05,
      "loss": 0.0234,
      "step": 3300
    },
    {
      "epoch": 3.4623430962343096,
      "grad_norm": 0.39993929862976074,
      "learning_rate": 1.3075313807531382e-05,
      "loss": 0.0056,
      "step": 3310
    },
    {
      "epoch": 3.4728033472803346,
      "grad_norm": 0.11339477449655533,
      "learning_rate": 1.3054393305439333e-05,
      "loss": 0.0083,
      "step": 3320
    },
    {
      "epoch": 3.48326359832636,
      "grad_norm": 3.430753231048584,
      "learning_rate": 1.3033472803347282e-05,
      "loss": 0.0098,
      "step": 3330
    },
    {
      "epoch": 3.493723849372385,
      "grad_norm": 0.1587919443845749,
      "learning_rate": 1.301255230125523e-05,
      "loss": 0.0016,
      "step": 3340
    },
    {
      "epoch": 3.50418410041841,
      "grad_norm": 0.30216047167778015,
      "learning_rate": 1.2991631799163181e-05,
      "loss": 0.0184,
      "step": 3350
    },
    {
      "epoch": 3.514644351464435,
      "grad_norm": 0.04218563437461853,
      "learning_rate": 1.297071129707113e-05,
      "loss": 0.0172,
      "step": 3360
    },
    {
      "epoch": 3.52510460251046,
      "grad_norm": 0.5947007536888123,
      "learning_rate": 1.294979079497908e-05,
      "loss": 0.0077,
      "step": 3370
    },
    {
      "epoch": 3.5355648535564854,
      "grad_norm": 0.9877542853355408,
      "learning_rate": 1.2928870292887031e-05,
      "loss": 0.0047,
      "step": 3380
    },
    {
      "epoch": 3.5460251046025104,
      "grad_norm": 0.12530483305454254,
      "learning_rate": 1.290794979079498e-05,
      "loss": 0.01,
      "step": 3390
    },
    {
      "epoch": 3.5564853556485354,
      "grad_norm": 0.5343488454818726,
      "learning_rate": 1.288702928870293e-05,
      "loss": 0.0109,
      "step": 3400
    },
    {
      "epoch": 3.566945606694561,
      "grad_norm": 1.4982129335403442,
      "learning_rate": 1.286610878661088e-05,
      "loss": 0.0093,
      "step": 3410
    },
    {
      "epoch": 3.577405857740586,
      "grad_norm": 4.390720844268799,
      "learning_rate": 1.284518828451883e-05,
      "loss": 0.0327,
      "step": 3420
    },
    {
      "epoch": 3.587866108786611,
      "grad_norm": 0.026350315660238266,
      "learning_rate": 1.282426778242678e-05,
      "loss": 0.0162,
      "step": 3430
    },
    {
      "epoch": 3.598326359832636,
      "grad_norm": 2.3948922157287598,
      "learning_rate": 1.2803347280334727e-05,
      "loss": 0.0134,
      "step": 3440
    },
    {
      "epoch": 3.608786610878661,
      "grad_norm": 1.6873904466629028,
      "learning_rate": 1.2782426778242678e-05,
      "loss": 0.0253,
      "step": 3450
    },
    {
      "epoch": 3.6192468619246863,
      "grad_norm": 1.7172274589538574,
      "learning_rate": 1.2761506276150628e-05,
      "loss": 0.0183,
      "step": 3460
    },
    {
      "epoch": 3.6297071129707112,
      "grad_norm": 0.6460022330284119,
      "learning_rate": 1.2740585774058577e-05,
      "loss": 0.0124,
      "step": 3470
    },
    {
      "epoch": 3.6401673640167362,
      "grad_norm": 2.2142889499664307,
      "learning_rate": 1.2719665271966528e-05,
      "loss": 0.0375,
      "step": 3480
    },
    {
      "epoch": 3.6506276150627617,
      "grad_norm": 2.9260127544403076,
      "learning_rate": 1.2698744769874478e-05,
      "loss": 0.0143,
      "step": 3490
    },
    {
      "epoch": 3.6610878661087867,
      "grad_norm": 0.02727108635008335,
      "learning_rate": 1.2677824267782429e-05,
      "loss": 0.0383,
      "step": 3500
    },
    {
      "epoch": 3.6715481171548117,
      "grad_norm": 0.1409662961959839,
      "learning_rate": 1.2656903765690378e-05,
      "loss": 0.0116,
      "step": 3510
    },
    {
      "epoch": 3.6820083682008367,
      "grad_norm": 2.1892430782318115,
      "learning_rate": 1.2635983263598328e-05,
      "loss": 0.0137,
      "step": 3520
    },
    {
      "epoch": 3.6924686192468616,
      "grad_norm": 0.030509470030665398,
      "learning_rate": 1.2615062761506279e-05,
      "loss": 0.0164,
      "step": 3530
    },
    {
      "epoch": 3.702928870292887,
      "grad_norm": 4.263144493103027,
      "learning_rate": 1.2594142259414227e-05,
      "loss": 0.0062,
      "step": 3540
    },
    {
      "epoch": 3.713389121338912,
      "grad_norm": 0.13773727416992188,
      "learning_rate": 1.2573221757322176e-05,
      "loss": 0.0026,
      "step": 3550
    },
    {
      "epoch": 3.723849372384937,
      "grad_norm": 2.3817481994628906,
      "learning_rate": 1.2552301255230125e-05,
      "loss": 0.0079,
      "step": 3560
    },
    {
      "epoch": 3.7343096234309625,
      "grad_norm": 0.012383387424051762,
      "learning_rate": 1.2531380753138076e-05,
      "loss": 0.0077,
      "step": 3570
    },
    {
      "epoch": 3.7447698744769875,
      "grad_norm": 0.1883630007505417,
      "learning_rate": 1.2510460251046026e-05,
      "loss": 0.0021,
      "step": 3580
    },
    {
      "epoch": 3.7552301255230125,
      "grad_norm": 0.05150285363197327,
      "learning_rate": 1.2489539748953975e-05,
      "loss": 0.0229,
      "step": 3590
    },
    {
      "epoch": 3.7656903765690375,
      "grad_norm": 0.25513899326324463,
      "learning_rate": 1.2468619246861926e-05,
      "loss": 0.0025,
      "step": 3600
    },
    {
      "epoch": 3.776150627615063,
      "grad_norm": 0.8966097831726074,
      "learning_rate": 1.2447698744769876e-05,
      "loss": 0.006,
      "step": 3610
    },
    {
      "epoch": 3.786610878661088,
      "grad_norm": 5.774024963378906,
      "learning_rate": 1.2426778242677825e-05,
      "loss": 0.0232,
      "step": 3620
    },
    {
      "epoch": 3.797071129707113,
      "grad_norm": 0.0459381528198719,
      "learning_rate": 1.2405857740585776e-05,
      "loss": 0.0092,
      "step": 3630
    },
    {
      "epoch": 3.8075313807531384,
      "grad_norm": 0.012280977331101894,
      "learning_rate": 1.2384937238493724e-05,
      "loss": 0.0132,
      "step": 3640
    },
    {
      "epoch": 3.8179916317991633,
      "grad_norm": 0.067037433385849,
      "learning_rate": 1.2364016736401673e-05,
      "loss": 0.0202,
      "step": 3650
    },
    {
      "epoch": 3.8284518828451883,
      "grad_norm": 0.11496233195066452,
      "learning_rate": 1.2343096234309625e-05,
      "loss": 0.0198,
      "step": 3660
    },
    {
      "epoch": 3.8389121338912133,
      "grad_norm": 5.289803504943848,
      "learning_rate": 1.2322175732217574e-05,
      "loss": 0.0189,
      "step": 3670
    },
    {
      "epoch": 3.8493723849372383,
      "grad_norm": 0.7928880453109741,
      "learning_rate": 1.2301255230125523e-05,
      "loss": 0.0123,
      "step": 3680
    },
    {
      "epoch": 3.8598326359832638,
      "grad_norm": 3.2605929374694824,
      "learning_rate": 1.2280334728033474e-05,
      "loss": 0.0148,
      "step": 3690
    },
    {
      "epoch": 3.8702928870292888,
      "grad_norm": 0.04249153286218643,
      "learning_rate": 1.2259414225941424e-05,
      "loss": 0.0099,
      "step": 3700
    },
    {
      "epoch": 3.8807531380753137,
      "grad_norm": 0.02000739797949791,
      "learning_rate": 1.2238493723849373e-05,
      "loss": 0.009,
      "step": 3710
    },
    {
      "epoch": 3.891213389121339,
      "grad_norm": 1.2350287437438965,
      "learning_rate": 1.2217573221757324e-05,
      "loss": 0.0035,
      "step": 3720
    },
    {
      "epoch": 3.901673640167364,
      "grad_norm": 1.378419041633606,
      "learning_rate": 1.2196652719665274e-05,
      "loss": 0.0186,
      "step": 3730
    },
    {
      "epoch": 3.912133891213389,
      "grad_norm": 0.2329317331314087,
      "learning_rate": 1.2175732217573222e-05,
      "loss": 0.0082,
      "step": 3740
    },
    {
      "epoch": 3.922594142259414,
      "grad_norm": 0.14920870959758759,
      "learning_rate": 1.2154811715481171e-05,
      "loss": 0.0162,
      "step": 3750
    },
    {
      "epoch": 3.933054393305439,
      "grad_norm": 0.03971941024065018,
      "learning_rate": 1.2133891213389122e-05,
      "loss": 0.0085,
      "step": 3760
    },
    {
      "epoch": 3.9435146443514646,
      "grad_norm": 4.872292995452881,
      "learning_rate": 1.2112970711297071e-05,
      "loss": 0.0099,
      "step": 3770
    },
    {
      "epoch": 3.9539748953974896,
      "grad_norm": 6.98232364654541,
      "learning_rate": 1.2092050209205023e-05,
      "loss": 0.0255,
      "step": 3780
    },
    {
      "epoch": 3.9644351464435146,
      "grad_norm": 3.5725927352905273,
      "learning_rate": 1.2071129707112972e-05,
      "loss": 0.0077,
      "step": 3790
    },
    {
      "epoch": 3.97489539748954,
      "grad_norm": 0.7412234544754028,
      "learning_rate": 1.2050209205020921e-05,
      "loss": 0.014,
      "step": 3800
    },
    {
      "epoch": 3.985355648535565,
      "grad_norm": 1.382890224456787,
      "learning_rate": 1.2029288702928872e-05,
      "loss": 0.0104,
      "step": 3810
    },
    {
      "epoch": 3.99581589958159,
      "grad_norm": 2.965728998184204,
      "learning_rate": 1.2008368200836822e-05,
      "loss": 0.0152,
      "step": 3820
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.9934451219512195,
      "eval_f1": 0.971173184357542,
      "eval_loss": 0.02596054971218109,
      "eval_precision": 0.9761904761904762,
      "eval_recall": 0.9662072032014228,
      "eval_runtime": 728.0322,
      "eval_samples_per_second": 27.033,
      "eval_steps_per_second": 1.128,
      "step": 3824
    },
    {
      "epoch": 4.006276150627615,
      "grad_norm": 4.834027290344238,
      "learning_rate": 1.1987447698744771e-05,
      "loss": 0.0223,
      "step": 3830
    },
    {
      "epoch": 4.01673640167364,
      "grad_norm": 1.6620186567306519,
      "learning_rate": 1.1966527196652719e-05,
      "loss": 0.0035,
      "step": 3840
    },
    {
      "epoch": 4.027196652719665,
      "grad_norm": 0.07470625638961792,
      "learning_rate": 1.194560669456067e-05,
      "loss": 0.0198,
      "step": 3850
    },
    {
      "epoch": 4.03765690376569,
      "grad_norm": 0.06871664524078369,
      "learning_rate": 1.192468619246862e-05,
      "loss": 0.003,
      "step": 3860
    },
    {
      "epoch": 4.048117154811716,
      "grad_norm": 0.026818152517080307,
      "learning_rate": 1.1903765690376569e-05,
      "loss": 0.0221,
      "step": 3870
    },
    {
      "epoch": 4.058577405857741,
      "grad_norm": 2.645538330078125,
      "learning_rate": 1.188284518828452e-05,
      "loss": 0.0053,
      "step": 3880
    },
    {
      "epoch": 4.069037656903766,
      "grad_norm": 0.8957608938217163,
      "learning_rate": 1.186192468619247e-05,
      "loss": 0.0026,
      "step": 3890
    },
    {
      "epoch": 4.079497907949791,
      "grad_norm": 0.04728297144174576,
      "learning_rate": 1.184100418410042e-05,
      "loss": 0.0026,
      "step": 3900
    },
    {
      "epoch": 4.089958158995816,
      "grad_norm": 0.015270276926457882,
      "learning_rate": 1.182008368200837e-05,
      "loss": 0.0106,
      "step": 3910
    },
    {
      "epoch": 4.100418410041841,
      "grad_norm": 0.0501379556953907,
      "learning_rate": 1.179916317991632e-05,
      "loss": 0.0113,
      "step": 3920
    },
    {
      "epoch": 4.110878661087866,
      "grad_norm": 1.2044776678085327,
      "learning_rate": 1.177824267782427e-05,
      "loss": 0.0039,
      "step": 3930
    },
    {
      "epoch": 4.121338912133891,
      "grad_norm": 0.26861926913261414,
      "learning_rate": 1.1757322175732218e-05,
      "loss": 0.0009,
      "step": 3940
    },
    {
      "epoch": 4.131799163179917,
      "grad_norm": 1.8802516460418701,
      "learning_rate": 1.1736401673640168e-05,
      "loss": 0.0033,
      "step": 3950
    },
    {
      "epoch": 4.142259414225942,
      "grad_norm": 0.19814085960388184,
      "learning_rate": 1.1715481171548117e-05,
      "loss": 0.0065,
      "step": 3960
    },
    {
      "epoch": 4.152719665271967,
      "grad_norm": 0.023476235568523407,
      "learning_rate": 1.1694560669456068e-05,
      "loss": 0.011,
      "step": 3970
    },
    {
      "epoch": 4.163179916317992,
      "grad_norm": 4.3363142013549805,
      "learning_rate": 1.1673640167364018e-05,
      "loss": 0.0067,
      "step": 3980
    },
    {
      "epoch": 4.173640167364017,
      "grad_norm": 0.2625507116317749,
      "learning_rate": 1.1652719665271967e-05,
      "loss": 0.0136,
      "step": 3990
    },
    {
      "epoch": 4.184100418410042,
      "grad_norm": 0.18637068569660187,
      "learning_rate": 1.1631799163179918e-05,
      "loss": 0.0051,
      "step": 4000
    },
    {
      "epoch": 4.194560669456067,
      "grad_norm": 0.1274058073759079,
      "learning_rate": 1.1610878661087867e-05,
      "loss": 0.0018,
      "step": 4010
    },
    {
      "epoch": 4.205020920502092,
      "grad_norm": 0.5826324820518494,
      "learning_rate": 1.1589958158995817e-05,
      "loss": 0.0029,
      "step": 4020
    },
    {
      "epoch": 4.2154811715481175,
      "grad_norm": 0.04620800167322159,
      "learning_rate": 1.1569037656903768e-05,
      "loss": 0.008,
      "step": 4030
    },
    {
      "epoch": 4.2259414225941425,
      "grad_norm": 0.011051984503865242,
      "learning_rate": 1.1548117154811716e-05,
      "loss": 0.0012,
      "step": 4040
    },
    {
      "epoch": 4.2364016736401675,
      "grad_norm": 0.029375994578003883,
      "learning_rate": 1.1527196652719665e-05,
      "loss": 0.0016,
      "step": 4050
    },
    {
      "epoch": 4.2468619246861925,
      "grad_norm": 1.221139907836914,
      "learning_rate": 1.1506276150627616e-05,
      "loss": 0.0054,
      "step": 4060
    },
    {
      "epoch": 4.2573221757322175,
      "grad_norm": 0.015404490754008293,
      "learning_rate": 1.1485355648535566e-05,
      "loss": 0.0145,
      "step": 4070
    },
    {
      "epoch": 4.2677824267782425,
      "grad_norm": 1.1126388311386108,
      "learning_rate": 1.1464435146443515e-05,
      "loss": 0.0049,
      "step": 4080
    },
    {
      "epoch": 4.2782426778242675,
      "grad_norm": 0.19093763828277588,
      "learning_rate": 1.1443514644351466e-05,
      "loss": 0.0077,
      "step": 4090
    },
    {
      "epoch": 4.2887029288702925,
      "grad_norm": 1.297489047050476,
      "learning_rate": 1.1422594142259416e-05,
      "loss": 0.0011,
      "step": 4100
    },
    {
      "epoch": 4.299163179916318,
      "grad_norm": 0.4971579611301422,
      "learning_rate": 1.1401673640167365e-05,
      "loss": 0.0027,
      "step": 4110
    },
    {
      "epoch": 4.309623430962343,
      "grad_norm": 2.8865981101989746,
      "learning_rate": 1.1380753138075316e-05,
      "loss": 0.0131,
      "step": 4120
    },
    {
      "epoch": 4.320083682008368,
      "grad_norm": 1.7468433380126953,
      "learning_rate": 1.1359832635983265e-05,
      "loss": 0.0065,
      "step": 4130
    },
    {
      "epoch": 4.330543933054393,
      "grad_norm": 0.006024553906172514,
      "learning_rate": 1.1338912133891213e-05,
      "loss": 0.0045,
      "step": 4140
    },
    {
      "epoch": 4.341004184100418,
      "grad_norm": 2.310210704803467,
      "learning_rate": 1.1317991631799163e-05,
      "loss": 0.0095,
      "step": 4150
    },
    {
      "epoch": 4.351464435146443,
      "grad_norm": 0.0027186109218746424,
      "learning_rate": 1.1297071129707114e-05,
      "loss": 0.0192,
      "step": 4160
    },
    {
      "epoch": 4.361924686192468,
      "grad_norm": 0.03848583623766899,
      "learning_rate": 1.1276150627615063e-05,
      "loss": 0.0139,
      "step": 4170
    },
    {
      "epoch": 4.372384937238493,
      "grad_norm": 2.6293091773986816,
      "learning_rate": 1.1255230125523014e-05,
      "loss": 0.0161,
      "step": 4180
    },
    {
      "epoch": 4.382845188284519,
      "grad_norm": 0.06663535535335541,
      "learning_rate": 1.1234309623430964e-05,
      "loss": 0.0022,
      "step": 4190
    },
    {
      "epoch": 4.393305439330544,
      "grad_norm": 1.7277195453643799,
      "learning_rate": 1.1213389121338913e-05,
      "loss": 0.0074,
      "step": 4200
    },
    {
      "epoch": 4.403765690376569,
      "grad_norm": 0.011556300334632397,
      "learning_rate": 1.1192468619246864e-05,
      "loss": 0.0017,
      "step": 4210
    },
    {
      "epoch": 4.414225941422594,
      "grad_norm": 0.07041244953870773,
      "learning_rate": 1.1171548117154814e-05,
      "loss": 0.0073,
      "step": 4220
    },
    {
      "epoch": 4.424686192468619,
      "grad_norm": 3.223465919494629,
      "learning_rate": 1.1150627615062763e-05,
      "loss": 0.008,
      "step": 4230
    },
    {
      "epoch": 4.435146443514644,
      "grad_norm": 0.25913006067276,
      "learning_rate": 1.112970711297071e-05,
      "loss": 0.007,
      "step": 4240
    },
    {
      "epoch": 4.445606694560669,
      "grad_norm": 0.04395473003387451,
      "learning_rate": 1.1108786610878662e-05,
      "loss": 0.0047,
      "step": 4250
    },
    {
      "epoch": 4.456066945606695,
      "grad_norm": 0.0034693428315222263,
      "learning_rate": 1.1087866108786611e-05,
      "loss": 0.0021,
      "step": 4260
    },
    {
      "epoch": 4.46652719665272,
      "grad_norm": 0.15297052264213562,
      "learning_rate": 1.106694560669456e-05,
      "loss": 0.0059,
      "step": 4270
    },
    {
      "epoch": 4.476987447698745,
      "grad_norm": 3.6476151943206787,
      "learning_rate": 1.1046025104602512e-05,
      "loss": 0.0193,
      "step": 4280
    },
    {
      "epoch": 4.48744769874477,
      "grad_norm": 0.03232654184103012,
      "learning_rate": 1.1025104602510461e-05,
      "loss": 0.0113,
      "step": 4290
    },
    {
      "epoch": 4.497907949790795,
      "grad_norm": 0.584942102432251,
      "learning_rate": 1.100418410041841e-05,
      "loss": 0.009,
      "step": 4300
    },
    {
      "epoch": 4.50836820083682,
      "grad_norm": 1.6196273565292358,
      "learning_rate": 1.0983263598326362e-05,
      "loss": 0.0014,
      "step": 4310
    },
    {
      "epoch": 4.518828451882845,
      "grad_norm": 0.0017218025168403983,
      "learning_rate": 1.0962343096234311e-05,
      "loss": 0.0017,
      "step": 4320
    },
    {
      "epoch": 4.52928870292887,
      "grad_norm": 0.03195333853363991,
      "learning_rate": 1.0941422594142262e-05,
      "loss": 0.0024,
      "step": 4330
    },
    {
      "epoch": 4.539748953974895,
      "grad_norm": 1.9363319873809814,
      "learning_rate": 1.092050209205021e-05,
      "loss": 0.0142,
      "step": 4340
    },
    {
      "epoch": 4.550209205020921,
      "grad_norm": 0.007783477660268545,
      "learning_rate": 1.089958158995816e-05,
      "loss": 0.0118,
      "step": 4350
    },
    {
      "epoch": 4.560669456066946,
      "grad_norm": 0.018261734396219254,
      "learning_rate": 1.0878661087866109e-05,
      "loss": 0.0136,
      "step": 4360
    },
    {
      "epoch": 4.571129707112971,
      "grad_norm": 1.7881332635879517,
      "learning_rate": 1.085774058577406e-05,
      "loss": 0.0094,
      "step": 4370
    },
    {
      "epoch": 4.581589958158996,
      "grad_norm": 0.07843299955129623,
      "learning_rate": 1.083682008368201e-05,
      "loss": 0.0104,
      "step": 4380
    },
    {
      "epoch": 4.592050209205021,
      "grad_norm": 0.8568052053451538,
      "learning_rate": 1.0815899581589959e-05,
      "loss": 0.0106,
      "step": 4390
    },
    {
      "epoch": 4.602510460251046,
      "grad_norm": 1.0419502258300781,
      "learning_rate": 1.079497907949791e-05,
      "loss": 0.0126,
      "step": 4400
    },
    {
      "epoch": 4.612970711297071,
      "grad_norm": 0.5691434741020203,
      "learning_rate": 1.0774058577405859e-05,
      "loss": 0.0208,
      "step": 4410
    },
    {
      "epoch": 4.623430962343097,
      "grad_norm": 0.05713112652301788,
      "learning_rate": 1.0753138075313809e-05,
      "loss": 0.0059,
      "step": 4420
    },
    {
      "epoch": 4.633891213389122,
      "grad_norm": 1.923607349395752,
      "learning_rate": 1.073221757322176e-05,
      "loss": 0.0096,
      "step": 4430
    },
    {
      "epoch": 4.644351464435147,
      "grad_norm": 0.7925992012023926,
      "learning_rate": 1.0711297071129707e-05,
      "loss": 0.0021,
      "step": 4440
    },
    {
      "epoch": 4.654811715481172,
      "grad_norm": 2.7162880897521973,
      "learning_rate": 1.0690376569037657e-05,
      "loss": 0.0112,
      "step": 4450
    },
    {
      "epoch": 4.665271966527197,
      "grad_norm": 0.03328307718038559,
      "learning_rate": 1.0669456066945608e-05,
      "loss": 0.0087,
      "step": 4460
    },
    {
      "epoch": 4.675732217573222,
      "grad_norm": 0.10940729826688766,
      "learning_rate": 1.0648535564853557e-05,
      "loss": 0.0144,
      "step": 4470
    },
    {
      "epoch": 4.686192468619247,
      "grad_norm": 0.4470902681350708,
      "learning_rate": 1.0627615062761507e-05,
      "loss": 0.0126,
      "step": 4480
    },
    {
      "epoch": 4.696652719665272,
      "grad_norm": 0.013714254833757877,
      "learning_rate": 1.0606694560669458e-05,
      "loss": 0.0041,
      "step": 4490
    },
    {
      "epoch": 4.707112970711297,
      "grad_norm": 0.08119285851716995,
      "learning_rate": 1.0585774058577407e-05,
      "loss": 0.0057,
      "step": 4500
    },
    {
      "epoch": 4.7175732217573225,
      "grad_norm": 0.01604362018406391,
      "learning_rate": 1.0564853556485357e-05,
      "loss": 0.0084,
      "step": 4510
    },
    {
      "epoch": 4.7280334728033475,
      "grad_norm": 0.9513491988182068,
      "learning_rate": 1.0543933054393308e-05,
      "loss": 0.0058,
      "step": 4520
    },
    {
      "epoch": 4.7384937238493725,
      "grad_norm": 0.06175937503576279,
      "learning_rate": 1.0523012552301257e-05,
      "loss": 0.0055,
      "step": 4530
    },
    {
      "epoch": 4.7489539748953975,
      "grad_norm": 0.2940186858177185,
      "learning_rate": 1.0502092050209205e-05,
      "loss": 0.0036,
      "step": 4540
    },
    {
      "epoch": 4.7594142259414225,
      "grad_norm": 0.02118990570306778,
      "learning_rate": 1.0481171548117154e-05,
      "loss": 0.0256,
      "step": 4550
    },
    {
      "epoch": 4.7698744769874475,
      "grad_norm": 2.0449931621551514,
      "learning_rate": 1.0460251046025105e-05,
      "loss": 0.0137,
      "step": 4560
    },
    {
      "epoch": 4.7803347280334725,
      "grad_norm": 0.029781391844153404,
      "learning_rate": 1.0439330543933055e-05,
      "loss": 0.0069,
      "step": 4570
    },
    {
      "epoch": 4.790794979079498,
      "grad_norm": 3.862020492553711,
      "learning_rate": 1.0418410041841006e-05,
      "loss": 0.0079,
      "step": 4580
    },
    {
      "epoch": 4.801255230125523,
      "grad_norm": 1.4885838031768799,
      "learning_rate": 1.0397489539748955e-05,
      "loss": 0.0125,
      "step": 4590
    },
    {
      "epoch": 4.811715481171548,
      "grad_norm": 5.406171798706055,
      "learning_rate": 1.0376569037656905e-05,
      "loss": 0.0064,
      "step": 4600
    },
    {
      "epoch": 4.822175732217573,
      "grad_norm": 1.5001273155212402,
      "learning_rate": 1.0355648535564856e-05,
      "loss": 0.007,
      "step": 4610
    },
    {
      "epoch": 4.832635983263598,
      "grad_norm": 0.1781167834997177,
      "learning_rate": 1.0334728033472805e-05,
      "loss": 0.0082,
      "step": 4620
    },
    {
      "epoch": 4.843096234309623,
      "grad_norm": 3.027427911758423,
      "learning_rate": 1.0313807531380755e-05,
      "loss": 0.0163,
      "step": 4630
    },
    {
      "epoch": 4.853556485355648,
      "grad_norm": 0.17825937271118164,
      "learning_rate": 1.0292887029288702e-05,
      "loss": 0.0082,
      "step": 4640
    },
    {
      "epoch": 4.864016736401673,
      "grad_norm": 0.4662420153617859,
      "learning_rate": 1.0271966527196653e-05,
      "loss": 0.0085,
      "step": 4650
    },
    {
      "epoch": 4.874476987447698,
      "grad_norm": 2.5933704376220703,
      "learning_rate": 1.0251046025104603e-05,
      "loss": 0.0127,
      "step": 4660
    },
    {
      "epoch": 4.884937238493724,
      "grad_norm": 0.10737451165914536,
      "learning_rate": 1.0230125523012552e-05,
      "loss": 0.0044,
      "step": 4670
    },
    {
      "epoch": 4.895397489539749,
      "grad_norm": 0.8017618060112,
      "learning_rate": 1.0209205020920503e-05,
      "loss": 0.0036,
      "step": 4680
    },
    {
      "epoch": 4.905857740585774,
      "grad_norm": 0.024924347177147865,
      "learning_rate": 1.0188284518828453e-05,
      "loss": 0.0015,
      "step": 4690
    },
    {
      "epoch": 4.916317991631799,
      "grad_norm": 0.007340265903621912,
      "learning_rate": 1.0167364016736402e-05,
      "loss": 0.0158,
      "step": 4700
    },
    {
      "epoch": 4.926778242677824,
      "grad_norm": 0.6990300416946411,
      "learning_rate": 1.0146443514644353e-05,
      "loss": 0.0086,
      "step": 4710
    },
    {
      "epoch": 4.937238493723849,
      "grad_norm": 0.6889196038246155,
      "learning_rate": 1.0125523012552303e-05,
      "loss": 0.0103,
      "step": 4720
    },
    {
      "epoch": 4.947698744769874,
      "grad_norm": 0.014444006606936455,
      "learning_rate": 1.0104602510460254e-05,
      "loss": 0.0017,
      "step": 4730
    },
    {
      "epoch": 4.9581589958159,
      "grad_norm": 0.021027810871601105,
      "learning_rate": 1.0083682008368201e-05,
      "loss": 0.0196,
      "step": 4740
    },
    {
      "epoch": 4.968619246861925,
      "grad_norm": 2.6483547687530518,
      "learning_rate": 1.0062761506276151e-05,
      "loss": 0.0173,
      "step": 4750
    },
    {
      "epoch": 4.97907949790795,
      "grad_norm": 0.11353432387113571,
      "learning_rate": 1.00418410041841e-05,
      "loss": 0.0064,
      "step": 4760
    },
    {
      "epoch": 4.989539748953975,
      "grad_norm": 0.21522721648216248,
      "learning_rate": 1.0020920502092051e-05,
      "loss": 0.0045,
      "step": 4770
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.31432417035102844,
      "learning_rate": 1e-05,
      "loss": 0.0023,
      "step": 4780
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.9940040650406504,
      "eval_f1": 0.973939929328622,
      "eval_loss": 0.02458832412958145,
      "eval_precision": 0.96752961825362,
      "eval_recall": 0.9804357492218764,
      "eval_runtime": 626.9797,
      "eval_samples_per_second": 31.39,
      "eval_steps_per_second": 1.309,
      "step": 4780
    },
    {
      "epoch": 5.010460251046025,
      "grad_norm": 0.09989863634109497,
      "learning_rate": 9.97907949790795e-06,
      "loss": 0.0016,
      "step": 4790
    },
    {
      "epoch": 5.02092050209205,
      "grad_norm": 0.08274755626916885,
      "learning_rate": 9.958158995815901e-06,
      "loss": 0.003,
      "step": 4800
    },
    {
      "epoch": 5.031380753138075,
      "grad_norm": 1.3546040058135986,
      "learning_rate": 9.937238493723849e-06,
      "loss": 0.0033,
      "step": 4810
    },
    {
      "epoch": 5.0418410041841,
      "grad_norm": 0.9713103771209717,
      "learning_rate": 9.9163179916318e-06,
      "loss": 0.0065,
      "step": 4820
    },
    {
      "epoch": 5.052301255230126,
      "grad_norm": 0.7635505795478821,
      "learning_rate": 9.89539748953975e-06,
      "loss": 0.0046,
      "step": 4830
    },
    {
      "epoch": 5.062761506276151,
      "grad_norm": 1.2963091135025024,
      "learning_rate": 9.874476987447699e-06,
      "loss": 0.0135,
      "step": 4840
    },
    {
      "epoch": 5.073221757322176,
      "grad_norm": 0.03352516144514084,
      "learning_rate": 9.85355648535565e-06,
      "loss": 0.0043,
      "step": 4850
    },
    {
      "epoch": 5.083682008368201,
      "grad_norm": 0.2846335470676422,
      "learning_rate": 9.8326359832636e-06,
      "loss": 0.0077,
      "step": 4860
    },
    {
      "epoch": 5.094142259414226,
      "grad_norm": 2.1249749660491943,
      "learning_rate": 9.811715481171549e-06,
      "loss": 0.0076,
      "step": 4870
    },
    {
      "epoch": 5.104602510460251,
      "grad_norm": 0.2591782808303833,
      "learning_rate": 9.790794979079498e-06,
      "loss": 0.0037,
      "step": 4880
    },
    {
      "epoch": 5.115062761506276,
      "grad_norm": 0.32358911633491516,
      "learning_rate": 9.76987447698745e-06,
      "loss": 0.0079,
      "step": 4890
    },
    {
      "epoch": 5.125523012552302,
      "grad_norm": 0.7227758169174194,
      "learning_rate": 9.748953974895399e-06,
      "loss": 0.0022,
      "step": 4900
    },
    {
      "epoch": 5.135983263598327,
      "grad_norm": 0.44605565071105957,
      "learning_rate": 9.728033472803348e-06,
      "loss": 0.0062,
      "step": 4910
    },
    {
      "epoch": 5.146443514644352,
      "grad_norm": 1.7217507362365723,
      "learning_rate": 9.707112970711298e-06,
      "loss": 0.0053,
      "step": 4920
    },
    {
      "epoch": 5.156903765690377,
      "grad_norm": 0.0025381569284945726,
      "learning_rate": 9.686192468619247e-06,
      "loss": 0.0015,
      "step": 4930
    },
    {
      "epoch": 5.167364016736402,
      "grad_norm": 0.15402618050575256,
      "learning_rate": 9.665271966527198e-06,
      "loss": 0.0085,
      "step": 4940
    },
    {
      "epoch": 5.177824267782427,
      "grad_norm": 0.014937483705580235,
      "learning_rate": 9.644351464435148e-06,
      "loss": 0.0116,
      "step": 4950
    },
    {
      "epoch": 5.188284518828452,
      "grad_norm": 0.07629099488258362,
      "learning_rate": 9.623430962343097e-06,
      "loss": 0.0012,
      "step": 4960
    },
    {
      "epoch": 5.198744769874477,
      "grad_norm": 0.01721988618373871,
      "learning_rate": 9.602510460251046e-06,
      "loss": 0.0013,
      "step": 4970
    },
    {
      "epoch": 5.209205020920502,
      "grad_norm": 0.8725212216377258,
      "learning_rate": 9.581589958158996e-06,
      "loss": 0.008,
      "step": 4980
    },
    {
      "epoch": 5.2196652719665275,
      "grad_norm": 0.02062646858394146,
      "learning_rate": 9.560669456066947e-06,
      "loss": 0.0004,
      "step": 4990
    },
    {
      "epoch": 5.2301255230125525,
      "grad_norm": 0.13211177289485931,
      "learning_rate": 9.539748953974896e-06,
      "loss": 0.0151,
      "step": 5000
    },
    {
      "epoch": 5.2405857740585775,
      "grad_norm": 2.362380266189575,
      "learning_rate": 9.518828451882846e-06,
      "loss": 0.0119,
      "step": 5010
    },
    {
      "epoch": 5.2510460251046025,
      "grad_norm": 1.991727352142334,
      "learning_rate": 9.497907949790795e-06,
      "loss": 0.0115,
      "step": 5020
    },
    {
      "epoch": 5.2615062761506275,
      "grad_norm": 4.2103047370910645,
      "learning_rate": 9.476987447698746e-06,
      "loss": 0.0112,
      "step": 5030
    },
    {
      "epoch": 5.2719665271966525,
      "grad_norm": 0.3169158697128296,
      "learning_rate": 9.456066945606696e-06,
      "loss": 0.0024,
      "step": 5040
    },
    {
      "epoch": 5.2824267782426775,
      "grad_norm": 0.3124053478240967,
      "learning_rate": 9.435146443514645e-06,
      "loss": 0.003,
      "step": 5050
    },
    {
      "epoch": 5.292887029288703,
      "grad_norm": 0.026923509314656258,
      "learning_rate": 9.414225941422594e-06,
      "loss": 0.0027,
      "step": 5060
    },
    {
      "epoch": 5.303347280334728,
      "grad_norm": 6.51240873336792,
      "learning_rate": 9.393305439330544e-06,
      "loss": 0.0021,
      "step": 5070
    },
    {
      "epoch": 5.313807531380753,
      "grad_norm": 0.004714499693363905,
      "learning_rate": 9.372384937238495e-06,
      "loss": 0.0001,
      "step": 5080
    },
    {
      "epoch": 5.324267782426778,
      "grad_norm": 0.7964769005775452,
      "learning_rate": 9.351464435146444e-06,
      "loss": 0.0152,
      "step": 5090
    },
    {
      "epoch": 5.334728033472803,
      "grad_norm": 4.941593170166016,
      "learning_rate": 9.330543933054394e-06,
      "loss": 0.0065,
      "step": 5100
    },
    {
      "epoch": 5.345188284518828,
      "grad_norm": 1.6570223569869995,
      "learning_rate": 9.309623430962343e-06,
      "loss": 0.0051,
      "step": 5110
    },
    {
      "epoch": 5.355648535564853,
      "grad_norm": 1.1732535362243652,
      "learning_rate": 9.288702928870293e-06,
      "loss": 0.0034,
      "step": 5120
    },
    {
      "epoch": 5.366108786610878,
      "grad_norm": 0.02073296345770359,
      "learning_rate": 9.267782426778244e-06,
      "loss": 0.0125,
      "step": 5130
    },
    {
      "epoch": 5.376569037656903,
      "grad_norm": 0.930721640586853,
      "learning_rate": 9.246861924686193e-06,
      "loss": 0.0088,
      "step": 5140
    },
    {
      "epoch": 5.387029288702929,
      "grad_norm": 0.9680682420730591,
      "learning_rate": 9.225941422594144e-06,
      "loss": 0.0014,
      "step": 5150
    },
    {
      "epoch": 5.397489539748954,
      "grad_norm": 3.618643045425415,
      "learning_rate": 9.205020920502092e-06,
      "loss": 0.0103,
      "step": 5160
    },
    {
      "epoch": 5.407949790794979,
      "grad_norm": 0.01308041624724865,
      "learning_rate": 9.184100418410043e-06,
      "loss": 0.001,
      "step": 5170
    },
    {
      "epoch": 5.418410041841004,
      "grad_norm": 3.015777349472046,
      "learning_rate": 9.163179916317992e-06,
      "loss": 0.0051,
      "step": 5180
    },
    {
      "epoch": 5.428870292887029,
      "grad_norm": 2.3725244998931885,
      "learning_rate": 9.142259414225942e-06,
      "loss": 0.0074,
      "step": 5190
    },
    {
      "epoch": 5.439330543933054,
      "grad_norm": 0.4063321650028229,
      "learning_rate": 9.121338912133893e-06,
      "loss": 0.0047,
      "step": 5200
    },
    {
      "epoch": 5.449790794979079,
      "grad_norm": 0.7831611037254333,
      "learning_rate": 9.10041841004184e-06,
      "loss": 0.0047,
      "step": 5210
    },
    {
      "epoch": 5.460251046025105,
      "grad_norm": 0.004935952834784985,
      "learning_rate": 9.079497907949792e-06,
      "loss": 0.0045,
      "step": 5220
    },
    {
      "epoch": 5.47071129707113,
      "grad_norm": 0.056735292077064514,
      "learning_rate": 9.058577405857741e-06,
      "loss": 0.0028,
      "step": 5230
    },
    {
      "epoch": 5.481171548117155,
      "grad_norm": 1.3866636753082275,
      "learning_rate": 9.03765690376569e-06,
      "loss": 0.0016,
      "step": 5240
    },
    {
      "epoch": 5.49163179916318,
      "grad_norm": 0.8815909624099731,
      "learning_rate": 9.016736401673642e-06,
      "loss": 0.0059,
      "step": 5250
    },
    {
      "epoch": 5.502092050209205,
      "grad_norm": 0.58634352684021,
      "learning_rate": 8.995815899581591e-06,
      "loss": 0.0018,
      "step": 5260
    },
    {
      "epoch": 5.51255230125523,
      "grad_norm": 0.018975064158439636,
      "learning_rate": 8.97489539748954e-06,
      "loss": 0.0001,
      "step": 5270
    },
    {
      "epoch": 5.523012552301255,
      "grad_norm": 0.7954884171485901,
      "learning_rate": 8.95397489539749e-06,
      "loss": 0.002,
      "step": 5280
    },
    {
      "epoch": 5.53347280334728,
      "grad_norm": 5.262272357940674,
      "learning_rate": 8.933054393305441e-06,
      "loss": 0.0091,
      "step": 5290
    },
    {
      "epoch": 5.543933054393305,
      "grad_norm": 0.005349405109882355,
      "learning_rate": 8.91213389121339e-06,
      "loss": 0.0036,
      "step": 5300
    },
    {
      "epoch": 5.554393305439331,
      "grad_norm": 0.000709743588231504,
      "learning_rate": 8.89121338912134e-06,
      "loss": 0.0001,
      "step": 5310
    },
    {
      "epoch": 5.564853556485356,
      "grad_norm": 0.05757120996713638,
      "learning_rate": 8.87029288702929e-06,
      "loss": 0.0177,
      "step": 5320
    },
    {
      "epoch": 5.575313807531381,
      "grad_norm": 0.002643507206812501,
      "learning_rate": 8.849372384937239e-06,
      "loss": 0.0103,
      "step": 5330
    },
    {
      "epoch": 5.585774058577406,
      "grad_norm": 0.40059441328048706,
      "learning_rate": 8.82845188284519e-06,
      "loss": 0.0132,
      "step": 5340
    },
    {
      "epoch": 5.596234309623431,
      "grad_norm": 0.06146438047289848,
      "learning_rate": 8.80753138075314e-06,
      "loss": 0.0028,
      "step": 5350
    },
    {
      "epoch": 5.606694560669456,
      "grad_norm": 0.004239351022988558,
      "learning_rate": 8.786610878661089e-06,
      "loss": 0.0021,
      "step": 5360
    },
    {
      "epoch": 5.617154811715481,
      "grad_norm": 0.007203723769634962,
      "learning_rate": 8.765690376569038e-06,
      "loss": 0.0056,
      "step": 5370
    },
    {
      "epoch": 5.627615062761507,
      "grad_norm": 2.18654465675354,
      "learning_rate": 8.744769874476987e-06,
      "loss": 0.0128,
      "step": 5380
    },
    {
      "epoch": 5.638075313807532,
      "grad_norm": 0.006987901404500008,
      "learning_rate": 8.723849372384939e-06,
      "loss": 0.0007,
      "step": 5390
    },
    {
      "epoch": 5.648535564853557,
      "grad_norm": 0.0076007419265806675,
      "learning_rate": 8.702928870292888e-06,
      "loss": 0.0004,
      "step": 5400
    },
    {
      "epoch": 5.658995815899582,
      "grad_norm": 0.0771111324429512,
      "learning_rate": 8.682008368200837e-06,
      "loss": 0.0003,
      "step": 5410
    },
    {
      "epoch": 5.669456066945607,
      "grad_norm": 0.002599374856799841,
      "learning_rate": 8.661087866108787e-06,
      "loss": 0.001,
      "step": 5420
    },
    {
      "epoch": 5.679916317991632,
      "grad_norm": 0.3701397776603699,
      "learning_rate": 8.640167364016738e-06,
      "loss": 0.0196,
      "step": 5430
    },
    {
      "epoch": 5.690376569037657,
      "grad_norm": 0.6267544031143188,
      "learning_rate": 8.619246861924687e-06,
      "loss": 0.0057,
      "step": 5440
    },
    {
      "epoch": 5.700836820083682,
      "grad_norm": 0.29756176471710205,
      "learning_rate": 8.598326359832637e-06,
      "loss": 0.0027,
      "step": 5450
    },
    {
      "epoch": 5.711297071129707,
      "grad_norm": 0.3236546814441681,
      "learning_rate": 8.577405857740586e-06,
      "loss": 0.0055,
      "step": 5460
    },
    {
      "epoch": 5.7217573221757325,
      "grad_norm": 3.07175612449646,
      "learning_rate": 8.556485355648536e-06,
      "loss": 0.0079,
      "step": 5470
    },
    {
      "epoch": 5.7322175732217575,
      "grad_norm": 0.011638138443231583,
      "learning_rate": 8.535564853556487e-06,
      "loss": 0.0023,
      "step": 5480
    },
    {
      "epoch": 5.7426778242677825,
      "grad_norm": 0.014141999185085297,
      "learning_rate": 8.514644351464436e-06,
      "loss": 0.0066,
      "step": 5490
    },
    {
      "epoch": 5.7531380753138075,
      "grad_norm": 0.048091862350702286,
      "learning_rate": 8.493723849372385e-06,
      "loss": 0.0089,
      "step": 5500
    },
    {
      "epoch": 5.7635983263598325,
      "grad_norm": 0.007488963194191456,
      "learning_rate": 8.472803347280335e-06,
      "loss": 0.0103,
      "step": 5510
    },
    {
      "epoch": 5.7740585774058575,
      "grad_norm": 0.004664590582251549,
      "learning_rate": 8.451882845188284e-06,
      "loss": 0.0015,
      "step": 5520
    },
    {
      "epoch": 5.7845188284518825,
      "grad_norm": 0.24487727880477905,
      "learning_rate": 8.430962343096235e-06,
      "loss": 0.0059,
      "step": 5530
    },
    {
      "epoch": 5.794979079497908,
      "grad_norm": 0.8872719407081604,
      "learning_rate": 8.410041841004185e-06,
      "loss": 0.0011,
      "step": 5540
    },
    {
      "epoch": 5.805439330543933,
      "grad_norm": 4.725320339202881,
      "learning_rate": 8.389121338912136e-06,
      "loss": 0.0125,
      "step": 5550
    },
    {
      "epoch": 5.815899581589958,
      "grad_norm": 0.042100679129362106,
      "learning_rate": 8.368200836820084e-06,
      "loss": 0.0025,
      "step": 5560
    },
    {
      "epoch": 5.826359832635983,
      "grad_norm": 3.6282479763031006,
      "learning_rate": 8.347280334728035e-06,
      "loss": 0.0068,
      "step": 5570
    },
    {
      "epoch": 5.836820083682008,
      "grad_norm": 0.04745479300618172,
      "learning_rate": 8.326359832635984e-06,
      "loss": 0.0057,
      "step": 5580
    },
    {
      "epoch": 5.847280334728033,
      "grad_norm": 0.003938591573387384,
      "learning_rate": 8.305439330543934e-06,
      "loss": 0.0052,
      "step": 5590
    },
    {
      "epoch": 5.857740585774058,
      "grad_norm": 0.5927234292030334,
      "learning_rate": 8.284518828451885e-06,
      "loss": 0.0136,
      "step": 5600
    },
    {
      "epoch": 5.868200836820083,
      "grad_norm": 0.08024077117443085,
      "learning_rate": 8.263598326359832e-06,
      "loss": 0.0037,
      "step": 5610
    },
    {
      "epoch": 5.878661087866108,
      "grad_norm": 0.02608785405755043,
      "learning_rate": 8.242677824267783e-06,
      "loss": 0.0095,
      "step": 5620
    },
    {
      "epoch": 5.889121338912134,
      "grad_norm": 0.013683994300663471,
      "learning_rate": 8.221757322175733e-06,
      "loss": 0.0069,
      "step": 5630
    },
    {
      "epoch": 5.899581589958159,
      "grad_norm": 0.7020805478096008,
      "learning_rate": 8.200836820083682e-06,
      "loss": 0.0033,
      "step": 5640
    },
    {
      "epoch": 5.910041841004184,
      "grad_norm": 2.0307071208953857,
      "learning_rate": 8.179916317991633e-06,
      "loss": 0.0106,
      "step": 5650
    },
    {
      "epoch": 5.920502092050209,
      "grad_norm": 0.18855613470077515,
      "learning_rate": 8.158995815899581e-06,
      "loss": 0.0027,
      "step": 5660
    },
    {
      "epoch": 5.930962343096234,
      "grad_norm": 0.07328097522258759,
      "learning_rate": 8.138075313807532e-06,
      "loss": 0.0105,
      "step": 5670
    },
    {
      "epoch": 5.941422594142259,
      "grad_norm": 0.22975389659404755,
      "learning_rate": 8.117154811715482e-06,
      "loss": 0.0034,
      "step": 5680
    },
    {
      "epoch": 5.951882845188284,
      "grad_norm": 0.8664940595626831,
      "learning_rate": 8.096234309623433e-06,
      "loss": 0.0027,
      "step": 5690
    },
    {
      "epoch": 5.96234309623431,
      "grad_norm": 0.010234449990093708,
      "learning_rate": 8.075313807531382e-06,
      "loss": 0.0031,
      "step": 5700
    },
    {
      "epoch": 5.972803347280335,
      "grad_norm": 0.0025091248098760843,
      "learning_rate": 8.054393305439332e-06,
      "loss": 0.0054,
      "step": 5710
    },
    {
      "epoch": 5.98326359832636,
      "grad_norm": 2.114807367324829,
      "learning_rate": 8.033472803347281e-06,
      "loss": 0.003,
      "step": 5720
    },
    {
      "epoch": 5.993723849372385,
      "grad_norm": 0.001254635164514184,
      "learning_rate": 8.01255230125523e-06,
      "loss": 0.0022,
      "step": 5730
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.9946646341463414,
      "eval_f1": 0.9765886287625418,
      "eval_loss": 0.023769436404109,
      "eval_precision": 0.9794275491949911,
      "eval_recall": 0.9737661182747888,
      "eval_runtime": 1055.5779,
      "eval_samples_per_second": 18.645,
      "eval_steps_per_second": 0.778,
      "step": 5736
    }
  ],
  "logging_steps": 10,
  "max_steps": 9560,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.688916005791008e+17,
  "train_batch_size": 96,
  "trial_name": null,
  "trial_params": null
}
